test_method,isMysteryGuest
"@Test
    public void testGetFromUserResources() throws Throwable {
        Path testXenonImagesPath = Files.createTempDirectory(""test-xenon-images"");

        HostInitCommonServiceConfig.startServices(host);
        waitForServiceAvailability(ConfigurationFactoryService.SELF_LINK);
        waitForServiceAvailability(UriUtils.buildUriPath(UriUtils.buildUriPath(
                ConfigurationFactoryService.SELF_LINK, FileUtil.USER_RESOURCES_PATH_VARIABLE)));

        // Set expected configuration
        ConfigurationState config = new ConfigurationState();
        config.documentSelfLink = UriUtils.buildUriPath(ConfigurationFactoryService.SELF_LINK,
                FileUtil.USER_RESOURCES_PATH_VARIABLE);
        config.key = FileUtil.USER_RESOURCES_PATH_VARIABLE;
        config.value = testXenonImagesPath.toAbsolutePath().toString();

        doPut(config);

        File imageDir = new File(UriUtils.buildUriPath(testXenonImagesPath.toString(),
                SystemImageRetrievalManager.SYSTEM_IMAGES_PATH));
        imageDir.mkdir();

        byte[] content = IOUtils.toByteArray(Thread.currentThread().getContextClassLoader()
                .getResourceAsStream(TEST_IMAGE));
        // Basically, rename it so it must be loaded from user resources for sure
        File tmpFile = new File(
                UriUtils.buildUriPath(imageDir.getAbsolutePath(), TEST_IMAGE_RES));
        tmpFile.createNewFile();
        try (OutputStream os = new FileOutputStream(tmpFile)) {
            os.write(content);
        }",1
"@Test
    public void testValidateServicesAreDeployedFirst() throws Throwable {
        String wordpressTemplate = CommonTestStateFactory
                .getFileContent(""WordPress_with_MySQL_kubernetes.yaml"");

        String compositeDescriptionLink = importTemplate(wordpressTemplate);

        CompositeDescription compositeDescription = getCompositeDescription(
                compositeDescriptionLink);

        CompositeComponent compositeComponent = new CompositeComponent();
        compositeComponent.name = compositeDescription.name + ""-mcm-102"";
        compositeComponent.compositeDescriptionLink = compositeDescription.documentSelfLink;
        compositeComponent.customProperties = new HashMap<>();
        compositeComponent.customProperties.put(CUSTOM_PROPERTY_HOST_LINK,
                kubernetesHostState.documentSelfLink);
        compositeComponent = doPost(compositeComponent, CompositeComponentFactoryService.SELF_LINK);

        addPodsAndReplicaSetsForWordpressApp(extractId(compositeComponent.documentSelfLink));

        provisioningTaskLink = createProvisioningTask();

        ApplicationRequest appRequest = createApplicationRequest(
                compositeComponent.documentSelfLink);

        doOperation(ManagementUriParts.ADAPTER_KUBERNETES_APPLICATION, appRequest);

        // wait for provisioning task stage to change to finish
        waitForPropertyValue(provisioningTaskLink, MockTaskState.class, ""taskInfo.stage"",
                TaskState.TaskStage.FINISHED);

        assertEquals(10, service.deployedElements.size());

        List<BaseKubernetesObject> kubernetesElements = new ArrayList<>();
        service.deployedElements.forEach(e -> kubernetesElements.add(e));

        // Assert that services are deployed first.
        assertEquals(KubernetesUtil.SERVICE_TYPE, kubernetesElements.get(4).kind);
        assertEquals(KubernetesUtil.SERVICE_TYPE, kubernetesElements.get(5).kind);

        // Assert that states are created and they have correct compositeComponentLink.
        CompositeComponent finalCompositeComponent = compositeComponent;

        List<String> resourceLinks = getDocumentLinksOfType(ServiceState.class);
        assertEquals(2, resourceLinks.size());
        resourceLinks.forEach(link -> doOperation(Operation.createGet(host, link)
                .setCompletion((o, ex) -> {
                    if (ex != null) {
                        host.failIteration(ex);
                    }",1
"@Test
    public void testDefaultProjectCreatedOnStartUp() throws Throwable {
        waitForServiceAvailability(ProjectService.DEFAULT_PROJECT_LINK);

        ProjectState project = getDocument(ProjectState.class,
                ProjectService.DEFAULT_PROJECT_LINK);

        assertNotNull(project);
        assertEquals(ProjectService.DEFAULT_PROJECT_ID, project.name);
        assertEquals(ProjectService.DEFAULT_PROJECT_ID, project.id);
    }",1
"@Test
    public void testImportContentWithProjectAndUsers() throws Throwable {
        AuthContentBody body = Utils.fromJson(authContent, AuthContentBody.class);
        loadAuthContent(body);

        List<String> projectLinks = getDocumentLinksOfType(ProjectState.class);
        projectLinks.remove(ProjectService.DEFAULT_PROJECT_LINK);

        assertEquals(body.projects.size(), projectLinks.size());

        List<String> projectToImportNames = body.projects.stream()
                .map(p -> p.name)
                .collect(Collectors.toList());

        for (String link : projectLinks) {
            ProjectState state = getDocument(ProjectState.class, link);
            assertTrue(projectToImportNames.contains(state.name));
        }",1
"@Test
    public void testDeletePrincipalShouldDeleteUserState() throws Throwable {
        String fritzEmail = ""fritz@admiral.com"";
        String fritzSelfLink = LocalPrincipalFactoryService.SELF_LINK + ""/"" + encode(fritzEmail);

        doDelete(UriUtils.buildUri(host, fritzSelfLink), false);

        LocalPrincipalState state = getDocumentNoWait(LocalPrincipalState.class, fritzSelfLink);
        assertNull(state);

        UserState userState = getDocumentNoWait(UserState.class, buildUserServicePath(fritzEmail));
        assertNull(userState);

        ResourceGroupState resourceGroupState = getDocumentNoWait(ResourceGroupState.class,
                UriUtils.buildUriPath(ResourceGroupService.FACTORY_LINK, encode(fritzEmail)));
        assertNull(resourceGroupState);

        UserGroupState userGroupState = getDocumentNoWait(UserGroupState.class,
                UriUtils.buildUriPath(UserGroupService.FACTORY_LINK, encode(fritzEmail)));
        assertNull(userGroupState);

        RoleState roleState = getDocumentNoWait(RoleState.class,
                UriUtils.buildUriPath(RoleService.FACTORY_LINK, encode(fritzEmail)));
        assertNull(roleState);

    }",1
"@Test
    public void testUserSpecificResourceAreCreatedWhenUserIsCreated() throws Throwable {
        // Assert user specific UserGroup, ResourceGroup and Role are created.
        String fritzEmail = ""fritz@admiral.com"";
        String fritzSelfLink = LocalPrincipalFactoryService.SELF_LINK + ""/"" + encode(fritzEmail);

        LocalPrincipalState state = getDocumentNoWait(LocalPrincipalState.class, fritzSelfLink);
        assertNotNull(state);

        UserState userState = getDocumentNoWait(UserState.class, buildUserServicePath(fritzEmail));
        assertNotNull(userState);

        ResourceGroupState resourceGroupState = getDocumentNoWait(ResourceGroupState.class,
                UriUtils.buildUriPath(ResourceGroupService.FACTORY_LINK, encode(fritzEmail)));
        assertNotNull(resourceGroupState);

        UserGroupState userGroupState = getDocumentNoWait(UserGroupState.class,
                UriUtils.buildUriPath(UserGroupService.FACTORY_LINK, encode(fritzEmail)));
        assertNotNull(userGroupState);

        RoleState roleState = getDocumentNoWait(RoleState.class,
                UriUtils.buildUriPath(RoleService.FACTORY_LINK, encode(fritzEmail)));
        assertNotNull(roleState);
        assertEquals(userGroupState.documentSelfLink, roleState.userGroupLink);
        assertEquals(resourceGroupState.documentSelfLink, roleState.resourceGroupLink);
    }",1
"@Test
    public void testAssignRoleToUserGroup() throws Throwable {
        PrincipalRoleAssignment roleAssignment = new PrincipalRoleAssignment();
        roleAssignment.add = new ArrayList<>();
        roleAssignment.add.add(AuthRole.CLOUD_ADMIN.name());

        doRoleAssignment(roleAssignment, USER_GROUP_DEVELOPERS);

        RoleState roleState = getDocument(RoleState.class,
                UriUtils.buildUriPath(RoleService.FACTORY_LINK, AuthRole.CLOUD_ADMIN
                        .buildRoleWithSuffix(encode(USER_GROUP_DEVELOPERS))));
        assertNotNull(roleState);
        assertEquals(UriUtils.buildUriPath(UserGroupService.FACTORY_LINK,
                encode(USER_GROUP_DEVELOPERS)), roleState.userGroupLink);
    }",1
"@Test
    public void testAssignRoleToUserTwice() throws Throwable {
        PrincipalRoleAssignment roleAssignment = new PrincipalRoleAssignment();
        roleAssignment.add = new ArrayList<>();
        roleAssignment.add.add(AuthRole.CLOUD_ADMIN.name());

        // Assign.
        doRoleAssignment(roleAssignment, USER_EMAIL_BASIC_USER);

        UserState state = getDocument(UserState.class, buildUserServicePath(USER_EMAIL_BASIC_USER));
        assertNotNull(state);
        assertTrue(state.userGroupLinks.contains(CLOUD_ADMINS_USER_GROUP_LINK));

        // Unassign.
        roleAssignment = new PrincipalRoleAssignment();
        roleAssignment.remove = new ArrayList<>();
        roleAssignment.remove.add(AuthRole.CLOUD_ADMIN.name());

        doRoleAssignment(roleAssignment, USER_EMAIL_BASIC_USER);

        state = getDocument(UserState.class, buildUserServicePath(USER_EMAIL_BASIC_USER));
        assertNotNull(state);
        assertTrue(!state.userGroupLinks.contains(CLOUD_ADMINS_USER_GROUP_LINK));

        // Assign again.
        roleAssignment = new PrincipalRoleAssignment();
        roleAssignment.add = new ArrayList<>();
        roleAssignment.add.add(AuthRole.CLOUD_ADMIN.name());

        doRoleAssignment(roleAssignment, USER_EMAIL_BASIC_USER);

        state = getDocument(UserState.class, buildUserServicePath(USER_EMAIL_BASIC_USER));
        assertNotNull(state);
        assertTrue(state.userGroupLinks.contains(CLOUD_ADMINS_USER_GROUP_LINK));
    }",1
"@Test
    public void testGetAllRolesForPrincipalWithIndirectRoles() throws Throwable {
        host.assumeIdentity(buildUserServicePath(USER_EMAIL_ADMIN2));
        // Scenario: create a group which will contain Connie which is basic user and the group
        // will be assigned to cloud admins. Create nested groups and add Connie in them, assign
        // the nested groups to project roles. Verify that PrincipalRoles for Connie contains all
        // roles where he is assigned indirectly.

        // root is the group where Connie belongs and we assign the group to cloud admins role.
        LocalPrincipalState root = new LocalPrincipalState();
        root.type = LocalPrincipalType.GROUP;
        root.name = ""root"";
        root.groupMembersLinks = Collections.singletonList(UriUtils.buildUriPath(
                LocalPrincipalFactoryService.SELF_LINK, encode(USER_EMAIL_CONNIE)));
        root = doPost(root, LocalPrincipalFactoryService.SELF_LINK);
        assertNotNull(root.documentSelfLink);

        // nestedGroup1 is the group where Connie belongs but we will add nestedGroup1 to
        // nestedGroup2 and we will indirectly assign roles to Connie as we assign a role to
        // nestedGroup2.
        LocalPrincipalState nestedGroup1 = new LocalPrincipalState();
        nestedGroup1.type = LocalPrincipalType.GROUP;
        nestedGroup1.name = ""nestedGroup1"";
        nestedGroup1.groupMembersLinks = Collections.singletonList(UriUtils.buildUriPath(
                LocalPrincipalFactoryService.SELF_LINK, encode(USER_EMAIL_CONNIE)));
        nestedGroup1 = doPost(nestedGroup1, LocalPrincipalFactoryService.SELF_LINK);
        assertNotNull(nestedGroup1.documentSelfLink);

        // nestedGroup2 is the group which contains nestedGroup1
        LocalPrincipalState nestedGroup2 = new LocalPrincipalState();
        nestedGroup2.type = LocalPrincipalType.GROUP;
        nestedGroup2.name = ""nestedGroup2"";
        nestedGroup2.groupMembersLinks = Collections.singletonList(nestedGroup1.documentSelfLink);
        nestedGroup2 = doPost(nestedGroup2, LocalPrincipalFactoryService.SELF_LINK);
        assertNotNull(nestedGroup2.documentSelfLink);

        // assign cloud admins role to root user group.
        PrincipalRoleAssignment roleAssignment = new PrincipalRoleAssignment();
        roleAssignment.add = Collections.singletonList(AuthRole.CLOUD_ADMIN.name());
        doPatch(roleAssignment, UriUtils.buildUriPath(PrincipalService.SELF_LINK, root.id,
                PrincipalService.ROLES_SUFFIX));

        // Create first project and assign nestedGroup1 as project admin.
        ProjectState firstProject = createProject(""first-project"");
        assertNotNull(firstProject.documentSelfLink);
        ProjectRoles projectRoles = new ProjectRoles();
        PrincipalRoleAssignment admins = new PrincipalRoleAssignment();
        admins.add = Collections.singletonList(nestedGroup1.id);
        projectRoles.administrators = admins;
        doPatch(projectRoles, firstProject.documentSelfLink);

        // Create second project and assign nestedGroup2 as project member.
        ProjectState secondProject = createProject(""second-project"");
        assertNotNull(secondProject.documentSelfLink);
        projectRoles = new ProjectRoles();
        PrincipalRoleAssignment members = new PrincipalRoleAssignment();
        members.add = Collections.singletonList(nestedGroup2.id);
        projectRoles.members = members;
        doPatch(projectRoles, secondProject.documentSelfLink);

        URI uri = UriUtils.buildUri(host, PrincipalService.SELF_LINK);
        uri = UriUtils.extendUriWithQuery(uri, PrincipalService.CRITERIA_QUERY, ""connie"",
                PrincipalService.ROLES_QUERY, PrincipalService.ROLES_QUERY_VALUE);

        List<PrincipalRoles> resultRoles = new ArrayList<>();

        TestContext ctx = testCreate(1);

        Operation getRoles = Operation
                .createGet(uri)
                .setReferer(host.getUri())
                .setCompletion((o, ex) -> {
                    if (ex != null) {
                        ctx.failIteration(ex);
                        return;
                    }",1
"@Test
    public void testGetPublicProjectOnly() throws Throwable {
        ProjectState testProject1 = createProject(""test-project1"", ""test"", true);
        ProjectState testProject2 = createProject(""test-project2"", ""test"", true);
        ProjectState testProject3 = createProject(""test-project3"", ""test"", false);
        ProjectState testProject4 = createProject(""test-project4"", ""test"", false);
        assertDocumentExists(testProject1.documentSelfLink);
        assertDocumentExists(testProject2.documentSelfLink);
        assertDocumentExists(testProject3.documentSelfLink);
        assertDocumentExists(testProject4.documentSelfLink);

        host.assumeIdentity(buildUserServicePath(USER_EMAIL_BASIC_USER));

        URI uri = UriUtils.buildUri(ProjectFactoryService.SELF_LINK);
        uri = UriUtils.extendUriWithQuery(uri, ProjectFactoryService.QUERY_PARAM_PUBLIC, """");

        ServiceDocumentQueryResult getResult = getDocumentNoWait(ServiceDocumentQueryResult.class,
                uri.toString());

        assertEquals(Long.valueOf(2), getResult.documentCount);
        assertEquals(2, getResult.documentLinks.size());
        assertEquals(2, getResult.documents.size());
        assertTrue(getResult.documentLinks.contains(testProject1.documentSelfLink));
        assertTrue(getResult.documentLinks.contains(testProject2.documentSelfLink));
        assertTrue(!getResult.documentLinks.contains(testProject3.documentSelfLink));
        assertTrue(!getResult.documentLinks.contains(testProject4.documentSelfLink));
    }",1
"@Test
    public void testDelete() throws Throwable {
        String admins = project.administratorsUserGroupLinks.iterator().next();
        String members = project.membersUserGroupLinks.iterator().next();
        String viewers = project.viewersUserGroupLinks.iterator().next();
        deleteProject(project);

        // Verify the default UserGroups are deleted
        UserGroupState adminsGroup = getDocumentNoWait(UserGroupState.class, admins);
        assertNull(adminsGroup);

        UserGroupState membersGroups = getDocumentNoWait(UserGroupState.class, members);
        assertNull(membersGroups);

        UserGroupState viewersGroups = getDocumentNoWait(UserGroupState.class, viewers);
        assertNull(viewersGroups);
    }",1
"@Test
    public void testProjectRolesPatchGroups() throws Throwable {
        // verify initial state
        ExpandedProjectState expandedState = getExpandedProjectState(project.documentSelfLink);
        assertNotNull(expandedState.administratorsUserGroupLinks);
        assertNotNull(expandedState.membersUserGroupLinks);
        assertNotNull(expandedState.administrators);
        assertNotNull(expandedState.members);
        assertNotNull(expandedState.viewers);
        assertEquals(1, expandedState.administrators.size());
        assertEquals(1, expandedState.members.size());
        assertEquals(1, expandedState.viewers.size());
        assertEquals(USER_EMAIL_ADMIN, expandedState.administrators.iterator().next().email);
        assertEquals(USER_EMAIL_ADMIN, expandedState.members.iterator().next().email);
        assertEquals(USER_EMAIL_BASIC_USER, expandedState.viewers.iterator().next().email);

        String expectedAdministratorsUserGroupLink = UriUtils.buildUriPath(
                UserGroupService.FACTORY_LINK,
                AuthRole.PROJECT_ADMIN
                        .buildRoleWithSuffix(Service.getId(project.documentSelfLink)));
        String expectedMembersUserGroupLink = UriUtils.buildUriPath(UserGroupService.FACTORY_LINK,
                AuthRole.PROJECT_MEMBER
                        .buildRoleWithSuffix(Service.getId(project.documentSelfLink)));

        assertEquals(expectedAdministratorsUserGroupLink,
                expandedState.administratorsUserGroupLinks.iterator().next());
        assertEquals(expectedMembersUserGroupLink,
                expandedState.membersUserGroupLinks.iterator().next());

        // make a batch user operation: add group
        ProjectRoles projectRoles = new ProjectRoles();
        projectRoles.members = new PrincipalRoleAssignment();
        projectRoles.members.add = Arrays.asList(USER_GROUP_DEVELOPERS);

        // assert that the new role does not exist
        String roleLink = UriUtils.buildUriPath(RoleService.FACTORY_LINK,
                AuthRole.PROJECT_MEMBER.buildRoleWithSuffix(Service.getId(project.documentSelfLink),
                        encode(USER_GROUP_DEVELOPERS)));
        assertDocumentNotExists(roleLink);

        host.testStart(1);

        Operation.createPatch(host, expandedState.documentSelfLink)
                .setReferer(host.getUri())
                .setBody(projectRoles)
                .setCompletion((o, e) -> {
                    if (e != null) {
                        host.log(Level.SEVERE, Utils.toString(e));
                        host.failIteration(e);
                    }",1
"@Test
    public void executeJavaExtSourceAsZIPTest() throws Throwable {
        // Create Closure Definition
        ClosureDescription closureDescState = new ClosureDescription();
        closureDescState.name = ""test"";

        int expectedInVar = 3;
        int expectedInVar2 = 4;
        int expectedResult = 7;

        closureDescState.sourceURL = testWebserverUri + ""/test_script_java.zip"";
        closureDescState.source = ""should not be used"";
        closureDescState.runtime = RUNTIME_JAVA_8;
        closureDescState.entrypoint = ""testpackage.Test.test"";

        ResourceConstraints constraints = new ResourceConstraints();
        constraints.timeoutSeconds = 10;
        constraints.ramMB = 300;
        closureDescState.resources = constraints;

        String taskDefPayload = Utils.toJson(closureDescState);
        ClosureDescription closureDescription = createClosureDescription(taskDefPayload,
                serviceClient);
        assertNotNull(closureDescription);

        // Create Closure
        Closure createdClosure = createClosure(closureDescription, serviceClient);
        assertEquals(closureDescription.documentSelfLink, createdClosure.descriptionLink);
        assertEquals(TaskState.TaskStage.CREATED, createdClosure.state);

        // Execute the created Closure
        Closure closureRequest = new Closure();
        Map inputs = new HashMap<>();
        inputs.put(""a"", new JsonPrimitive(expectedInVar));
        inputs.put(""b"", new JsonPrimitive(expectedInVar2));
        closureRequest.inputs = inputs;

        executeClosure(createdClosure, closureRequest, serviceClient);

        // Wait for the completion timeout
        String imageRequestLink = waitForBuildCompletion(IMAGE_NAME, closureDescription);

        waitForTaskState(createdClosure.documentSelfLink, TaskState.TaskStage.FINISHED,
                serviceClient);

        Closure fetchedClosure = getClosure(createdClosure.documentSelfLink, serviceClient);

        assertEquals(closureDescription.documentSelfLink, fetchedClosure.descriptionLink);
        assertEquals(TaskState.TaskStage.FINISHED, fetchedClosure.state);

        assertEquals(expectedInVar, fetchedClosure.inputs.get(""a"").getAsInt());
        assertEquals(expectedInVar2, fetchedClosure.inputs.get(""b"").getAsInt());
        assertEquals(expectedResult, fetchedClosure.outputs.get(""result"").getAsInt(), 0);

        cleanResource(imageRequestLink, serviceClient);
        cleanResource(createdClosure.documentSelfLink, serviceClient);
        cleanResource(closureDescription.documentSelfLink, serviceClient);
    }",1
"@Test
    public void executePowershellExtSourceAsZIPTest() throws Throwable {
        // Create Closure Definition
        ClosureDescription closureDescState = new ClosureDescription();
        closureDescState.name = ""test"";

        String expectedInVar = ""a"";
        String expectedResult = ""ac"";

        closureDescState.sourceURL = testWebserverUri + ""/test_script_powershell.zip"";
        closureDescState.source = ""should not be used"";
        closureDescState.runtime = RUNTIME_POWERSHELL_6;
        closureDescState.outputNames = new ArrayList<>(Collections.singletonList(""result""));
        ResourceConstraints constraints = new ResourceConstraints();
        constraints.timeoutSeconds = 60;
        constraints.ramMB = 300;
        closureDescState.resources = constraints;

        String taskDefPayload = Utils.toJson(closureDescState);
        ClosureDescription closureDescription = createClosureDescription(taskDefPayload,
                serviceClient);
        assertNotNull(closureDescription);

        // Create Closure
        Closure createdClosure = createClosure(closureDescription, serviceClient);
        assertEquals(closureDescription.documentSelfLink, createdClosure.descriptionLink);
        assertEquals(TaskState.TaskStage.CREATED, createdClosure.state);

        // Execute the created Closure
        Closure closureRequest = new Closure();
        Map inputs = new HashMap<>();
        inputs.put(""a"", new JsonPrimitive(expectedInVar));
        closureRequest.inputs = inputs;

        executeClosure(createdClosure, closureRequest, serviceClient);

        // Wait for the completion timeout
        String imageRequestLink = waitForBuildCompletion(IMAGE_NAME, closureDescription);

        waitForTaskState(createdClosure.documentSelfLink, TaskState.TaskStage.FINISHED,
                serviceClient);

        Closure fetchedClosure = getClosure(createdClosure.documentSelfLink, serviceClient);

        assertEquals(closureDescription.documentSelfLink, fetchedClosure.descriptionLink);
        assertEquals(TaskState.TaskStage.FINISHED, fetchedClosure.state);

        assertEquals(expectedInVar, fetchedClosure.inputs.get(""a"").getAsString());
        assertEquals(expectedResult, fetchedClosure.outputs.get(""result"").getAsString());

        //        cleanResource(imageRequestLink, serviceClient);
        //        cleanResource(createdClosure.documentSelfLink, serviceClient);
        //        cleanResource(closureDescription.documentSelfLink, serviceClient);
    }",1
"@Test
    public void testUntrustedCertificates() throws Exception {

        // Validate a custom certificate.
        // Is should fail as it is signed by untrusted CA
        try {
            trustManager.checkServerTrusted(
                    getCertificates(""/certs/untrusted-server.crt""), ""RSA"");
            fail(""Should not trust untrusted certificate"");
        }",1
"@Test
    public void testShouldUpdateContainerLinksWhenUpdatesToContainers() throws Throwable {
        compositeComponent = createCompositeComponent();
        ContainerState containerState1 = createContainer(compositeComponent.documentSelfLink);

        // add a new container:
        waitFor(() -> {
            compositeComponent = getDocument(CompositeComponent.class,
                    compositeComponent.documentSelfLink);
            if (compositeComponent.componentLinks == null
                    || compositeComponent.componentLinks.isEmpty()) {
                return false;
            }",1
"@Test
    public void testCloneCompositeDescriptionWithTwoContainers() throws Throwable {
        initObjectsWithTwoContainers();

        CompositeDescription clonedCompositeDesc = cloneCompositeDesc(
                createdCompositeWithTwoContainers, false);

        checkCompositeForEquality(createdCompositeWithTwoContainers, clonedCompositeDesc, false);

        List<String> containerDescriptions = clonedCompositeDesc.descriptionLinks;

        ContainerDescription clonedFirstContainer = getDocument(ContainerDescription.class,
                containerDescriptions.get(0));
        checkContainersForЕquality(createdFirstContainer, clonedFirstContainer, false);

        ContainerDescription clonedSecondContainer = getDocument(ContainerDescription.class,
                containerDescriptions.get(1));
        checkContainersForЕquality(createdSecondContainer, clonedSecondContainer, false);
    }",1
"@Test
    public void testPutExpanded() throws Throwable {
        ContainerDescription container = new ContainerDescription();
        container.name = ""container"";
        container.image = ""registry.hub.docker.com/kitematic/hello-world-nginx"";
        container = doPost(container, ContainerDescriptionService.FACTORY_LINK);

        ComponentDescription containerComponent = new ComponentDescription();
        containerComponent.name = ""container"";
        container.name = ""updated"";
        containerComponent.updateServiceDocument(container);
        containerComponent.type = ResourceType.CONTAINER_TYPE.getContentType();

        CompositeDescription cd = new CompositeDescription();
        cd.name = ""testComposite"";
        cd = doPost(cd, CompositeDescriptionFactoryService.SELF_LINK);

        // Make PUT but as expanded state, so that components are also updated
        CompositeDescriptionExpanded cdUpdate = new CompositeDescriptionExpanded();
        cdUpdate.documentSelfLink = cd.documentSelfLink;
        cdUpdate.name = cd.name;
        cdUpdate.componentDescriptions = new ArrayList<>();
        cdUpdate.componentDescriptions.add(containerComponent);
        cdUpdate = doPut(cdUpdate);

        // Explicitly search for document to validate that the list returns the right document kind
        CompositeDescription foundCd = searchForDocument(CompositeDescription.class,
                cd.documentSelfLink);
        assertEquals(Utils.buildKind(CompositeDescription.class), foundCd.documentKind);

        container = getDocument(ContainerDescription.class, container.documentSelfLink);
        assertEquals(""updated"", container.name);
    }",1
"@Test
    public void testTenantsLinksInCompositeDescriptionEmbedded() throws Throwable {

        testTenantsLinksInCompositeDescription(true);
    }",1
"@Test
    public void testTenantsLinksInCompositeDescriptionNotEmbedded() throws Throwable {

        testTenantsLinksInCompositeDescription(false);
    }",1
"@Test
    public void testContainersCountOnHostWithContainersNoSystem() throws Throwable {
        String hostId = UUID.randomUUID().toString();
        String hostLink = UriUtils.buildUriPath(ComputeService.FACTORY_LINK, hostId);
        // add preexisting container
        addContainerToMockAdapter(hostLink, preexistingContainerId, preexistingContainerNames);

        ComputeDescription hostDescription = createComputeDescription();
        hostDescription = doPost(hostDescription, ComputeDescriptionService.FACTORY_LINK);

        ComputeState cs = createComputeState(hostId, hostDescription);

        cs = doPost(cs, ComputeService.FACTORY_LINK);

        ContainerState containerState = new ContainerState();
        containerState.id = UUID.randomUUID().toString();
        containerState.names = containerNames;
        containerState.parentLink = UriUtils.buildUriPath(
                ComputeService.FACTORY_LINK,
                hostId);
        containerState.powerState = PowerState.STOPPED;
        containerState = doPost(containerState, ContainerFactoryService.SELF_LINK);
        addContainerToMockAdapter(hostLink, containerState.id, containerState.names);

        doOperation(new ContainerHostDataCollectionState(), UriUtils.buildUri(host,
                ContainerHostDataCollectionService.HOST_INFO_DATA_COLLECTION_LINK),
                false,
                Service.Action.PATCH);

        String csLink = cs.documentSelfLink;
        waitFor(() -> {
            ComputeState computeState = getDocument(ComputeState.class, csLink);
            String containers = computeState.customProperties == null ? null
                    : computeState.customProperties
                            .get(ContainerHostService.NUMBER_OF_CONTAINERS_PER_HOST_PROP_NAME);
            String systemContainers = computeState.customProperties == null ? null
                    : computeState.customProperties
                            .get(ContainerHostService.NUMBER_OF_SYSTEM_CONTAINERS_PROP_NAME);
            //the test container created above and the missing container coming from the host.
            host.log(""testContainersCountOnHostWithContainer - countainer count: %s"", containers);
            return ""2"".equals(containers) && ""0"".equals(systemContainers);
        }",1
"@Test
    public void testContainersCountSystemAndNotSystem() throws Throwable {
        String hostId = UUID.randomUUID().toString();
        String hostLink = UriUtils.buildUriPath(ComputeService.FACTORY_LINK, hostId);
        // add preexisting container
        addContainerToMockAdapter(hostLink, preexistingContainerId, preexistingContainerNames);

        ComputeDescription hostDescription = createComputeDescription();
        hostDescription = doPost(hostDescription, ComputeDescriptionService.FACTORY_LINK);

        ComputeState cs = createComputeState(hostId, hostDescription);

        cs = doPost(cs, ComputeService.FACTORY_LINK);

        ContainerState containerState = new ContainerState();
        containerState.id = UUID.randomUUID().toString();
        containerState.names = containerNames;
        containerState.parentLink = UriUtils.buildUriPath(
                ComputeService.FACTORY_LINK, hostId);
        containerState.powerState = PowerState.STOPPED;
        containerState.system = Boolean.TRUE;
        containerState = doPost(containerState, ContainerFactoryService.SELF_LINK);
        addContainerToMockAdapter(hostLink, containerState.id, containerState.names);

        doOperation(new ContainerHostDataCollectionState(), UriUtils.buildUri(host,
                ContainerHostDataCollectionService.HOST_INFO_DATA_COLLECTION_LINK),
                false,
                Service.Action.PATCH);

        String csLink = cs.documentSelfLink;
        waitFor(() -> {
            ComputeState computeState = getDocument(ComputeState.class, csLink);
            String containers = computeState.customProperties == null ? null
                    : computeState.customProperties
                            .get(ContainerHostService.NUMBER_OF_CONTAINERS_PER_HOST_PROP_NAME);
            String systemContainers = computeState.customProperties == null ? null
                    : computeState.customProperties
                            .get(ContainerHostService.NUMBER_OF_SYSTEM_CONTAINERS_PROP_NAME);
            // the test container created above and the missing container coming from the host.
            host.log(""testContainersCountOnHostWithContainer - countainer count: %s"", containers);
            return ""2"".equals(containers) && ""1"".equals(systemContainers);
        }",1
"@Test
    public void testDataCollectionDuringProvisioning() throws Throwable {
        // stop the mock adapter service and start the mock inspector adapter service:
        stopService(mockAdapterService);
        mockAdapterService = null;
        final MockInspectAdapterService mockInspectAdapterService = new MockInspectAdapterService();
        String containerBeingProvisioned = ""containerBeingProvisioned"";
        try {
            String hostId = UUID.randomUUID().toString();
            String hostLink = UriUtils.buildUriPath(ComputeService.FACTORY_LINK, hostId);
            // add preexisting container
            addContainerToMockAdapter(hostLink, preexistingContainerId, preexistingContainerNames);

            URI adapterServiceUri = UriUtils.buildUri(host, ManagementUriParts.ADAPTER_DOCKER);
            host.startService(Operation.createPost(adapterServiceUri), mockInspectAdapterService);
            waitForServiceAvailability(ManagementUriParts.ADAPTER_DOCKER);

            ComputeDescription hostDescription = createComputeDescription();
            hostDescription = doPost(hostDescription, ComputeDescriptionService.FACTORY_LINK);

            ComputeState cs = createComputeState(hostId, hostDescription);

            cs = doPost(cs, ComputeService.FACTORY_LINK);

            // container being provisioned should not be discovered by the data collection
            // Do not set id - the container is still being provisioned
            ContainerState containerState = new ContainerState();
            containerState.names = containerNames;
            containerState.parentLink = UriUtils.buildUriPath(
                    ComputeService.FACTORY_LINK,
                    hostId);
            containerState.powerState = PowerState.PROVISIONING;
            containerState = doPost(containerState, ContainerFactoryService.SELF_LINK);
            addContainerToMockAdapter(hostLink, containerBeingProvisioned, containerState.names);

            doOperation(new ContainerHostDataCollectionState(), UriUtils.buildUri(host,
                    ContainerHostDataCollectionService.HOST_INFO_DATA_COLLECTION_LINK),
                    false,
                    Service.Action.PATCH);

            host.log("">>>> testDiscoverCreateAndInspectContainer: Container Host %s created.""
                            + "" Waiting for data collection..."", cs.documentSelfLink);
            String csLink = cs.documentSelfLink;
            waitFor(() -> {
                ComputeState computeState = getDocument(ComputeState.class, csLink);
                String containers = computeState.customProperties == null ? null
                        : computeState.customProperties
                                .get(ContainerHostService.NUMBER_OF_CONTAINERS_PER_HOST_PROP_NAME);

                if (containers != null) {
                    host.log("">>>> # of containers per host %s is %s"",
                            computeState.documentSelfLink, containers);
                }",1
"@Test
    public void testDataCollectionWhenAHostIsMarkedForDeletion() throws Throwable {
        String hostId = UUID.randomUUID().toString();

        ComputeDescription hostDescription = createComputeDescription();
        hostDescription = doPost(hostDescription, ComputeDescriptionService.FACTORY_LINK);

        ComputeState cs = createComputeState(hostId, hostDescription);
        cs.lifecycleState = LifecycleState.SUSPEND;
        cs = doPost(cs, ComputeService.FACTORY_LINK);

        // create a dummy ContainerState on the ComputeState (that will be marked as missing by the
        // collection)
        missingContainerState = new ContainerState();
        missingContainerState.id = UUID.randomUUID().toString();
        missingContainerState.names = containerNames;
        missingContainerState.parentLink = UriUtils.buildUriPath(
                ComputeService.FACTORY_LINK,
                hostId);
        missingContainerState.powerState = PowerState.STOPPED;
        missingContainerState.system = false;
        missingContainerState = doPost(missingContainerState, ContainerFactoryService.SELF_LINK);

        doOperation(new ContainerHostDataCollectionState(), UriUtils.buildUri(host,
                ContainerHostDataCollectionService.HOST_INFO_DATA_COLLECTION_LINK),
                false,
                Service.Action.PATCH);

        String csLink = cs.documentSelfLink;
        final long timoutInMillis = 5000; // 5sec
        long startTime = System.currentTimeMillis();

        waitFor(() -> {
            ComputeState computeState = getDocument(ComputeState.class, csLink);
            String containers = computeState.customProperties == null ? null
                    : computeState.customProperties
                            .get(ContainerHostService.NUMBER_OF_CONTAINERS_PER_HOST_PROP_NAME);

            if (containers != null && Integer.parseInt(containers) >= 1) {
                fail(""Should not have any containers."");
            }",1
"@Test
    public void testDataCollectionDuringProvisioning() throws Throwable {
        // stop the mock adapter service and start the mock inspector adapter service:
        stopService(mockAdapterService);
        mockAdapterService = null;
        final MockInspectAdapterService mockInspectAdapterService = new MockInspectAdapterService();
        String containerBeingProvisioned = ""containerBeingProvisioned"";
        try {
            String hostId = UUID.randomUUID().toString();
            String hostLink = UriUtils.buildUriPath(ComputeService.FACTORY_LINK, hostId);
            // add preexisting container
            addContainerToMockAdapter(hostLink, preexistingContainerId, preexistingContainerNames);

            URI adapterServiceUri = UriUtils.buildUri(host, ManagementUriParts.ADAPTER_DOCKER);
            host.startService(Operation.createPost(adapterServiceUri), mockInspectAdapterService);
            waitForServiceAvailability(ManagementUriParts.ADAPTER_DOCKER);

            ComputeDescription hostDescription = createComputeDescription();
            hostDescription = doPost(hostDescription, ComputeDescriptionService.FACTORY_LINK);

            ComputeState cs = createComputeState(hostId, hostDescription);

            cs = doPost(cs, ComputeService.FACTORY_LINK);

            // container being provisioned should not be discovered by the data collection
            // Do not set id - the container is still being provisioned
            ContainerState containerState = new ContainerState();
            containerState.names = containerNames;
            containerState.parentLink = UriUtils.buildUriPath(
                    ComputeService.FACTORY_LINK,
                    hostId);
            containerState.powerState = PowerState.PROVISIONING;
            containerState = doPost(containerState, ContainerFactoryService.SELF_LINK);
            addContainerToMockAdapter(hostLink, containerBeingProvisioned, containerState.names);

            doOperation(new ContainerHostDataCollectionState(), UriUtils.buildUri(host,
                    ContainerHostDataCollectionService.HOST_INFO_DATA_COLLECTION_LINK),
                    false,
                    Service.Action.PATCH);

            host.log("">>>> testDiscoverCreateAndInspectContainer: Container Host %s created.""
                            + "" Waiting for data collection..."", cs.documentSelfLink);
            String csLink = cs.documentSelfLink;
            waitFor(() -> {
                ComputeState computeState = getDocument(ComputeState.class, csLink);
                String containers = computeState.customProperties == null ? null
                        : computeState.customProperties
                                .get(ContainerHostService.NUMBER_OF_CONTAINERS_PER_HOST_PROP_NAME);

                if (containers != null) {
                    host.log("">>>> # of containers per host %s is %s"",
                            computeState.documentSelfLink, containers);
                }",1
"@Test
    public void testGetShellWhenEmbeddedShouldFail() throws Throwable {

        ConfigurationState config = new ConfigurationState();
        config.key = ConfigurationUtil.EMBEDDED_MODE_PROPERTY;
        config.value = ""true"";
        config.documentSelfLink = config.key;
        doPost(config, ConfigurationFactoryService.SELF_LINK);

        try {
            getDocument(String.class, ContainerShellService.SELF_LINK);
            fail(""It should have been forbidden!"");
        }",1
"@Test
    public void testGetShellInvalidIdShouldFail() throws Throwable {
        try {
            getDocument(String.class, ContainerShellService.SELF_LINK + ""?id=invalid"");
            fail(""It should have failed!"");
        }",1
"@Test
    public void testGetShellNoIdShouldFail() throws Throwable {
        try {
            getDocument(String.class, ContainerShellService.SELF_LINK);
            fail(""It should have failed!"");
        }",1
"@Test
    public void testContainerStatsNoParam() {
        TestRequestSender sender = host.getTestRequestSender();
        FailureResponse failureResponse = sender.sendAndWaitFailure(Operation
                .createGet(host, ContainerStatsService.SELF_LINK));
        assertEquals(Operation.STATUS_CODE_FAILURE_THRESHOLD, failureResponse.op.getStatusCode());
        assertTrue(failureResponse.failure instanceof IllegalArgumentException);
    }",1
"@Test
    public void testDelete() throws Throwable {
        ComputeDescription computeDescription =
                doPost(new ComputeDescription(), ComputeDescriptionService.FACTORY_LINK);
        DeploymentPolicy deploymentPolicy =
                doPost(createDeploymentPolicy(), DeploymentPolicyService.FACTORY_LINK);

        ResourcePoolState resourcePool = new ResourcePoolState();
        resourcePool.name = ""test-resource-pool"";
        resourcePool = doPost(resourcePool, ResourcePoolService.FACTORY_LINK);

        ComputeState compute = new ComputeState();
        compute.customProperties = new HashMap<>();
        compute.customProperties.put(ContainerHostService.CUSTOM_PROPERTY_DEPLOYMENT_POLICY,
                deploymentPolicy.documentSelfLink);
        compute.descriptionLink = computeDescription.documentSelfLink;
        compute = doPost(compute, ComputeService.FACTORY_LINK);

        GroupResourcePlacementState resourcePlacement = new GroupResourcePlacementState();
        resourcePlacement.deploymentPolicyLink = deploymentPolicy.documentSelfLink;
        resourcePlacement.maxNumberInstances = 1;
        resourcePlacement.name = ""test-group-resource-placement"";
        resourcePlacement.resourcePoolLink = resourcePool.documentSelfLink;
        resourcePlacement = doPost(resourcePlacement, GroupResourcePlacementService.FACTORY_LINK);

        try {
            doDelete(UriUtils.buildUri(host, deploymentPolicy.documentSelfLink), true);
            fail(""expect validation error during deletion"");
        }",1
"@Test
    public void testDeleteWhenActiveReservation() throws Throwable {
        GroupResourcePlacementState placementState = createAndStoreGroupResourcePlacement();
        placementState = makeResourcePlacementReservationRequest(placementState, 5);

        boolean expectedFailure = true;
        try {
            DeploymentProfileConfig.getInstance().setTest(false);
            doDelete(UriUtils.buildUri(host, placementState.documentSelfLink), expectedFailure);
            fail(""expect validation error during deletion"");
        }",1
"@Test
    public void testGetGroupResourcePlacementState() throws Throwable {
        GroupResourcePlacementState placementState = new GroupResourcePlacementState();
        placementState.name = ""reservation-test"";
        placementState.tenantLinks = Collections.singletonList(""testGroup"");
        placementState.maxNumberInstances = 10;
        placementState.resourcePoolLink = resourcePool.documentSelfLink;

        GroupResourcePlacementState outPlacementState = doPost(placementState,
                GroupResourcePlacementService.FACTORY_LINK);

        GroupResourcePlacementState[] result = new GroupResourcePlacementState[] { null }",1
"@Test
    public void testMemoryPlacementPatchRequest() throws Throwable {
        GroupResourcePlacementState placementState = createAndStoreGroupResourcePlacement();
        String descLink = containerDescription.documentSelfLink;
        int count = 8;

        boolean expectFailure = false;

        placementState = makeResourcePlacementReservationRequest(count, descLink, placementState,
                expectFailure);

        // Set the memory limit to something smaller than what's already reserved
        placementState.memoryLimit = 700;
        expectFailure = true;

        host.testStart(1);
        host.send(Operation
                .createPut(UriUtils.buildUri(host, placementState.documentSelfLink))
                .setBody(placementState)
                .setCompletion(expectFailure ? host.getExpectedFailureCompletion()
                        : host.getCompletion()));
        host.testWait(""Asd"", (int) TimeUnit.MINUTES.toSeconds(1));

        releasePlacement(placementState, descLink, count);
    }",1
"@Test
    public void testUpdateWhenNoActiveReservations() throws Throwable {
        GroupResourcePlacementState placementState = createAndStoreGroupResourcePlacement();
        String newName = ""newName"";
        int newMaxInstance = 7;
        String newResourcePoolLink = resourcePool.documentSelfLink;
        int newPriority = 23;
        long newMemoryLimit = MIN_MEMORY;
        long newStorageLimit = 5789L;
        int newCpuShares = 45;
        List<String> newTenantLinks = Arrays.asList(BUSINESS_GROUP);

        placementState.name = newName;
        placementState.maxNumberInstances = newMaxInstance;
        placementState.priority = newPriority;
        placementState.resourcePoolLink = newResourcePoolLink;
        placementState.memoryLimit = newMemoryLimit;
        placementState.storageLimit = newStorageLimit;
        placementState.cpuShares = newCpuShares;
        placementState.tenantLinks = newTenantLinks;

        doOperation(placementState, UriUtils.buildUri(host, placementState.documentSelfLink),
                false, Action.PUT);

        placementState = getDocument(GroupResourcePlacementState.class,
                placementState.documentSelfLink);

        assertEquals(newName, placementState.name);
        assertEquals(newMaxInstance, placementState.maxNumberInstances);
        assertEquals(newPriority, placementState.priority);
        assertEquals(newResourcePoolLink, placementState.resourcePoolLink);
        assertEquals(newMemoryLimit, placementState.memoryLimit);
        assertEquals(newStorageLimit, placementState.storageLimit);
        assertEquals(newCpuShares, placementState.cpuShares);
        assertEquals(newTenantLinks, placementState.tenantLinks);

        doDelete(UriUtils.buildUri(host, placementState.documentSelfLink), false);
    }",1
"@Test
    public void testHostPortProfileServices() throws Throwable {
        verifyService(
                FactoryService.create(HostPortProfileService.class),
                HostPortProfileService.HostPortProfileState.class,
                (prefix, index) -> {
                    return createHostPortProfile();
                }",1
"@Test
    public void testDiscoverExistingVolumeOnHost() throws Throwable {
        // add preexisting volume to the adapter service
        addVolumeToMockAdapter(COMPUTE_HOST_LINK, TEST_PREEXISTING_VOLUME_NAME, LOCAL_DRIVER, LOCAL_SCOPE);

        // run data collection on preexisting volume
        startAndWaitHostVolumeListDataCollection();

        List<ContainerVolumeState> volumeStates = getVolumeStates();
        assertEquals(1, volumeStates.size());
        ContainerVolumeState preexistingVolumeState = volumeStates.get(0);
        assertNotNull(""Preexisting volume not created or can't be retrieved."", preexistingVolumeState);
        assertEquals(TEST_PREEXISTING_VOLUME_NAME, preexistingVolumeState.name);
        assertTrue(Boolean.TRUE.equals(preexistingVolumeState.external));
        assertTrue(""Preexisting volume belongs to the host."",
                COMPUTE_HOST_LINK.equals(preexistingVolumeState.originatingHostLink));
    }",1
"@Test
    public void testContainerServices() throws Throwable {
        verifyService(
                FactoryService.create(ContainerNetworkService.class),
                ContainerNetworkState.class,
                (prefix, index) -> {
                    ContainerNetworkState networkState = new ContainerNetworkState();
                    networkState.id = prefix + ""id"" + index;
                    networkState.name = prefix + ""name"" + index;

                    Ipam ipam = new Ipam();
                    ipam.driver = IPAM_DRIVER;

                    IpamConfig ipamConfig = new IpamConfig();
                    ipamConfig.subnet = String.format(SUBNET_TEMPLATE, index % 256);
                    ipamConfig.ipRange = String.format(IP_RANGE_TEMPLATE, index % 256, index % 256);
                    ipamConfig.gateway = String.format(GATEWAY_TEMPLATE, index % 256);
                    ipamConfig.auxAddresses = new HashMap<>();
                    ipamConfig.auxAddresses.put(prefix + IPAM_ADDITIONAL_HOST_KEY,
                            String.format(IPAM_ADDITIONAL_HOST_IP_ADDRESS_TEMPLATE, index % 256));

                    ipam.config = new IpamConfig[] { ipamConfig }",1
"@Test
    public void testPatchDriverOptions() throws Throwable {
        ContainerNetworkState network = createNetwork(""tenant1"");
        URI networkUri = UriUtils.buildUri(host, network.documentSelfLink);

        // Test add option to empty map
        ContainerNetworkState patch = new ContainerNetworkState();
        patch.options = new HashMap<>();
        patch.options.put(DRIVER_OPTIONS_KEY_1, DRIVER_OPTIONS_VALUE_1);

        doOperation(patch, networkUri, false, Action.PATCH);
        ContainerNetworkState updatedNetwork = getDocument(ContainerNetworkState.class,
                network.documentSelfLink);
        assertEquals(1, updatedNetwork.options.size());
        assertEquals(DRIVER_OPTIONS_VALUE_1,
                updatedNetwork.options.get(DRIVER_OPTIONS_KEY_1));

        // Test append option to existing list
        patch = new ContainerNetworkState();
        patch.options = new HashMap<>();
        patch.options.put(DRIVER_OPTIONS_KEY_2, DRIVER_OPTIONS_VALUE_2);

        doOperation(patch, networkUri, false, Action.PATCH);
        updatedNetwork = getDocument(ContainerNetworkState.class, network.documentSelfLink);
        assertEquals(2, updatedNetwork.options.size());
        assertEquals(DRIVER_OPTIONS_VALUE_1,
                updatedNetwork.options.get(DRIVER_OPTIONS_KEY_1));
        assertEquals(DRIVER_OPTIONS_VALUE_2,
                updatedNetwork.options.get(DRIVER_OPTIONS_KEY_2));

        // Test overwrite existing options
        patch = new ContainerNetworkState();
        patch.options = new HashMap<>();
        patch.options.put(DRIVER_OPTIONS_KEY_1, DRIVER_OPTIONS_VALUE_2);
        patch.options.put(DRIVER_OPTIONS_KEY_2, DRIVER_OPTIONS_VALUE_1);

        doOperation(patch, networkUri, false, Action.PATCH);
        updatedNetwork = getDocument(ContainerNetworkState.class, network.documentSelfLink);
        assertEquals(2, updatedNetwork.options.size());
        assertEquals(DRIVER_OPTIONS_VALUE_1,
                updatedNetwork.options.get(DRIVER_OPTIONS_KEY_2));
        assertEquals(DRIVER_OPTIONS_VALUE_2,
                updatedNetwork.options.get(DRIVER_OPTIONS_KEY_1));
    }",1
"@Test
    public void testPatchExpiration() throws Throwable {
        ContainerNetworkState network = createNetwork(null);
        URI networkUri = UriUtils.buildUri(host, network.documentSelfLink);

        ContainerNetworkState patch = new ContainerNetworkState();
        long nowMicrosUtc = Utils.fromNowMicrosUtc(TimeUnit.SECONDS.toMicros(30));
        patch.documentExpirationTimeMicros = nowMicrosUtc;

        doOperation(patch, networkUri, false, Action.PATCH);
        ContainerNetworkState updatedNetwork = getDocument(ContainerNetworkState.class,
                network.documentSelfLink);
        assertEquals(nowMicrosUtc, updatedNetwork.documentExpirationTimeMicros);

        patch = new ContainerNetworkState();
        patch.documentExpirationTimeMicros = -1;

        doOperation(patch, networkUri, false, Action.PATCH);
        updatedNetwork = getDocument(ContainerNetworkState.class, network.documentSelfLink);
        assertEquals(0, updatedNetwork.documentExpirationTimeMicros);
    }",1
"@Test
    public void testQueryResultLimit() throws Throwable {
        final String queryTaskDocumentSelfLink = UriUtils.buildUriPath(
                ServiceUriPaths.CORE_QUERY_TASKS, ""/testQueryTaskResultLimit"");
        QuerySpecification qs = new QuerySpecification();
        qs.query = Query.Builder.create().addKindFieldClause(ContainerDescription.class).build();
        QueryTask qt = QueryTask.create(qs);
        qt.documentSelfLink = queryTaskDocumentSelfLink + 1;

        final AtomicReference<QueryTask> q = new AtomicReference<>();
        host.testStart(1);
        new ServiceDocumentQuery<>(host, ContainerDescription.class)
                .query(qt, handler(false, q, qt.documentSelfLink));
        host.testWait();
        qt = q.getAndSet(null);
        assertNotNull(qt);
        assertEquals(ServiceDocumentQuery.DEFAULT_QUERY_RESULT_LIMIT,
                qt.querySpec.resultLimit);

        Integer resourceLimit = 1000;
        qs = new QuerySpecification();
        qs.query = Query.Builder.create().addKindFieldClause(ContainerDescription.class).build();
        qt = QueryTask.create(qs);
        qt.querySpec.resultLimit = resourceLimit;
        qt.documentSelfLink = queryTaskDocumentSelfLink + 2;
        host.testStart(1);
        new ServiceDocumentQuery<>(host, ContainerDescription.class)
                .query(qt, handler(false, q, qt.documentSelfLink));
        host.testWait();
        qt = q.getAndSet(null);
        assertNotNull(qt);
        assertEquals(resourceLimit, qt.querySpec.resultLimit);

        qs = new QuerySpecification();
        qs.query = Query.Builder.create().addKindFieldClause(ContainerDescription.class).build();
        qt = QueryTask.create(qs);
        QueryUtil.addCountOption(qt);
        qt.documentSelfLink = queryTaskDocumentSelfLink + 3;
        host.testStart(1);
        new ServiceDocumentQuery<>(host, ContainerDescription.class)
                .query(qt, handler(true, q, qt.documentSelfLink));
        host.testWait();
        qt = q.getAndSet(null);
        assertNotNull(qt);
        assertNull(qt.querySpec.resultLimit);
    }",1
"@Test
    public void testQueryTaskDeleted() throws Throwable {
        final String queryTaskDocumentSelfLink = UriUtils.buildUriPath(
                ServiceUriPaths.CORE_QUERY_TASKS, ""/testQueryTaskResultLimit"");
        final String queryTaskLink = queryTaskDocumentSelfLink + 1;
        QuerySpecification qs = new QuerySpecification();
        qs.query = Query.Builder.create().addKindFieldClause(ContainerDescription.class).build();
        QueryTask qt = QueryTask.create(qs);
        qt.documentSelfLink = queryTaskLink;
        int fiveSec = 5000000;
        qt.documentExpirationTimeMicros = ServiceUtils.getExpirationTimeFromNowInMicros(fiveSec);

        final AtomicReference<QueryTask> q = new AtomicReference<>();
        host.testStart(1);
        new ServiceDocumentQuery<>(host, ContainerDescription.class)
                .query(qt, handler(false, q, qt.documentSelfLink));
        host.testWait();
        qt = q.getAndSet(null);
        //validate query task exists
        assertNotNull(qt);
        // validate query task is deleted
        waitFor(() -> {
            QueryTask queryTask = getDocumentNoWait(QueryTask.class, queryTaskLink);
            return queryTask == null;
        }",1
"@Test
    public void verifyDefaultContainerDescriptionCreatedOnStartup() throws Throwable {

        ContainerDescription agentContainerDesc = getDocument(
                ContainerDescription.class,
                SystemContainerDescriptions.AGENT_CONTAINER_DESCRIPTION_LINK);
        assertNotNull(agentContainerDesc);
        assertEquals(SystemContainerDescriptions.AGENT_CONTAINER_NAME,
                agentContainerDesc.name);
        String expectedImageName = String.format(""%s:%s"",
                SystemContainerDescriptions.AGENT_IMAGE_NAME,
                SystemContainerDescriptions.getAgentImageVersion());
        assertEquals(expectedImageName, agentContainerDesc.image);
    }",1
"@Test
    public void testNoDuplicateResultWhenBothMatch() throws Throwable {
        verifyTemplateSearchResult(TEST_COMMON);
    }",1
"@Test
    public void testUpdateContainerPorts() throws Throwable {
        ComputeService.ComputeState computeState = createComputeHost();
        ContainerState containerState = createContainerState(computeState.documentSelfLink, true);
        HostPortProfileService.HostPortProfileState profile = createHostPortProfile(computeState,
                containerState, new Long[] { new Long(5000), new Long(3045) }",1
"@Test
    public void testReleaseRetiredContainerPorts() throws Throwable {
        ComputeService.ComputeState computeState = createComputeHost();
        ContainerState containerState = createContainerState(computeState.documentSelfLink, false);
        doPut(containerState);
        HostPortProfileService.HostPortProfileState profile = createHostPortProfile(computeState,
                containerState, new Long[] { new Long(5000), new Long(3045) }",1
"@Test
    public void testToReadableErrorMessage() throws Throwable {
        ContainerHostService service = new ContainerHostService();
        Method m = service.getClass().getDeclaredMethod(""toReadableErrorMessage"", Throwable.class,
                Operation.class);
        m.setAccessible(true);

        Exception e = new Exception();
        String result = (String) m.invoke(service, e, null);
        assertNotNull(result);
        assertTrue(result.contains(""Unexpected error:""));

        e = new io.netty.handler.codec.DecoderException(""Received fatal alert: bad_certificate"");
        result = (String) m.invoke(service, e, null);
        assertNotNull(result);
        assertTrue(result.contains(""Check login credentials""));

        e = new IllegalStateException(""Socket channel closed:"");
        result = (String) m.invoke(service, e, null);
        assertNotNull(result);
        assertTrue(result.contains(""Check login credentials""));

        e = new Exception(new ConnectTimeoutException());
        result = (String) m.invoke(service, e, null);
        assertNotNull(result);
        assertTrue(result.contains(""Connection timeout""));

        e = new Exception(new ProtocolException());
        result = (String) m.invoke(service, e, null);
        assertNotNull(result);
        assertTrue(result.contains(""Protocol exception""));

        e = new IllegalArgumentException();
        result = (String) m.invoke(service, e, null);
        assertNotNull(result);
        assertTrue(result.contains(""Illegal argument exception:""));
    }",1
"@Test
    public void testConvertDockerComposeToCompositeTemplate() throws IOException {
        CompositeTemplate expectedTemplate = deserializeCompositeTemplate(
                getContent(""composite.wordpress.yaml""));

        String expectedTemplateYaml = serializeCompositeTemplate(expectedTemplate);

        // Docker Compose with environment values as array

        DockerCompose compose1 = deserializeDockerCompose(
                getContent(""docker.wordpress.1.yaml""));

        CompositeTemplate template1 = fromDockerComposeToCompositeTemplate(compose1);
        template1.name = expectedTemplate.name; // because of the timestamp

        assertComponentTypes(template1.components);

        assertContainersComponents(ResourceType.CONTAINER_TYPE.getContentType(), 2,
                template1.components);
        assertContainersComponents(ResourceType.NETWORK_TYPE.getContentType(), 0,
                template1.components);
        assertContainersComponents(ResourceType.VOLUME_TYPE.getContentType(), 0,
                template1.components);

        String template1Yaml = serializeCompositeTemplate(template1);

        assertEqualsYamls(expectedTemplateYaml, template1Yaml, true);

        // Docker Compose with environment values as dictionary

        DockerCompose compose2 = deserializeDockerCompose(
                getContent(""docker.wordpress.2.yaml""));

        CompositeTemplate template2 = fromDockerComposeToCompositeTemplate(compose2);
        template2.name = expectedTemplate.name; // because of the timestamp

        assertComponentTypes(template2.components);
        assertContainersComponents(ResourceType.CONTAINER_TYPE.getContentType(), 2,
                template1.components);
        assertContainersComponents(ResourceType.NETWORK_TYPE.getContentType(), 0,
                template1.components);
        assertContainersComponents(ResourceType.VOLUME_TYPE.getContentType(), 0,
                template1.components);

        String template2Yaml = serializeCompositeTemplate(template2);

        assertEqualsYamls(expectedTemplateYaml, template2Yaml, true);
    }",1
"@Test
    public void testConvertDockerComposeToCompositeTemplateWithNetwork() throws IOException {

        // Docker Compose with simple network entities

        CompositeTemplate expectedTemplate = deserializeCompositeTemplate(
                getContent(""composite.simple.network.yaml""));

        String expectedTemplateYaml = serializeCompositeTemplate(expectedTemplate);

        DockerCompose compose1 = deserializeDockerCompose(
                getContent(""docker.simple.network.yaml""));

        CompositeTemplate template1 = fromDockerComposeToCompositeTemplate(compose1);
        template1.name = expectedTemplate.name; // because of the timestamp

        assertComponentTypes(template1.components);
        assertContainersComponents(ResourceType.CONTAINER_TYPE.getContentType(), 3,
                template1.components);
        assertContainersComponents(ResourceType.NETWORK_TYPE.getContentType(), 2,
                template1.components);
        assertContainersComponents(ResourceType.VOLUME_TYPE.getContentType(), 0,
                template1.components);

        String template1Yaml = serializeCompositeTemplate(template1);

        assertEqualsYamls(toUnixLineEnding(expectedTemplateYaml),
                toUnixLineEnding(getContent(""composite.simple.network.expected2.yaml"")), true);
        assertEqualsYamls(toUnixLineEnding(template1Yaml),
                toUnixLineEnding(getContent(""composite.simple.network.yaml"")), true);

        // Docker Compose with complex network entities

        expectedTemplate = deserializeCompositeTemplate(
                getContent(""composite.complex.network.yaml""));

        expectedTemplateYaml = serializeCompositeTemplate(expectedTemplate);

        DockerCompose compose2 = deserializeDockerCompose(
                getContent(""docker.complex.network.yaml""));

        CompositeTemplate template2 = fromDockerComposeToCompositeTemplate(compose2);
        template2.name = expectedTemplate.name; // because of the timestamp

        assertComponentTypes(template2.components);
        assertContainersComponents(ResourceType.CONTAINER_TYPE.getContentType(), 3,
                template2.components);
        assertContainersComponents(ResourceType.NETWORK_TYPE.getContentType(), 3,
                template2.components);
        assertContainersComponents(ResourceType.VOLUME_TYPE.getContentType(), 0,
                template2.components);

        String template2Yaml = serializeCompositeTemplate(template2);

        assertEqualsYamls(toUnixLineEnding(getContent(""composite.simple.network.expected.yaml"")),
                toUnixLineEnding(template2Yaml), true);
    }",1
"@Test
    public void testDeserializeCompositeTemplateWithBindings() throws IOException {
        String expectedContent = getContent(""composite.bindings.yaml"");

        CompositeTemplate compositeTemplate = deserializeCompositeTemplate(expectedContent);

        ContainerDescription wpData = (ContainerDescription) compositeTemplate.components
                .get(""wordpress"").data;
        assertEquals(null, wpData._cluster);
        assertFalse(wpData.customProperties.containsKey(""mysql_user""));

        assertEquals(1, compositeTemplate.bindings.size());

        List<Binding> bindings = compositeTemplate.bindings.iterator().next().bindings;
        assertEquals(3, bindings.size());

        Map<Boolean, List<Binding>> partitionedBindings = bindings.stream()
                .collect(Collectors.partitioningBy(b -> b.isProvisioningTimeBinding()));

        assertEquals(1, partitionedBindings.get(false).size());

        Binding binding = partitionedBindings.get(false).get(0);
        assertFalse(binding.isProvisioningTimeBinding());
        assertEquals(""db~_cluster"", binding.placeholder.bindingExpression);

        // provisioning time bindings
        assertEquals(2, partitionedBindings.get(true).size());

        binding = partitionedBindings.get(true).get(0);
        assertTrue(binding.isProvisioningTimeBinding());
        assertEquals(""_resource~db~address"", binding.placeholder.bindingExpression);

        binding = partitionedBindings.get(true).get(1);
        assertTrue(binding.isProvisioningTimeBinding());
        assertEquals(""_resource~db~env~MYSQL_USER"", binding.placeholder.bindingExpression);

        // validate ""normalizeBindings""

        String expectedContentSerialized = serializeCompositeTemplate(compositeTemplate);

        assertFalse(expectedContent.contains(""bindings:""));
        assertFalse(expectedContentSerialized.contains(""bindings:""));

        assertTrue(expectedContent.contains(""${db~_cluster}",1
"@Test
    public void testDeserializeSerializeComplexDockerComposeWithNetwork() throws IOException {

        String expectedContent = getContent(""docker.complex.network.yaml"");

        DockerCompose compose = deserializeDockerCompose(expectedContent);

        String content = serializeDockerCompose(compose);

        assertEqualsYamls(toUnixLineEnding(expectedContent), toUnixLineEnding(content));

        CompositeTemplate template = fromDockerComposeToCompositeTemplate(compose);

        assertNull(template.id);
        assertNull(template.status);
        assertComponentTypes(template.components);
        assertContainersComponents(ResourceType.CONTAINER_TYPE.getContentType(), 3,
                template.components);
        assertContainersComponents(ResourceType.NETWORK_TYPE.getContentType(), 3,
                template.components);
        assertContainersComponents(ResourceType.VOLUME_TYPE.getContentType(), 0,
                template.components);

        String contentTemplate = serializeCompositeTemplate(template);

        assertTrue((contentTemplate != null) && (!contentTemplate.isEmpty()));
    }",1
"@Test
    public void testSeriaizeDeserializeCompositeTemplateWithHealthCheck() throws IOException {

        // Assert that healthConfig.ignoreOnProvision is serialized when false
        String expectedContent = getContent(""composite.simple.health.yaml"");

        CompositeTemplate template = deserializeCompositeTemplate(expectedContent);

        ContainerDescription data = (ContainerDescription) template.components.get(""hello"").data;
        assertNotNull(data.healthConfig);
        assertEquals(false, data.healthConfig.ignoreOnProvision);

        // Assert that healthConfig.ignoreOnProvision is serialized when true
        data.healthConfig.ignoreOnProvision = true;
        String content = serializeCompositeTemplate(template);

        template = deserializeCompositeTemplate(content);

        data = (ContainerDescription) template.components.get(""hello"").data;
        assertNotNull(data.healthConfig);
        assertEquals(true, data.healthConfig.ignoreOnProvision);

        // Assert that healthConfig.ignoreOnProvision is not serialized when null
        data.healthConfig = null;
        content = serializeCompositeTemplate(template);
        assertFalse(content.contains(""health_config""));
    }",1
"@Test
    public void testWrongDeserializationExceptions() throws IOException {
        try {
            deserializeCompositeTemplate(getContent(""composite.bad.yaml""));
            fail(""wrong content!"");
        }",1
"@Test
    public void testConstructKubeConfigWithBearerToken() {
        String clusterAddress = ""https://testhost:8443"";
        String token = ""bearer_token"";

        AuthCredentialsServiceState creds = new AuthCredentialsServiceState();
        creds.privateKey = token;
        creds.type = AuthCredentialsType.Bearer.toString();
        KubeConfig config = KubernetesUtil.constructKubeConfig(clusterAddress, creds);

        assertNotNull(config);
        assertEquals(token, config.users.get(0).user.token);
    }",1
"@Test
    public void testConstructKubeConfigWithPassword() {
        String clusterAddress = ""https://testhost:8443"";
        String username = ""user1"";
        String password = ""password123"";

        AuthCredentialsServiceState creds = new AuthCredentialsServiceState();
        creds.userEmail = username;
        creds.privateKey = password;
        creds.type = AuthCredentialsType.Password.toString();
        KubeConfig config = KubernetesUtil.constructKubeConfig(clusterAddress, creds);

        assertNotNull(config);
        assertEquals(username, config.users.get(0).user.username);
        assertEquals(password, config.users.get(0).user.password);
    }",1
"@Test
    public void testDeserializeKubernetesEntityHasCorrectClass() throws IOException {
        assertEquals(Pod.class,
                KubernetesUtil.deserializeKubernetesEntity(podYaml).getClass());
        assertEquals(PodTemplate.class,
                KubernetesUtil.deserializeKubernetesEntity(podTemplateYaml).getClass());
        assertEquals(ReplicationController.class,
                KubernetesUtil.deserializeKubernetesEntity(replicationControllerYaml).getClass());
        assertEquals(Deployment.class,
                KubernetesUtil.deserializeKubernetesEntity(deploymentYaml).getClass());
        assertEquals(Service.class,
                KubernetesUtil.deserializeKubernetesEntity(serviceYamlFormat).getClass());
        assertEquals(BaseKubernetesObject.class,
                KubernetesUtil.deserializeKubernetesEntity(secretYaml).getClass());
    }",1
"@Test
    public void testFromContainerDescriptionHealthConfigToPodContainerProbe() {
        Probe expectedProbe1 = new Probe();
        expectedProbe1.exec = new ExecAction();
        expectedProbe1.exec.command = new String[] { ""test"", ""command"" }",1
"@Test
    public void testGetStateTypeFromSelfLink() {
        String selfLink = ""/resources/kubernetes-pods/376fdq673"";
        Class<? extends BaseKubernetesState> expectedClass = PodState.class;
        Class<? extends BaseKubernetesState> actualClass = getStateTypeFromSelfLink(selfLink);
        assertEquals(expectedClass, actualClass);
    }",1
"@Test
    public void testSetApplicationLabelOnReplicationController() throws IOException {
        KubernetesDescription kd = new KubernetesDescription();
        kd.kubernetesEntity = replicationControllerYaml;
        kd.type = KubernetesUtil.REPLICATION_CONTROLLER_TYPE;

        String testCompositeId = ""123456"";

        kd = KubernetesUtil.setApplicationLabel(kd, testCompositeId);

        ReplicationController replicationController = kd
                .getKubernetesEntity(ReplicationController.class);

        assertNotNull(replicationController);
        assertNotNull(replicationController.metadata);
        assertNotNull(replicationController.metadata.labels);
        assertEquals(testCompositeId,
                replicationController.metadata.labels.get(KUBERNETES_LABEL_APP_ID));

        assertNotNull(replicationController.spec);
        assertNotNull(replicationController.spec.template);
        assertNotNull(replicationController.spec.template.metadata);
        assertNotNull(replicationController.spec.template.metadata.labels);
        assertEquals(testCompositeId,
                replicationController.spec.template.metadata.labels.get(KUBERNETES_LABEL_APP_ID));
    }",1
"@Test
    public void testSetApplicationLabelOnService() throws IOException {
        KubernetesDescription kd = new KubernetesDescription();
        kd.kubernetesEntity = serviceYamlFormat;
        kd.type = SERVICE_TYPE;

        String testCompositeId = ""123456"";

        kd = KubernetesUtil.setApplicationLabel(kd, testCompositeId);

        Service service = kd.getKubernetesEntity(Service.class);

        assertNotNull(service);
        assertNotNull(service.metadata);
        assertNotNull(service.metadata.labels);
        assertEquals(testCompositeId, service.metadata.labels.get(KUBERNETES_LABEL_APP_ID));
    }",1
"@Test
    public void testCreateDelete() throws Throwable {
        ElasticPlacementZoneConfigurationState state = createState(true);
        state.resourcePoolState = buildRpState();
        state.epzState = buildEpzState(null, ""tag1"", ""tag2"");
        ElasticPlacementZoneConfigurationState createdState = sendState(state, Action.POST);

        delete(ElasticPlacementZoneConfigurationService.SELF_LINK
                + createdState.resourcePoolState.documentSelfLink);

        assertNull(
                searchForDocument(ResourcePoolState.class, createdState.resourcePoolState.documentSelfLink));
        assertNull(searchForDocument(ElasticPlacementZoneState.class,
                createdState.epzState.documentSelfLink));
    }",1
"@Test
    public void testCreateNoEpz() throws Throwable {
        ElasticPlacementZoneConfigurationState state = createState(true);
        state.resourcePoolState = buildRpState();
        ElasticPlacementZoneConfigurationState returnedState = sendState(state, Action.POST);

        assertNotNull(returnedState.resourcePoolState.documentSelfLink);
        assertNotNull(searchForDocument(ResourcePoolState.class,
                returnedState.resourcePoolState.documentSelfLink));
        assertEquals(state.resourcePoolState.name, returnedState.resourcePoolState.name);
    }",1
"@Test
    public void testUpdateNoEpz() throws Throwable {
        // create through the config service
        ElasticPlacementZoneConfigurationState state = createState(false);
        state.resourcePoolState = buildRpState();
        ElasticPlacementZoneConfigurationState createdState = sendState(state, Action.POST);

        // update through the config service
        ElasticPlacementZoneConfigurationState patchState = createState(false);
        patchState.resourcePoolState.documentSelfLink =
                createdState.resourcePoolState.documentSelfLink;
        patchState.resourcePoolState.name = ""new-name"";
        ElasticPlacementZoneConfigurationState latestState = sendState(patchState, Action.PATCH);

        // validate returned state
        assertEquals(""new-name"", latestState.resourcePoolState.name);
        assertNotNull(latestState.resourcePoolState.query);

        // validate the actual RP state
        ResourcePoolState rp = getDocument(ResourcePoolState.class,
                createdState.resourcePoolState.documentSelfLink);
        assertEquals(""new-name"", rp.name);
    }",1
"@Test
    public void testPatchNoChange() throws Throwable {
        // create a non-elastic RP
        ResourcePoolState rp = createRp();
        assertEquals(EnumSet.noneOf(ResourcePoolProperty.class), rp.properties);
        assertTrue(isNonElasticQuery(rp.query));

        // create EPZ for the RP
        String epzLink = createEpz(rp.documentSelfLink, ""tag1"", ""tag2"").documentSelfLink;

        ElasticPlacementZoneState patchState = new ElasticPlacementZoneState();
        patchState.tagLinksToMatch = tagSet(""tag1"");

        Operation patchOp = Operation.createPatch(host, epzLink).setBody(patchState).forceRemote();
        Operation returnedOp = ((CompletableFuture<Operation>) host.sendWithDeferredResult(patchOp)
                .toCompletionStage()).get(60, TimeUnit.SECONDS);
        assertEquals(Operation.STATUS_CODE_NOT_MODIFIED, returnedOp.getStatusCode());
    }",1
"@Test
    public void testCreate() {
        Endpoint endpoint = new Endpoint();
        endpoint.apiEndpoint = ""http://localhost"";
        endpoint.uaaEndpoint = ""https://localhost"";

        createEndpoint(endpoint);

        endpoint.apiEndpoint = null;
        endpoint.uaaEndpoint = ""http://localhost"";
        createEndpointExpectFailure(endpoint, ser -> {
            assertEquals(Operation.STATUS_CODE_BAD_REQUEST, ser.statusCode);
            assertEquals(""'API endpoint' is required"", ser.message);
        }",1
"@Test
    public void testDelete() throws Throwable {
        Endpoint endpoint = new Endpoint();
        endpoint.apiEndpoint = ""http://localhost"";
        endpoint.uaaEndpoint = ""https://localhost"";

        endpoint = createEndpoint(endpoint);

        delete(endpoint.documentSelfLink);
        endpoint = getDocumentNoWait(Endpoint.class, endpoint.documentSelfLink);
        assertNull(endpoint);
    }",1
"@Test
    public void testDeleteEndpointAndClusters() throws Throwable {
        Endpoint endpoint = new Endpoint();
        endpoint.apiEndpoint = ""http://localhost"";
        endpoint.uaaEndpoint = ""https://localhost"";

        endpoint = createEndpoint(endpoint);
        ClusterDto clusterDto = createCluster(endpoint.documentSelfLink);

        delete(endpoint.documentSelfLink);
        endpoint = getDocumentNoWait(Endpoint.class, endpoint.documentSelfLink);
        assertNull(endpoint);

        clusterDto = getDocumentNoWait(ClusterDto.class, clusterDto.documentSelfLink);
        assertNull(endpoint);
    }",1
"@Test
    public void testContainerHostDataCollectionServiceCreatedOnStartUp() throws Throwable {
        waitForServiceAvailability(ContainerHostDataCollectionService
                .HOST_INFO_DATA_COLLECTION_LINK);
        ContainerHostDataCollectionState dataCollectionState = getDocument(
                ContainerHostDataCollectionState.class,
                ContainerHostDataCollectionService.HOST_INFO_DATA_COLLECTION_LINK);

        assertNotNull(dataCollectionState);
    }",1
"@Test
    public void testDefaultGroupPlacementServiceCreatedOnStartUp() throws Throwable {
        waitForServiceAvailability(GroupResourcePlacementService.DEFAULT_RESOURCE_PLACEMENT_LINK);
        GroupResourcePlacementState groupResourcePlacementState = getDocument(
                GroupResourcePlacementState.class,
                GroupResourcePlacementService.DEFAULT_RESOURCE_PLACEMENT_LINK);

        assertNotNull(groupResourcePlacementState);
        assertEquals(GroupResourcePlacementService.DEFAULT_RESOURCE_PLACEMENT_ID,
                groupResourcePlacementState.name);
        assertEquals(GroupResourcePlacementService.DEFAULT_RESOURCE_POOL_LINK,
                groupResourcePlacementState.resourcePoolLink);
        assertEquals(1000000, groupResourcePlacementState.maxNumberInstances);
        assertEquals(100, groupResourcePlacementState.priority);
        assertNull(groupResourcePlacementState.tenantLinks);// assert global default group placement
    }",1
"@Test
    public void testHostContainerListDataCollectionServiceCreatedOnStartUp() throws Throwable {
        waitForServiceAvailability(HostContainerListDataCollection
                .DEFAULT_HOST_CONTAINER_LIST_DATA_COLLECTION_LINK);
        HostContainerListDataCollectionState dataCollectionState = getDocument(
                HostContainerListDataCollectionState.class,
                HostContainerListDataCollection.DEFAULT_HOST_CONTAINER_LIST_DATA_COLLECTION_LINK);

        assertNotNull(dataCollectionState);
        assertEquals(TaskStage.STARTED, dataCollectionState.taskInfo.stage);
    }",1
"@Test
    public void testCreateClusterWithIntercept() throws Throwable {
        ContainerHostSpec hostSpec = new ContainerHostSpec();
        hostSpec.hostState = new ComputeState();
        hostSpec.hostState.id = UUID.randomUUID().toString();
        hostSpec.hostState.address = ""test"";
        hostSpec.hostState.customProperties = new HashMap<>();
        hostSpec.hostState.customProperties.put(ContainerHostService
                        .HOST_DOCKER_ADAPTER_TYPE_PROP_NAME, ""API"");
        hostSpec.hostState.customProperties.put(ContainerHostService.CONTAINER_HOST_TYPE_PROP_NAME,
                ""DOCKER"");

        ClusterDto dto = doPostWithProjectHeader(hostSpec, ClusterService.SELF_LINK, project
                .documentSelfLink, ClusterDto.class);

        ComputeState computeState = getDocument(ComputeState.class, dto.nodeLinks.get(0));

        assertTrue(computeState.tenantLinks.contains(project.documentSelfLink));
    }",1
"@Test
    public void testCreateCompositeComponentIntercept() {
        CompositeComponent state = new CompositeComponent();
        state.name = ""test"";
        state.componentLinks = Collections.singletonList(""test"");

        CompositeComponent doc = doPostWithProjectHeader(state, CompositeComponentFactoryService
                .SELF_LINK, project.documentSelfLink, CompositeComponent.class);

        assertTenantLinks(doc, project.documentSelfLink);
        assertEquals(state.name, doc.name);
        assertEquals(state.componentLinks, doc.componentLinks);
    }",1
"@Test
    public void testCreateContainerVolumeDescriptionIntercept() {
        ContainerVolumeDescription state = new ContainerVolumeDescription();
        state.name = ""test"";
        state.externalName = ""test"";

        ContainerVolumeDescription doc = doPostWithProjectHeader(state,
                ContainerVolumeDescriptionService.FACTORY_LINK, project.documentSelfLink,
                ContainerVolumeDescription.class);

        assertTenantLinks(doc, project.documentSelfLink);
        assertEquals(state.name, doc.name);
        assertEquals(state.externalName, doc.externalName);
    }",1
"@Test
    public void testGetCompositeComponent() {
        CompositeComponent state1 = new CompositeComponent();
        state1.name = ""test"";
        state1.componentLinks = Collections.singletonList(""test"");

        CompositeComponent state2 = new CompositeComponent();
        state2.name = ""test"";
        state2.componentLinks = Collections.singletonList(""test"");

        state1 = doPostWithProjectHeader(state1, CompositeComponentFactoryService
                .SELF_LINK, testProject1.documentSelfLink, CompositeComponent.class);

        state2 = doPostWithProjectHeader(state2, CompositeComponentFactoryService
                .SELF_LINK, testProject2.documentSelfLink, CompositeComponent.class);

        ServiceDocumentQueryResult project1Docs = getDocumentsWithinProject(
                CompositeComponentFactoryService.SELF_LINK, testProject1.documentSelfLink);
        assertEquals(1, project1Docs.documentLinks.size());
        assertTrue(project1Docs.documentLinks.contains(state1.documentSelfLink));

        ServiceDocumentQueryResult project2Docs = getDocumentsWithinProject(
                CompositeComponentFactoryService.SELF_LINK, testProject2.documentSelfLink);
        assertEquals(1, project2Docs.documentLinks.size());
        assertTrue(project2Docs.documentLinks.contains(state2.documentSelfLink));
    }",1
"@Test
    public void testGetContainerVolumeDescription() {
        ContainerVolumeDescription state1 = new ContainerVolumeDescription();
        state1.name = ""test"";
        state1.externalName = ""test"";

        ContainerVolumeDescription state2 = new ContainerVolumeDescription();
        state2.name = ""test"";
        state2.externalName = ""test"";

        state1 = doPostWithProjectHeader(state1,
                ContainerVolumeDescriptionService.FACTORY_LINK, testProject1.documentSelfLink,
                ContainerVolumeDescription.class);

        state2 = doPostWithProjectHeader(state2,
                ContainerVolumeDescriptionService.FACTORY_LINK, testProject2.documentSelfLink,
                ContainerVolumeDescription.class);

        ServiceDocumentQueryResult project1Docs = getDocumentsWithinProject(
                ContainerVolumeDescriptionService.FACTORY_LINK, testProject1.documentSelfLink);
        assertEquals(1, project1Docs.documentLinks.size());
        assertTrue(project1Docs.documentLinks.contains(state1.documentSelfLink));

        ServiceDocumentQueryResult project2Docs = getDocumentsWithinProject(
                ContainerVolumeDescriptionService.FACTORY_LINK, testProject2.documentSelfLink);
        assertEquals(1, project2Docs.documentLinks.size());
        assertTrue(project2Docs.documentLinks.contains(state2.documentSelfLink));
    }",1
"@Test
    public void testGetContainerVolumeDescription() {
        ContainerVolumeDescription state1 = new ContainerVolumeDescription();
        state1.name = ""test"";
        state1.externalName = ""test"";

        ContainerVolumeDescription state2 = new ContainerVolumeDescription();
        state2.name = ""test"";
        state2.externalName = ""test"";

        state1 = doPostWithProjectHeader(state1,
                ContainerVolumeDescriptionService.FACTORY_LINK, testProject1.documentSelfLink,
                ContainerVolumeDescription.class);

        state2 = doPostWithProjectHeader(state2,
                ContainerVolumeDescriptionService.FACTORY_LINK, testProject2.documentSelfLink,
                ContainerVolumeDescription.class);

        ServiceDocumentQueryResult project1Docs = getDocumentsWithinProject(
                ContainerVolumeDescriptionService.FACTORY_LINK, testProject1.documentSelfLink);
        assertEquals(1, project1Docs.documentLinks.size());
        assertTrue(project1Docs.documentLinks.contains(state1.documentSelfLink));

        ServiceDocumentQueryResult project2Docs = getDocumentsWithinProject(
                ContainerVolumeDescriptionService.FACTORY_LINK, testProject2.documentSelfLink);
        assertEquals(1, project2Docs.documentLinks.size());
        assertTrue(project2Docs.documentLinks.contains(state2.documentSelfLink));
    }",1
"@Test
    public void testGetExternalPopularImages() throws Throwable {

        // with the 'container.user.resources.path' configuration attribute set
        // the images of the popular-images.json file there will be returned

        HostInitCommonServiceConfig.startServices(host);

        waitForServiceAvailability(ConfigurationFactoryService.SELF_LINK);
        waitForServiceAvailability(UriUtils.buildUriPath(UriUtils.buildUriPath(
                ConfigurationFactoryService.SELF_LINK, FileUtil.USER_RESOURCES_PATH_VARIABLE)));

        ConfigurationState config = new ConfigurationState();
        config.documentSelfLink = UriUtils.buildUriPath(ConfigurationFactoryService.SELF_LINK, FileUtil.USER_RESOURCES_PATH_VARIABLE);
        config.key = FileUtil.USER_RESOURCES_PATH_VARIABLE;
        config.value = Paths.get(PopularImagesServiceTest.class.getResource(""/containers"").toURI())
                .toString();

        ConfigurationState storedConfig = doPut(config);
        assertNotNull(storedConfig);

        HostInitImageServicesConfig.startServices(host);
        waitForServiceAvailability(PopularImagesService.SELF_LINK);

        Collection<?> images = getDocument(Collection.class, PopularImagesService.SELF_LINK);
        assertNotNull(images);
        assertEquals(5, images.size());
    }",1
"@Test
    public void testGetSingleEventLog() throws Throwable {
        EventLogState createdState = doPost(eventLogState, EventLogFactoryService.SELF_LINK);

        EventLogState retrievedState = getDocument(EventLogState.class, createdState.documentSelfLink);
        assertEventLogEquals(createdState, retrievedState);

        doDelete(UriUtils.buildUri(host, createdState.documentSelfLink), false);
    }",1
"@Test
    public void testAllocationOfContainersWithAffinityAndVolumeFrom() throws Throwable {
        createDockerHost(createDockerHostDescription(), createResourcePool(), true);
        createDockerHost(createDockerHostDescription(), createResourcePool(), true);
        createDockerHost(createDockerHostDescription(), createResourcePool(), true);

        // create first container:
        ContainerDescription desc1 = TestRequestStateFactory.createContainerDescription();
        desc1.documentSelfLink = UUID.randomUUID().toString();
        desc1.name = ""name1"";
        desc1.portBindings = null;
        desc1 = doPost(desc1, ContainerDescriptionService.FACTORY_LINK);
        assertNotNull(desc1);
        addForDeletion(desc1);

        String contextId = UUID.randomUUID().toString();
        // all instances of this request should be allocated on the same hosts because of the pod.
        ContainerAllocationTaskState allocationTask1 = createContainerAllocationTask(
                desc1.documentSelfLink, 1);
        allocationTask1.customProperties.put(RequestUtils.FIELD_NAME_CONTEXT_ID_KEY, contextId);
        allocationTask1 = allocate(allocationTask1);
        ContainerState container1 = getDocument(ContainerState.class,
                allocationTask1.resourceLinks.iterator().next());

        // create second container with afinity dependent on the first container:
        ContainerDescription desc2 = TestRequestStateFactory.createContainerDescription();
        desc2.documentSelfLink = UUID.randomUUID().toString();
        desc2.name = ""name2-links"";
        desc2.portBindings = null;
        desc2.affinity = new String[] { desc1.name }",1
"@Test
    public void testAllocationOfContainersWithSameHostPodConstraint() throws Throwable {
        createDockerHost(createDockerHostDescription(), createResourcePool(), true);
        createDockerHost(createDockerHostDescription(), createResourcePool(), true);
        createDockerHost(createDockerHostDescription(), createResourcePool(), true);

        String pod = ""host-pod1"";

        // create a description with a pod defined:
        ContainerDescription desc1 = TestRequestStateFactory.createContainerDescription();
        desc1.documentSelfLink = UUID.randomUUID().toString();
        desc1.name = ""linked-container1"";
        desc1.pod = pod;
        desc1.portBindings = null;
        desc1 = doPost(desc1, ContainerDescriptionService.FACTORY_LINK);
        assertNotNull(desc1);
        addForDeletion(desc1);

        String contextId = UUID.randomUUID().toString();
        // all instances of this request should be allocated on the same hosts because of the pod.
        ContainerAllocationTaskState allocationTask1 = createContainerAllocationTask(
                desc1.documentSelfLink, 1);
        allocationTask1.customProperties.put(RequestUtils.FIELD_NAME_CONTEXT_ID_KEY, contextId);
        allocationTask1 = allocate(allocationTask1);
        ContainerState container = getDocument(ContainerState.class,
                allocationTask1.resourceLinks.iterator().next());

        String hostLink = container.parentLink;

        // loop a few times to make sure the right host is not chosen by a chance
        for (int i = 0; i < 5; i++) {
            ContainerDescription desc2 = TestRequestStateFactory.createContainerDescription();
            desc2.documentSelfLink = UUID.randomUUID().toString();
            desc2.name = ""linked-container"" + i;
            desc2.pod = pod;
            desc2.portBindings = null;
            desc2 = doPost(desc2, ContainerDescriptionService.FACTORY_LINK);
            assertNotNull(desc2);
            addForDeletion(desc2);

            // all instances of this request should be allocated on the same host as the one already
            // selected by the previous request since the desc1.pod == desc2.pod
            ContainerAllocationTaskState allocationTask2 = createContainerAllocationTask(
                    desc2.documentSelfLink, 2);
            allocationTask2.customProperties.put(RequestUtils.FIELD_NAME_CONTEXT_ID_KEY, contextId);
            allocationTask2 = allocate(allocationTask2);
            for (String resourceLink : allocationTask2.resourceLinks) {
                ContainerState currContainer = getDocument(ContainerState.class, resourceLink);
                assertEquals(""Same host not selected for allocation request: ""
                        + allocationTask2.documentSelfLink + "" - in iteration: "" + i, hostLink,
                        currContainer.parentLink);
            }",1
"@Test
    public void testContainerAllocationWithFollowingProvisioningRequest() throws Throwable {
        host.log("">>>>>>Start: testContainerAllocationWithFollowingProvisioningRequest <<<<< "");
        doOperation(containerDesc, UriUtils.buildUri(host, containerDesc.documentSelfLink),
                false, Action.PUT);

        ContainerAllocationTaskState allocationTask = createContainerAllocationTask();
        allocationTask.customProperties = new HashMap<>();
        allocationTask.customProperties.put(RequestUtils.FIELD_NAME_ALLOCATION_REQUEST,
                Boolean.TRUE.toString());
        allocationTask = allocate(allocationTask);

        assertContainerStateAfterAllocation(allocationTask);

        // Request provisioning after allocation:
        RequestBrokerState provisioningRequest = new RequestBrokerState();
        provisioningRequest.resourceType = allocationTask.resourceType;
        provisioningRequest.resourceLinks = allocationTask.resourceLinks;
        provisioningRequest.resourceDescriptionLink = containerDesc.documentSelfLink;
        provisioningRequest.operation = ContainerOperationType.CREATE.id;

        provisioningRequest = doPost(provisioningRequest, RequestBrokerFactoryService.SELF_LINK);
        assertNotNull(provisioningRequest);

        waitForTaskSuccess(allocationTask.documentSelfLink, ContainerAllocationTaskState.class);
    }",1
"@Test
    public void testRedeploymentWithAutoRedeployOptionDisabled() throws Throwable {
        final long timeoutInMillis = 5000; // 5sec
        ContainerDescription cd = createContainerDescription(false);

        ContainerState state = provisionContainer(cd.documentSelfLink);
        // change the power state of one of them
        setContainerPowerState(state, PowerState.ERROR);

        doOperation(new ContainerControlLoopState(), UriUtils.buildUri(host,
                ContainerControlLoopService.CONTROL_LOOP_INFO_LINK),
                false,
                Service.Action.PATCH);

        state = getDocument(ContainerState.class, state.documentSelfLink);
        host.log(""Power state = %s"", state.powerState.name());

        long startTime = System.currentTimeMillis();
        final String link = state.documentSelfLink;
        AtomicBoolean healthyContainersFound = new AtomicBoolean(false);
        waitFor(() -> {
            ContainerState st = getDocument(ContainerState.class, link);
            host.log(""Power state = %s"", st.powerState.name());
            retrieveContainerStates(cd.documentSelfLink)
                    .thenAccept(containerStates -> {
                        long healthyContainers = containerStates.stream()
                                .filter(cs -> PowerState.RUNNING == cs.powerState)
                                .count();
                        host.log(""healthyContainers = %d"", healthyContainers);
                        if (healthyContainers != 0) {
                            healthyContainersFound.set(true);
                        }",1
"@Test
    public void testContainerHostRemovalResourceOperationCycle() throws Throwable {
        request = startRequest(request);
        waitForRequestToComplete(request);

        request = getDocument(RequestBrokerState.class, request.documentSelfLink);
        assertNotNull(request);

        // verify the resources are created as expected:
        assertEquals(request.resourceCount, request.resourceLinks.size());
        List<String> containerStateLinks = findResourceLinks(ContainerState.class,
                request.resourceLinks);

        // create a host removal task
        ContainerHostRemovalTaskState state = new ContainerHostRemovalTaskState();
        state.resourceLinks = new HashSet<>(Collections.singletonList(
                computeHost.documentSelfLink));
        state = doPost(state, ContainerHostRemovalTaskFactoryService.SELF_LINK);

        assertNotNull(""task is null"", state);
        waitForTaskSuccess(state.documentSelfLink, ContainerHostRemovalTaskState.class);

        validateHostRemoved(containerStateLinks);
    }",1
"@Test
    public void testRequestBrokerContainerHostRemovalWithSystemContainerAndVolumes()
            throws Throwable {
        request = startRequest(request);
        waitForRequestToComplete(request);

        request = getDocument(RequestBrokerState.class, request.documentSelfLink);
        assertNotNull(request);

        // create a system container
        ContainerState container = TestRequestStateFactory.createContainer();
        container.descriptionLink = containerDesc.documentSelfLink;
        container.adapterManagementReference = containerDesc.instanceAdapterReference;
        container.groupResourcePlacementLink = groupPlacementState.documentSelfLink;
        container.parentLink = computeHost.documentSelfLink;
        container.system = Boolean.TRUE;
        container = doPost(container, ContainerFactoryService.SELF_LINK);

        // verify the resources are created as expected:
        assertEquals(request.resourceCount, request.resourceLinks.size());
        List<String> containerStateLinks = findResourceLinks(ContainerState.class,
                request.resourceLinks);

        // create a volume
        ContainerVolumeDescription volumeDesc = TestRequestStateFactory
                .createContainerVolumeDescription(""test-volume"");
        volumeDesc = doPost(volumeDesc, ContainerVolumeDescriptionService.FACTORY_LINK);
        addForDeletion(volumeDesc);

        ContainerVolumeState volume = TestRequestStateFactory.createVolume(""test-volume-003"");
        volume.adapterManagementReference = volumeDesc.instanceAdapterReference;
        volume.descriptionLink = volumeDesc.documentSelfLink;
        volume = doPost(volume, ContainerVolumeService.FACTORY_LINK);

        // verify the volume is created as expected
        List<String> containerVolumeStateLinks = findResourceLinks(ContainerVolumeState.class,
                Arrays.asList(volume.documentSelfLink));
        assertEquals(1, containerVolumeStateLinks.size());

        // create a host removal task - RequestBroker
        RequestBrokerState request = new RequestBrokerState();
        request.resourceType = ResourceType.CONTAINER_HOST_TYPE.getName();
        request.resourceLinks = new HashSet<>(Collections.singletonList(
                computeHost.documentSelfLink));
        request.operation = RequestBrokerState.REMOVE_RESOURCE_OPERATION;

        request = startRequest(request);
        waitForRequestToComplete(request);

        // verify that the volume state was removed
        containerVolumeStateLinks = findResourceLinks(ContainerVolumeState.class,
                Arrays.asList(volume.documentSelfLink));
        assertTrue(""ContainerVolumeState not removed: "" + containerVolumeStateLinks,
                containerVolumeStateLinks.isEmpty());

        // verify that the container states were removed
        containerStateLinks = findResourceLinks(ContainerState.class, containerStateLinks);
        assertTrue(""ContainerState not removed: "" + containerStateLinks,
                containerStateLinks.isEmpty());

        // verify that the host was removed
        Collection<String> computeSelfLinks = findResourceLinks(ComputeState.class,
                Collections.singletonList(computeHost.documentSelfLink));

        assertTrue(""ComputeState was not deleted: "" + computeSelfLinks, computeSelfLinks.isEmpty());

        // verify that the containers where removed from the docker mock
        Set<ContainerState> containerStates = getExistingContainersInAdapter();
        for (ContainerState containerState : containerStates) {
            for (String containerLink : containerStateLinks) {
                if (containerState.documentSelfLink.endsWith(containerLink)) {
                    fail(""Container State not removed with link: "" + containerLink);
                }",1
"@Test
    public void testNetworkProvisioningTaskWithProvidedHostIds() throws Throwable {

        ContainerNetworkDescription networkDesc = TestRequestStateFactory
                .createContainerNetworkDescription(""My-Net"");
        networkDesc.documentSelfLink = UUID.randomUUID().toString();
        networkDesc = doPost(networkDesc, ContainerNetworkDescriptionService.FACTORY_LINK);

        ComputeDescription dockerHostDesc = createDockerHostDescription();
        if (dockerHostDesc.customProperties == null) {
            dockerHostDesc.customProperties = new HashMap<>();
        }",1
"@Test
    public void testRemoveAllocatedOnlyContainers() throws Throwable {
        request.customProperties = new HashMap<>();
        request.customProperties.put(RequestUtils.FIELD_NAME_ALLOCATION_REQUEST,
                Boolean.TRUE.toString());
        request = startRequest(request);
        waitForRequestToComplete(request);

        request = getDocument(RequestBrokerState.class, request.documentSelfLink);
        assertNotNull(request);

        // verify the resources are created as expected:
        assertEquals(request.resourceCount, request.resourceLinks.size());
        Collection<String> containerStateLinks = findResourceLinks(ContainerState.class,
                request.resourceLinks);
        assertEquals(request.resourceCount, containerStateLinks.size());
        // verify containers are not provisioned:
        for (String containerSelfLink : containerStateLinks) {
            ContainerState container = getDocument(ContainerState.class, containerSelfLink);
            assertNotNull(container);
            assertNull(container.id);
            waitForContainerPowerState(PowerState.PROVISIONING, container.documentSelfLink);
        }",1
"@Test
    public void testRemoveApplicationWithScaledContainer() throws Throwable {
        ContainerDescription desc1 = TestRequestStateFactory.createContainerDescription(""name1"",
                true, true);
        ContainerDescription desc2 = TestRequestStateFactory.createContainerDescription(""name2"",
                true, false);
        desc2.affinity = new String[] { desc1.name }",1
"@Test
    public void testRemoveOfStaleContainerOperationFromHostRemoval() throws Throwable {
        // try to remove a container as part of host removal when the container was already removed
        testRemoveOfStaleContainerOperationFromContainerRemovalTask(
                ManagementUriParts.REQUEST_HOST_REMOVAL_OPERATIONS);
    }",1
"@Test
    public void testVolumeProvisioningTaskWithProvidedHostIds() throws Throwable {

        ContainerVolumeDescription volumeDesc = TestRequestStateFactory
                .createContainerVolumeDescription(""My-Vol"");
        volumeDesc.documentSelfLink = UUID.randomUUID().toString();
        volumeDesc = doPost(volumeDesc, ContainerVolumeDescriptionService.FACTORY_LINK);

        ComputeDescription dockerHostDesc = createDockerHostDescription();
        if (dockerHostDesc.customProperties == null) {
            dockerHostDesc.customProperties = new HashMap<>();
        }",1
"@Test
    public void testRequestLifeCycle() throws Throwable {
        host.log(""########  Start of testRequestLifeCycle ######## "");
        // setup Docker Host:
        ResourcePoolState resourcePool = createResourcePool();
        ComputeDescription dockerHostDesc = createDockerHostDescription();
        createDockerHost(dockerHostDesc, resourcePool);

        // setup Container desc:
        ContainerDescription containerDesc = createContainerDescription();

        // setup Group Placement:
        GroupResourcePlacementState groupPlacementState = createGroupResourcePlacement(
                resourcePool);

        // 1. Request a container instance:
        RequestBrokerState request = TestRequestStateFactory.createRequestState();
        request.resourceDescriptionLink = containerDesc.documentSelfLink;
        request.tenantLinks = groupPlacementState.tenantLinks;
        host.log(""########  Start of request ######## "");
        request = startRequest(request);

        // wait for request completed state:
        request = waitForRequestToComplete(request);

        RequestBrokerGraphResponse graph = getDocument(RequestBrokerGraphResponse.class,
                ManagementUriParts.REQUEST_GRAPH, RequestBrokerGraphService.QUERY_PARAM,
                extractId(request.documentSelfLink));
        assertNotNull(graph);
        assertNotNull(graph.tasks);

        TaskServiceDocumentHistory requestTask = graph.tasks.remove(0);
        assertTaskPassingStages(requestTask, RequestBrokerFactoryService.SELF_LINK,
                RequestBrokerState.SubStage.values());

        TaskServiceDocumentHistory reservationTask = graph.tasks.remove(0);
        assertTaskPassingStages(reservationTask, ReservationTaskFactoryService.SELF_LINK,
                ReservationTaskState.SubStage.values());

        TaskServiceDocumentHistory placementReservationTask = graph.tasks.remove(0);
        assertTaskPassingStages(placementReservationTask,
                PlacementHostSelectionTaskService.FACTORY_LINK,
                PlacementHostSelectionTaskState.SubStage.values());

        TaskServiceDocumentHistory allocationTask = graph.tasks.remove(0);
        assertTaskPassingStages(allocationTask, ContainerAllocationTaskFactoryService.SELF_LINK,
                ContainerAllocationTaskState.SubStage.values());

        TaskServiceDocumentHistory placementTask = graph.tasks.remove(0);
        assertTaskPassingStages(placementTask, PlacementHostSelectionTaskService.FACTORY_LINK,
                PlacementHostSelectionTaskState.SubStage.values());
    }",1
"@Test
    public void testCompositeComponentWithClusterAndLocalContainerVolumeRequestLifeCycle()
            throws Throwable {
        host.log(
                ""########  Start of testCompositeComponentWithClusterAndLocalContainerVolumeRequestLifeCycle ######## "");

        // setup Docker Host:
        ResourcePoolState resourcePool = createResourcePool();
        ComputeDescription dockerHostDesc = createDockerHostDescription();

        delete(computeHost.documentSelfLink);
        computeHost = null;

        ComputeState dockerHost1 = createDockerHost(dockerHostDesc, resourcePool, true);
        addForDeletion(dockerHost1);

        ComputeState dockerHost2 = createDockerHost(dockerHostDesc, resourcePool, true);
        addForDeletion(dockerHost2);

        String sharedVolumeName = ""Postgres"";
        String volumeName = String.format(""%s:/etc/pgdata/postgres"", sharedVolumeName);

        ContainerVolumeDescription volumeDesc = TestRequestStateFactory
                .createContainerVolumeDescription(sharedVolumeName);
        volumeDesc.documentSelfLink = UUID.randomUUID().toString();

        ContainerDescription container1Desc = TestRequestStateFactory.createContainerDescription();
        container1Desc.documentSelfLink = UUID.randomUUID().toString();
        container1Desc.name = ""Container1"";
        container1Desc.volumes = new String[] { volumeName }",1
"@Test
    public void testCompositeComponentWithContainerNetworkRequestLifeCycle() throws Throwable {
        host.log(
                ""########  Start of testCompositeComponentWithContainerNetworkRequestLifeCycle ######## "");

        // setup Docker Host:
        ResourcePoolState resourcePool = createResourcePool();
        ComputeDescription dockerHostDesc = createDockerHostDescription();

        delete(computeHost.documentSelfLink);
        computeHost = null;

        // ""set"" the same KV-store for the Docker Hosts created
        dockerHostDesc.customProperties.put(
                ContainerHostService.DOCKER_HOST_CLUSTER_STORE_PROP_NAME, ""my-kv-store"");

        ComputeState dockerHost1 = createDockerHost(dockerHostDesc, resourcePool, true);
        addForDeletion(dockerHost1);

        ComputeState dockerHost2 = createDockerHost(dockerHostDesc, resourcePool, true);
        addForDeletion(dockerHost2);

        // setup Composite description with 2 containers and 1 network

        String networkName = ""MyNet"";

        ContainerNetworkDescription networkDesc = TestRequestStateFactory
                .createContainerNetworkDescription(networkName);
        networkDesc.documentSelfLink = UUID.randomUUID().toString();

        ContainerDescription container1Desc = TestRequestStateFactory.createContainerDescription();
        container1Desc.documentSelfLink = UUID.randomUUID().toString();
        container1Desc.name = ""Container1"";
        container1Desc.networks = new HashMap<>();
        container1Desc.networks.put(networkName, new ServiceNetwork());

        ContainerDescription container2Desc = TestRequestStateFactory.createContainerDescription();
        container2Desc.documentSelfLink = UUID.randomUUID().toString();
        container2Desc.name = ""Container2"";
        container2Desc.affinity = new String[] { ""!Container1:hard"" }",1
"@Test
    public void testCompositeComponentWithContainerVolumeRequestLifeCycle() throws Throwable {
        host.log(
                ""########  Start of testCompositeComponentWithContainerVolumeRequestLifeCycle ######## "");

        // setup Docker Host:
        ResourcePoolState resourcePool = createResourcePool();
        ComputeDescription dockerHostDesc = createDockerHostDescription();

        delete(computeHost.documentSelfLink);
        computeHost = null;

        ComputeState dockerHost1 = createDockerHost(dockerHostDesc, resourcePool, true);
        addForDeletion(dockerHost1);

        ComputeState dockerHost2 = createDockerHost(dockerHostDesc, resourcePool, true);
        addForDeletion(dockerHost2);

        String sharedVolumeName = ""Postgres"";
        String volumeName = String.format(""%s:/etc/pgdata/postgres"", sharedVolumeName);

        ContainerVolumeDescription volumeDesc = TestRequestStateFactory
                .createContainerVolumeDescription(sharedVolumeName);
        volumeDesc.documentSelfLink = UUID.randomUUID().toString();

        ContainerDescription container1Desc = TestRequestStateFactory.createContainerDescription();
        container1Desc.documentSelfLink = UUID.randomUUID().toString();
        container1Desc.name = ""Container1"";
        container1Desc.volumes = new String[] { volumeName }",1
"@Test
    public void testRequestFailShouldNotDeleteDescriptionsInUse() throws Throwable {
        host.log(""########  Start of testRequestFailShouldNotDeleteDescriptionsInUse ######## "");

        // ****** Start of testing a single container instance clean up ******
        host.log(""### Request a single container instance."");
        RequestBrokerState request = TestRequestStateFactory.createRequestState();
        request.resourceDescriptionLink = containerDesc.documentSelfLink;

        host.log(""########  Start of request ######## "");
        request = startRequest(request);

        // wait for request completed state:
        waitForRequestToComplete(request);

        host.log(""### Request a single container instance. Expected to fail because there is no placement created.""
                + ""Should not delete the description as there is already a container associated with it ###."");
        request = TestRequestStateFactory.createRequestState();
        request.resourceDescriptionLink = containerDesc.documentSelfLink;
        request.tenantLinks = Arrays.asList(""unknown"");

        host.log(""########  Start of request ######## "");
        request = startRequest(request);

        // wait for request completed state:
        waitForRequestToFail(request);

        final String containerDescLink = containerDesc.documentSelfLink;

        final long timoutInMillis = 3000; // 3sec
        long startRequestTime = System.currentTimeMillis();

        waitFor(() -> {
            if (System.currentTimeMillis() - startRequestTime > timoutInMillis) {
                return true;
            }",1
"@Test
    public void testRequestLifeCycleFailureShouldCleanReservations() throws Throwable {
        // setup Docker Host:
        ResourcePoolState resourcePool = createResourcePool();
        ComputeDescription dockerHostDesc = createDockerHostDescription();
        createDockerHost(dockerHostDesc, resourcePool);

        // setup Container desc:
        ContainerDescription containerDesc = createContainerDescription();

        // setup Group Placement:
        GroupResourcePlacementState groupPlacementState = createGroupResourcePlacement(
                resourcePool);
        assertEquals(0, groupPlacementState.allocatedInstancesCount);

        // 1. Request a container instance:
        RequestBrokerState request = TestRequestStateFactory.createRequestState();
        request.resourceDescriptionLink = containerDesc.documentSelfLink;
        request.tenantLinks = groupPlacementState.tenantLinks;
        request.customProperties = new HashMap<>();
        request.customProperties.put(MockDockerAdapterService.FAILURE_EXPECTED,
                Boolean.TRUE.toString());

        request = startRequest(request);

        // 2. Wait for reservation removed substage
        waitForRequestToFail(request);

        // 3. Verify that the group placement has been released.
        groupPlacementState = getDocument(GroupResourcePlacementState.class,
                groupPlacementState.documentSelfLink);
        assertEquals(0, groupPlacementState.allocatedInstancesCount);
    }",1
"@Test
    public void testRequestLifeCycleWithContainerNetworkAndServiceAntiAffinityFilterFailureShouldCleanNetworks()
            throws Throwable {
        host.log(
                ""########  Start of testRequestLifeCycleWithContainerNetworkAndServiceAntiAffinityFilterFailureShouldCleanNetworks ######## "");

        // setup Docker Host:
        ResourcePoolState resourcePool = createResourcePool();
        ComputeDescription dockerHostDesc = createDockerHostDescription();

        delete(computeHost.documentSelfLink);
        computeHost = null;

        // DO NOT ""set"" the same KV-store for the Docker Hosts created!
        // In this way the ContainerToNetworkAffinityFilter and ServiceAntiAffinityHostFilter will
        // work as expected by saying that there are no hosts available.
        // Containers should be set on the same host because of the network but they ""can't""
        // because of the container2 anti-affinity rule.

        ComputeState dockerHost1 = createDockerHost(dockerHostDesc, resourcePool, true);
        addForDeletion(dockerHost1);

        ComputeState dockerHost2 = createDockerHost(dockerHostDesc, resourcePool, true);
        addForDeletion(dockerHost2);

        // setup Composite description with 2 containers and 1 network

        String networkName = ""MyNet"";

        ContainerNetworkDescription networkDesc = TestRequestStateFactory
                .createContainerNetworkDescription(networkName);
        networkDesc.documentSelfLink = UUID.randomUUID().toString();

        ContainerDescription container1Desc = TestRequestStateFactory
                .createContainerDescription(""Container1"");
        container1Desc.documentSelfLink = UUID.randomUUID().toString();
        container1Desc.networks = new HashMap<>();
        container1Desc.networks.put(networkName, new ServiceNetwork());

        ContainerDescription container2Desc = TestRequestStateFactory
                .createContainerDescription(""Container2"");
        container2Desc.documentSelfLink = UUID.randomUUID().toString();
        container2Desc.affinity = new String[] { ""!Container1:hard"" }",1
"@Test
    public void testRequestLifeCycleWithContainerNetworkFailureShouldCleanNetworks()
            throws Throwable {
        host.log(
                ""########  Start of testRequestLifeCycleWithContainerNetworkFailureShouldCleanNetworks ######## "");

        // setup Docker Host:
        ResourcePoolState resourcePool = createResourcePool();
        ComputeDescription dockerHostDesc = createDockerHostDescription();

        delete(computeHost.documentSelfLink);
        computeHost = null;

        // ""set"" the same KV-store for the Docker Hosts created
        dockerHostDesc.customProperties.put(
                ContainerHostService.DOCKER_HOST_CLUSTER_STORE_PROP_NAME, ""my-kv-store"");

        ComputeState dockerHost1 = createDockerHost(dockerHostDesc, resourcePool, true);
        addForDeletion(dockerHost1);

        ComputeState dockerHost2 = createDockerHost(dockerHostDesc, resourcePool, true);
        addForDeletion(dockerHost2);

        // setup Composite description with 2 containers and 1 network

        String networkName = ""MyNet"";

        ContainerNetworkDescription networkDesc = TestRequestStateFactory
                .createContainerNetworkDescription(networkName);
        networkDesc.documentSelfLink = UUID.randomUUID().toString();

        ContainerDescription container1Desc = TestRequestStateFactory.createContainerDescription();
        container1Desc.documentSelfLink = UUID.randomUUID().toString();
        container1Desc.name = ""Container1"";
        container1Desc.networks = new HashMap<>();
        container1Desc.networks.put(networkName, new ServiceNetwork());

        ContainerDescription container2Desc = TestRequestStateFactory.createContainerDescription();
        container2Desc.documentSelfLink = UUID.randomUUID().toString();
        container2Desc.name = ""Container2"";
        container2Desc.affinity = new String[] { ""!Container1:hard"" }",1
"@Test
    public void testRequestLifecycleWithContainerNetworkShouldCleanNetworkStatesOnProvisionAndDeletionFailure()
            throws Throwable {
        host.log(
                ""########  Start of ""
                        + ""testRequestLifecycleWithContainerNetworkShouldCleanNetworkStatesOnProvisionAndDeletionFailure ######## "");

        // 1. Request a network with expected failure:
        RequestBrokerState request = TestRequestStateFactory.createRequestState(
                ResourceType.NETWORK_TYPE.getName(),
                containerNetworkDesc.documentSelfLink);
        request.tenantLinks = groupPlacementState.tenantLinks;
        request.customProperties.put(
                ReservationAllocationTaskService.CONTAINER_HOST_ID_CUSTOM_PROPERTY, computeHost.id);

        // This should ensure that both the provisioning and the deletion (cleanup) requests to the
        // mock adapter will fail - during the allocation, the custom properties will be copied into
        // the network state. During the provisioning, the mock adapter will read the
        // EXPECTED_FAILURE from the request's custom properties and during deletion (cleanup) -
        // from the network state.
        request.customProperties.put(MockDockerNetworkAdapterService.FAILURE_EXPECTED,
                Boolean.TRUE.toString());

        host.log(""########  Start of request ######## "");
        request = startRequest(request);

        // 2. Wait for reservation removed substage
        waitForRequestToFail(request);

        // and there must be no container network state left
        ServiceDocumentQueryResult networkStates = getDocument(ServiceDocumentQueryResult.class,
                ContainerNetworkService.FACTORY_LINK);
        assertEquals(0L, networkStates.documentCount.longValue());
    }",1
"@Test
    public void testValidateOnStart() throws Throwable {
        RequestBrokerState request = TestRequestStateFactory.createRequestState();
        request.resourceType = ""-"";
        request.resourceDescriptionLink = ""-"";
        RequestBrokerService r = new RequestBrokerService();
        Method m = r.getClass().getDeclaredMethod(""validateStateOnStart"", RequestBrokerState.class);
        m.setAccessible(true);

        validateLocalizableException(() -> {
            try {
                m.invoke(r, request);
            }",1
"@Test
    public void testReservationAllocationTask() throws Throwable {

        verifyService(FactoryService.create(ReservationAllocationTaskService.class),
                ReservationAllocationTaskState.class,
                (prefix, index) -> {
                    ReservationAllocationTaskState reservationState = new ReservationAllocationTaskState();
                    reservationState.tenantLinks = Collections.singletonList(""testGroup"");
                    reservationState.resourceDescriptionLink = prefix + ""test"";
                    reservationState.customProperties = containerDesc.customProperties;
                    reservationState.name = containerDesc.name;
                    reservationState.resourceCount = 1;
                    Assert.assertNull(reservationState.groupResourcePlacementLink);

                    return reservationState;

                }",1
"@Test
    public void testReservationAllocationThroughReservationTask() throws Throwable {
        ReservationTaskState task = new ReservationTaskState();
        task.tenantLinks = containerDesc.tenantLinks;
        task.resourceDescriptionLink = containerDesc.documentSelfLink;
        task.resourceCount = 1;
        task.serviceTaskCallback = ServiceTaskCallback.createEmpty();
        task.groupResourcePlacementLink = null;
        task.resourcePoolsPerGroupPlacementLinks = null;

        task = doPost(task, ReservationTaskFactoryService.SELF_LINK);
        assertNotNull(task);

        ReservationTaskState result = waitForTaskSuccess(task.documentSelfLink,
                ReservationTaskState.class);

        assertNotNull(result.groupResourcePlacementLink);
        assertNotNull(result.resourcePoolsPerGroupPlacementLinks);
        assertTrue(result.resourcePoolsPerGroupPlacementLinks.size() == 1);
        assertTrue(result.groupResourcePlacementLink.contains(containerDesc.name));
        assertTrue(result.resourcePoolsPerGroupPlacementLinks.keySet()
                .contains(result.groupResourcePlacementLink));

        ReservationAllocationTaskState rsvAllocation = getDocument(
                ReservationAllocationTaskState.class,
                task.documentSelfLink);
        assertNotNull(rsvAllocation);
        assertEquals(result.groupResourcePlacementLink, rsvAllocation.groupResourcePlacementLink);
        assertEquals(result.resourcePoolsPerGroupPlacementLinks,
                rsvAllocation.resourcePoolsPerGroupPlacementLinks);

        GroupResourcePlacementState groupResourcePlacement = getDocument(
                GroupResourcePlacementState.class, result.groupResourcePlacementLink);
        assertNotNull(groupResourcePlacement);
        assertNotNull(groupResourcePlacement.resourcePoolLink);

        assertEquals(groupResourcePlacement.documentSelfLink, rsvAllocation.groupResourcePlacementLink);

    }",1
"@Test
    public void testDeploymentPoliciesOnHost() throws Throwable {
        DeploymentPolicy policy = createDeploymentPolicy();

        containerDesc.deploymentPolicyId = extractId(policy.documentSelfLink);
        doPut(containerDesc);

        GroupResourcePlacementState placementState = TestRequestStateFactory
                .createGroupResourcePlacementState();
        placementState = doPost(placementState, GroupResourcePlacementService.FACTORY_LINK);
        addForDeletion(placementState);

        ReservationTaskState task = new ReservationTaskState();
        task.tenantLinks = placementState.tenantLinks;
        task.resourceDescriptionLink = containerDesc.documentSelfLink;
        task.resourceCount = 1;
        task.serviceTaskCallback = ServiceTaskCallback.createEmpty();

        task = doPost(task, ReservationTaskFactoryService.SELF_LINK);
        assertNotNull(task);

        task = waitForTaskSuccess(task.documentSelfLink, ReservationTaskState.class);

        // update the container host and succeed
        computeHost.customProperties.put(ContainerHostService.CUSTOM_PROPERTY_DEPLOYMENT_POLICY,
                policy.documentSelfLink);
        doPut(computeHost);

        task = new ReservationTaskState();
        task.tenantLinks = placementState.tenantLinks;
        task.resourceDescriptionLink = containerDesc.documentSelfLink;
        task.resourceCount = 1;
        task.serviceTaskCallback = ServiceTaskCallback.createEmpty();
        task = doPost(task, ReservationTaskFactoryService.SELF_LINK);
        assertNotNull(task);

        task = waitForTaskSuccess(task.documentSelfLink, ReservationTaskState.class);

        placementState = getDocument(GroupResourcePlacementState.class,
                placementState.documentSelfLink);
        assertEquals(placementState.documentSelfLink, task.groupResourcePlacementLink);
    }",1
"@Test
    public void testDeploymentPoliciesOnPolicy() throws Throwable {
        DeploymentPolicy policy = createDeploymentPolicy();

        containerDesc.deploymentPolicyId = extractId(policy.documentSelfLink);
        doPut(containerDesc);

        GroupResourcePlacementState placementState = TestRequestStateFactory
                .createGroupResourcePlacementState();
        placementState = doPost(placementState, GroupResourcePlacementService.FACTORY_LINK);
        addForDeletion(placementState);

        ReservationTaskState task = new ReservationTaskState();
        task.tenantLinks = placementState.tenantLinks;
        task.resourceDescriptionLink = containerDesc.documentSelfLink;
        task.resourceCount = 1;
        task.serviceTaskCallback = ServiceTaskCallback.createEmpty();

        task = doPost(task, ReservationTaskFactoryService.SELF_LINK);
        assertNotNull(task);

        task = waitForTaskSuccess(task.documentSelfLink, ReservationTaskState.class);

        // update the placement and succeed
        placementState.deploymentPolicyLink = policy.documentSelfLink;
        doPut(placementState);

        task = new ReservationTaskState();
        task.tenantLinks = placementState.tenantLinks;
        task.resourceDescriptionLink = containerDesc.documentSelfLink;
        task.resourceCount = 1;
        task.serviceTaskCallback = ServiceTaskCallback.createEmpty();

        task = doPost(task, ReservationTaskFactoryService.SELF_LINK);
        assertNotNull(task);

        task = waitForTaskSuccess(task.documentSelfLink, ReservationTaskState.class);

        placementState = getDocument(GroupResourcePlacementState.class,
                placementState.documentSelfLink);
        assertEquals(placementState.documentSelfLink, task.groupResourcePlacementLink);
    }",1
"@Test
    public void testReservationTaskLifeCycleWhenNoAvailableGroupPlacements() throws Throwable {
        GroupResourcePlacementState groupPlacementState = doPost(
                TestRequestStateFactory.createGroupResourcePlacementState(),
                GroupResourcePlacementService.FACTORY_LINK);
        addForDeletion(groupPlacementState);

        ReservationTaskState task = new ReservationTaskState();
        task.tenantLinks = groupPlacementState.tenantLinks;
        task.resourceDescriptionLink = containerDesc.documentSelfLink;
        task.resourceCount = groupPlacementState.maxNumberInstances + 1;
        task.serviceTaskCallback = ServiceTaskCallback.createEmpty();

        task = doPost(task, ReservationTaskFactoryService.SELF_LINK);
        assertNotNull(task);

        waitForTaskError(task.documentSelfLink, ReservationTaskState.class);
    }",1
"@Test
    public void testReservationTaskLifeCycleWhenNoAvailableGroupPlacements() throws Throwable {
        GroupResourcePlacementState groupPlacementState = doPost(
                TestRequestStateFactory.createGroupResourcePlacementState(),
                GroupResourcePlacementService.FACTORY_LINK);
        addForDeletion(groupPlacementState);

        ReservationTaskState task = new ReservationTaskState();
        task.tenantLinks = groupPlacementState.tenantLinks;
        task.resourceDescriptionLink = containerDesc.documentSelfLink;
        task.resourceCount = groupPlacementState.maxNumberInstances + 1;
        task.serviceTaskCallback = ServiceTaskCallback.createEmpty();

        task = doPost(task, ReservationTaskFactoryService.SELF_LINK);
        assertNotNull(task);

        waitForTaskError(task.documentSelfLink, ReservationTaskState.class);
    }",1
"@Test
    public void testDeleteEventsWhenNoneAreAvailable() throws Throwable {
        verifyEventsCount(0);

        ServiceDocumentDeleteTaskState deleteTaskState = doPost(request,
                ServiceDocumentDeleteTaskService.FACTORY_LINK);

        waitForTaskSuccess(deleteTaskState.documentSelfLink,
                ServiceDocumentDeleteTaskService.ServiceDocumentDeleteTaskState.class);

        verifyEventsCount(0);
    }",1
"@Test
    public void testAssignmentAndUnassignment() throws Throwable {
        ComputeState compute = createCompute();

        // addition
        updateTags(compute.documentSelfLink,
                Arrays.asList(""prop"", ""key1:value2"", ""key2:value2""),
                Arrays.asList(),
                Arrays.asList(""prop"", ""key1:value2"", ""key2:value2""));

        // addition + removal
        updateTags(compute.documentSelfLink,
                Arrays.asList(""location:somewhere""),
                Arrays.asList(""key2:value2""),
                Arrays.asList(""prop"", ""key1:value2"", ""location:somewhere""));

        // no change
        updateTags(compute.documentSelfLink,
                Arrays.asList(""location:somewhere""),
                Arrays.asList(""key2:value2""),
                Arrays.asList(""prop"", ""key1:value2"", ""location:somewhere""));

        // empty request
        updateTags(compute.documentSelfLink,
                Arrays.asList(),
                Arrays.asList(),
                Arrays.asList(""prop"", ""key1:value2"", ""location:somewhere""));

        // null addition
        updateTags(compute.documentSelfLink,
                null,
                Arrays.asList(),
                Arrays.asList(""prop"", ""key1:value2"", ""location:somewhere""));

        // null removal
        updateTags(compute.documentSelfLink,
                Arrays.asList(),
                null,
                Arrays.asList(""prop"", ""key1:value2"", ""location:somewhere""));

        // removal
        updateTags(compute.documentSelfLink,
                Arrays.asList(),
                Arrays.asList(""key1:value2""),
                Arrays.asList(""prop"", ""location:somewhere""));

        // removal of all
        updateTags(compute.documentSelfLink,
                null,
                Arrays.asList(""prop"", ""location:somewhere""),
                Arrays.asList());
    }",1
"@Test
    public void testDefaultResourcePrefixNameCreatedOnStartUp() throws Throwable {
        waitForServiceAvailability(
                ResourceNamePrefixService.DEFAULT_RESOURCE_NAME_PREFIX_SELF_LINK);
        ResourceNamePrefixState defaultNamePrefixState = getDocument(ResourceNamePrefixState.class,
                ResourceNamePrefixService.DEFAULT_RESOURCE_NAME_PREFIX_SELF_LINK);
        assertNotNull(defaultNamePrefixState);
        assertNull(defaultNamePrefixState.tenantLinks);
    }",1
"@Test
    public void testCreateEventRegistryTopic() {
        EventTopicState state = createEventTopicState(""dummy-link"", EVENT_NAME, EVENT_TASK,
                TaskStage.FINISHED.name(), DefaultSubStage.COMPLETED.name(), false, new String());

        URI uri = UriUtils.buildUri(host, EventTopicService.FACTORY_LINK);
        EventTopicState result = sender
                .sendPostAndWait(uri, state, EventTopicState.class);

        assertNotNull(result);
        assertNotNull(result.documentSelfLink);
        assertNotNull(result.topicTaskInfo);

        assertEquals(state.topicTaskInfo.task, result.topicTaskInfo.task);
        assertEquals(state.topicTaskInfo.stage, result.topicTaskInfo.stage);
        assertEquals(state.topicTaskInfo.substage, result.topicTaskInfo.substage);

        uri = UriUtils.buildUri(host, result.documentSelfLink);
        result = sender.sendGetAndWait(uri, EventTopicState.class);
        assertNotNull(result);
    }",1
"@Test
    public void testCreateionOfChangeComputeNameTopic() {
        // On start service creates new topic. No need for explicit post for creation.
        TestContext context = new TestContext(1, Duration.ofSeconds(120));
        verifyThatTopicExists(CHANGE_COMPUTE_NAME_SELF_LINK, context);
        context.await();
    }",1
"@Test
    public void testTimeout() throws Throwable {
        ExtensibilitySubscriptionCallback state = createExtensibilityCallback(
                ExtensibilitySubscriptionCallback.Status.BLOCKED);

        state.due = LocalDateTime.now().plus(5, SECONDS);
        state.serviceTaskCallback = new ServiceTaskCallback();
        state.serviceTaskCallback.serviceSelfLink = TestStatelessService.SELF_LINK;
        state.replyPayload = new ServiceTaskCallbackResponse();
        state.replyPayload.taskInfo = TaskState.createAsStarted();
        state.replyPayload.taskSubStage = DefaultSubStage.CREATED;
        state.taskStateJson = TestStatelessService.class.getSimpleName();

        DeferredResult<Void> done = new DeferredResult<>();
        this.host.startService(new TestStatelessService(done));
        this.host.waitForServiceAvailable(TestStatelessService.SELF_LINK);

        URI uri = UriUtils.buildUri(host, ExtensibilitySubscriptionCallbackService.FACTORY_LINK);
        sender.sendPostAndWait(uri, state, ExtensibilitySubscriptionCallback.class);

        this.waitFor(""Task did not time out."", () -> done.isDone());
    }",1
"@Test
    public void testCreateAndGet() {
        ExtensibilitySubscription state = createExtensibilityState();

        URI uri = UriUtils.buildUri(host, ExtensibilitySubscriptionService.FACTORY_LINK);
        ExtensibilitySubscription result = sender
                .sendPostAndWait(uri, state, ExtensibilitySubscription.class);

        assertNotNull(result);
        assertNotNull(result.documentSelfLink);
        assertEquals(state.task, result.task);
        assertEquals(state.stage, result.stage);
        assertEquals(state.substage, result.substage);

        uri = UriUtils.buildUri(host, result.documentSelfLink);
        result = sender.sendGetAndWait(uri, ExtensibilitySubscription.class);
        assertNotNull(result);
    }",1
"@Test
    public void testGetEmpty() throws InterruptedException {
        ServiceDocumentQueryResult result = sender.sendAndWait(
                Operation.createGet(host, ExtensibilitySubscriptionService.FACTORY_LINK),
                ServiceDocumentQueryResult.class);
        assertNotNull(result);
        assertNotNull(result.documentCount);
        assertEquals(0L, (long) result.documentCount);
    }",1
"@Test
    public void testRedirect() {
        String movedLocation = ""moved"";
        URI targetUri = UriUtils.buildUri(SAMPLE_HARBOR_URI, SAMPLE_API_PATH);

        HarborApiProxyService service = new HarborApiProxyService();
        service.setHost(VerificationHost.create());
        ServiceClient client = new MockServiceClient() {
            @Override
            public void sendRequest(Operation op) {
                String authHeader = op.getRequestHeader(Operation.AUTHORIZATION_HEADER);
                assertNull(""Authorization header should be empty"", authHeader);
                op.setStatusCode(Operation.STATUS_CODE_MOVED_PERM);
                op.addResponseHeader(Operation.LOCATION_HEADER, movedLocation);
                op.setUri(targetUri);
                op.complete();
            }",1
"@Test
    public void testDeleteDefaultRegistryOnStartup() throws Throwable {
        RegistryState registryState = new RegistryState();
        registryState.documentSelfLink = RegistryService.DEFAULT_INSTANCE_LINK;
        registryState.endpointType = RegistryState.DOCKER_REGISTRY_ENDPOINT_TYPE;
        registryState.address = RegistryService.DEFAULT_REGISTRY_ADDRESS;
        registryState = doPost(registryState, RegistryFactoryService.SELF_LINK);

        assertNotNull(""Failed to create default registry"", registryState);

        ConfigurationState config = new ConfigurationState();
        config.key = RegistryService.DISABLE_DEFAULT_REGISTRY_PROP_NAME;
        config.value = Boolean.toString(true);

        ConfigurationUtil.initialize(config);

        RegistryService.buildDefaultStateInstance(host);

        waitFor(""Ensure default registry is deleted."", () -> {
            List<String> resourceLinks = findResourceLinks(RegistryState.class,
                    Collections.singletonList(RegistryService.DEFAULT_INSTANCE_LINK));
            return resourceLinks.size() == 0;
        }",1
"@Test
    public void testPatch() throws Throwable {
        String testEntry = ""test-entry"";
        UniquePropertiesRequest patch = new UniquePropertiesRequest();
        patch.toAdd = Collections.singletonList(testEntry);

        doPatch(patch, testState.documentSelfLink);

        testState = getDocumentNoWait(UniquePropertiesState.class, testState.documentSelfLink);
        assertEquals(1, testState.uniqueProperties.size());
        assertTrue(testState.uniqueProperties.contains(testEntry));

        try {
            doPatch(patch, testState.documentSelfLink);
            fail(""Adding entry that already exist should fail the operation."");
        }",1
"@Test
    public void testProvisionApplication() throws Exception {
        setupCoreOsHost(DockerAdapterType.API, false, null);
        checkNumberOfNetworks(serviceClient, NUMBER_OF_NETWORKS_PER_APPLICATION);

        ContainerVolumeState volume = setupExternalVolume();
        compositeDescriptionLink = importTemplateWithExternalVolume(serviceClient,
                TEMPLATE_FILENAME, volume.name);

        logger.info(
                ""---------- 5. Request simple application with a container, a network and an external volume. --------"");
        requestContainerAndDelete(getResourceDescriptionLink(false, null));
    }",1
"@Test
    public void testAddHostWithTrailingForwardSlashes() throws Throwable {
        RegistryState rs = new RegistryState();
        rs.address = TEST_REGISTRY_ADDRESS + ""///"";
        rs.name = getClass().getName();
        rs.endpointType = RegistryState.DOCKER_REGISTRY_ENDPOINT_TYPE;

        RegistryHostSpec hs = new RegistryHostSpec();
        hs.hostState = rs;
        hs.acceptHostAddress = true;

        String[] result = new String[] { null }",1
"@Test
    public void testValidateSelfSignNotAccepted() throws Throwable {
        registryState.address = TEST_REGISTRY_ADDRESS;

        Operation op = Operation.createPut(helperWithValidationUri)
                .setBody(hostState)
                .setCompletion((o, e) -> {
                    if (e != null) {
                        host.failIteration(e);
                        return;
                    }",1
"@Test
    public void testSearchImagesWhenRegistriesAreDisabled() throws Exception {
        logger.info(""Assert the default registry is there"");
        RegistryState dockerHub = getDocument(RegistryService.DEFAULT_INSTANCE_LINK,
                RegistryState.class);
        assertNotNull(dockerHub);
        dockerHub.name = dockerHub.address; // required name when updating a registry

        logger.info(""Assert the preconfigured registry is there"");
        RegistryState configuredReg = getDocument(configuredRegistry.documentSelfLink,
                RegistryState.class);
        assertNotNull(configuredReg);

        List<RegistryState> disabledRegistries = disableRegistries(Arrays.asList(dockerHub,
                configuredReg));
        registriesToEnable.addAll(disabledRegistries);

        URI templateSearchUri = UriUtils.buildUri(new URI(getBaseUrl()),
                TemplateSearchService.SELF_LINK);

        final List<String> keyValues = new ArrayList<>(Arrays.asList(
                TemplateSearchService.IMAGES_ONLY_PARAM, Boolean.toString(true),
                TemplateSearchService.QUERY_PARAM, ""vmware""));

        templateSearchUri = UriUtils.extendUriWithQuery(templateSearchUri,
                keyValues.toArray(new String[0]));

        logger.info(""Search URI built: "" + templateSearchUri);

        HashMap<String, String> headers = new HashMap<>();
        headers.put(OperationUtil.PROJECT_ADMIRAL_HEADER, ProjectService.DEFAULT_PROJECT_LINK);

        HttpResponse httpResponse = SimpleHttpsClient.execute(HttpMethod.GET,
                templateSearchUri.toString(), null, headers, null);
        RegistrySearchResponse searchResponse = Utils.fromJson(httpResponse.responseBody,
                RegistrySearchResponse.class);

        assertEquals(0, searchResponse.results.size());
    }",1
"@Test
    public void testSearchImagesWithRegistryFilter() throws Exception {
        // assert the default registry is there
        RegistryState dockerHub = getDocument(RegistryService.DEFAULT_INSTANCE_LINK,
                RegistryState.class);
        assertNotNull(dockerHub);

        URI templateSearchUri = UriUtils.buildUri(new URI(getBaseUrl()),
                TemplateSearchService.SELF_LINK);

        // exclude results from the default registry
        final List<String> keyValues = new ArrayList<>(Arrays.asList(
                TemplateSearchService.IMAGES_ONLY_PARAM, Boolean.toString(true),
                ContainerImageService.REGISTRY_FILTER_QUERY_PARAM_NAME, REGISTRY_NAME,
                TemplateSearchService.QUERY_PARAM, ""vmware""));

        templateSearchUri = UriUtils.extendUriWithQuery(templateSearchUri,
                keyValues.toArray(new String[0]));

        final HashMap<String, String> headers = new HashMap<>();
        headers.put(OperationUtil.PROJECT_ADMIRAL_HEADER, ProjectService.DEFAULT_PROJECT_LINK);
        HttpResponse httpResponse = SimpleHttpsClient.execute(HttpMethod.GET,
                templateSearchUri.toString(), null, headers, null);
        RegistrySearchResponse searchResponse = Utils.fromJson(httpResponse.responseBody,
                RegistrySearchResponse.class);
        assertEquals(2, searchResponse.results.size());
    }",1
"@Test
    public void testForwardIndexHtmlWithXFrameOptions() {
        UiService service = new UiService();
        service.setSelfLink(""/"");
        VerificationHost vh = new VerificationHost() {
            @Override
            public void sendRequest(Operation op) {
                if (op.getUri().getPath().equals(""/index.html"")) {
                    op.setBody(""OK"");
                    op.complete();
                }",1
"@Test
    public void testRedirect() {
        UiService service = new UiService();
        service.setSelfLink(""/sample"");
        service.setHost(new VerificationHost());

        AtomicBoolean completionCalled = new AtomicBoolean();

        service.handleGet(new Operation().setUri(UriUtils.buildUri(""http://localhost/sample""))
                .setCompletion((o, e) -> {
                    assertEquals(""/sample/"", o.getResponseHeader(Operation.LOCATION_HEADER));
                    completionCalled.set(true);

                }",1
"@Test
    public void testMultipleApplications() throws Throwable {
        List<String> tenantLinks = new ArrayList<String>();
        tenantLinks.add(""project1"");
        tenantLinks.add(""project2"");

        ContainerState containerState1 = createContainer(tenantLinks);
        containerState1 = doPost(containerState1, ContainerFactoryService.SELF_LINK);
        ContainerState containerState2 = createContainer(tenantLinks);

        tenantLinks.add(""project3"");
        containerState2 = doPost(containerState2, ContainerFactoryService.SELF_LINK);

        ContainerNetworkState network = createNetwork(tenantLinks);
        network = doPost(network, ContainerNetworkFactoryService.SELF_LINK);

        ContainerVolumeState volume = createVolume(tenantLinks);
        volume = doPost(volume, ContainerVolumeFactoryService.SELF_LINK);

        List<String> componentLinks = new ArrayList<>();
        componentLinks.add(containerState1.documentSelfLink);

        CompositeComponent application = createCompositeComponent(componentLinks);
        application = doPost(application, CompositeComponentFactoryService.SELF_LINK);

        componentLinks = new ArrayList<>();
        componentLinks.add(containerState2.documentSelfLink);
        componentLinks.add(network.documentSelfLink);
        componentLinks.add(volume.documentSelfLink);

        CompositeComponent application2 = createCompositeComponent(componentLinks);
        application2 = doPost(application2, CompositeComponentFactoryService.SELF_LINK);

        doOperation(new ServiceDocument(),
                UriUtils.buildUri(host, CompositeComponentsTransformationService.SELF_LINK), false,
                Service.Action.POST);

        application = getDocument(CompositeComponent.class, application.documentSelfLink);
        application2 = getDocument(CompositeComponent.class, application2.documentSelfLink);
        Assert.assertTrue(application.tenantLinks.size() == 2);
        Assert.assertTrue(application.tenantLinks.containsAll(containerState1.tenantLinks));

        Assert.assertTrue(application2.tenantLinks.size() == 3);
        Assert.assertTrue(application2.tenantLinks.containsAll(containerState2.tenantLinks));
        Assert.assertTrue(application2.tenantLinks.containsAll(network.tenantLinks));
        Assert.assertTrue(application2.tenantLinks.containsAll(volume.tenantLinks));
    }",1
"@Test
    public void testSingleApplicationOneContainer() throws Throwable {
        List<String> tenantLinks = new ArrayList<String>();
        tenantLinks.add(""project1"");
        ContainerState containerState = createContainer(tenantLinks);
        containerState = doPost(containerState, ContainerFactoryService.SELF_LINK);
        List<String> componentLinks = new ArrayList<>();
        componentLinks.add(containerState.documentSelfLink);
        CompositeComponent application = createCompositeComponent(componentLinks);
        application = doPost(application, CompositeComponentFactoryService.SELF_LINK);

        doOperation(new ServiceDocument(),
                UriUtils.buildUri(host, CompositeComponentsTransformationService.SELF_LINK), false,
                Service.Action.POST);
        application = getDocument(CompositeComponent.class, application.documentSelfLink);
        Assert.assertTrue(application.tenantLinks.size() == 1);
        Assert.assertTrue(application.tenantLinks.containsAll(tenantLinks));
    }",1
"@Test
    public void testThereShouldNotBeDuplicatedTenantLinks() throws Throwable {
        List<String> tenantLinks = new ArrayList<String>();
        tenantLinks.add(""project1"");
        tenantLinks.add(""project2"");

        ContainerState containerState1 = createContainer(tenantLinks);
        containerState1 = doPost(containerState1, ContainerFactoryService.SELF_LINK);
        ContainerState containerState2 = createContainer(tenantLinks);
        containerState2 = doPost(containerState2, ContainerFactoryService.SELF_LINK);

        tenantLinks.add(""project3"");
        ContainerNetworkState network = createNetwork(tenantLinks);
        network = doPost(network, ContainerNetworkFactoryService.SELF_LINK);

        ContainerVolumeState volume = createVolume(tenantLinks);
        volume = doPost(volume, ContainerVolumeFactoryService.SELF_LINK);

        List<String> componentLinks = new ArrayList<>();
        componentLinks.add(containerState1.documentSelfLink);
        componentLinks.add(containerState2.documentSelfLink);
        componentLinks.add(network.documentSelfLink);
        componentLinks.add(volume.documentSelfLink);
        CompositeComponent application = createCompositeComponent(componentLinks);
        application.tenantLinks = new ArrayList<>();
        application.tenantLinks.add(""project1"");
        application.tenantLinks.add(""project2"");
        application = doPost(application, CompositeComponentFactoryService.SELF_LINK);

        doOperation(new ServiceDocument(),
                UriUtils.buildUri(host, CompositeComponentsTransformationService.SELF_LINK), false,
                Service.Action.POST);

        application = getDocument(CompositeComponent.class, application.documentSelfLink);
        Assert.assertTrue(application.tenantLinks.size() == 3);
        Assert.assertTrue(application.tenantLinks.containsAll(tenantLinks));
        Assert.assertTrue(application.tenantLinks.containsAll(containerState1.tenantLinks));
        Assert.assertTrue(application.tenantLinks.containsAll(network.tenantLinks));
        Assert.assertTrue(application.tenantLinks.containsAll(volume.tenantLinks));
    }",1
"@Test
    public void testDefaultPlacementDefaultPoolOneHost() throws Throwable {
        List<String> links = getDocumentLinksOfType(ResourcePoolState.class);
        Assert.assertTrue(links.size() == 1);
        ComputeState compute = createComputeState(""host1"",
                GroupResourcePlacementService.DEFAULT_RESOURCE_POOL_LINK);
        compute = doPost(compute, ComputeService.FACTORY_LINK);
        Assert.assertTrue(compute.tagLinks == null);
        doOperation(new ServiceDocument(),
                UriUtils.buildUri(host, ComputePlacementPoolRelationTransformationService.SELF_LINK), false,
                Service.Action.POST);
        compute = getDocument(ComputeState.class, compute.documentSelfLink);
        // check that a tag is added to the compute
        Assert.assertTrue(compute.tagLinks != null);
        Assert.assertTrue(compute.tagLinks.size() == 1);
        Assert.assertTrue(compute.tenantLinks.size() == 1);
        Assert.assertTrue(compute.tenantLinks.get(0).equals(ProjectService.DEFAULT_PROJECT_LINK));
        // Check that the pool has the default project
        ElasticPlacementZoneConfigurationState pool = getDocument(
                ElasticPlacementZoneConfigurationState.class,
                ElasticPlacementZoneConfigurationService.SELF_LINK
                        + links.get(0));
        Assert.assertTrue(pool.epzState.tagLinksToMatch.size() == 1);
        Assert.assertTrue(pool.epzState.tagLinksToMatch.containsAll(compute.tagLinks));
        GroupResourcePlacementState placement = getDocument(GroupResourcePlacementState.class,
                GroupResourcePlacementService.DEFAULT_RESOURCE_PLACEMENT_LINK);
        Assert.assertTrue(placement.tenantLinks.size() == 1);
        Assert.assertTrue(placement.tenantLinks.get(0).equals(ProjectService.DEFAULT_PROJECT_LINK));

        host.testStart(1);
        doPost(compute, ResourcePoolTransformationService.SELF_LINK);
        DeferredResult<List<ComputeState>> hostsWithinPlacementZone = ClusterUtils
                .getHostsWithinPlacementZone(pool.epzState.resourcePoolLink,
                        ProjectService.DEFAULT_PROJECT_LINK, host);
        hostsWithinPlacementZone.whenComplete((computeStates, ex) -> {
            if (ex != null) {
                host.failIteration(ex);
                return;
            }",1
"@Test
    public void testHostWithoutNetworks() throws Throwable {
        List<String> tenantLinks = new ArrayList<String>();
        tenantLinks.add(""project1"");
        ComputeState cs = createComputeState(""TestID1"", tenantLinks);
        cs = doPost(cs, ComputeService.FACTORY_LINK);
        List<String> links = getDocumentLinksOfType(ContainerNetworkState.class);
        Assert.assertTrue(links.isEmpty());
        doOperation(new ServiceDocument(),
                UriUtils.buildUri(host, ContainerNetworksTransformationService.SELF_LINK), false,
                Service.Action.POST);
    }",1
"@Test
    public void testNetworkHasTenantLinks() throws Throwable {
        List<String> tenantLinks = new ArrayList<String>();
        String tenant = ""project1"";
        tenantLinks.add(tenant);
        ComputeState cs = createComputeState(""TestID1"", tenantLinks);
        cs = doPost(cs, ComputeService.FACTORY_LINK);

        ContainerNetworkState containerNetwork1 = createNetwork(cs.documentSelfLink);
        containerNetwork1.tenantLinks = new ArrayList<>();
        containerNetwork1.tenantLinks.add(tenant);
        containerNetwork1 = doPost(containerNetwork1, ContainerNetworkFactoryService.SELF_LINK);
        ContainerNetworkState containerNetwork2 = createNetwork(cs.documentSelfLink);
        containerNetwork2 = doPost(containerNetwork2, ContainerNetworkFactoryService.SELF_LINK);
        doOperation(new ServiceDocument(),
                UriUtils.buildUri(host, ContainerNetworksTransformationService.SELF_LINK), false,
                Service.Action.POST);

        containerNetwork1 = getDocument(ContainerNetworkState.class,
                containerNetwork1.documentSelfLink);
        containerNetwork2 = getDocument(ContainerNetworkState.class,
                containerNetwork2.documentSelfLink);

        Assert.assertTrue(containerNetwork1.tenantLinks.size() == 1);
        Assert.assertTrue(containerNetwork2.tenantLinks.size() == 1);
        Assert.assertTrue(containerNetwork1.tenantLinks.containsAll(tenantLinks));
        Assert.assertTrue(containerNetwork2.tenantLinks.containsAll(tenantLinks));
    }",1
"@Test
    public void testNoNetworksNoHosts() throws Throwable {
        List<String> links = getDocumentLinksOfType(ComputeState.class);
        Assert.assertTrue(links.isEmpty());
        doOperation(new ServiceDocument(),
                UriUtils.buildUri(host, ContainerNetworksTransformationService.SELF_LINK), false,
                Service.Action.POST);
    }",1
"@Test
    public void testMultipleHosts() throws Throwable {
        List<String> tenantLinksHost1 = new ArrayList<String>();
        tenantLinksHost1.add(""project1"");
        tenantLinksHost1.add(""project2"");
        List<String> tenantLinksHost2 = new ArrayList<String>();
        tenantLinksHost2.add(""host2-project"");

        ComputeState cs = createComputeState(""TestID1"", tenantLinksHost1);
        cs = doPost(cs, ComputeService.FACTORY_LINK);
        ComputeState cs2 = createComputeState(""TestID2"", tenantLinksHost2);
        cs2 = doPost(cs2, ComputeService.FACTORY_LINK);

        ContainerState firstContainerHost1 = createContainer(cs.documentSelfLink);
        firstContainerHost1 = doPost(firstContainerHost1, ContainerFactoryService.SELF_LINK);
        ContainerState secondContainerHost1 = createContainer(cs.documentSelfLink);
        secondContainerHost1 = doPost(secondContainerHost1, ContainerFactoryService.SELF_LINK);

        ContainerState firstContainerHost2 = createContainer(cs2.documentSelfLink);
        firstContainerHost2 = doPost(firstContainerHost2, ContainerFactoryService.SELF_LINK);
        ContainerState secondContainerHost2 = createContainer(cs2.documentSelfLink);
        // set tenant links to the container to check that the old tenant links are not overwritten
        secondContainerHost2.tenantLinks = new ArrayList<>();
        String containerTenantLink = ""test-business-group"";
        secondContainerHost2.tenantLinks.add(containerTenantLink);
        secondContainerHost2 = doPost(secondContainerHost2, ContainerFactoryService.SELF_LINK);
        doOperation(new ServiceDocument(),
                UriUtils.buildUri(host, ContainersTransformationService.SELF_LINK), false,
                Service.Action.POST);

        firstContainerHost1 = getDocument(ContainerState.class,
                firstContainerHost1.documentSelfLink);
        secondContainerHost1 = getDocument(ContainerState.class,
                secondContainerHost1.documentSelfLink);
        firstContainerHost2 = getDocument(ContainerState.class,
                firstContainerHost2.documentSelfLink);
        secondContainerHost2 = getDocument(ContainerState.class,
                secondContainerHost2.documentSelfLink);

        Assert.assertTrue(firstContainerHost1.tenantLinks.containsAll(tenantLinksHost1));
        Assert.assertTrue(secondContainerHost1.tenantLinks.containsAll(tenantLinksHost1));
        Assert.assertTrue(secondContainerHost1.tenantLinks.equals(firstContainerHost1.tenantLinks));

        Assert.assertTrue(firstContainerHost2.tenantLinks.containsAll(tenantLinksHost2));
        Assert.assertTrue(secondContainerHost2.tenantLinks.containsAll(tenantLinksHost2));
        Assert.assertTrue(secondContainerHost2.tenantLinks
                .size() == firstContainerHost2.tenantLinks.size() + 1);
        Assert.assertTrue(secondContainerHost2.tenantLinks.contains(containerTenantLink));
    }",1
"@Test
    public void testNoContainersNoHosts() throws Throwable {
        List<String> links = getDocumentLinksOfType(ComputeState.class);
        Assert.assertTrue(links.isEmpty());
        doOperation(new ServiceDocument(),
                UriUtils.buildUri(host, ContainersTransformationService.SELF_LINK), false,
                Service.Action.POST);
    }",1
"@Test
    public void testVolumeHasTenantLinks() throws Throwable {
        List<String> tenantLinks = new ArrayList<String>();
        tenantLinks.add(""project1"");
        String commonTenantLink = ""tenant"";
        tenantLinks.add(commonTenantLink);
        ComputeState cs = createComputeState(""TestID1"", tenantLinks);
        cs = doPost(cs, ComputeService.FACTORY_LINK);

        ContainerVolumeState containerVolume1 = createVolume(cs.documentSelfLink);
        containerVolume1.tenantLinks = new ArrayList<>();
        containerVolume1.tenantLinks.add(commonTenantLink);
        containerVolume1 = doPost(containerVolume1, ContainerVolumeFactoryService.SELF_LINK);
        ContainerVolumeState containerVolume2 = createVolume(cs.documentSelfLink);
        containerVolume2 = doPost(containerVolume2, ContainerVolumeFactoryService.SELF_LINK);
        doOperation(new ServiceDocument(),
                UriUtils.buildUri(host, ContainerVolumesTransformationService.SELF_LINK), false,
                Service.Action.POST);

        containerVolume1 = getDocument(ContainerVolumeState.class,
                containerVolume1.documentSelfLink);
        containerVolume2 = getDocument(ContainerVolumeState.class,
                containerVolume2.documentSelfLink);

        Assert.assertTrue(containerVolume1.tenantLinks.size() == 2);
        Assert.assertTrue(containerVolume2.tenantLinks.size() == 2);
        Assert.assertTrue(containerVolume1.tenantLinks.containsAll(tenantLinks));
        Assert.assertTrue(containerVolume2.tenantLinks.containsAll(tenantLinks));
    }",1
"@Test
    public void testNoPlacementsForPool() throws Throwable {
        ResourcePoolState pool1 = createResourcePool();
        doPost(pool1, ResourcePoolTransformationService.SELF_LINK);
        pool1 = getDocument(ResourcePoolState.class, pool1.documentSelfLink);
        Assert.assertTrue(pool1.tenantLinks == null);
    }",1
"@Test
    public void testPlacementsBolongToDifferentPools() throws Throwable {
        ResourcePoolState pool1 = createResourcePool();

        GroupResourcePlacementState placement1 = new GroupResourcePlacementState();
        placement1.name = ""placement"";
        placement1.resourcePoolLink = pool1.documentSelfLink;
        placement1.tenantLinks = new ArrayList<>();
        placement1.tenantLinks.add(""tenant1"");
        placement1 = doPost(placement1, GroupResourcePlacementService.FACTORY_LINK);

        ResourcePoolState pool2 = createResourcePool();
        GroupResourcePlacementState placement2 = new GroupResourcePlacementState();
        placement2.name = ""placement2"";
        placement2.resourcePoolLink = pool2.documentSelfLink;
        placement2.tenantLinks = new ArrayList<>();
        placement2.tenantLinks.add(""tenant2"");
        placement2 = doPost(placement2, GroupResourcePlacementService.FACTORY_LINK);

        doPost(placement2, ResourcePoolTransformationService.SELF_LINK);
        pool1 = getDocument(ResourcePoolState.class, pool1.documentSelfLink);
        pool2 = getDocument(ResourcePoolState.class, pool2.documentSelfLink);

        // verify the tenant links of the pools
        Assert.assertTrue(pool1.tenantLinks.size() == 1);
        Assert.assertTrue(pool2.tenantLinks.size() == 1);
        Assert.assertTrue(pool1.tenantLinks.contains(""tenant1""));
        Assert.assertTrue(pool2.tenantLinks.contains(""tenant2""));

        // Verify that the pools are not changed
        Assert.assertTrue(getDocument(GroupResourcePlacementState.class,
                placement1.documentSelfLink).resourcePoolLink.equals(pool1.documentSelfLink));
        Assert.assertTrue(getDocument(GroupResourcePlacementState.class,
                placement2.documentSelfLink).resourcePoolLink.equals(pool2.documentSelfLink));
    }",1
"@Test
    public void testCloseServer() throws Throwable {
        CountDownLatch latch = new CountDownLatch(1);
        SslEchoServer server = new SslEchoServer();
        try {
            server.start();
            String message = ""Hello world\n"";
            ByteBuffer readBuffer = ByteBuffer.allocate(message.length());
            Filter clientSocket = openClientSocket(""localhost"", readBuffer, latch, null);

            clientSocket.write(stringToBuffer(message), new CompletionHandler<ByteBuffer>() {
                @Override
                public void failed(Throwable t) {
                    t.printStackTrace();
                }",1
"@Test
    public void testCustomHostameVerificationFail() throws Throwable {
        CountDownLatch latch = new CountDownLatch(1);
        SslEchoServer server = new SslEchoServer();
        try {
            server.start();
            HostnameVerifier verifier = new HostnameVerifier() {
                @Override
                public boolean verify(String s, SSLSession sslSession) {
                    return false;
                }",1
"@Test
    public void testEcho100k() throws Throwable {
        CountDownLatch latch = new CountDownLatch(1);
        SslEchoServer server = new SslEchoServer();
        try {
            server.start();
            StringBuilder sb = new StringBuilder();
            for (int i = 0; i < 1000; i++) {
                sb.append(""1234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890"");
            }",1
"@Test
    public void testRehandshakeClient() throws Throwable {
        CountDownLatch latch = new CountDownLatch(1);
        final SslEchoServer server = new SslEchoServer();
        try {
            server.start();
            StringBuilder sb = new StringBuilder();
            for (int i = 0; i < 1000; i++) {
                sb.append(""1234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890"");
            }",1
"@Test
    public void testRehandshakeServer() throws Throwable {
        CountDownLatch latch = new CountDownLatch(1);
        final SslEchoServer server = new SslEchoServer();
        try {
            server.start();
            StringBuilder sb = new StringBuilder();
            for (int i = 0; i < 1000; i++) {
                sb.append(""1234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890"");
            }",1
"@Test
    public void testGetSendStream() throws IOException {

        TestRemoteEndpoint tre = new TestRemoteEndpoint();
        TyrusSession testSession = createTestSession(tre, endpointWrapper);
        TyrusRemoteEndpoint.Basic rew = new TyrusRemoteEndpoint.Basic(testSession, tre, endpointWrapper);
        OutputStream stream = rew.getSendStream();

        for (byte b : sentBytes) {
            stream.write(b);
        }",1
"@Test
  public void singleSslSocketFactory() {
    HttpRequest request1 = get(""https://localhost"").trustAllCerts();
    HttpRequest request2 = get(""https://localhost"").trustAllCerts();
    assertNotNull(((HttpsURLConnection) request1.getConnection())
        .getSSLSocketFactory());
    assertNotNull(((HttpsURLConnection) request2.getConnection())
        .getSSLSocketFactory());
    assertEquals(
        ((HttpsURLConnection) request1.getConnection()).getSSLSocketFactory(),
        ((HttpsURLConnection) request2.getConnection()).getSSLSocketFactory());
  }",1
"@Test
    public void testNormal01() throws Exception {
        // start mock fluentd
        int port = MockFluentd.randomPort();
        final List<Event> elist = new ArrayList<Event>();
        MockFluentd fluentd = new MockFluentd(port, new MockFluentd.MockProcess() {
            public void process(MessagePack msgpack, Socket socket) throws IOException {
                BufferedInputStream in = new BufferedInputStream(socket.getInputStream());
                try {
                    Unpacker unpacker = msgpack.createUnpacker(in);
                    while (true) {
                        Event e = unpacker.read(Event.class);
                        elist.add(e);
                    }",1
"@Test
    public void testReconnection() throws Exception {
        // start mock fluentd
        int port = MockFluentd.randomPort();
        String host = ""localhost"";
        final List<Event> elist1 = new ArrayList<Event>();
        final AtomicReference<Exception> lastError = new AtomicReference<Exception>();

        FixedThreadManager threadManager = new FixedThreadManager(2);

        // run a fluentd
        MockFluentd fluentd1 = new MockFluentd(port, new MockFluentd.MockProcess() {
            public void process(MessagePack msgpack, Socket socket) throws IOException {
                BufferedInputStream in = new BufferedInputStream(socket.getInputStream());
                try {
                    Unpacker unpacker = msgpack.createUnpacker(in);
                    while (true) {
                        Event e = unpacker.read(Event.class);
                        elist1.add(e);

                        if (elist1.size() >= 1)
                            break;
                    }",1
"@Test
    public void process() throws IOException {
        List<String> sources = Arrays.asList(
                new File(packagePath, ""IntegerExtensions.java"").getPath(),
                new File(packagePath, ""ExampleEntity2.java"").getPath());
        process(QuerydslAnnotationProcessor.class, sources, ""integerExtensions"");
                String qtypeContent = new String(Files.readAllBytes(Paths.get(""target"", ""integerExtensions"", ""com"", ""querydsl"", ""QExampleEntity2.java"")), StandardCharsets.UTF_8);
        //The superclass' id property is inherited, but can't be assigned to the custom QInteger
        assertTrue(qtypeContent.contains(""public final ext.java.lang.QInteger id = new ext.java.lang.QInteger(_super.id);""));
    }",1
"@Test
    public void process_abstractClasses2() throws IOException {
        String path = new File(""src/test/java/com/querydsl/apt/domain/AbstractClasses2Test.java"").getPath();
        process(JPAAnnotationProcessor.class, Collections.singletonList(path),""abstractClasses2"");
    }",1
"@Test
    public void process_generic13Test() throws IOException {
        String path = new File(""src/test/java/com/querydsl/apt/domain/Generic13Test.java"").getPath();
        process(QuerydslAnnotationProcessor.class, Collections.singletonList(path),""Generic13Test"");
    }",1
"@Test
    public void process_inheritance2Test() throws IOException {
        String path = new File(""src/test/java/com/querydsl/apt/inheritance/Inheritance2Test.java"").getPath();
        process(QuerydslAnnotationProcessor.class, Collections.singletonList(path),""InheritanceTest2"");
    }",1
"@Test
    public void process_monitoredCompany() throws IOException {
        String path = new File(PACKAGE_PATH, ""MonitoredCompany.java"").getPath();
        process(QuerydslAnnotationProcessor.class, Collections.singletonList(path),""MonitoredCompany"");
    }",1
"@Test
    public void process_queryEmbedded4() throws IOException {
        String path = new File(""src/test/java/com/querydsl/apt/domain/QueryEmbedded4Test.java"").getPath();
        process(QuerydslAnnotationProcessor.class, Collections.singletonList(path),""QueryEmbedded4Test"");
    }",1
"@Test
    public void rooAnnotationProcessor() throws IOException {
        process(RooAnnotationProcessor.class, CLASSES, ""roo"");

        assertTrue(new File(""target/roo/com/querydsl/apt/domain/QRooEntities_MyEntity.java"").exists());
    }",1
"@Test
    public void verify_package() throws Exception {
        String version = System.getProperty(""version"");
        verify(new File(""target/querydsl-jdo-"" + version + ""-apt-one-jar.jar""));
    }",1
"@Test
    public void executeWithNamingStrategy() throws Exception {
        mojo.setTargetFolder(""target/export8"");
        mojo.setNamingStrategyClass(OriginalNamingStrategy.class.getName());
        mojo.execute();

        assertTrue(new File(""target/export8"").exists());
    }",1
"@Test
    public void executeWithNumericMappings() throws Exception {
        mojo.setTargetFolder(""target/export5"");
        NumericMapping mapping = new NumericMapping();
        mapping.setTotal(1);
        mapping.setDecimal(1);
        mapping.setJavaType(Number.class.getName());
        mojo.setNumericMappings(new NumericMapping[]{mapping}",1
"@Test
    public void executeWithRenames() throws Exception {
        RenameMapping mapping = new RenameMapping();
        mapping.setFromSchema(""ABC"");
        mapping.setToSchema(""DEF"");

        mojo.setTargetFolder(""target/export13"");
        mojo.setRenameMappings(new RenameMapping[]{mapping}",1
"@Test
    public void verify_package() throws Exception {
        String version = System.getProperty(""version"");
        verify(new File(""target/querydsl-jdo-"" + version + ""-apt-one-jar.jar""));
    }",1
"@Test
    public void execute_with_beans() {
        AntMetaDataExporter exporter = new AntMetaDataExporter();
        exporter.setJdbcDriver(""org.h2.Driver"");
        exporter.setJdbcUser(""sa"");
        exporter.setJdbcUrl(url);
        exporter.setPackageName(""test"");
        exporter.setTargetFolder(""target/AntMetaDataExporterTest2"");
        exporter.setExportBeans(true);
        exporter.setNamePrefix(""Q"");
        exporter.setNameSuffix("""");
        exporter.setBeanPrefix("""");
        exporter.setBeanSuffix(""Bean"");
        exporter.execute();

        assertTrue(new File(""target/AntMetaDataExporterTest2"").exists());
        assertTrue(new File(""target/AntMetaDataExporterTest2/test/QTest.java"").exists());
        assertTrue(new File(""target/AntMetaDataExporterTest2/test/TestBean.java"").exists());
    }",1
"@Test
    public void getPropertyName() {
        assertEquals(""while_col"", namingStrategy.getPropertyName(""while"", entityModel));
        assertEquals(""name"", namingStrategy.getPropertyName(""name"", entityModel));
        assertEquals(""user_id"", namingStrategy.getPropertyName(""user_id"", entityModel));
        assertEquals(""accountEvent_id"", namingStrategy.getPropertyName(""accountEvent_id"", entityModel));

        assertEquals(""_123abc"", namingStrategy.getPropertyName(""123abc"", entityModel));
        assertEquals(""_123_abc"", namingStrategy.getPropertyName(""123 abc"", entityModel));

        assertEquals(""_123_abc_def"", namingStrategy.getPropertyName(""#123#abc#def"", entityModel));

        assertEquals(""new_line"", namingStrategy.getPropertyName(""new line"", entityModel));
    }",1
"@Test
    public void tableToTable() {
        mapping.setFromTable(""TABLE1"");
        mapping.setToTable(""TABLE2"");
        mapping.apply(configuration);

        assertEquals(
                new SchemaAndTable(""DEF"", ""TABLE2""),
                configuration.getOverride(new SchemaAndTable(""DEF"", ""TABLE1"")));
        assertEquals(
                new SchemaAndTable(""DEF"", ""TABLE3""),
                configuration.getOverride(new SchemaAndTable(""DEF"", ""TABLE3"")));
    }",1
"@Test
    public void custom_type() {
        Configuration configuration = new Configuration(new H2Templates());
//        configuration.setJavaType(Types.BLOB, InputStream.class);
        configuration.register(new InputStreamType());
        assertEquals(InputStream.class, configuration.getJavaType(Types.BLOB, null, 0,0,"""", """"));
    }",1
"@Test
    public void get_schema() {
        Configuration configuration = new Configuration(new H2Templates());
        configuration.registerSchemaOverride(""public"", ""pub"");
        configuration.registerTableOverride(""employee"", ""emp"");
        configuration.registerTableOverride(""public"", ""employee"", ""employees"");

        assertEquals(""pub"", configuration.getOverride(new SchemaAndTable(""public"", """")).getSchema());
        assertEquals(""emp"", configuration.getOverride(new SchemaAndTable("""", ""employee"")).getTable());
        assertEquals(""employees"", configuration.getOverride(new SchemaAndTable(""public"", ""employee"")).getTable());

        configuration.setDynamicNameMapping(new PreConfiguredNameMapping());
        SchemaAndTable notOverriddenSchemaAndTable = new SchemaAndTable(""notoverridden"", ""notoverridden"");
        assertEquals(notOverriddenSchemaAndTable, configuration.getOverride(notOverriddenSchemaAndTable));

        configuration.setDynamicNameMapping(new ChangeLetterCaseNameMapping(LetterCase.UPPER, Locale.ENGLISH));
        String notDirectOverriden = ""notDirectOverriden"";
        assertEquals(notDirectOverriden.toUpperCase(Locale.ENGLISH),
                configuration.getOverride(new SchemaAndTable(""public"", notDirectOverriden)).getTable());

    }",1
"@Test
    public void batch() throws SQLException {
        insert(survey).values(2, ""A"",""B"").execute();
        insert(survey).values(3, ""B"",""C"").execute();

        SQLDeleteClause delete = delete(survey);
        delete.where(survey.name.eq(""A"")).addBatch();
        assertEquals(1, delete.getBatchCount());
        delete.where(survey.name.eq(""B"")).addBatch();
        assertEquals(2, delete.getBatchCount());
        assertEquals(2, delete.execute());
    }",1
"@Test
    public void nextVal() {
        Operation<String> nextval = ExpressionUtils.operation(String.class, SQLOps.NEXTVAL, ConstantImpl.create(""myseq""));
        assertEquals(""next value for myseq"", new SQLSerializer(new Configuration(new DerbyTemplates())).handle(nextval).toString());
    }",1
"@Test
    public void precedence() {
        // unary + and -
        int p1 = getPrecedence(Ops.NEGATE);
        // *, /, || (concatenation)
        int p2 = getPrecedence(Ops.MULT, Ops.DIV);
        // binary + and -
        int p3 = getPrecedence(Ops.ADD, Ops.SUB);
        // comparisons, quantified comparisons, EXISTS, IN, IS NULL, LIKE, BETWEEN, IS
        int p4 = getPrecedence(Ops.EQ, Ops.NE, Ops.LT, Ops.GT, Ops.LOE, Ops.GOE, Ops.EXISTS,
                Ops.IN, Ops.IS_NULL, Ops.LIKE, Ops.BETWEEN, Ops.IS_NOT_NULL);
        // NOT
        int p5 = getPrecedence(Ops.NOT);
        // AND
        int p6 = getPrecedence(Ops.AND);
        // OR
        int p7 = getPrecedence(Ops.OR);

        assertTrue(p1 < p2);
        assertTrue(p2 < p3);
        assertTrue(p3 < p4);
        assertTrue(p4 < p5);
        assertTrue(p5 < p6);
        assertTrue(p6 < p7);
    }",1
"@Test
    public void extract2() {
        Map<Path<?>, Object> values = AnnotationMapper.DEFAULT.createMap(emp, new EmployeeX());
        assertTrue(values.isEmpty());
    }",1
"@Test
    public void extract_failure() {
        Map<Path<?>, Object> values = AnnotationMapper.DEFAULT.createMap(emp, employee);
        assertTrue(values.isEmpty());
    }",1
"@Test
    public void getSQL() {
        QEmployee emp1 = new QEmployee(""emp1"");
        SQLDeleteClause delete = new SQLDeleteClause(null, SQLTemplates.DEFAULT, emp1);
        delete.where(emp1.id.eq(1));

        SQLBindings sql = delete.getSQL().get(0);
        assertEquals(""delete from EMPLOYEE\nwhere EMPLOYEE.ID = ?"", sql.getSQL());
        assertEquals(Collections.singletonList(1), sql.getNullFriendlyBindings());
    }",1
"@Test
    public void getSQLWithPreservedColumnOrder() {
        com.querydsl.sql.domain.QEmployee emp1 = new com.querydsl.sql.domain.QEmployee(""emp1"");
        SQLInsertClause insert = new SQLInsertClause(null, SQLTemplates.DEFAULT, emp1);
        insert.populate(emp1);

        SQLBindings sql = insert.getSQL().get(0);
        assertEquals(""The order of columns in generated sql should be predictable"",
                ""insert into EMPLOYEE (ID, FIRSTNAME, LASTNAME, SALARY, DATEFIELD, TIMEFIELD, SUPERIOR_ID)\n"" +
                ""values (EMPLOYEE.ID, EMPLOYEE.FIRSTNAME, EMPLOYEE.LASTNAME, EMPLOYEE.SALARY, EMPLOYEE.DATEFIELD, EMPLOYEE.TIMEFIELD, EMPLOYEE.SUPERIOR_ID)"", sql.getSQL());
    }",1
"@Test
    public void intertable2() {
        QEmployee emp1 = new QEmployee(""emp1"");
        QEmployee emp2 = new QEmployee(""emp2"");
        SQLUpdateClause update = new SQLUpdateClause(null, SQLTemplates.DEFAULT, emp1);
        update.set(emp1.id, select(emp2.id).from(emp2)
              .where(emp2.superiorId.isNotNull()));

        SQLBindings sql = update.getSQL().get(0);
        assertEquals(""update EMPLOYEE\n"" +
                ""set ID = (select emp2.ID\n"" +
                ""from EMPLOYEE emp2\n"" +
                ""where emp2.SUPERIOR_ID is not null)"", sql.getSQL());
    }",1
"@Test
    public void on() {
        QEmployee employee = new QEmployee(""employee"");
        QEmployee employee2 = new QEmployee(""employee2"");

        ForeignKey<Employee> foreignKey = new ForeignKey<Employee>(employee, employee.superiorId, ""ID"");
        assertEquals(""employee.superiorId = employee2.ID"", foreignKey.on(employee2).toString());

        foreignKey = new ForeignKey<Employee>(employee,
                Arrays.asList(employee.superiorId, employee.firstname),
                Arrays.asList(""ID"", ""FN""));
        assertEquals(""employee.superiorId = employee2.ID && employee.firstname = employee2.FN"", foreignKey.on(employee2).toString());
    }",1
"@Test
    public void builder() {
        SQLTemplates templates = H2Templates.builder().quote()
            .newLineToSingleSpace()
            .build();

        assertNotNull(templates);
    }",1
"@Test
    public void precedence() {
        // unary
        // *, /, %
        // +, -
        // ||
        // comparison
        // NOT
        // AND
        // OR

        int p1 = getPrecedence(Ops.NEGATE);
        int p2 = getPrecedence(Ops.MULT, Ops.DIV, Ops.MOD);
        int p3 = getPrecedence(Ops.ADD, Ops.SUB);
        int p4 = getPrecedence(Ops.CONCAT);
        int p5 = getPrecedence(Ops.EQ, Ops.NE, Ops.LT, Ops.GT); // ...
        int p6 = getPrecedence(Ops.NOT);
        int p7 = getPrecedence(Ops.AND);
        int p8 = getPrecedence(Ops.OR);

        assertTrue(p1 < p2);
        assertTrue(p2 < p3);
        assertTrue(p3 < p4);
        assertTrue(p4 < p5);
        assertTrue(p5 < p6);
        assertTrue(p6 < p7);
        assertTrue(p7 < p8);
    }",1
"@Test
    public void precedence() {
        // Evaluation from left to right. Parentheses group operations.
        // Multiplication and division take precedence over addition and subtraction.
        // AND takes precedence over OR.
        // NOT applies to the immediate term.
        // LIKE applies to the result of any string concatenation to the right.
        // Comparison ops are not combined without logical ops so there is no precedence issue.

        //int p1 = getPrecedence(Ops.NEGATE);
        int p2 = getPrecedence(Ops.MULT, Ops.DIV, Ops.CONCAT);
        int p3 = getPrecedence(Ops.ADD, Ops.SUB);
        int p4 = getPrecedence(Ops.NOT);
        int p5 = getPrecedence(Ops.EQ, Ops.NE, Ops.LT, Ops.GT, Ops.LOE, Ops.GOE);
        int p6 = getPrecedence(Ops.IS_NULL, Ops.IS_NOT_NULL, Ops.LIKE, Ops.LIKE_ESCAPE, Ops.BETWEEN, Ops.IN, Ops.NOT_IN, Ops.EXISTS);
        int p7 = getPrecedence(Ops.AND);
        int p8 = getPrecedence(Ops.OR);

        //assertTrue(p1 < p2);
        assertTrue(p2 < p3);
        assertTrue(p3 < p4);
        assertTrue(p4 < p5);
        assertTrue(p5 < p6);
        assertTrue(p6 < p7);
        assertTrue(p7 < p8);
    }",1
"@Test
    public void complex1() {
        // related to #584795
        QSurvey survey = new QSurvey(""survey"");
        QEmployee emp1 = new QEmployee(""emp1"");
        QEmployee emp2 = new QEmployee(""emp2"");
        SQLInsertClause insert = insert(survey);
        insert.columns(survey.id, survey.name);
        insert.select(select(survey.id, emp2.firstname).from(survey)
          .innerJoin(emp1)
           .on(survey.id.eq(emp1.id))
          .innerJoin(emp2)
           .on(emp1.superiorId.eq(emp2.superiorId), emp1.firstname.eq(emp2.firstname)));

        assertEquals(0, insert.execute());
    }",1
"@Test
    public void insert_alternative_syntax() {
        // with columns
        assertEquals(1, insert(survey)
            .set(survey.id, 3)
            .set(survey.name, ""Hello"")
            .execute());
    }",1
"@Test
    public void insert_batch_to_bulk() {
        SQLInsertClause insert = insert(survey);
        insert.setBatchToBulk(true);

        insert.set(survey.id, 5)
                .set(survey.name, ""55"")
                .addBatch();

        assertEquals(1, insert.getBatchCount());

        insert.set(survey.id, 6)
                .set(survey.name, ""66"")
                .addBatch();

        assertEquals(2, insert.getBatchCount());
        assertEquals(2, insert.execute());

        assertEquals(1L, query().from(survey).where(survey.name.eq(""55"")).fetchCount());
        assertEquals(1L, query().from(survey).where(survey.name.eq(""66"")).fetchCount());
    }",1
"@Test
    public void like_with_escape() {
        assertEquals(1, query().from(survey).where(survey.name.like(""a!%"", '!')).fetchCount());
        assertEquals(1, query().from(survey).where(survey.name.like(""a!_"", '!')).fetchCount());
        assertEquals(3, query().from(survey).where(survey.name.like(""a%"", '!')).fetchCount());
        assertEquals(2, query().from(survey).where(survey.name.like(""a_"", '!')).fetchCount());
    }",1
"@Test
    public void merge_listener() {
        final AtomicInteger calls = new AtomicInteger(0);
        SQLListener listener = new SQLBaseListener() {
            @Override
            public void end(SQLListenerContext context) {
                if (context.getData(AbstractSQLQuery.PARENT_CONTEXT) == null) {
                    calls.incrementAndGet();
                }",1
"@Test
    public void merge_with_keys_columns_and_values_using_null() {
        // keys + columns + values
        assertEquals(1, merge(survey).keys(survey.id)
            .set(survey.id, 5)
            .set(survey.name, (String) null).execute());
    }",1
"@Test
    public void tableHints_multiple() {
        SQLServerQuery<?> query = new SQLServerQuery<Void>(null, new SQLServerTemplates());
        query.from(survey).tableHints(SQLServerTableHints.NOWAIT, SQLServerTableHints.NOLOCK).where(survey.name.isNull());
        assertEquals(""from SURVEY SURVEY with (NOWAIT, NOLOCK)\nwhere SURVEY.NAME is null"", query.toString());
    }",1
"@Test
    public void tableHints_multiple2() {
        QSurvey survey2 = new QSurvey(""survey2"");
        SQLServerQuery<?> query = new SQLServerQuery<Void>(null, new SQLServerTemplates());
        query.from(survey).tableHints(SQLServerTableHints.NOWAIT)
             .from(survey2).tableHints(SQLServerTableHints.NOLOCK)
             .where(survey.name.isNull());
        assertEquals(""from SURVEY SURVEY with (NOWAIT), SURVEY survey2 with (NOLOCK)\nwhere SURVEY.NAME is null"", query.toString());
    }",1
"@Test
    public void forUpdate_with_limit() {
        query.forUpdate();
        query.limit(2);
        assertEquals(""select survey.NAME from SURVEY survey order by survey.NAME asc limit ? for update"",
                toString(query));
    }",1
"@Test
    public void ignoreIndex() {
        query = new MySQLQuery<Void>(null, MySQLTemplates.builder().newLineToSingleSpace().build());
        query.from(survey);
        query.ignoreIndex(""col1_index"");
        query.orderBy(survey.name.asc());
        query.getMetadata().setProjection(survey.name);

        assertEquals(""select survey.NAME from SURVEY survey ignore index (col1_index) "" +
                     ""order by survey.NAME asc"", toString(query));
    }",1
"@Test
    public void intoString() {
        query.into(""var1"");
        assertEquals(""select survey.NAME from SURVEY survey "" +
                     ""order by survey.NAME asc into var1"", toString(query));
    }",1
"@Test
    public void syntax() {
//        SELECT
//        [ALL | DISTINCT | DISTINCTROW ]
//          [HIGH_PRIORITY]
        query.highPriority();
//          [STRAIGHT_JOIN]
        query.straightJoin();
//          [SQL_SMALL_RESULT] [SQL_BIG_RESULT] [SQL_BUFFER_RESULT]
        query.smallResult();
        query.bigResult();
        query.bufferResult();
//          [SQL_CACHE | SQL_NO_CACHE] [SQL_CALC_FOUND_ROWS]
        query.cache();
        query.noCache();
        query.calcFoundRows();
//        select_expr [, select_expr ...]
//        [FROM table_references
        query.from(new QSurvey(""survey2""));
//        [WHERE where_condition]
        query.where(survey.id.isNotNull());
//        [GROUP BY {col_name | expr | position}",1
"@Test
    public void test() {
        SQLTemplates templates = MySQLTemplates.builder()
                .printSchema()
                .build();
        Configuration conf = new Configuration(templates);
        System.out.println(new SQLQuery(conf).from(survey1).toString());
    }",1
"@Test
    public void aggregate_uniqueResult() {
        int min = 30000, avg = 65000, max = 160000;
        // fetchOne
        assertEquals(min, query().from(employee).select(employee.salary.min()).fetchOne().intValue());
        assertEquals(avg, query().from(employee).select(employee.salary.avg()).fetchOne().intValue());
        assertEquals(max, query().from(employee).select(employee.salary.max()).fetchOne().intValue());
    }",1
"@Test
    public void all() {
        for (Expression<?> expr : survey.all()) {
            Path<?> path = (Path<?>) expr;
            assertEquals(survey, path.getMetadata().getParent());
        }",1
"@Test
    public void array_projection() {
        List<String[]> results = query().from(employee).select(
                new ArrayConstructorExpression<String>(String[].class, employee.firstname)).fetch();
        assertFalse(results.isEmpty());
        for (String[] result : results) {
            assertNotNull(result[0]);
        }",1
"@Test
    public void array_projection() {
        List<String[]> results = query().from(employee).select(
                new ArrayConstructorExpression<String>(String[].class, employee.firstname)).fetch();
        assertFalse(results.isEmpty());
        for (String[] result : results) {
            assertNotNull(result[0]);
        }",1
"@Test
    public void between() {
        // 11-13
        assertEquals(Arrays.asList(11, 12, 13),
                query().from(employee).where(employee.id.between(11, 13)).orderBy(employee.id.asc())
                       .select(employee.id).fetch());
    }",1
"@Test
    public void casts() throws SQLException {
        NumberExpression<?> num = employee.id;
        List<Expression<?>> exprs = new ArrayList<>();

        add(exprs, num.byteValue(), MYSQL);
        add(exprs, num.doubleValue());
        add(exprs, num.floatValue());
        add(exprs, num.intValue());
        add(exprs, num.longValue(), MYSQL);
        add(exprs, num.shortValue(), MYSQL);
        add(exprs, num.stringValue(), DERBY);

        for (Expression<?> expr : exprs) {
            for (Object o : query().from(employee).select(expr).fetch()) {
                assertEquals(expr.getType(), o.getClass());
            }",1
"@Test
    public void coalesce() {
        Coalesce<String> c = new Coalesce<String>(employee.firstname, employee.lastname).add(""xxx"");
        assertEquals(Collections.emptyList(),
                query().from(employee).where(c.getValue().eq(""xxx"")).select(employee.id).fetch());
    }",1
"@Test
    public void count2() {
        assertEquals(10, query().from(employee).select(employee.count()).fetchFirst().intValue());
    }",1
"@Test
    public void count_distinct2() {
        query().from(employee).select(employee.countDistinct()).fetchFirst();
    }",1
"@Test
    public void count_distinct_with_pK() {
        assertEquals(10, query().from(employee).distinct().fetchCount());
    }",1
"@Test
    public void dateTime() {
        SQLQuery<?> query = query().from(employee).orderBy(employee.id.asc());
        assertEquals(Integer.valueOf(10),     query.select(employee.datefield.dayOfMonth()).fetchFirst());
        assertEquals(Integer.valueOf(2),      query.select(employee.datefield.month()).fetchFirst());
        assertEquals(Integer.valueOf(2000),   query.select(employee.datefield.year()).fetchFirst());
        assertEquals(Integer.valueOf(200002), query.select(employee.datefield.yearMonth()).fetchFirst());
    }",1
"@Test
    public void select_concat() throws SQLException {
        for (Tuple row : query().from(survey).select(survey.name, survey.name.append(""Hello World"")).fetch()) {
            assertEquals(
                    row.get(survey.name) + ""Hello World"",
                    row.get(survey.name.append(""Hello World"")));
        }",1
"@Test
    public void factoryExpression_in_groupBy() {
        Expression<Employee> empBean = Projections.bean(Employee.class, employee.id, employee.superiorId);
        assertTrue(query().from(employee).groupBy(empBean).select(empBean).fetchFirst() != null);
    }",1
"@Test
    public void aggregate_list() {
        int min = 30000, avg = 65000, max = 160000;
        // fetch
        assertEquals(min, query().from(employee).select(employee.salary.min()).fetch().get(0).intValue());
        assertEquals(avg, query().from(employee).select(employee.salary.avg()).fetch().get(0).intValue());
        assertEquals(max, query().from(employee).select(employee.salary.max()).fetch().get(0).intValue());
    }",1
"@Test
    public void in_empty() {
        assertEquals(0, query().from(employee).where(employee.id.in(Collections.emptyList())).fetchCount());
    }",1
"@Test
    public void inner_join() throws SQLException {
        assertEquals(8, query().from(employee).innerJoin(employee2)
            .on(employee.superiorIdKey.on(employee2))
            .select(employee.id, employee2.id).fetch().size());
    }",1
"@Test
    public void inner_join_2Conditions() {
        assertEquals(8, query().from(employee).innerJoin(employee2)
            .on(employee.superiorIdKey.on(employee2))
            .on(employee2.firstname.isNotNull())
            .select(employee.id, employee2.id).fetch().size());
    }",1
"@Test
    public void compact_join() {
        // verbose
        assertEquals(8, query().from(employee)
            .innerJoin(employee2)
            .on(employee.superiorId.eq(employee2.id))
            .select(employee.id, employee2.id).fetch().size());

        // compact
        assertEquals(8, query().from(employee)
            .innerJoin(employee.superiorIdKey, employee2)
            .select(employee.id, employee2.id).fetch().size());

    }",1
"@Test
    public void joins() throws SQLException {
        for (Tuple row : query().from(employee).innerJoin(employee2)
                .on(employee.superiorId.eq(employee2.superiorId))
                .where(employee2.id.eq(10))
                .select(employee.id, employee2.id).fetch()) {
            assertNotNull(row.get(employee.id));
            assertNotNull(row.get(employee2.id));
        }",1
"@Test
    public void like_ignore_case() {
        assertEquals(3, query().from(employee).where(employee.firstname.likeIgnoreCase(""%m%"")).fetchCount());
    }",1
"@Test
    public void limit_and_offset() throws SQLException {
        assertEquals(Arrays.asList(20, 13, 10, 2),
            query().from(employee)
                   .orderBy(employee.firstname.asc())
                   .limit(4).offset(3)
                   .select(employee.id).fetch());
    }",1
"@Test
    public void listResults() {
        QueryResults<Integer> results = query().from(employee)
                .limit(10).offset(1).orderBy(employee.id.asc())
                .select(employee.id).fetchResults();
        assertEquals(10, results.getTotal());
    }",1
"@Test
    public void listResults2() {
        QueryResults<Integer> results = query().from(employee)
                .limit(2).offset(10).orderBy(employee.id.asc())
                .select(employee.id).fetchResults();
        assertEquals(10, results.getTotal());
    }",1
"@Test
    public void listResults_factoryExpression() {
        QueryResults<Employee> results = query().from(employee)
                .limit(10).offset(1).orderBy(employee.id.asc())
                .select(employee).fetchResults();
        assertEquals(10, results.getTotal());
    }",1
"@Test
    public void literals_literals() {
        if (configuration.getUseLiterals()) {
            literals();
        }",1
"@Test
    public void random() {
        firstResult(MathExpressions.random());
    }",1
"@Test
    public void nested_tuple_projection() {
        Concatenation concat = new Concatenation(employee.firstname, employee.lastname);
        List<Tuple> tuples = query().from(employee)
                .select(employee.firstname, employee.lastname, concat).fetch();
        assertFalse(tuples.isEmpty());
        for (Tuple tuple : tuples) {
            String firstName = tuple.get(employee.firstname);
            String lastName = tuple.get(employee.lastname);
            assertEquals(firstName + lastName, tuple.get(concat));
        }",1
"@Test
    public void num_cast() {
        query().from(employee).select(employee.id.castToNum(Long.class)).fetch();
        query().from(employee).select(employee.id.castToNum(Float.class)).fetch();
        query().from(employee).select(employee.id.castToNum(Double.class)).fetch();
    }",1
"@Test
    public void number_as_boolean_Null() {
        QNumberTest numberTest = QNumberTest.numberTest;
        delete(numberTest).execute();
        insert(numberTest).setNull(numberTest.col1Boolean).execute();
        insert(numberTest).setNull(numberTest.col1Number).execute();
        assertEquals(2, query().from(numberTest).select(numberTest.col1Boolean).fetch().size());
        assertEquals(2, query().from(numberTest).select(numberTest.col1Number).fetch().size());
    }",1
"@Test
    public void offset_only() {
        assertEquals(Arrays.asList(20, 13, 10, 2, 1, 11, 12), query().from(employee)
            .orderBy(employee.firstname.asc())
            .offset(3)
            .select(employee.id).fetch());
    }",1
"@Test
    public void order_nullsLast() {
        assertEquals(Collections.singletonList(""Hello World""), query().from(survey)
            .orderBy(survey.name.asc().nullsLast())
            .select(survey.name).fetch());
    }",1
"@Test
    public void statementOptions() {
        StatementOptions options = StatementOptions.builder().setFetchSize(15).setMaxRows(150).build();
        SQLQuery<?> query = query().from(employee).orderBy(employee.id.asc());
        query.setStatementOptions(options);
        query.addListener(new SQLBaseListener() {
            public void preExecute(SQLListenerContext context) {
                try {
                    assertEquals(15, context.getPreparedStatement().getFetchSize());
                    assertEquals(150, context.getPreparedStatement().getMaxRows());
                }",1
"@Test
    public void array_projection() {
        List<String[]> results = query().from(employee).select(
                new ArrayConstructorExpression<String>(String[].class, employee.firstname)).fetch();
        assertFalse(results.isEmpty());
        for (String[] result : results) {
            assertNotNull(result[0]);
        }",1
"@Test
    public void constructor_projection2() {
        List<SimpleProjection> projections = query().from(employee).select(
                Projections.constructor(SimpleProjection.class,
                        employee.firstname, employee.lastname)).fetch();
        assertFalse(projections.isEmpty());
        for (SimpleProjection projection : projections) {
            assertNotNull(projection);
        }",1
"@Test
    public void query_with_constant() throws Exception {
        for (Tuple row : query().from(survey)
                .where(survey.id.eq(1))
                .select(survey.id, survey.name).fetch()) {
            assertNotNull(row.get(survey.id));
            assertNotNull(row.get(survey.name));
        }",1
"@Test
    public void relationalPath_eq() {
        assertEquals(10, query().from(employee, employee2)
                .where(employee.eq(employee2))
                .select(employee.id, employee2.id).fetch().size());
    }",1
"@Test
    public void relationalPath_eq2() {
        assertEquals(1, query().from(survey, survey2)
                .where(survey.eq(survey2))
                .select(survey.id, survey2.id).fetch().size());
    }",1
"@Test
    public void select_for_share() {
        if (configuration.getTemplates().isForShareSupported()) {
            assertEquals(1, query().from(survey).forShare().where(survey.id.isNotNull()).select(survey.id).fetch().size());
        }",1
"@Test
    public void serialization2() throws Exception {
        List<Tuple> rows = query().from(survey).select(survey.id, survey.name).fetch();
        serialize(rows);
    }",1
"@Test
    public void serialization2() throws Exception {
        List<Tuple> rows = query().from(survey).select(survey.id, survey.name).fetch();
        serialize(rows);
    }",1
"@Test
    public void statementOptions() {
        StatementOptions options = StatementOptions.builder().setFetchSize(15).setMaxRows(150).build();
        SQLQuery<?> query = query().from(employee).orderBy(employee.id.asc());
        query.setStatementOptions(options);
        query.addListener(new SQLBaseListener() {
            public void preExecute(SQLListenerContext context) {
                try {
                    assertEquals(15, context.getPreparedStatement().getFetchSize());
                    assertEquals(150, context.getPreparedStatement().getMaxRows());
                }",1
"@Test
    public void array_projection() {
        List<String[]> results = query().from(employee).select(
                new ArrayConstructorExpression<String>(String[].class, employee.firstname)).fetch();
        assertFalse(results.isEmpty());
        for (String[] result : results) {
            assertNotNull(result[0]);
        }",1
"@Test
    public void stringFunctions2() throws SQLException {
        for (BooleanExpression where : Arrays.asList(
                employee.firstname.startsWith(""a""),
                employee.firstname.startsWithIgnoreCase(""a""),
                employee.firstname.endsWith(""a""),
                employee.firstname.endsWithIgnoreCase(""a""))) {
            query().from(employee).where(where).select(employee.firstname).fetch();
        }",1
"@Test
    public void syntax_for_employee() throws SQLException {
        assertEquals(3, query().from(employee).groupBy(employee.superiorId)
            .orderBy(employee.superiorId.asc())
            .select(employee.salary.avg(), employee.id.max()).fetch().size());

        assertEquals(2, query().from(employee).groupBy(employee.superiorId)
            .having(employee.id.max().gt(5))
            .orderBy(employee.superiorId.asc())
            .select(employee.salary.avg(), employee.id.max()).fetch().size());

        assertEquals(2, query().from(employee).groupBy(employee.superiorId)
            .having(employee.superiorId.isNotNull())
            .orderBy(employee.superiorId.asc())
            .select(employee.salary.avg(), employee.id.max()).fetch().size());
    }",1
"@Test
    public void nested_tuple_projection() {
        Concatenation concat = new Concatenation(employee.firstname, employee.lastname);
        List<Tuple> tuples = query().from(employee)
                .select(employee.firstname, employee.lastname, concat).fetch();
        assertFalse(tuples.isEmpty());
        for (Tuple tuple : tuples) {
            String firstName = tuple.get(employee.firstname);
            String lastName = tuple.get(employee.lastname);
            assertEquals(firstName + lastName, tuple.get(concat));
        }",1
"@Test
    public void unique_wildcard() {
        // unique wildcard
        Tuple row = query().from(survey).limit(1).select(survey.all()).fetchFirst();
        assertNotNull(row);
        assertEquals(3, row.size());
        assertNotNull(row.get(0, Object.class));
        assertNotNull(row.get(0, Object.class) + "" is not null"", row.get(1, Object.class));
    }",1
"@Test
    public void where_exists() throws SQLException {
        SQLQuery<Integer> sq1 = query().from(employee).select(employee.id.max());
        assertEquals(10, query().from(employee).where(sq1.exists()).fetchCount());
    }",1
"@Test
    public void wildcard_all2() {
        assertEquals(10, query().from(new RelationalPathBase<Object>(Object.class, ""employee"", ""public"", ""EMPLOYEE""))
                .select(Wildcard.all).fetch().size());
    }",1
"@Test
    public void windowFunctions_manual_paging() {
        Expression<Long> rowNumber = SQLExpressions.rowNumber().over().orderBy(employee.lastname.asc()).as(""rn"");
        Expression<Object[]> all = Wildcard.all;

        // simple
        System.out.println(""#1"");
        for (Tuple row : query().from(employee).select(employee.firstname, employee.lastname, rowNumber).fetch()) {
            System.out.println(row);
        }",1
"@Test
    public void lineString_instances() {
        List<Geometry> results = withLineStrings().select(shapes.geometry).fetch();
        assertFalse(results.isEmpty());
        for (Geometry row : results) {
            assertNotNull(row);
            assertTrue(row instanceof LineString);
        }",1
"@Test
    public void multiLineString_instances() {
        List<Geometry> results = withMultiLineStrings().select(shapes.geometry).fetch();
        assertFalse(results.isEmpty());
        for (Geometry row : results) {
            assertNotNull(row);
            assertTrue(row instanceof MultiLineString);
        }",1
"@Test
    public void multiPoint_methods() {
        MultiPointPath<MultiPoint> multipoint = shapes.geometry.asMultiPoint();

        List<Expression<?>> expressions = new ArrayList<>();
        add(expressions, multipoint.asBinary(), H2);
        add(expressions, multipoint.asText());
        add(expressions, multipoint.boundary(), H2, MYSQL);
        add(expressions, multipoint.convexHull(), H2, MYSQL);
        add(expressions, multipoint.dimension());
        add(expressions, multipoint.envelope(), H2);
        add(expressions, multipoint.geometryType(), H2);
        add(expressions, multipoint.isEmpty());
        add(expressions, multipoint.isSimple());
        // multipoint specific
        add(expressions, multipoint.numGeometries(), H2);
        add(expressions, multipoint.geometryN(1), H2);

        for (Expression<?> expr : expressions) {
            boolean logged = false;
            for (Object row : withMultipoints().select(expr).fetch()) {
                if (row == null && !logged) {
                    System.err.println(expr.toString());
                    logged = true;
                }",1
"@Test
    public void multiPolygon_instances() {
        List<Geometry> results = withMultiPolygons().select(shapes.geometry).fetch();
        assertFalse(results.isEmpty());
        for (Geometry row : results) {
            assertNotNull(row);
            assertTrue(row instanceof MultiPolygon);
        }",1
"@Test
    public void params() {
        Param<String> name = new Param<String>(String.class, ""name"");
        query.from(survey).where(survey.name.eq(name), survey.name2.eq(""A"")).select(survey.id);
        query.set(name, ""Bob"");
        SQLBindings bindings = query.getSQL();
        assertEquals(""select SURVEY.ID\nfrom SURVEY SURVEY\nwhere SURVEY.NAME = ? and SURVEY.NAME2 = ?"", bindings.getSQL());
        assertEquals(Arrays.asList(""Bob"", ""A""), bindings.getNullFriendlyBindings());
    }",1
"@Test
    public void iterate() {
        try (CloseableIterator<Employee> it = query.iterate()) {
            while (it.hasNext()) {
                it.next();
            }",1
"@Test
    public void subQuery_alias() {
        query().from(query().from(employee).select(employee.all()).as(employee2)).select(employee2.all()).fetch();
    }",1
"@Test
    public void subQuery_leftJoin() {
        SubQueryExpression<Integer> sq = query().from(employee2).select(employee2.id);
        QEmployee sqEmp = new QEmployee(""sq"");
        query().from(employee).leftJoin(sq, sqEmp).on(sqEmp.id.eq(employee.id)).select(employee.id).fetch();

    }",1
"@Test
    public void union_single_column_projections_iterate() throws IOException {
        SubQueryExpression<Integer> sq1 = query().from(employee).select(employee.id.max());
        SubQueryExpression<Integer> sq2 = query().from(employee).select(employee.id.min());

        try (CloseableIterator<Integer> iterator = query().union(sq1, sq2).iterate()) {
            assertTrue(iterator.hasNext());
            assertTrue(iterator.next() != null);
            assertTrue(iterator.next() != null);
            assertFalse(iterator.hasNext());
        }",1
"@Test
    public void setNull2() {
        long count = query().from(survey).fetchCount();
        assertEquals(count, update(survey).set(survey.name, (String) null).execute());
    }",1
"@Test
    public void update() throws SQLException {
        // original state
        long count = query().from(survey).fetchCount();
        assertEquals(0, query().from(survey).where(survey.name.eq(""S"")).fetchCount());

        // update call with 0 update count
        assertEquals(0, update(survey).where(survey.name.eq(""XXX"")).set(survey.name, ""S"").execute());
        assertEquals(0, query().from(survey).where(survey.name.eq(""S"")).fetchCount());

        // update call with full update count
        assertEquals(count, update(survey).set(survey.name, ""S"").execute());
        assertEquals(count, query().from(survey).where(survey.name.eq(""S"")).fetchCount());
    }",1
"@Test
    public void update2() throws SQLException {
        List<Path<?>> paths = Collections.<Path<?>>singletonList(survey.name);
        List<?> values = Collections.singletonList(""S"");

        // original state
        long count = query().from(survey).fetchCount();
        assertEquals(0, query().from(survey).where(survey.name.eq(""S"")).fetchCount());

        // update call with 0 update count
        assertEquals(0, update(survey).where(survey.name.eq(""XXX"")).set(paths, values).execute());
        assertEquals(0, query().from(survey).where(survey.name.eq(""S"")).fetchCount());

        // update call with full update count
        assertEquals(count, update(survey).set(paths, values).execute());
        assertEquals(count, query().from(survey).where(survey.name.eq(""S"")).fetchCount());

    }",1
"@Test
    public void update_with_subQuery_exists() {
        QSurvey survey1 = new QSurvey(""s1"");
        QEmployee employee = new QEmployee(""e"");
        SQLUpdateClause update = update(survey1);
        update.set(survey1.name, ""AA"");
        update.where(selectOne().from(employee).where(survey1.id.eq(employee.id)).exists());
        assertEquals(1, update.execute());
    }",1
"@Test
    public void update_with_subQuery_exists2() {
        QSurvey survey1 = new QSurvey(""s1"");
        QEmployee employee = new QEmployee(""e"");
        SQLUpdateClause update = update(survey1);
        update.set(survey1.name, ""AA"");
        update.where(selectOne().from(employee).where(survey1.name.eq(employee.lastname)).exists());
        assertEquals(0, update.execute());
    }",1
"@Test
    public void update_with_subQuery_exists_Params() {
        QSurvey survey1 = new QSurvey(""s1"");
        QEmployee employee = new QEmployee(""e"");

        Param<Integer> param = new Param<Integer>(Integer.class, ""param"");
        SQLQuery<?> sq = query().from(employee).where(employee.id.eq(param));
        sq.set(param, -12478923);

        SQLUpdateClause update = update(survey1);
        update.set(survey1.name, ""AA"");
        update.where(sq.exists());
        assertEquals(0, update.execute());
    }",1
"@Test
    public void update_with_subQuery_notExists() {
        QSurvey survey1 = new QSurvey(""s1"");
        QEmployee employee = new QEmployee(""e"");
        SQLUpdateClause update = update(survey1);
        update.set(survey1.name, ""AA"");
        update.where(query().from(employee).where(survey1.id.eq(employee.id)).notExists());
        assertEquals(0, update.execute());
    }",1
"@Test
  public void testRDF() throws Exception {
    Path tempDir = getTempDir();
    Path dataDir = tempDir.resolve(""data"");
    Path modelDir = tempDir.resolve(""model"");

    Map<String,Object> overlayConfig = new HashMap<>();
    overlayConfig.put(""oryx.batch.update-class"", RDFUpdate.class.getName());
    ConfigUtils.set(overlayConfig, ""oryx.batch.storage.data-dir"", dataDir);
    ConfigUtils.set(overlayConfig, ""oryx.batch.storage.model-dir"", modelDir);
    overlayConfig.put(""oryx.batch.streaming.generation-interval-sec"", GEN_INTERVAL_SEC);
    overlayConfig.put(""oryx.rdf.num-trees"", 10);
    // Low values like 1 are deliberately bad, won't work
    overlayConfig.put(""oryx.rdf.hyperparams.max-depth"", ""[1,"" + MAX_DEPTH + ""]"");
    overlayConfig.put(""oryx.rdf.hyperparams.max-split-candidates"", MAX_SPLIT_CANDIDATES);
    overlayConfig.put(""oryx.input-schema.num-features"", 5);
    overlayConfig.put(""oryx.input-schema.categorical-features"", ""[\""4\""]"");
    overlayConfig.put(""oryx.input-schema.id-features"", ""[\""0\""]"");
    overlayConfig.put(""oryx.input-schema.target-feature"", ""\""4\"""");
    overlayConfig.put(""oryx.ml.eval.candidates"", 2);
    overlayConfig.put(""oryx.ml.eval.parallelism"", 2);
    Config config = ConfigUtils.overlayOn(overlayConfig, getConfig());

    startMessaging();

    startServerProduceConsumeTopics(
        config,
        new RandomCategoricalRDFDataGenerator(3),
        DATA_TO_WRITE,
        WRITE_INTERVAL_MSEC);

    List<Path> modelInstanceDirs = IOUtils.listFiles(modelDir, ""*"");

    checkIntervals(modelInstanceDirs.size(), DATA_TO_WRITE, WRITE_INTERVAL_MSEC, GEN_INTERVAL_SEC);

    Path latestModelDir = modelInstanceDirs.get(modelInstanceDirs.size() - 1);
    Path modelFile = latestModelDir.resolve(MLUpdate.MODEL_FILE_NAME);
    assertTrue(""No such model file: "" + modelFile, Files.exists(modelFile));

    PMML pmml = PMMLUtils.read(modelFile);

    assertEquals(3, pmml.getExtensions().size());
    Map<String,Object> expected = new HashMap<>();
    expected.put(""maxSplitCandidates"", MAX_SPLIT_CANDIDATES);
    expected.put(""maxDepth"", MAX_DEPTH);
    expected.put(""impurity"", IMPURITY);
    checkExtensions(pmml, expected);

    Pair<DecisionForest,CategoricalValueEncodings> forestEncoding = RDFPMMLUtils.read(pmml);
    DecisionForest forest = forestEncoding.getFirst();
    CategoricalValueEncodings encoding = forestEncoding.getSecond();
    Map<String,Integer> targetEncoding = encoding.getValueEncodingMap(4);

    int[] zeroOne = { 0, 1 }",1
"@Test
  public void testReadPMMLFromMessage() throws Exception {
    PMML pmml = PMMLUtils.buildSkeletonPMML();
    String pmmlString = PMMLUtils.toString(pmml);
    assertEquals(PMMLUtils.VERSION, AppPMMLUtils.readPMMLFromUpdateKeyMessage(
        ""MODEL"", pmmlString, null).getVersion());

    Path pmmlPath = getTempDir().resolve(""out.pmml"");
    Files.write(pmmlPath, Collections.singleton(pmmlString));
    assertEquals(PMMLUtils.VERSION, AppPMMLUtils.readPMMLFromUpdateKeyMessage(
        ""MODEL-REF"", pmmlPath.toAbsolutePath().toString(), null).getVersion());

    assertNull(AppPMMLUtils.readPMMLFromUpdateKeyMessage(""MODEL-REF"", ""no-such-path"", null));
  }",1
"@Test
  public void testAllItemIDs() {
    List<String> users = target(""/user/allIDs"").request()
        .accept(MediaType.APPLICATION_JSON_TYPE).get(LIST_STRING_TYPE);
    Assert.assertEquals(7, users.size());
    for (int user = 0; user < 7; user++) {
      OryxTest.assertContains(users, ""U"" + user);
    }",1
"@Test
  public void testEmptyItems() {
    List<String> items = target(""/knownItems/X1"").request()
        .accept(MediaType.APPLICATION_JSON_TYPE).get(LIST_STRING_TYPE);
    Assert.assertEquals(0, items.size());
  }",1
"@Test
  public void testRescorer() {
    List<IDCount> top = target(""/mostPopularItems"").queryParam(""rescorerParams"", ""foo"").request()
        .accept(MediaType.APPLICATION_JSON_TYPE).get(LIST_ID_COUNT_TYPE);
    Assert.assertEquals(4, top.size());
    Assert.assertEquals(6, top.get(0).getCount());
    Assert.assertEquals(5, top.get(1).getCount());
  }",1
"@Test
  public void testDelete() {
    Response response = target(""/pref/U1/I2"").request().delete();
    checkResponse(response, ""U1"", ""I2"", """");
  }",1
"@Test
  public void testPostWithBadItemValue() {
    try (Response response = target(""/pref/U2/I2"").request().post(Entity.text(""aBc!""))) {
      Assert.assertEquals(Response.Status.BAD_REQUEST.getStatusCode(), response.getStatus());
    }",1
"@Test
  public void testRecommendToAnonymous() {
    List<IDValue> recs = target(""/recommendToAnonymous/I4=1.0/I5=2.0"").request()
        .accept(MediaType.APPLICATION_JSON_TYPE).get(LIST_ID_VALUE_TYPE);
    testTopByValue(7, recs, false);
    Assert.assertEquals(""I7"", recs.get(0).getID());
    Assert.assertEquals(0.35964763f, recs.get(0).getValue(), FLOAT_EPSILON);
  }",1
"@Test
  public void testRescorer() {
    List<IDValue> recs = target(""/recommendToAnonymous/I4=1.0/I5=2.0"")
        .queryParam(""rescorerParams"", ""foo"").request()
        .accept(MediaType.APPLICATION_JSON_TYPE).get(LIST_ID_VALUE_TYPE);
    testTopByValue(3, recs, false);
    Assert.assertEquals(""I7"", recs.get(0).getID());
    Assert.assertEquals(2.0f * 0.35964763f, recs.get(0).getValue(), FLOAT_EPSILON);
  }",1
"@Test
  public void testHowMany() {
    testHowMany(""/recommendWithContext/U5/"", 10, 2);
    testHowMany(""/recommendWithContext/U5/"", 2, 2);
    testHowMany(""/recommendWithContext/U5/"", 1, 1);
  }",1
"@Test
  public void testRecommendWithContextWithUnknown() {
    String response = target(""/recommendWithContext/U0/foo/I4=1.0/I5=2.0"").request().get(String.class);
    testCSVTopByScore(5, response);
  }",1
"@Test
  public void testRescorer() {
    List<IDValue> recs = target(""similarity/I0/I4/I6"")
        .queryParam(""rescorerParams"", ""foo"").request()
        .accept(MediaType.APPLICATION_JSON_TYPE).get(LIST_ID_VALUE_TYPE);
    testTopByValue(4, recs, false);
    Assert.assertEquals(""I5"", recs.get(1).getID());
    Assert.assertEquals(2.0 * 0.9125432970065859, recs.get(2).getValue(), DOUBLE_EPSILON);
  }",1
"@Test
  public void testSimilarityCSV() {
    String response = target(""/similarity/I0/I4/I6"").request().get(String.class);
    testCSVTopByScore(6, response);
  }",1
"@Test
  public void testZeroSimilarityToItem() {
    List<Double> items = target(""/similarityToItem/I1/I10"").request()
        .accept(MediaType.APPLICATION_JSON_TYPE).get(LIST_DOUBLE_TYPE);
    Assert.assertEquals(1, items.size());
    Assert.assertEquals(0.0f, items.get(0), FLOAT_EPSILON);
  }",1
"@Test
  public void testAssign2() {
    String prediction = target(""/assign/10,-1.0"").request().get(String.class);
    Assert.assertEquals(3, Integer.parseInt(prediction));
  }",1
"@Test
  public void testConsole() {
    String html;
    try (Response response = target(""/index.html"").request().accept(MediaType.TEXT_HTML).get()) {
      Assert.assertEquals(""public"", response.getHeaderString(""Cache-Control""));
      Assert.assertEquals(""SAMEORIGIN"", response.getHeaderString(""X-Frame-Options""));
      html = response.readEntity(String.class);
    }",1
"@Test
  public void testGet() {
    try (Response response = target(""/ready"").request().get()) {
      Assert.assertEquals(Response.Status.OK.getStatusCode(), response.getStatus());
    }",1
"@Test
  public void testDistribution() {
    List<IDValue> recs = target(""/classificationDistribution/B,0,"").request()
        .accept(MediaType.APPLICATION_JSON_TYPE).get(LIST_ID_VALUE_TYPE);
    Assert.assertEquals(""X"", recs.get(0).getID());
    Assert.assertEquals((10.0 / 90.0 + 2 * (1000.0 / 111000.0)) / 3,
                        recs.get(0).getValue(),
                        OryxTest.DOUBLE_EPSILON);
    Assert.assertEquals(""Y"", recs.get(1).getID());
    Assert.assertEquals((30.0 / 90.0 + 2 * (10000.0 / 111000.0)) / 3,
                        recs.get(1).getValue(),
                        OryxTest.DOUBLE_EPSILON);
    Assert.assertEquals(""Z"", recs.get(2).getID());
    Assert.assertEquals((50.0 / 90.0 + 2 * (100000.0 / 111000.0)) / 3,
                        recs.get(2).getValue(),
                        OryxTest.DOUBLE_EPSILON);
  }",1
"@Test
  public void testConsole() {
    String html;
    try (Response response = target(""/index.html"").request().accept(MediaType.TEXT_HTML).get()) {
      Assert.assertEquals(""public"", response.getHeaderString(""Cache-Control""));
      Assert.assertEquals(""SAMEORIGIN"", response.getHeaderString(""X-Frame-Options""));
      html = response.readEntity(String.class);
    }",1
"@Test
  public void testFormPredict() throws Exception {
    checkResponse(getFormPostResponse(PREDICT_DATA, ""/predict"", null, null));
  }",1
"@Test
  public void testALSSpeed() throws Exception {
    Map<String,Object> overlayConfig = new HashMap<>();
    overlayConfig.put(""oryx.speed.model-manager-class"", ALSSpeedModelManager.class.getName());
    overlayConfig.put(""oryx.speed.streaming.generation-interval-sec"", 5);
    overlayConfig.put(""oryx.als.hyperparams.features"", 2);
    Config config = ConfigUtils.overlayOn(overlayConfig, getConfig());

    startMessaging();

    List<KeyMessage<String,String>> updates =
        startServerProduceConsumeTopics(config,
                                        new MockALSInputGenerator(),
                                        new MockALSModelUpdateGenerator(),
                                        10, 10);

    if (log.isDebugEnabled()) {
      updates.forEach(update -> log.debug(""{}",1
"@Test
  public void testDeleteRecursively() throws IOException {
    Path testDir = createTestDirs();
    IOUtils.deleteRecursively(testDir);
    assertFalse(Files.exists(testDir));
    assertFalse(Files.exists(testDir.resolve(""subFile1"")));
  }",1
"@Test
  public void testListSubdirs2() throws IOException {
    Path testDir = createTestDirs();
    List<Path> files = IOUtils.listFiles(testDir, ""*/subFile*"");
    assertEquals(1, files.size());
    assertContains(files, testDir.resolve(""subDir1"").resolve(""subFile2""));
  }",1
"@Test
  public void testDeleteOldData() throws Exception {
    Path tempDir = getTempDir();
    Path dataDir = tempDir.resolve(""data"");
    Path modelDir = tempDir.resolve(""model"");
    Map<String,Object> overlayConfig = new HashMap<>();
    overlayConfig.put(""oryx.batch.update-class"", MockBatchUpdate.class.getName());
    ConfigUtils.set(overlayConfig, ""oryx.batch.storage.data-dir"", dataDir);
    ConfigUtils.set(overlayConfig, ""oryx.batch.storage.model-dir"", modelDir);
    overlayConfig.put(""oryx.batch.storage.max-age-data-hours"", 0);
    overlayConfig.put(""oryx.batch.storage.max-age-model-hours"", 0);
    overlayConfig.put(""oryx.batch.streaming.generation-interval-sec"", GEN_INTERVAL_SEC);
    Config config = ConfigUtils.overlayOn(overlayConfig, getConfig());

    startMessaging();
    startServerProduceConsumeTopics(config, DATA_TO_WRITE, WRITE_INTERVAL_MSEC);
    assertEquals(0, IOUtils.listFiles(dataDir, ""*"").size());
    assertEquals(0, IOUtils.listFiles(modelDir, ""*"").size());
  }",1
"@Test
  public void testUserPassword() throws Exception {
    startServer(buildUserPasswordConfig());

    Authenticator.setDefault(new Authenticator() {
      @Override
      protected PasswordAuthentication getPasswordAuthentication() {
        return new PasswordAuthentication(""oryx"", ""pass"".toCharArray());
      }",1
"@Test
  public void testMLUpdate() throws Exception {
    Path tempDir = getTempDir();
    Path dataDir = tempDir.resolve(""data"");
    Map<String,Object> overlayConfig = new HashMap<>();
    overlayConfig.put(""oryx.batch.update-class"", MockMLUpdate.class.getName());
    ConfigUtils.set(overlayConfig, ""oryx.batch.storage.data-dir"", dataDir);
    ConfigUtils.set(overlayConfig, ""oryx.batch.storage.model-dir"", tempDir.resolve(""model""));
    overlayConfig.put(""oryx.batch.streaming.generation-interval-sec"", GEN_INTERVAL_SEC);
    overlayConfig.put(""oryx.ml.eval.test-fraction"", TEST_FRACTION);
    overlayConfig.put(""oryx.ml.eval.threshold"", DATA_TO_WRITE / 2); // Should easily pass threshold
    Config config = ConfigUtils.overlayOn(overlayConfig, getConfig());

    startMessaging();

    List<Integer> trainCounts = MockMLUpdate.getResetTrainCounts();
    List<Integer> testCounts = MockMLUpdate.getResetTestCounts();

    startServerProduceConsumeTopics(config, DATA_TO_WRITE, WRITE_INTERVAL_MSEC);

    // If lists are unequal at this point, there must have been an empty test set
    // which yielded no call to evaluate(). Fill in the blank
    while (trainCounts.size() > testCounts.size()) {
      testCounts.add(0);
    }",1
"@Test
    public void deleteCurrentBranch() throws Exception {
        FileUtil.delete(localGitPath);
        RepositoryManager.create(localGitPath, null);
        RepositoryManager.checkoutExistingBranchOrCreateOrphan(localGitPath, true, null, null, ""newBranch"");

        // as the current branch is ""newBranch"" the deletion will go to tmp branch
        RepositoryManager.deleteBranch(localGitPath, ""newBranch"", false);

        assertThat(RepositoryManager.isOnBranch(localGitPath, ""tmp"")).isTrue();
    }",1
"@Test
    public void migrate_data_previously_store_in_elastic_search() throws Exception {
        String existingId = AbstractDeploymentConfig.generateId(""versionOK"", ""envId"");

        // given a configuration store in ES
        when(alienDao.findById(DeploymentMatchingConfiguration.class, existingId)).thenReturn(new DeploymentMatchingConfiguration(""versionOK"", ""envId""));
        // given the location of the existing config is defined
        Path configLocalPath = Paths.get(""target/deployment_config_dao_test/config"");
        Files.deleteIfExists(configLocalPath);
        when(localGitRepositoryPathResolver.resolve(eq(DeploymentMatchingConfiguration.class), eq(existingId))).thenReturn(configLocalPath);

        // when looking for a config that not existing in Git yet
        DeploymentMatchingConfiguration config = dao.findById(DeploymentMatchingConfiguration.class, existingId);

        // then we get data from ES
        assertThat(config).isNotNull();
        assertThat(config.getId()).isEqualTo(existingId);
        // ES data has been deleted
        verify(alienDao).delete(DeploymentMatchingConfiguration.class, existingId);
        // check data has been migrated into git
        assertThat(Files.exists(configLocalPath)).isTrue();
    }",1
"@Test
    public void testAddAndReturn(){
        CrawlDatums datums = new CrawlDatums();


        String url = ""http://cn.bing.com/"";

        CrawlDatum addedDatum = datums.addAndReturn(url);
        assertEquals(url,addedDatum.url());


        ArrayList<String> urlList = new ArrayList<String>();
        for(int i = 0;i<10;i++){
            urlList.add(""https://www.google.com/""+i);
        }",1
"@Test
    public void testBerkeleyDBInjector() throws Exception {
        BerkeleyDBManager dbManager = new BerkeleyDBManager(tempCrawlPath);
        testInject(dbManager);
    }",1
"@Test
    public void testRocksDBInjector() throws Exception {
        RocksDBManager dbManager = new RocksDBManager(tempCrawlPath);
        testInject(dbManager);
    }",1
"@Test
    public void testReportTrigger() {
        XmlHttpResponse resp = new XmlHttpResponse();
        UnitTestHelper.fillResponse(resp, ""leanapps/report.xml"");
        resp.setStatusCode(200);
        fixture.setResponse(resp);
        assertEquals(""NEW_POLICY_NEW_CUSTOMER"", fixture.reportTrigger());
    }",1
"@Test
    public void testParseLalResponse() {
        XmlHttpResponse lalResponse = new XmlHttpResponse();
        UnitTestHelper.fillResponse(lalResponse, ""leanapps/getPolicyCheckResponse.xml"");
        String key = lalResponse.toString();
        assertSame(lalResponse, HttpResponse.parse(key));
    }",1
"@Test
    public void testFindChromedriverExecutable() {
        String exec = factoryFactory.getExecutable(""chromedriver"");

        assertNotNull(""No executable found for chromedriver"", exec);
        File execFile = new File(exec);
        assertTrue(""Executable for chromedriver does not exist: "" + exec, execFile.exists());
        assertTrue(""Executable for chromedriver not executable: "" + exec, execFile.canExecute());
    }",1
"@Test
    public void testTrimElements() {
        String expected = FileUtil.loadFile(""GetWeatherSoapResponse.xml"");
        String formatted = FileUtil.loadFile(""GetWeatherSoapResponseFormatted.xml"");

        String trimmed = XMLFormatter.trimElements(formatted);

        assertEquals(expected, trimmed);
        assertEquals(expected, XMLFormatter.trimElements(trimmed));
    }",1
"@Test
    public void testAllXmlNoText() {
        LalPolicyXPaths.registerNamespace();
        String responseString = FileUtil.loadFile(""leanapps/getPolicyCheckResponse.xml"");
        List<String> all = xPathHelper.getAllXPath(NS_CONTEXT, responseString, ""//*/@xsi:type"");

        assertEquals(13, all.size());
        assertEquals(""ns:PostalAddress"", all.get(1));
    }",1
"@Test
    public void testBadXPath() {
        String responseString = FileUtil.loadFile(""leanapps/getPolicyCheckResponse.xml"");
        assertEquals("""", xPathHelper.getXPath(null, responseString, ""//status""));
        try {
            xPathHelper.getXPath(null, responseString, ""\\status"");
            fail(""expected exception"");
        }",1
"@Test
    public void testXPathWithNamespace() {
        LalPolicyXPaths.registerNamespace();
        String responseString = FileUtil.loadFile(""leanapps/getPolicyCheckResponse.xml"");
        assertEquals(""OK"", xPathHelper.getXPath(NS_CONTEXT, responseString, ""//lal:status/lal:status""));
    }",1
"@Test
    public void testFindTestResultPages() throws Exception {
        String path = getTestReportsPath();
        List<TestReportHtml> reports = new ReportFinder().findTestResultPages(new File(path));
        List<TestReportHtml> overviews = reports.stream().filter(TestReportHtml::isOverviewPage).collect(Collectors.toList());
        assertEquals(""Unexpected number of run: "" + overviews, 3, overviews.size());
        assertEquals(""Unexpected number of results"", EXPECTED_TEST_COUNT, reports.size());

        TestReportHtml mockXmlServerTest = getReport(reports, ""MockXmlServerTest"", ""MockXmlServerTest"");
        assertEquals(-1, mockXmlServerTest.getTime());
        assertEquals(Integer.MAX_VALUE - 2, mockXmlServerTest.getIndex());
        TestReportHtml mockXmlSuiteSetup = getReport(reports, ""MockXmlServerTest"", ""SuiteSetUp"");
        assertEquals(Integer.MAX_VALUE - 3, mockXmlSuiteSetup.getIndex());

        assertTrue(reports.stream().filter(r -> !""MockXmlServerTest"".equals(r.getRunName())).noneMatch(r -> r.getTime() == -1));

        TestReportHtml fitOverview = getActual(overviews, ""Fit"");
        assertEquals(0, fitOverview.getTime());
        assertEquals(Integer.MAX_VALUE, fitOverview.getIndex());
        assertEquals(2400, getActual(overviews, ""Http"").getTime());
        assertEquals(2075, getActual(overviews, ""Util"").getTime());

        TestReportHtml fitSuiteSetup = getReport(reports, ""Fit"", ""SuiteSetUp"");
        assertEquals(0, fitSuiteSetup.getIndex());
        TestReportHtml fitArraysAndSymbols = getReport(reports, ""Fit"", ""ArraysAndSymbolsComparison"");
        assertEquals(1, fitArraysAndSymbols.getIndex());
        TestReportHtml fitReturn = getReport(reports, ""Fit"", ""ReturnArrayAsSymbol"");
        assertEquals(2, fitReturn.getIndex());
    }",1
"@Test
  public void testGetAttribute_failsForAttributesNotDefinedByProvider() {
    File file = createFile();
    try {
      service.getAttribute(file, ""test:blah"");
      fail();
    }",1
"@Test
  public void testGetFileAttributeView_isNullForUnsupportedView() {
    final File file = createFile();
    FileLookup fileLookup =
        new FileLookup() {
          @Override
          public File lookup() throws IOException {
            return file;
          }",1
"@Test
  public void testReadAttributes_asObject() {
    File file = createFile();
    service.setInitialAttributes(file);

    BasicFileAttributes basicAttrs = service.readAttributes(file, BasicFileAttributes.class);
    assertThat(basicAttrs.fileKey()).isEqualTo(0);
    assertThat(basicAttrs.isDirectory()).isTrue();
    assertThat(basicAttrs.isRegularFile()).isFalse();

    TestAttributes testAttrs = service.readAttributes(file, TestAttributes.class);
    assertThat(testAttrs.foo()).isEqualTo(""hello"");
    assertThat(testAttrs.bar()).isEqualTo(0);
    assertThat(testAttrs.baz()).isEqualTo(1);

    file.setAttribute(""test"", ""baz"", 100);
    assertThat(service.readAttributes(file, TestAttributes.class).baz()).isEqualTo(100);
  }",1
"@Test
  public void testReadAttributes_failsForUnsupportedAttributesType() {
    File file = createFile();
    try {
      service.readAttributes(file, PosixFileAttributes.class);
      fail();
    }",1
"@Test
  public void testSetAttribute_failsForAttributeThatIsNotSettable() {
    File file = createFile();
    try {
      service.setAttribute(file, ""test:foo"", ""world"", false);
      fail();
    }",1
"@Test
  public void testSetAttribute_onCreate_failsForAttributeThatIsNotSettableOnCreate() {
    File file = createFile();
    try {
      service.setInitialAttributes(file, new BasicFileAttribute<>(""test:foo"", ""world""));
      fail();
    }",1
"@Test
  public void testFileBasics() {
    File file = regularFile(0);

    assertThat(file.id()).isEqualTo(0);
    assertThat(file.links()).isEqualTo(0);
  }",1
"@Test
  public void testRegularFile() {
    File file = regularFile(10);
    assertThat(file.isDirectory()).isFalse();
    assertThat(file.isRegularFile()).isTrue();
    assertThat(file.isSymbolicLink()).isFalse();
  }",1
"@Test
  public void testLookup_absolute_finalSymlink_nofollowLinks() throws IOException {
    assertExists(lookup(""/work/four/five"", NOFOLLOW_LINKS), ""four"", ""five"");
    assertExists(lookup(""/work/four/six"", NOFOLLOW_LINKS), ""four"", ""six"");
    assertExists(lookup(""/work/four/loop"", NOFOLLOW_LINKS), ""four"", ""loop"");
  }",1
"@Test
  public void testLookup_absolute_intermediateSymlink() throws IOException {
    assertExists(lookup(""/work/four/five/bar""), ""foo"", ""bar"");
    assertExists(lookup(""/work/four/six/two/three""), ""two"", ""three"");

    // NOFOLLOW_LINKS doesn't affect intermediate symlinks
    assertExists(lookup(""/work/four/five/bar"", NOFOLLOW_LINKS), ""foo"", ""bar"");
    assertExists(lookup(""/work/four/six/two/three"", NOFOLLOW_LINKS), ""two"", ""three"");
  }",1
"@Test
  public void testLookup_absolute_parentExists() throws IOException {
    assertParentExists(lookup(""/a""), ""/"");
    assertParentExists(lookup(""/foo/baz""), ""foo"");
    assertParentExists(lookup(""$c""), ""$"");
    assertParentExists(lookup(""$a/b/c/d""), ""c"");
  }",1
"@Test
  public void testFullDisk_doesNotAllocatePartiallyWhenTooManyBlocksRequested() throws IOException {
    HeapDisk disk = new HeapDisk(4, 10, 4);
    disk.allocate(blocks, 6);

    RegularFile blocks2 = RegularFile.create(-2, fileTimeSource.now(), disk);

    try {
      disk.allocate(blocks2, 5);
      fail();
    }",1
"@Test
  public void testAsyncChannel() throws Throwable {
    RegularFile file = regularFile(15);
    ExecutorService executor = Executors.newSingleThreadExecutor();
    JimfsAsynchronousFileChannel channel = channel(file, executor, READ, WRITE);

    try {
      assertEquals(15, channel.size());

      assertSame(channel, channel.truncate(5));
      assertEquals(5, channel.size());

      file.write(5, new byte[5], 0, 5);
      checkAsyncRead(channel);
      checkAsyncWrite(channel);
      checkAsyncLock(channel);

      channel.close();
      assertFalse(channel.isOpen());
    }",1
"@Test
  public void testAsyncClose_read() throws Throwable {
    RegularFile file = regularFile(15);
    ExecutorService executor = Executors.newFixedThreadPool(2);

    try {
      JimfsAsynchronousFileChannel channel = channel(file, executor, READ, WRITE);

      file.writeLock().lock(); // cause another thread trying to read to block

      // future-returning read
      Future<Integer> future = channel.read(ByteBuffer.allocate(10), 0);

      // completion handler read
      SettableFuture<Integer> completionHandlerFuture = SettableFuture.create();
      channel.read(ByteBuffer.allocate(10), 0, null, setFuture(completionHandlerFuture));

      // Despite this 10ms sleep to allow plenty of time, it's possible, though very rare, for a
      // race to cause the channel to be closed before the asynchronous calls get to the initial
      // check that the channel is open, causing ClosedChannelException to be thrown rather than
      // AsynchronousCloseException. This is not a problem in practice, just a quirk of how these
      // tests work and that we don't have a way of waiting for the operations to get past that
      // check.
      Uninterruptibles.sleepUninterruptibly(10, MILLISECONDS);

      channel.close();

      assertAsynchronousClose(future);
      assertAsynchronousClose(completionHandlerFuture);
    }",1
"@Test
  public void testAsyncClose_write() throws Throwable {
    RegularFile file = regularFile(15);
    ExecutorService executor = Executors.newFixedThreadPool(4);

    try {
      JimfsAsynchronousFileChannel channel = channel(file, executor, READ, WRITE);

      file.writeLock().lock(); // cause another thread trying to write to block

      // future-returning write
      Future<Integer> future = channel.write(ByteBuffer.allocate(10), 0);

      // completion handler write
      SettableFuture<Integer> completionHandlerFuture = SettableFuture.create();
      channel.write(ByteBuffer.allocate(10), 0, null, setFuture(completionHandlerFuture));

      // Despite this 10ms sleep to allow plenty of time, it's possible, though very rare, for a
      // race to cause the channel to be closed before the asynchronous calls get to the initial
      // check that the channel is open, causing ClosedChannelException to be thrown rather than
      // AsynchronousCloseException. This is not a problem in practice, just a quirk of how these
      // tests work and that we don't have a way of waiting for the operations to get past that
      // check.
      Uninterruptibles.sleepUninterruptibly(10, MILLISECONDS);

      channel.close();

      assertAsynchronousClose(future);
      assertAsynchronousClose(completionHandlerFuture);
    }",1
"@Test
  public void testPosition() throws IOException {
    FileChannel channel = channel(regularFile(10), READ);
    assertEquals(0, channel.position());
    assertSame(channel, channel.position(100));
    assertEquals(100, channel.position());
  }",1
"@Test
  public void testRead() throws IOException {
    RegularFile file = regularFile(20);
    FileChannel channel = channel(file, READ);
    assertEquals(0, channel.position());

    ByteBuffer buf = buffer(""1234567890"");
    ByteBuffer buf2 = buffer(""123457890"");
    assertEquals(10, channel.read(buf));
    assertEquals(10, channel.position());

    buf.flip();
    assertEquals(10, channel.read(new ByteBuffer[] {buf, buf2}",1
"@Test
  public void testReadNegative() throws IOException {
    FileChannel channel = channel(regularFile(0), READ, WRITE);

    try {
      channel.read(buffer(""111""), -1);
      fail();
    }",1
"@Test
  public void testReadsInWriteOnlyMode() throws IOException {
    FileChannel channel = channel(regularFile(0), WRITE);

    try {
      channel.read(buffer(""111""));
      fail();
    }",1
"@Test
  public void testSize() throws IOException {
    RegularFile file = regularFile(10);
    FileChannel channel = channel(file, READ);

    assertEquals(10, channel.size());

    file.write(10, new byte[90], 0, 90);
    assertEquals(100, channel.size());
  }",1
"@Test
  public void testTransferFromNegative() throws IOException {
    FileChannel channel = channel(regularFile(0), READ, WRITE);

    try {
      channel.transferFrom(new ByteBufferChannel(10), -1, 0);
      fail();
    }",1
"@Test
  public void testCloseCancelsAllKeysAndStopsPolling() throws IOException {
    Key key1 = watcher.register(createDirectory(), ImmutableList.of(ENTRY_CREATE));
    Key key2 = watcher.register(createDirectory(), ImmutableList.of(ENTRY_DELETE));

    assertThat(key1.isValid()).isTrue();
    assertThat(key2.isValid()).isTrue();
    assertThat(watcher.isPolling()).isTrue();

    watcher.close();

    assertThat(key1.isValid()).isFalse();
    assertThat(key2.isValid()).isFalse();
    assertThat(watcher.isPolling()).isFalse();
  }",1
"@Test
  public void testInitialAttributes() {
    // no initial attributes
    assertThat(ImmutableList.copyOf(file.getAttributeKeys())).isEmpty();
    assertThat(provider.attributes(file)).isEmpty();
  }",1
"@Test
  public void testSetOnCreate() {
    assertSetFailsOnCreate(""anything"", new byte[0]);
  }",1
"@Test
    public void testDatabaseCreation() {
        DatabaseSynchronizer dbSynchronizer = new DatabaseSynchronizer();

        dbSynchronizer.drop();
        assertFalse(dbSynchronizer.databaseExists());

        dbSynchronizer.create();
        assertTrue(dbSynchronizer.databaseExists());

        dbSynchronizer.drop();
    }",1
"@Test
    public void testEndpointFilename() {
        assertEquals(""person/Person.java"", new EndpointNaming(""Person"").getFilename());
        assertEquals(""personaddress/PersonAddress.java"", new EndpointNaming(""PersonAddress"").getFilename());
    }",1
"@Test
    public void testEndpointPath() {
        assertEquals(""people"", new EndpointNaming(""Person"").getPath());
        assertEquals(""person-addresses"", new EndpointNaming(""PersonAddress"").getPath());
        assertEquals(""parents"", new EndpointNaming(""Parent"").getPath());
        assertEquals(""parent-addresses"", new EndpointNaming(""ParentAddress"").getPath());
        assertEquals(""children"", new EndpointNaming(""Child"").getPath());
        assertEquals(""grandchildren"", new EndpointNaming(""Grandchild"").getPath());
    }",1
"@Test
    public void testShowFacade() {
        saveObject(1l, ""xpto"", 10);

        login(""amy"");

        String json = get(""/shielded_objects/1"");
        ShieldedObject retrievedObject = from(json, ShieldedObject.class);

        assertEquals(""xpto"", retrievedObject.getStringValue());
        assertNull(retrievedObject.getIntValue());
    }",1
"@Test
    public void testPersistenceWithVisibility() throws Exception {
        // @formatter:off
        put(""sys.cpu.user "" + TEST_TIME + "" 1.0 tag1=value1 tag2=value2"",
                   ""sys.cpu.idle "" + (TEST_TIME + 1) + "" 1.0 tag3=value3 tag4=value4 viz=(A|B)"",
                   ""sys.cpu.idle "" + (TEST_TIME + 2) + "" 1.0 tag3=value3 tag4=value4 viz=(C&B)"");
        // @formatter:on
        dataStore.flush();
        sleepUninterruptibly(WAIT_SECONDS, TimeUnit.SECONDS);
        int count = 0;
        for (final Map.Entry<Key,Value> entry : accumuloClient.createScanner(timelyProperties.getMetricsTable(), Authorizations.EMPTY)) {
            final double value = ByteBuffer.wrap(entry.getValue().get()).getDouble();
            assertEquals(1.0, value, 1e-9);
            count++;
        }",1
"@Test
    public void testClientAuthAccess() throws Exception {
        WebSocketSubscriptionClient client = new WebSocketSubscriptionClient(outboundSSLContext, ""localhost"", httpProperties.getPort(),
                        websocketProperties.getPort(), true, false, false, 65536);
        testWorkflow(client);
    }",1
"@Test
    public void testMultipleAgeOffWithoutCache() throws Exception {
        cacheProperties.setEnabled(false);
        testMultipleAgeOff(timelyProperties, cacheProperties);
    }",1
"@Test
    public void testMetrics() throws Exception {
      // @formatter:off
        put(""sys.cpu.user "" + TEST_TIME + "" 1.0 tag1=value1 tag2=value2"",
            ""sys.cpu.idle "" + (TEST_TIME + 1) + "" 1.0 tag3=value3 tag4=value4"",
            ""sys.cpu.idle "" + (TEST_TIME + 2) + "" 1.0 tag3=value3 tag4=value4 viz=(a|b|c)"",
            ""zzzz 1234567892 1.0 host=localhost"");
        // @formatter:on
        dataStore.flush();
        dataStoreCache.flushCaches(-1);
        // Latency in TestConfiguration is 2s, wait for it
        sleepUninterruptibly(TestConfiguration.WAIT_SECONDS, TimeUnit.SECONDS);

        String metrics = baseUrl + ""/api/metrics"";
        // Test prefix matching
        String result = query(metrics);
        Document doc = Jsoup.parse(result);
        Elements tableData = doc.select(""td"");

        assertEquals(1, tableData.select("":contains(sys.cpu.user)"").size());
        assertEquals(1, tableData.select("":contains(tag1=value1 tag2=value2)"").size());
        assertEquals(1, tableData.select("":contains(sys.cpu.idle)"").size());
        assertEquals(1, tableData.select("":contains(tag3=value3 tag4=value4)"").size());
        assertEquals(1, tableData.select("":contains(zzzz)"").size());
        assertEquals(1, tableData.select("":contains(host=localhost)"").size());
    }",1
"@Test
    public void testPutMetric() throws Exception {
        Metric m = Metric.newBuilder().name(""sys.cpu.user"").value(TEST_TIME, 1.0D).tag(new Tag(""tag1"", ""value1"")).build();
        new Metric();
        URL url = new URL(baseUrl + ""/api/put"");
        HttpsURLConnection con = getUrlConnection(url);
        con.setRequestMethod(""POST"");
        con.setDoOutput(true);
        con.setRequestProperty(""Content-Type"", ""application/json"");
        String requestJSON = JsonUtil.getObjectMapper().writeValueAsString(m);
        con.setRequestProperty(""Content-Length"", String.valueOf(requestJSON.length()));
        OutputStream wr = con.getOutputStream();
        wr.write(requestJSON.getBytes(UTF_8));
        int responseCode = con.getResponseCode();
        Assert.assertEquals(200, responseCode);
    }",1
"@Test
    public void testQueryWithNoTagsMultipleSeries() throws Exception {
        // @formatter:off
        put(""sys.cpu.user "" + TEST_TIME + "" 1.0 tag1=value1 tag2=value2 host=h1"",
           ""sys.cpu.user "" + TEST_TIME + "" 2.0 tag1=value1 tag2=value2 host=h2"",
           ""sys.cpu.user "" + (TEST_TIME + 1000) + "" 4.0 tag1=value1 tag2=value2 host=h1"",
           ""sys.cpu.user "" + (TEST_TIME + 1000) + "" 3.0 tag1=value1 tag2=value2 host=h2"",
           ""sys.cpu.user "" + (TEST_TIME + 2000) + "" 5.0 tag1=value1 tag2=value2 host=h1"",
           ""sys.cpu.user "" + (TEST_TIME + 2000) + "" 6.0 tag1=value1 tag2=value2 host=h2"");
        // @formatter:on
        dataStore.flush();
        dataStoreCache.flushCaches(-1);
        // Latency in TestConfiguration is 2s, wait for it
        sleepUninterruptibly(TestConfiguration.WAIT_SECONDS, TimeUnit.SECONDS);
        QueryRequest request = new QueryRequest();
        request.setStart(TEST_TIME);
        request.setEnd(TEST_TIME + 4000);
        SubQuery subQuery = new SubQuery();
        subQuery.setMetric(""sys.cpu.user"");
        subQuery.setDownsample(Optional.of(""1s-max""));
        request.addQuery(subQuery);
        List<QueryResponse> response = query(baseUrl + ""/api/query"", request);
        assertEquals(1, response.size());
        Map<String,String> tags = response.get(0).getTags();
        assertEquals(0, tags.size());
        Map<String,Object> dps = response.get(0).getDps();
        assertEquals(3, dps.size());
        Iterator<Entry<String,Object>> entries = dps.entrySet().iterator();
        Entry<String,Object> entry = entries.next();
        assertEquals(Long.toString((TEST_TIME / 1000)), entry.getKey());
        assertEquals(2.0, entry.getValue());
        entry = entries.next();
        assertEquals(Long.toString((TEST_TIME / 1000) + 1), entry.getKey());
        assertEquals(4.0, entry.getValue());
        entry = entries.next();
        assertEquals(Long.toString((TEST_TIME / 1000) + 2), entry.getKey());
        assertEquals(6.0, entry.getValue());
    }",1
"@Test
    public void testQueryWithoutMsResolution() throws Exception {
        // @formatter:off
        put(""sys.cpu.user "" + TEST_TIME + "" 1.0 tag1=value1 tag2=value2"",
           ""sys.cpu.user "" + (TEST_TIME + 1) + "" 1.0 tag3=value3"",
           ""sys.cpu.idle "" + (TEST_TIME + 2) + "" 1.0 tag3=value3 tag4=value4"",
           ""sys.cpu.idle "" + (TEST_TIME + 1000) + "" 3.0 tag3=value3 tag4=value4"");
        // @formatter:on
        dataStore.flush();
        dataStoreCache.flushCaches(-1);
        // Latency in TestConfiguration is 2s, wait for it
        sleepUninterruptibly(TestConfiguration.WAIT_SECONDS, TimeUnit.SECONDS);
        QueryRequest request = new QueryRequest();
        request.setStart(TEST_TIME);
        request.setEnd(TEST_TIME + 6000);
        SubQuery subQuery = new SubQuery();
        subQuery.setMetric(""sys.cpu.idle"");
        subQuery.setTags(Collections.singletonMap(""tag3"", ""value3""));
        subQuery.setDownsample(Optional.of(""1s-max""));
        request.addQuery(subQuery);
        List<QueryResponse> response = query(baseUrl + ""/api/query"", request);
        assertEquals(1, response.size());
        Map<String,String> tags = response.get(0).getTags();
        assertEquals(1, tags.size());
        assertTrue(tags.containsKey(""tag3""));
        assertTrue(tags.get(""tag3"").equals(""value3""));
        Map<String,Object> dps = response.get(0).getDps();
        assertEquals(2, dps.size());
        Iterator<Entry<String,Object>> entries = dps.entrySet().iterator();
        Entry<String,Object> entry = entries.next();
        assertEquals(Long.toString((TEST_TIME / 1000)), entry.getKey());
        assertEquals(1.0, entry.getValue());
        entry = entries.next();
        assertEquals(Long.toString((TEST_TIME / 1000) + 1), entry.getKey());
        assertEquals(3.0, entry.getValue());
    }",1
"@Test
    public void testQueryWithTagWildcard() throws Exception {
        // @formatter:off
        put(""sys.cpu.user "" + TEST_TIME + "" 1.0 tag1=value1 tag2=value2 rack=r1"",
            ""sys.cpu.user "" + (TEST_TIME + 1) + "" 1.0 tag3=value3 rack=r2"",
            ""sys.cpu.idle "" + (TEST_TIME + 2) + "" 1.0 tag3=value3 tag4=value4 rack=r1"",
            ""sys.cpu.idle "" + (TEST_TIME + 1000) + "" 3.0 tag3=value3 tag4=value4 rack=r2"");
        // @formatter:on
        dataStore.flush();
        dataStoreCache.flushCaches(-1);
        // Latency in TestConfiguration is 2s, wait for it
        sleepUninterruptibly(TestConfiguration.WAIT_SECONDS, TimeUnit.SECONDS);
        QueryRequest request = new QueryRequest();
        request.setStart(TEST_TIME);
        request.setEnd(TEST_TIME + 6000);
        SubQuery subQuery = new SubQuery();
        subQuery.setMetric(""sys.cpu.idle"");
        subQuery.setTags(Collections.singletonMap(""rack"", ""r.*""));
        subQuery.setDownsample(Optional.of(""1s-max""));
        request.addQuery(subQuery);

        List<QueryResponse> response = query(baseUrl + ""/api/query"", request);
        assertEquals(2, response.size());

        AtomicInteger rack1Count = new AtomicInteger(0);
        AtomicInteger rack2Count = new AtomicInteger(0);
        response.forEach(r -> {
            Map<String,String> tags = r.getTags();
            Map<String,Object> dps = r.getDps();
            assertEquals(1, tags.size());
            assertEquals(1, dps.size());
            assertTrue(tags.containsKey(""rack""));
            Value value = parseDps(dps);
            switch (tags.get(""rack"")) {
                case ""r2"":
                    assertEquals((Long) ((TEST_TIME / 1000L) + 1L), value.getTimestamp());
                    assertEquals(3.0D, value.getMeasure(), 0.0);
                    rack2Count.incrementAndGet();
                    break;
                case ""r1"":
                    assertEquals((Long) (TEST_TIME / 1000L), value.getTimestamp());
                    assertEquals(1.0D, value.getMeasure(), 0.0);
                    rack1Count.incrementAndGet();
                    break;
                default:
                    assertTrue(""Found invalid rack number: "" + tags.get(""rack""), false);
                    break;
            }",1
"@Test
    public void testHSTSRequestGet() throws Exception {
        String secureMe = baseHttpsUrl + ""/secure-me"";
        URL url = new URL(secureMe);
        HttpsURLConnection con = getUrlConnection(url);
        int responseCode = con.getResponseCode();
        assertEquals(404, responseCode);
        assertEquals(""max-age="" + httpProperties.getStrictTransportMaxAge(), con.getHeaderField(StrictTransportHandler.HSTS_HEADER_NAME));
    }",1
"@Test
    public void testPutMultiple() throws Exception {

        try (Socket sock = new Socket(serverProperties.getIp(), serverProperties.getTcpPort());
                PrintWriter writer = new PrintWriter(sock.getOutputStream(), true)) {
            // @formatter:off
            writer.write(""put sys.cpu.user "" + TEST_TIME + "" 1.0 tag1=value1 tag2=value2\n""
                       + ""put sys.cpu.idle "" + (TEST_TIME + 1) + "" 1.0 tag3=value3 tag4=value4\n"");
            writer.flush();
            while (2 != tcpRequests.getCount()) {
                Thread.sleep(5);
            }",1
"@Test
    public void testPersistenceWithVisibility() throws Exception {
        put(""sys.cpu.user "" + TEST_TIME + "" 1.0 tag1=value1 tag2=value2"", ""sys.cpu.idle "" + (TEST_TIME + 1) + "" 1.0 tag3=value3 tag4=value4 viz=(a|b)"",
                        ""sys.cpu.idle "" + (TEST_TIME + 2) + "" 1.0 tag3=value3 tag4=value4 viz=(c&b)"");
        sleepUninterruptibly(TestConfiguration.WAIT_SECONDS, TimeUnit.SECONDS);
        accumuloClient.securityOperations().changeUserAuthorizations(""root"", new Authorizations(""a"", ""b"", ""c""));

        int count = 0;
        for (final Map.Entry<Key,Value> entry : accumuloClient.createScanner(""timely.metrics"", Authorizations.EMPTY)) {
            log.debug(""Entry: "" + entry);
            final double value = ByteBuffer.wrap(entry.getValue().get()).getDouble();
            assertEquals(1.0, value, 1e-9);
            count++;
        }",1
"@Test
    public void testPutMultipleBinary() throws Exception {

        FlatBufferBuilder builder = new FlatBufferBuilder(1);

        int[] metric = new int[2];
        Map<String,String> t = new HashMap<>();
        t.put(""tag1"", ""value1"");
        t.put(""tag2"", ""value2"");
        metric[0] = createMetric(builder, ""sys.cpu.user"", TEST_TIME, 1.0D, t);
        t = new HashMap<>();
        t.put(""tag3"", ""value3"");
        t.put(""tag4"", ""value4"");
        metric[1] = createMetric(builder, ""sys.cpu.idle"", TEST_TIME + 1, 1.0D, t);

        int metricVector = timely.api.flatbuffer.Metrics.createMetricsVector(builder, metric);

        timely.api.flatbuffer.Metrics.startMetrics(builder);
        timely.api.flatbuffer.Metrics.addMetrics(builder, metricVector);
        int metrics = timely.api.flatbuffer.Metrics.endMetrics(builder);
        timely.api.flatbuffer.Metrics.finishMetricsBuffer(builder, metrics);

        ByteBuffer binary = builder.dataBuffer();
        byte[] data = new byte[binary.remaining()];
        binary.get(data, 0, binary.remaining());
        log.debug(""Sending {}",1
"@Test
  public void testGetDeletableFiles() throws IOException {
    // 1. Create a file
    Path file = new Path(root, ""testIsFileDeletableWithNoHFileRefs"");
    fs.createNewFile(file);
    // 2. Assert file is successfully created
    assertTrue(""Test file not created!"", fs.exists(file));
    BackupHFileCleaner cleaner = new BackupHFileCleaner();
    cleaner.setConf(conf);
    cleaner.setCheckForFullyBackedUpTables(false);
    List<FileStatus> stats = new ArrayList<>();
    // Prime the cleaner
    cleaner.getDeletableFiles(stats);
    // 3. Assert that file as is should be deletable
    FileStatus stat = fs.getFileStatus(file);
    stats.add(stat);
    Iterable<FileStatus> deletable = cleaner.getDeletableFiles(stats);
    boolean found = false;
    for (FileStatus stat1 : deletable) {
      if (stat.equals(stat1)) {
        found = true;
      }",1
"@Test
  public void testOperationJSON() throws IOException {
    // produce a Scan Operation
    Scan scan = new Scan().withStartRow(ROW);
    scan.addColumn(FAMILY, QUALIFIER);
    // get its JSON representation, and parse it
    String json = scan.toJSON();
    Type typeOfHashMap = new TypeToken<Map<String, Object>>() {
    }",1
"@Test
  public void testQuery() throws Exception {
    String result = readOutput(new URL(baseUrl, ""/jmx?qry=java.lang:type=Runtime""));
    LOG.info(""/jmx?qry=java.lang:type=Runtime RESULT: "" + result);
    assertReFind(""\""name\""\\s*:\\s*\""java.lang:type=Runtime\"""", result);
    assertReFind(""\""modelerType\"""", result);

    result = readOutput(new URL(baseUrl, ""/jmx?qry=java.lang:type=Memory""));
    LOG.info(""/jmx?qry=java.lang:type=Memory RESULT: "" + result);
    assertReFind(""\""name\""\\s*:\\s*\""java.lang:type=Memory\"""", result);
    assertReFind(""\""modelerType\"""", result);

    result = readOutput(new URL(baseUrl, ""/jmx""));
    LOG.info(""/jmx RESULT: "" + result);
    assertReFind(""\""name\""\\s*:\\s*\""java.lang:type=Memory\"""", result);

    // test to get an attribute of a mbean
    result = readOutput(new URL(baseUrl, ""/jmx?get=java.lang:type=Memory::HeapMemoryUsage""));
    LOG.info(""/jmx RESULT: "" + result);
    assertReFind(""\""name\""\\s*:\\s*\""java.lang:type=Memory\"""", result);
    assertReFind(""\""committed\""\\s*:"", result);

    // negative test to get an attribute of a mbean
    result = readOutput(new URL(baseUrl, ""/jmx?get=java.lang:type=Memory::""));
    LOG.info(""/jmx RESULT: "" + result);
    assertReFind(""\""ERROR\"""", result);

    // test to get JSONP result
    result = readOutput(new URL(baseUrl, ""/jmx?qry=java.lang:type=Memory&callback=mycallback1""));
    LOG.info(""/jmx?qry=java.lang:type=Memory&callback=mycallback RESULT: "" + result);
    assertReFind(""^mycallback1\\(\\{"", result);
    assertReFind(""\\}",1
"@Test
  public void testRetrieveFromFile() throws Exception {
    Path testDir = createAndGetTestDir();
    String ioEngineName = ""file:"" + testDir + ""/bucket.cache"";
    testRetrievalUtils(testDir, ioEngineName);
    int[] smallBucketSizes = new int[] { 3 * 1024, 5 * 1024 }",1
"@Test
  public void testHDFSLinkReadDuringRename() throws Exception {
    HBaseTestingUtil testUtil = new HBaseTestingUtil();
    Configuration conf = testUtil.getConfiguration();
    conf.setInt(""dfs.blocksize"", 1024 * 1024);
    conf.setInt(""dfs.client.read.prefetch.size"", 2 * 1024 * 1024);

    testUtil.startMiniDFSCluster(1);
    MiniDFSCluster cluster = testUtil.getDFSCluster();
    FileSystem fs = cluster.getFileSystem();
    assertEquals(""hdfs"", fs.getUri().getScheme());

    try {
      testLinkReadDuringRename(fs, testUtil.getDefaultRootDirPath());
    }",1
"@Test
  public void testHalfScanAndReseek() throws Exception {
    ResourceLeakDetector.setLevel(ResourceLeakDetector.Level.PARANOID);
    Configuration conf = TEST_UTIL.getConfiguration();
    FileSystem fs = FileSystem.get(conf);
    String root_dir = TEST_UTIL.getDataTestDir().toString();
    Path parentPath = new Path(new Path(root_dir, ""parent""), ""CF"");
    fs.mkdirs(parentPath);
    String tableName = Paths.get(root_dir).getFileName().toString();
    RegionInfo splitAHri = RegionInfoBuilder.newBuilder(TableName.valueOf(tableName)).build();
    Thread.currentThread().sleep(1000);
    RegionInfo splitBHri = RegionInfoBuilder.newBuilder(TableName.valueOf(tableName)).build();
    Path splitAPath = new Path(new Path(root_dir, splitAHri.getRegionNameAsString()), ""CF"");
    Path splitBPath = new Path(new Path(root_dir, splitBHri.getRegionNameAsString()), ""CF"");
    Path filePath = StoreFileWriter.getUniqueFile(fs, parentPath);
    String ioEngineName = ""file:"" + TEST_UTIL.getDataTestDir() + ""/bucketNoRecycler.cache"";
    BucketCache bucketCache = new BucketCache(ioEngineName, 32 * 1024 * 1024, 1024,
      new int[] { 4 * 1024, 8 * 1024, 64 * 1024, 96 * 1024 }",1
"@Test
  public void testNettyRpcServer() throws Exception {
    doTest(name.getTableName());
  }",1
"@Test
  public void testExcludeAllFromMinorCompaction() throws Exception {
    Configuration conf = util.getConfiguration();
    conf.setInt(""hbase.hstore.compaction.min"", 2);
    generateRandomStartKeys(5);

    util.startMiniCluster();
    try (Connection conn = ConnectionFactory.createConnection(); Admin admin = conn.getAdmin();
      Table table = util.createTable(TABLE_NAMES[0], FAMILIES);
      RegionLocator locator = conn.getRegionLocator(TABLE_NAMES[0])) {
      final FileSystem fs = util.getDFSCluster().getFileSystem();
      assertEquals(""Should start with empty table"", 0, util.countRows(table));

      // deep inspection: get the StoreFile dir
      final Path storePath =
        new Path(CommonFSUtils.getTableDir(CommonFSUtils.getRootDir(conf), TABLE_NAMES[0]),
          new Path(admin.getRegions(TABLE_NAMES[0]).get(0).getEncodedName(),
            Bytes.toString(FAMILIES[0])));
      assertEquals(0, fs.listStatus(storePath).length);

      // Generate two bulk load files
      conf.setBoolean(""hbase.mapreduce.hfileoutputformat.compaction.exclude"", true);

      for (int i = 0; i < 2; i++) {
        Path testDir = util.getDataTestDirOnTestFS(""testExcludeAllFromMinorCompaction_"" + i);
        runIncrementalPELoad(conf,
          Arrays.asList(new HFileOutputFormat2.TableInfo(table.getDescriptor(),
            conn.getRegionLocator(TABLE_NAMES[0]))),
          testDir, false);
        // Perform the actual load
        BulkLoadHFiles.create(conf).bulkLoad(table.getName(), testDir);
      }",1
"@Test
  public void testWritingPEData() throws Exception {
    Configuration conf = util.getConfiguration();
    Path testDir = util.getDataTestDirOnTestFS(""testWritingPEData"");
    FileSystem fs = testDir.getFileSystem(conf);

    // Set down this value or we OOME in eclipse.
    conf.setInt(""mapreduce.task.io.sort.mb"", 20);
    // Write a few files.
    long hregionMaxFilesize = 10 * 1024;
    conf.setLong(HConstants.HREGION_MAX_FILESIZE, hregionMaxFilesize);

    Job job = new Job(conf, ""testWritingPEData"");
    setupRandomGeneratorMapper(job, false);
    // This partitioner doesn't work well for number keys but using it anyways
    // just to demonstrate how to configure it.
    byte[] startKey = new byte[RandomKVGeneratingMapper.KEYLEN_DEFAULT];
    byte[] endKey = new byte[RandomKVGeneratingMapper.KEYLEN_DEFAULT];

    Arrays.fill(startKey, (byte) 0);
    Arrays.fill(endKey, (byte) 0xff);

    job.setPartitionerClass(SimpleTotalOrderPartitioner.class);
    // Set start and end rows for partitioner.
    SimpleTotalOrderPartitioner.setStartKey(job.getConfiguration(), startKey);
    SimpleTotalOrderPartitioner.setEndKey(job.getConfiguration(), endKey);
    job.setReducerClass(CellSortReducer.class);
    job.setOutputFormatClass(HFileOutputFormat2.class);
    job.setNumReduceTasks(4);
    job.getConfiguration().setStrings(""io.serializations"", conf.get(""io.serializations""),
      MutationSerialization.class.getName(), ResultSerialization.class.getName(),
      CellSerialization.class.getName());

    FileOutputFormat.setOutputPath(job, testDir);
    assertTrue(job.waitForCompletion(false));
    FileStatus[] files = fs.listStatus(testDir);
    assertTrue(files.length > 0);

    // check output file num and size.
    for (byte[] family : FAMILIES) {
      long kvCount = 0;
      RemoteIterator<LocatedFileStatus> iterator =
        fs.listFiles(testDir.suffix(""/"" + new String(family)), true);
      while (iterator.hasNext()) {
        LocatedFileStatus keyFileStatus = iterator.next();
        HFile.Reader reader =
          HFile.createReader(fs, keyFileStatus.getPath(), new CacheConfig(conf), true, conf);
        HFileScanner scanner = reader.getScanner(conf, false, false, false);

        kvCount += reader.getEntries();
        scanner.seekTo();
        long perKVSize = scanner.getCell().getSerializedSize();
        assertTrue(""Data size of each file should not be too large."",
          perKVSize * reader.getEntries() <= hregionMaxFilesize);
      }",1
"@Test
  public void testCPRequestCost() {
    // in order to pass needsBalance judgement
    conf.setFloat(""hbase.master.balancer.stochastic.cpRequestCost"", 10000f);
    loadBalancer.onConfigurationChange(conf);
    // mock cluster State
    Map<ServerName, List<RegionInfo>> clusterState = new HashMap<ServerName, List<RegionInfo>>();
    ServerName serverA = randomServer(3).getServerName();
    ServerName serverB = randomServer(3).getServerName();
    ServerName serverC = randomServer(3).getServerName();
    List<RegionInfo> regionsOnServerA = randomRegions(3);
    List<RegionInfo> regionsOnServerB = randomRegions(3);
    List<RegionInfo> regionsOnServerC = randomRegions(3);
    clusterState.put(serverA, regionsOnServerA);
    clusterState.put(serverB, regionsOnServerB);
    clusterState.put(serverC, regionsOnServerC);
    // mock ClusterMetrics
    Map<ServerName, ServerMetrics> serverMetricsMap = new TreeMap<>();
    serverMetricsMap.put(serverA, mockServerMetricsWithCpRequests(regionsOnServerA, 0));
    serverMetricsMap.put(serverB, mockServerMetricsWithCpRequests(regionsOnServerB, 0));
    serverMetricsMap.put(serverC, mockServerMetricsWithCpRequests(regionsOnServerC, 0));
    ClusterMetrics clusterStatus = mock(ClusterMetrics.class);
    when(clusterStatus.getLiveServerMetrics()).thenReturn(serverMetricsMap);
    loadBalancer.updateClusterMetrics(clusterStatus);

    // CPRequestCostFunction are Rate based, So doing setClusterMetrics again
    // this time, regions on serverA with more cpRequestCount load
    // serverA : 1000,1000,1000
    // serverB : 0,0,0
    // serverC : 0,0,0
    // so should move two regions from serverA to serverB & serverC
    serverMetricsMap = new TreeMap<>();
    serverMetricsMap.put(serverA, mockServerMetricsWithCpRequests(regionsOnServerA, 1000));
    serverMetricsMap.put(serverB, mockServerMetricsWithCpRequests(regionsOnServerB, 0));
    serverMetricsMap.put(serverC, mockServerMetricsWithCpRequests(regionsOnServerC, 0));
    clusterStatus = mock(ClusterMetrics.class);
    when(clusterStatus.getLiveServerMetrics()).thenReturn(serverMetricsMap);
    loadBalancer.updateClusterMetrics(clusterStatus);

    List<RegionPlan> plans =
      loadBalancer.balanceTable(HConstants.ENSEMBLE_TABLE_NAME, clusterState);
    Set<RegionInfo> regionsMoveFromServerA = new HashSet<>();
    Set<ServerName> targetServers = new HashSet<>();
    for (RegionPlan plan : plans) {
      if (plan.getSource().equals(serverA)) {
        regionsMoveFromServerA.add(plan.getRegionInfo());
        targetServers.add(plan.getDestination());
      }",1
"@Test
  public void testMinimumNumberOfThreads() throws Exception {
    Configuration conf = UTIL.getConfiguration();
    String confKey = ""hbase.test.cleaner.delegates"";
    conf.set(confKey, AlwaysDelete.class.getName());
    conf.set(CleanerChore.CHORE_POOL_SIZE, ""2"");
    int numProcs = Runtime.getRuntime().availableProcessors();
    // Sanity
    assertEquals(numProcs, CleanerChore.calculatePoolSize(Integer.toString(numProcs)));
    // The implementation does not allow us to set more threads than we have processors
    assertEquals(numProcs, CleanerChore.calculatePoolSize(Integer.toString(numProcs + 2)));
    // Force us into the branch that is multiplying 0.0 against the number of processors
    assertEquals(1, CleanerChore.calculatePoolSize(""0.0""));
  }",1
"@Test
  public void testStoppedCleanerDoesNotDeleteFiles() throws Exception {
    Stoppable stop = new StoppableImplementation();
    Configuration conf = UTIL.getConfiguration();
    Path testDir = UTIL.getDataTestDir();
    FileSystem fs = UTIL.getTestFileSystem();
    String confKey = ""hbase.test.cleaner.delegates"";
    conf.set(confKey, AlwaysDelete.class.getName());

    AllValidPaths chore =
      new AllValidPaths(""test-file-cleaner"", stop, conf, fs, testDir, confKey, POOL);

    // also create a file in the top level directory
    Path topFile = new Path(testDir, ""topFile"");
    fs.create(topFile).close();
    assertTrue(""Test file didn't get created."", fs.exists(topFile));

    // stop the chore
    stop.stop(""testing stop"");

    // run the chore
    chore.chore();

    // test that the file still exists
    assertTrue(""File got deleted while chore was stopped"", fs.exists(topFile));
  }",1
"@Test
  public void testRemovesEmptyDirectories() throws Exception {
    Configuration conf = UTIL.getConfiguration();
    // no cleaner policies = delete all files
    conf.setStrings(HFileCleaner.MASTER_HFILE_CLEANER_PLUGINS, """");
    Server server = new DummyServer();
    Path archivedHfileDir =
      new Path(UTIL.getDataTestDirOnTestFS(), HConstants.HFILE_ARCHIVE_DIRECTORY);

    // setup the cleaner
    FileSystem fs = UTIL.getDFSCluster().getFileSystem();
    HFileCleaner cleaner = new HFileCleaner(1000, server, conf, fs, archivedHfileDir, POOL);

    // make all the directories for archiving files
    Path table = new Path(archivedHfileDir, ""table"");
    Path region = new Path(table, ""regionsomthing"");
    Path family = new Path(region, ""fam"");
    Path file = new Path(family, ""file12345"");
    fs.mkdirs(family);
    if (!fs.exists(family)) throw new RuntimeException(""Couldn't create test family:"" + family);
    fs.create(file).close();
    if (!fs.exists(file)) throw new RuntimeException(""Test file didn't get created:"" + file);

    // run the chore to cleanup the files (and the directories above it)
    cleaner.chore();

    // make sure all the parent directories get removed
    assertFalse(""family directory not removed for empty directory"", fs.exists(family));
    assertFalse(""region directory not removed for empty directory"", fs.exists(region));
    assertFalse(""table directory not removed for empty directory"", fs.exists(table));
    assertTrue(""archive directory"", fs.exists(archivedHfileDir));
  }",1
"@Test
  public void testThreadCleanup() throws Exception {
    Configuration conf = UTIL.getConfiguration();
    conf.setStrings(HFileCleaner.MASTER_HFILE_CLEANER_PLUGINS, """");
    Server server = new DummyServer();
    Path archivedHfileDir =
      new Path(UTIL.getDataTestDirOnTestFS(), HConstants.HFILE_ARCHIVE_DIRECTORY);

    // setup the cleaner
    FileSystem fs = UTIL.getDFSCluster().getFileSystem();
    HFileCleaner cleaner = new HFileCleaner(1000, server, conf, fs, archivedHfileDir, POOL);
    // clean up archive directory
    fs.delete(archivedHfileDir, true);
    fs.mkdirs(archivedHfileDir);
    // create some file to delete
    fs.createNewFile(new Path(archivedHfileDir, ""dfd-dfd""));
    // launch the chore
    cleaner.chore();
    // call cleanup
    cleaner.cleanup();
    // wait awhile for thread to die
    Thread.sleep(100);
    for (Thread thread : cleaner.getCleanerThreads()) {
      Assert.assertFalse(thread.isAlive());
    }",1
"@Test
  public void testTTLCleaner() throws IOException {
    FileSystem fs = UTIL.getDFSCluster().getFileSystem();
    Path root = UTIL.getDataTestDirOnTestFS();
    Path file = new Path(root, ""file"");
    fs.createNewFile(file);
    long createTime = EnvironmentEdgeManager.currentTime();
    assertTrue(""Test file not created!"", fs.exists(file));
    TimeToLiveHFileCleaner cleaner = new TimeToLiveHFileCleaner();
    // update the time info for the file, so the cleaner removes it
    fs.setTimes(file, createTime - 100, -1);
    Configuration conf = UTIL.getConfiguration();
    conf.setLong(TimeToLiveHFileCleaner.TTL_CONF_KEY, 100);
    cleaner.setConf(conf);
    assertTrue(""File not set deletable - check mod time:"" + getFileStats(file, fs)
      + "" with create time:"" + createTime, cleaner.isFileDeletable(fs.getFileStatus(file)));
  }",1
"@Test
  public void testModifyNamespaceWithInvalidTableCount() throws Exception {
    final NamespaceDescriptor nsd =
      NamespaceDescriptor.create(""testModifyNamespaceWithInvalidTableCount"").build();
    final String nsKey = ""hbase.namespace.quota.maxtables"";
    final String nsValue = ""-1"";
    final ProcedureExecutor<MasterProcedureEnv> procExec = getMasterProcedureExecutor();

    createNamespaceForTesting(nsd);

    // Modify
    nsd.setConfiguration(nsKey, nsValue);

    long procId =
      procExec.submitProcedure(new ModifyNamespaceProcedure(procExec.getEnvironment(), nsd));
    // Wait the completion
    ProcedureTestingUtility.waitProcedure(procExec, procId);
    Procedure<?> result = procExec.getResult(procId);
    assertTrue(result.isFailed());
    LOG.debug(""Modify namespace failed with exception: "" + result.getException());
    assertTrue(ProcedureTestingUtility.getExceptionCause(result) instanceof ConstraintException);
  }",1
"@Test
  public void testCloneSnapshot() throws Exception {
    String nsp = prefix + ""_testCloneSnapshot"";
    NamespaceDescriptor nspDesc =
      NamespaceDescriptor.create(nsp).addConfiguration(TableNamespaceManager.KEY_MAX_TABLES, ""2"")
        .addConfiguration(TableNamespaceManager.KEY_MAX_REGIONS, ""20"").build();
    ADMIN.createNamespace(nspDesc);
    assertNotNull(""Namespace descriptor found null."", ADMIN.getNamespaceDescriptor(nsp));
    TableName tableName = TableName.valueOf(nsp + TableName.NAMESPACE_DELIM + ""table1"");
    TableName cloneTableName = TableName.valueOf(nsp + TableName.NAMESPACE_DELIM + ""table2"");

    ColumnFamilyDescriptor columnFamilyDescriptor =
      ColumnFamilyDescriptorBuilder.newBuilder(Bytes.toBytes(""fam1"")).build();
    TableDescriptorBuilder tableDescOne = TableDescriptorBuilder.newBuilder(tableName);
    tableDescOne.setColumnFamily(columnFamilyDescriptor);

    ADMIN.createTable(tableDescOne.build(), Bytes.toBytes(""AAA""), Bytes.toBytes(""ZZZ""), 4);
    String snapshot = ""snapshot_testCloneSnapshot"";
    ADMIN.snapshot(snapshot, tableName);
    ADMIN.cloneSnapshot(snapshot, cloneTableName);

    int tableLength;
    try (RegionLocator locator = ADMIN.getConnection().getRegionLocator(tableName)) {
      tableLength = locator.getStartKeys().length;
    }",1
"@Test
  public void testCorruptedEntries() throws Exception {
    // Insert something
    for (int i = 0; i < 100; ++i) {
      procStore.insert(new TestSequentialProcedure(), null);
    }",1
"@Test
  public void testCorruptedProcedures() throws Exception {
    // Insert root-procedures
    TestProcedure[] rootProcs = new TestProcedure[10];
    for (int i = 1; i <= rootProcs.length; i++) {
      rootProcs[i - 1] = new TestProcedure(i, 0);
      procStore.insert(rootProcs[i - 1], null);
      rootProcs[i - 1].addStackId(0);
      procStore.update(rootProcs[i - 1]);
    }",1
"@Test
  public void testCorruptedTrailer() throws Exception {
    // Insert something
    for (int i = 0; i < 100; ++i) {
      procStore.insert(new TestSequentialProcedure(), null);
    }",1
"@Test
  public void testLoad() throws Exception {
    Set<Long> procIds = new HashSet<>();

    // Insert something in the log
    Procedure<?> proc1 = new TestSequentialProcedure();
    procIds.add(proc1.getProcId());
    procStore.insert(proc1, null);

    Procedure<?> proc2 = new TestSequentialProcedure();
    Procedure<?>[] child2 = new Procedure[2];
    child2[0] = new TestSequentialProcedure();
    child2[1] = new TestSequentialProcedure();

    procIds.add(proc2.getProcId());
    procIds.add(child2[0].getProcId());
    procIds.add(child2[1].getProcId());
    procStore.insert(proc2, child2);

    // Verify that everything is there
    verifyProcIdsOnRestart(procIds);

    // Update and delete something
    procStore.update(proc1);
    procStore.update(child2[1]);
    procStore.delete(child2[1].getProcId());
    procIds.remove(child2[1].getProcId());

    // Verify that everything is there
    verifyProcIdsOnRestart(procIds);

    // Remove 4 byte from the trailers
    procStore.stop(false);
    FileStatus[] logs = fs.listStatus(logDir);
    assertEquals(3, logs.length);
    for (int i = 0; i < logs.length; ++i) {
      corruptLog(logs[i], 4);
    }",1
"@Test
  public void testWalCleanerNoHoles() throws Exception {
    final Procedure<?>[] procs = new Procedure[5];
    ArrayList<ProcedureWALFile> logs = null;
    // Insert procedures and roll wal after every insert.
    for (int i = 0; i < procs.length; i++) {
      procs[i] = new TestSequentialProcedure();
      procStore.insert(procs[i], null);
      procStore.rollWriterForTesting();
      logs = procStore.getActiveLogs();
      assertEquals(i + 2, logs.size()); // Extra 1 for current ongoing wal.
    }",1
"@Test
  public void testWalCleanerSequentialClean() throws Exception {
    final Procedure<?>[] procs = new Procedure[5];
    ArrayList<ProcedureWALFile> logs = null;

    // Insert procedures and roll wal after every insert.
    for (int i = 0; i < procs.length; i++) {
      procs[i] = new TestSequentialProcedure();
      procStore.insert(procs[i], null);
      procStore.rollWriterForTesting();
      logs = procStore.getActiveLogs();
      assertEquals(logs.size(), i + 2); // Extra 1 for current ongoing wal.
    }",1
"@Test
  public void testWalCleanerUpdates() throws Exception {
    TestSequentialProcedure p1 = new TestSequentialProcedure();
    TestSequentialProcedure p2 = new TestSequentialProcedure();
    procStore.insert(p1, null);
    procStore.insert(p2, null);
    procStore.rollWriterForTesting();
    ProcedureWALFile firstLog = procStore.getActiveLogs().get(0);
    procStore.update(p1);
    procStore.rollWriterForTesting();
    procStore.update(p2);
    procStore.rollWriterForTesting();
    procStore.removeInactiveLogsForTesting();
    assertFalse(procStore.getActiveLogs().contains(firstLog));
  }",1
"@Test
  public void testWalCleanerWithEmptyRolls() throws Exception {
    final Procedure<?>[] procs = new Procedure[3];
    for (int i = 0; i < procs.length; ++i) {
      procs[i] = new TestSequentialProcedure();
      procStore.insert(procs[i], null);
    }",1
"@Test
  public void testMetricForFailedYiledProcedure() {
    // procedure that yields and fails
    ProcedureMetrics proc = new ProcedureMetrics(false, true);
    long id = ProcedureTestingUtility.submitAndWait(procExecutor, proc);
    assertNotEquals(""ProcId zero!"", 0, id);
    beginCount++;
    failedCount++;
    ProcedureTestingUtility.waitProcedure(procExecutor, proc);
    assertEquals(""beginCount doesn't match!"", beginCount, proc.beginCount);
    assertEquals(""successCount doesn't match!"", successCount, proc.successCount);
    assertEquals(""failedCont doesn't match!"", failedCount, proc.failedCount);
  }",1
"@Test
  public void testBucketingFilesToSnapshots() throws Exception {
    // Create a table and set a quota
    TableName tn1 = helper.createTableWithRegions(1);
    admin.setQuota(QuotaSettingsFactory.limitTableSpace(tn1, SpaceQuotaHelperForTests.ONE_GIGABYTE,
      SpaceViolationPolicy.NO_INSERTS));

    // Write some data and flush it
    helper.writeData(tn1, 256L * SpaceQuotaHelperForTests.ONE_KILOBYTE);
    admin.flush(tn1);

    final AtomicReference<Long> lastSeenSize = new AtomicReference<>();
    // Wait for the Master chore to run to see the usage (with a fudge factor)
    TEST_UTIL.waitFor(30_000, new SpaceQuotaSnapshotPredicate(conn, tn1) {
      @Override
      boolean evaluate(SpaceQuotaSnapshot snapshot) throws Exception {
        lastSeenSize.set(snapshot.getUsage());
        return snapshot.getUsage() > 230L * SpaceQuotaHelperForTests.ONE_KILOBYTE;
      }",1
"@Test
  public void testSnapshotsFromNamespaces() throws Exception {
    NamespaceDescriptor ns = NamespaceDescriptor.create(""snapshots_from_namespaces"").build();
    admin.createNamespace(ns);

    TableName tn1 = helper.createTableWithRegions(ns.getName(), 1);
    TableName tn2 = helper.createTableWithRegions(ns.getName(), 1);
    TableName tn3 = helper.createTableWithRegions(1);

    // Set a throttle quota on 'default' namespace
    admin.setQuota(QuotaSettingsFactory.throttleNamespace(tn3.getNamespaceAsString(),
      ThrottleType.WRITE_NUMBER, 100, TimeUnit.SECONDS));
    // Set a user throttle quota
    admin.setQuota(
      QuotaSettingsFactory.throttleUser(""user"", ThrottleType.WRITE_NUMBER, 100, TimeUnit.MINUTES));

    // Set a space quota on the namespace
    admin.setQuota(QuotaSettingsFactory.limitNamespaceSpace(ns.getName(),
      SpaceQuotaHelperForTests.ONE_GIGABYTE, SpaceViolationPolicy.NO_INSERTS));

    // Create snapshots on each table (we didn't write any data, so just skipflush)
    admin.snapshot(new SnapshotDescription(tn1.getQualifierAsString() + ""snapshot"", tn1,
      SnapshotType.SKIPFLUSH));
    admin.snapshot(new SnapshotDescription(tn2.getQualifierAsString() + ""snapshot"", tn2,
      SnapshotType.SKIPFLUSH));
    admin.snapshot(new SnapshotDescription(tn3.getQualifierAsString() + ""snapshot"", tn3,
      SnapshotType.SKIPFLUSH));

    Multimap<TableName, String> mapping = testChore.getSnapshotsToComputeSize();
    assertEquals(2, mapping.size());
    assertEquals(1, mapping.get(tn1).size());
    assertEquals(tn1.getQualifierAsString() + ""snapshot"", mapping.get(tn1).iterator().next());
    assertEquals(1, mapping.get(tn2).size());
    assertEquals(tn2.getQualifierAsString() + ""snapshot"", mapping.get(tn2).iterator().next());

    admin.snapshot(new SnapshotDescription(tn2.getQualifierAsString() + ""snapshot1"", tn2,
      SnapshotType.SKIPFLUSH));
    admin.snapshot(new SnapshotDescription(tn3.getQualifierAsString() + ""snapshot2"", tn3,
      SnapshotType.SKIPFLUSH));

    mapping = testChore.getSnapshotsToComputeSize();
    assertEquals(3, mapping.size());
    assertEquals(1, mapping.get(tn1).size());
    assertEquals(tn1.getQualifierAsString() + ""snapshot"", mapping.get(tn1).iterator().next());
    assertEquals(2, mapping.get(tn2).size());
    assertEquals(new HashSet<String>(Arrays.asList(tn2.getQualifierAsString() + ""snapshot"",
      tn2.getQualifierAsString() + ""snapshot1"")), mapping.get(tn2));
  }",1
"@Test
  public void testSnapshotSize() throws Exception {
    // Create a table and set a quota
    TableName tn1 = helper.createTableWithRegions(5);
    admin.setQuota(QuotaSettingsFactory.limitTableSpace(tn1, SpaceQuotaHelperForTests.ONE_GIGABYTE,
      SpaceViolationPolicy.NO_INSERTS));

    // Write some data and flush it
    helper.writeData(tn1, 256L * SpaceQuotaHelperForTests.ONE_KILOBYTE);
    admin.flush(tn1);

    final long snapshotSize = TEST_UTIL.getMiniHBaseCluster().getRegions(tn1).stream()
      .flatMap(r -> r.getStores().stream()).mapToLong(HStore::getHFilesSize).sum();

    // Wait for the Master chore to run to see the usage (with a fudge factor)
    TEST_UTIL.waitFor(30_000, new SpaceQuotaSnapshotPredicate(conn, tn1) {
      @Override
      boolean evaluate(SpaceQuotaSnapshot snapshot) throws Exception {
        return snapshot.getUsage() == snapshotSize;
      }",1
"@Test
  public void testSingleStripeDropDeletes() throws Exception {
    Configuration conf = HBaseConfiguration.create();
    // Test depends on this not being set to pass. Default breaks test. TODO: Revisit.
    conf.unset(""hbase.hstore.compaction.min.size"");
    StripeCompactionPolicy policy = createPolicy(conf);
    // Verify the deletes can be dropped if there are no L0 files.
    Long[][] stripes = new Long[][] { new Long[] { 3L, 2L, 2L, 2L }",1
"@Test
  public void testAppendWithReadOnlyTable() throws Exception {
    final TableName tableName = TableName.valueOf(name.getMethodName());
    this.region = initHRegion(tableName, method, CONF, true, Bytes.toBytes(""somefamily""));
    boolean exceptionCaught = false;
    Append append = new Append(Bytes.toBytes(""somerow""));
    append.setDurability(Durability.SKIP_WAL);
    append.addColumn(Bytes.toBytes(""somefamily""), Bytes.toBytes(""somequalifier""),
      Bytes.toBytes(""somevalue""));
    try {
      region.append(append);
    }",1
"@Test
  public void testAtomicBatchPut() throws IOException {
    final Put[] puts = new Put[10];
    MetricsWALSource source = CompatibilitySingletonFactory.getInstance(MetricsWALSource.class);
    long syncs = prepareRegionForBachPut(puts, source, false);

    // 1. Straight forward case, should succeed
    OperationStatus[] codes = this.region.batchMutate(puts, true);
    assertEquals(10, codes.length);
    for (int i = 0; i < 10; i++) {
      assertEquals(OperationStatusCode.SUCCESS, codes[i].getOperationStatusCode());
    }",1
"@Test
  public void testBatchPut_whileNoRowLocksHeld() throws IOException {
    final Put[] puts = new Put[10];
    MetricsWALSource source = CompatibilitySingletonFactory.getInstance(MetricsWALSource.class);
    long syncs = prepareRegionForBachPut(puts, source, false);

    OperationStatus[] codes = this.region.batchMutate(puts);
    assertEquals(10, codes.length);
    for (int i = 0; i < 10; i++) {
      assertEquals(OperationStatusCode.SUCCESS, codes[i].getOperationStatusCode());
    }",1
"@Test
  public void testBatchPutWithTsSlop() throws Exception {
    // add data with a timestamp that is too recent for range. Ensure assert
    CONF.setInt(""hbase.hregion.keyvalue.timestamp.slop.millisecs"", 1000);
    final Put[] puts = new Put[10];
    MetricsWALSource source = CompatibilitySingletonFactory.getInstance(MetricsWALSource.class);

    long syncs = prepareRegionForBachPut(puts, source, true);

    OperationStatus[] codes = this.region.batchMutate(puts);
    assertEquals(10, codes.length);
    for (int i = 0; i < 10; i++) {
      assertEquals(OperationStatusCode.SANITY_CHECK_FAILURE, codes[i].getOperationStatusCode());
    }",1
"@Test
  public void testCellTTLs() throws IOException {
    IncrementingEnvironmentEdge edge = new IncrementingEnvironmentEdge();
    EnvironmentEdgeManager.injectEdge(edge);

    final byte[] row = Bytes.toBytes(""testRow"");
    final byte[] q1 = Bytes.toBytes(""q1"");
    final byte[] q2 = Bytes.toBytes(""q2"");
    final byte[] q3 = Bytes.toBytes(""q3"");
    final byte[] q4 = Bytes.toBytes(""q4"");

    // 10 seconds
    TableDescriptor tableDescriptor =
      TableDescriptorBuilder.newBuilder(TableName.valueOf(name.getMethodName()))
        .setColumnFamily(ColumnFamilyDescriptorBuilder.newBuilder(fam1).setTimeToLive(10).build())
        .build();

    Configuration conf = new Configuration(TEST_UTIL.getConfiguration());
    conf.setInt(HFile.FORMAT_VERSION_KEY, HFile.MIN_FORMAT_VERSION_WITH_TAGS);

    region = HBaseTestingUtil.createRegionAndWAL(
      RegionInfoBuilder.newBuilder(tableDescriptor.getTableName()).build(),
      TEST_UTIL.getDataTestDir(), conf, tableDescriptor);
    assertNotNull(region);
    long now = EnvironmentEdgeManager.currentTime();
    // Add a cell that will expire in 5 seconds via cell TTL
    region.put(new Put(row).add(new KeyValue(row, fam1, q1, now, HConstants.EMPTY_BYTE_ARRAY,
      new ArrayBackedTag[] {
        // TTL tags specify ts in milliseconds
        new ArrayBackedTag(TagType.TTL_TAG_TYPE, Bytes.toBytes(5000L)) }",1
"@Test
  public void testCheckAndMutateTimestampsAreMonotonic() throws IOException {
    region = initHRegion(tableName, method, CONF, fam1);
    ManualEnvironmentEdge edge = new ManualEnvironmentEdge();
    EnvironmentEdgeManager.injectEdge(edge);

    edge.setValue(10);
    Put p = new Put(row);
    p.setDurability(Durability.SKIP_WAL);
    p.addColumn(fam1, qual1, qual1);
    region.put(p);

    Result result = region.get(new Get(row));
    Cell c = result.getColumnLatestCell(fam1, qual1);
    assertNotNull(c);
    assertEquals(10L, c.getTimestamp());

    edge.setValue(1); // clock goes back
    p = new Put(row);
    p.setDurability(Durability.SKIP_WAL);
    p.addColumn(fam1, qual1, qual2);
    region.checkAndMutate(row, fam1, qual1, CompareOperator.EQUAL, new BinaryComparator(qual1), p);
    result = region.get(new Get(row));
    c = result.getColumnLatestCell(fam1, qual1);
    assertEquals(10L, c.getTimestamp());

    assertTrue(Bytes.equals(c.getValueArray(), c.getValueOffset(), c.getValueLength(), qual2, 0,
      qual2.length));
  }",1
"@Test
  public void testCloseWithFailingFlush() throws Exception {
    final Configuration conf = HBaseConfiguration.create(CONF);
    final WAL wal = createWALCompatibleWithFaultyFileSystem(method, conf, tableName);
    // Only retry once.
    conf.setInt(""hbase.hstore.flush.retries.number"", 1);
    final User user = User.createUserForTesting(conf, this.method, new String[] { ""foo"" }",1
"@Test
  public void testDelete_CheckTimestampUpdated() throws IOException {
    byte[] row1 = Bytes.toBytes(""row1"");
    byte[] col1 = Bytes.toBytes(""col1"");
    byte[] col2 = Bytes.toBytes(""col2"");
    byte[] col3 = Bytes.toBytes(""col3"");

    byte[] forUnitTestsOnly = Bytes.toBytes(""ForUnitTestsOnly"");

    // Setting up region
    this.region = initHRegion(tableName, method, CONF, fam1);
    // Building checkerList
    List<Cell> kvs = new ArrayList<>();
    kvs.add(new KeyValue(row1, fam1, col1, null));
    kvs.add(new KeyValue(row1, fam1, col2, null));
    kvs.add(new KeyValue(row1, fam1, col3, null));

    NavigableMap<byte[], List<Cell>> deleteMap = new TreeMap<>(Bytes.BYTES_COMPARATOR);
    deleteMap.put(fam1, kvs);
    region.delete(new Delete(forUnitTestsOnly, HConstants.LATEST_TIMESTAMP, deleteMap));

    // extract the key values out the memstore:
    // This is kinda hacky, but better than nothing...
    long now = EnvironmentEdgeManager.currentTime();
    AbstractMemStore memstore = (AbstractMemStore) region.getStore(fam1).memstore;
    Cell firstCell = memstore.getActive().first();
    assertTrue(firstCell.getTimestamp() <= now);
    now = firstCell.getTimestamp();
    for (Cell cell : memstore.getActive().getCellSet()) {
      assertTrue(cell.getTimestamp() <= now);
      now = cell.getTimestamp();
    }",1
"@Test
  public void testFlushResult() throws IOException {
    byte[] family = Bytes.toBytes(""family"");

    this.region = initHRegion(tableName, method, family);

    // empty memstore, flush doesn't run
    HRegion.FlushResult fr = region.flush(true);
    assertFalse(fr.isFlushSucceeded());
    assertFalse(fr.isCompactionNeeded());

    // Flush enough files to get up to the threshold, doesn't need compactions
    for (int i = 0; i < 2; i++) {
      Put put = new Put(tableName.toBytes()).addColumn(family, family, tableName.toBytes());
      region.put(put);
      fr = region.flush(true);
      assertTrue(fr.isFlushSucceeded());
      assertFalse(fr.isCompactionNeeded());
    }",1
"@Test
  public void testFlushSizeAccounting() throws Exception {
    final Configuration conf = HBaseConfiguration.create(CONF);
    final WAL wal = createWALCompatibleWithFaultyFileSystem(method, conf, tableName);
    // Only retry once.
    conf.setInt(""hbase.hstore.flush.retries.number"", 1);
    final User user = User.createUserForTesting(conf, method, new String[] { ""foo"" }",1
"@Test
  public void testGet_Empty() throws IOException {
    byte[] row = Bytes.toBytes(""row"");
    byte[] fam = Bytes.toBytes(""fam"");

    this.region = initHRegion(tableName, method, CONF, fam);
    Get get = new Get(row);
    get.addFamily(fam);
    Result r = region.get(get);

    assertTrue(r.isEmpty());
  }",1
"@Test
  public void testGet_FamilyChecker() throws IOException {
    byte[] row1 = Bytes.toBytes(""row1"");
    byte[] fam1 = Bytes.toBytes(""fam1"");
    byte[] fam2 = Bytes.toBytes(""False"");
    byte[] col1 = Bytes.toBytes(""col1"");

    // Setting up region
    this.region = initHRegion(tableName, method, CONF, fam1);
    Get get = new Get(row1);
    get.addColumn(fam2, col1);

    // Test
    try {
      region.get(get);
      fail(""Expecting DoNotRetryIOException in get but did not get any"");
    }",1
"@Test
  public void testGetScanner_WithNoFamilies() throws IOException {
    byte[] row1 = Bytes.toBytes(""row1"");
    byte[] fam1 = Bytes.toBytes(""fam1"");
    byte[] fam2 = Bytes.toBytes(""fam2"");
    byte[] fam3 = Bytes.toBytes(""fam3"");
    byte[] fam4 = Bytes.toBytes(""fam4"");

    byte[][] families = { fam1, fam2, fam3, fam4 }",1
"@Test
  public void testGetScanner_WithRegionClosed() throws IOException {
    byte[] fam1 = Bytes.toBytes(""fam1"");
    byte[] fam2 = Bytes.toBytes(""fam2"");

    byte[][] families = { fam1, fam2 }",1
"@Test
  public void testGetWhileRegionClose() throws IOException {
    Configuration hc = initSplit();
    int numRows = 100;
    byte[][] families = { fam1, fam2, fam3 }",1
"@Test
  public void testRecoveredEditsReplayCompaction() throws Exception {
    testRecoveredEditsReplayCompaction(false);
    testRecoveredEditsReplayCompaction(true);
  }",1
"@Test
  public void testReverseScanner_FromMemStore_SingleCF_FullScan() throws IOException {
    byte[] rowC = Bytes.toBytes(""rowC"");
    byte[] rowA = Bytes.toBytes(""rowA"");
    byte[] rowB = Bytes.toBytes(""rowB"");
    byte[] cf = Bytes.toBytes(""CF"");
    byte[][] families = { cf }",1
"@Test
  public void testReverseScanner_FromMemStoreAndHFiles_MultiCFs1() throws IOException {
    byte[] row0 = Bytes.toBytes(""row0""); // 1 kv
    byte[] row1 = Bytes.toBytes(""row1""); // 2 kv
    byte[] row2 = Bytes.toBytes(""row2""); // 4 kv
    byte[] row3 = Bytes.toBytes(""row3""); // 2 kv
    byte[] row4 = Bytes.toBytes(""row4""); // 5 kv
    byte[] row5 = Bytes.toBytes(""row5""); // 2 kv
    byte[] cf1 = Bytes.toBytes(""CF1"");
    byte[] cf2 = Bytes.toBytes(""CF2"");
    byte[] cf3 = Bytes.toBytes(""CF3"");
    byte[][] families = { cf1, cf2, cf3 }",1
"@Test
  public void testReverseScanner_FromMemStoreAndHFiles_MultiCFs2() throws IOException {
    byte[] row1 = Bytes.toBytes(""row1"");
    byte[] row2 = Bytes.toBytes(""row2"");
    byte[] row3 = Bytes.toBytes(""row3"");
    byte[] row4 = Bytes.toBytes(""row4"");
    byte[] cf1 = Bytes.toBytes(""CF1"");
    byte[] cf2 = Bytes.toBytes(""CF2"");
    byte[] cf3 = Bytes.toBytes(""CF3"");
    byte[] cf4 = Bytes.toBytes(""CF4"");
    byte[][] families = { cf1, cf2, cf3, cf4 }",1
"@Test
  public void testReverseScanShouldNotScanMemstoreIfReadPtLesser() throws Exception {
    byte[] cf1 = Bytes.toBytes(""CF1"");
    byte[][] families = { cf1 }",1
"@Test
  public void testScanner_DeleteOneFamilyNotAnother() throws IOException {
    byte[] fam1 = Bytes.toBytes(""columnA"");
    byte[] fam2 = Bytes.toBytes(""columnB"");
    this.region = initHRegion(tableName, method, CONF, fam1, fam2);
    byte[] rowA = Bytes.toBytes(""rowA"");
    byte[] rowB = Bytes.toBytes(""rowB"");

    byte[] value = Bytes.toBytes(""value"");

    Delete delete = new Delete(rowA);
    delete.addFamily(fam1);

    region.delete(delete);

    // now create data.
    Put put = new Put(rowA);
    put.addColumn(fam2, null, value);
    region.put(put);

    put = new Put(rowB);
    put.addColumn(fam1, null, value);
    put.addColumn(fam2, null, value);
    region.put(put);

    Scan scan = new Scan();
    scan.addFamily(fam1).addFamily(fam2);
    try (InternalScanner s = region.getScanner(scan)) {
      List<Cell> results = new ArrayList<>();
      s.next(results);
      assertTrue(CellUtil.matchingRows(results.get(0), rowA));

      results.clear();
      s.next(results);
      assertTrue(CellUtil.matchingRows(results.get(0), rowB));
    }",1
"@Test
  public void testScanner_Wildcard_FromMemStore_EnforceVersions() throws IOException {
    byte[] row1 = Bytes.toBytes(""row1"");
    byte[] qf1 = Bytes.toBytes(""qualifier1"");
    byte[] qf2 = Bytes.toBytes(""qualifier2"");
    byte[] fam1 = Bytes.toBytes(""fam1"");
    byte[][] families = { fam1 }",1
"@Test
  public void testSequenceId() throws IOException {
    region = initHRegion(tableName, method, CONF, COLUMN_FAMILY_BYTES);
    assertEquals(HConstants.NO_SEQNUM, region.getMaxFlushedSeqId());
    // Weird. This returns 0 if no store files or no edits. Afraid to change it.
    assertEquals(0, (long) region.getMaxStoreSeqId().get(COLUMN_FAMILY_BYTES));
    HBaseTestingUtil.closeRegionAndWAL(this.region);
    assertEquals(HConstants.NO_SEQNUM, region.getMaxFlushedSeqId());
    assertEquals(0, (long) region.getMaxStoreSeqId().get(COLUMN_FAMILY_BYTES));
    HRegion oldRegion = region;
    try {
      // Open region again.
      region = initHRegion(tableName, method, CONF, COLUMN_FAMILY_BYTES);
      byte[] value = Bytes.toBytes(method);
      // Make a random put against our cf.
      Put put = new Put(value);
      put.addColumn(COLUMN_FAMILY_BYTES, null, value);
      region.put(put);
      // No flush yet so init numbers should still be in place.
      assertEquals(HConstants.NO_SEQNUM, region.getMaxFlushedSeqId());
      assertEquals(0, (long) region.getMaxStoreSeqId().get(COLUMN_FAMILY_BYTES));
      region.flush(true);
      long max = region.getMaxFlushedSeqId();
      HBaseTestingUtil.closeRegionAndWAL(this.region);
      assertEquals(max, region.getMaxFlushedSeqId());
      this.region = null;
    }",1
"@Test
  public void testSkipRecoveredEditsReplayTheLastFileIgnored() throws Exception {
    byte[] family = Bytes.toBytes(""family"");
    this.region = initHRegion(tableName, method, CONF, family);
    final WALFactory wals = new WALFactory(CONF, method);
    try {
      Path regiondir = region.getRegionFileSystem().getRegionDir();
      FileSystem fs = region.getRegionFileSystem().getFileSystem();
      byte[] regionName = region.getRegionInfo().getEncodedNameAsBytes();
      byte[][] columns = region.getTableDescriptor().getColumnFamilyNames().toArray(new byte[0][]);

      assertEquals(0, region.getStoreFileList(columns).size());

      Path recoveredEditsDir = WALSplitUtil.getRegionDirRecoveredEditsDir(regiondir);

      long maxSeqId = 1050;
      long minSeqId = 1000;

      for (long i = minSeqId; i <= maxSeqId; i += 10) {
        Path recoveredEdits = new Path(recoveredEditsDir, String.format(""%019d"", i));
        fs.create(recoveredEdits);
        WALProvider.Writer writer = wals.createRecoveredEditsWriter(fs, recoveredEdits);

        long time = System.nanoTime();
        WALEdit edit = null;
        if (i == maxSeqId) {
          edit = WALEdit.createCompaction(region.getRegionInfo(),
            CompactionDescriptor.newBuilder().setTableName(ByteString.copyFrom(tableName.getName()))
              .setFamilyName(ByteString.copyFrom(regionName))
              .setEncodedRegionName(ByteString.copyFrom(regionName))
              .setStoreHomeDirBytes(ByteString.copyFrom(Bytes.toBytes(regiondir.toString())))
              .setRegionName(ByteString.copyFrom(region.getRegionInfo().getRegionName())).build());
        }",1
"@Test
  public void testStatusSettingToAbortIfAnyExceptionDuringRegionInitilization() throws Exception {
    RegionInfo info;
    try {
      FileSystem fs = mock(FileSystem.class);
      when(fs.exists(any())).thenThrow(new IOException());
      TableDescriptorBuilder tableDescriptorBuilder = TableDescriptorBuilder.newBuilder(tableName);
      ColumnFamilyDescriptor columnFamilyDescriptor =
        ColumnFamilyDescriptorBuilder.newBuilder(Bytes.toBytes(""cf"")).build();
      tableDescriptorBuilder.setColumnFamily(columnFamilyDescriptor);
      info = RegionInfoBuilder.newBuilder(tableName).build();
      Path path = new Path(dir + ""testStatusSettingToAbortIfAnyExceptionDuringRegionInitilization"");
      region = HRegion.newHRegion(path, null, fs, CONF, info, tableDescriptorBuilder.build(), null);
      // region initialization throws IOException and set task state to ABORTED.
      region.initialize();
      fail(""Region initialization should fail due to IOException"");
    }",1
"@Test
  public void testToShowNPEOnRegionScannerReseek() throws Exception {
    byte[] family = Bytes.toBytes(""family"");
    this.region = initHRegion(tableName, method, CONF, family);

    Put put = new Put(Bytes.toBytes(""r1""));
    put.addColumn(family, Bytes.toBytes(""q1""), Bytes.toBytes(""v1""));
    region.put(put);
    put = new Put(Bytes.toBytes(""r2""));
    put.addColumn(family, Bytes.toBytes(""q1""), Bytes.toBytes(""v1""));
    region.put(put);
    region.flush(true);

    Scan scan = new Scan();
    scan.readVersions(3);
    // open the first scanner
    try (RegionScanner scanner1 = region.getScanner(scan)) {
      LOG.info(""Smallest read point:"" + region.getSmallestReadPoint());

      region.compact(true);

      scanner1.reseek(Bytes.toBytes(""r2""));
      List<Cell> results = new ArrayList<>();
      scanner1.next(results);
      Cell keyValue = results.get(0);
      assertTrue(Bytes.compareTo(CellUtil.cloneRow(keyValue), Bytes.toBytes(""r2"")) == 0);
      scanner1.close();
    }",1
"@Test
  public void testSwitchingPreadtoStreamParallelyWithCompactionDischarger() throws Exception {
    Configuration conf = HBaseConfiguration.create();
    conf.set(""hbase.hstore.engine.class"", DummyStoreEngine.class.getName());
    conf.setLong(StoreScanner.STORESCANNER_PREAD_MAX_BYTES, 0);
    // Set the lower threshold to invoke the ""MERGE"" policy
    MyStore store = initMyStore(name.getMethodName(), conf, new MyStoreHook() {
    }",1
"@Test
  public void testBloomFilter() throws Exception {
    conf.setFloat(BloomFilterFactory.IO_STOREFILE_BLOOM_ERROR_RATE, (float) 0.01);
    conf.setBoolean(BloomFilterFactory.IO_STOREFILE_BLOOM_ENABLED, true);

    // write the file
    if (!fs.exists(ROOT_DIR)) {
      fs.mkdirs(ROOT_DIR);
    }",1
"@Test
  public void testPreemptTask() throws Exception {
    LOG.info(""testPreemptTask"");
    SplitLogCounters.resetCounters();
    final ServerName SRV = ServerName.valueOf(""tpt_svr,1,1"");
    final String PATH = ZKSplitLog.getEncodedNodeName(zkw, ""tpt_task"");
    RegionServerServices mockedRS = getRegionServer(SRV);
    SplitLogWorker slw =
      new SplitLogWorker(ds, TEST_UTIL.getConfiguration(), mockedRS, neverEndingTask);
    slw.start();
    try {
      Thread.yield(); // let the worker start
      Thread.sleep(1000);
      waitForCounter(SplitLogCounters.tot_wkr_task_grabing, 0, 1, WAIT_TIME);

      // this time create a task node after starting the splitLogWorker
      zkw.getRecoverableZooKeeper().create(PATH, new SplitLogTask.Unassigned(MANAGER).toByteArray(),
        Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);

      waitForCounter(SplitLogCounters.tot_wkr_task_acquired, 0, 1, WAIT_TIME);
      assertEquals(1, slw.getTaskReadySeq());
      byte[] bytes = ZKUtil.getData(zkw, PATH);
      SplitLogTask slt = SplitLogTask.parseFrom(bytes);
      assertTrue(slt.isOwned(SRV));
      slt = new SplitLogTask.Owned(MANAGER);
      ZKUtil.setData(zkw, PATH, slt.toByteArray());
      waitForCounter(SplitLogCounters.tot_wkr_preempt_task, 0, 1, WAIT_TIME);
    }",1
"@Test
  public void testEqualsWithLink() throws IOException {
    Path origin = new Path(""/origin"");
    Path tmp = TEST_UTIL.getDataTestDir();
    Path mob = new Path(""/mob"");
    Path archive = new Path(""/archive"");
    HFileLink link1 = new HFileLink(new Path(origin, ""f1""), new Path(tmp, ""f1""),
      new Path(mob, ""f1""), new Path(archive, ""f1""));
    HFileLink link2 = new HFileLink(new Path(origin, ""f1""), new Path(tmp, ""f1""),
      new Path(mob, ""f1""), new Path(archive, ""f1""));

    StoreFileInfo info1 =
      new StoreFileInfo(TEST_UTIL.getConfiguration(), TEST_UTIL.getTestFileSystem(), null, link1);
    StoreFileInfo info2 =
      new StoreFileInfo(TEST_UTIL.getConfiguration(), TEST_UTIL.getTestFileSystem(), null, link2);

    assertEquals(info1, info2);
    assertEquals(info1.hashCode(), info2.hashCode());
  }",1
"@Test
  public void testReplicationStatus() throws Exception {
    // This test wants two RS's up. We only run one generally so add one.
    UTIL1.getMiniHBaseCluster().startRegionServer();
    Waiter.waitFor(UTIL1.getConfiguration(), 30000, new Waiter.Predicate<Exception>() {
      @Override
      public boolean evaluate() throws Exception {
        return UTIL1.getMiniHBaseCluster().getLiveRegionServerThreads().size() > 1;
      }",1
"@Test
  public void testCompleteSnapshotWithNoSnapshotDirectoryFailure() throws Exception {
    Path snapshotDir = new Path(root, HConstants.SNAPSHOT_DIR_NAME);
    Path tmpDir = new Path(snapshotDir, "".tmp"");
    Path workingDir = new Path(tmpDir, ""not_a_snapshot"");
    Configuration conf = new Configuration();
    FileSystem workingFs = workingDir.getFileSystem(conf);
    assertFalse(
      ""Already have working snapshot dir: "" + workingDir + "" but shouldn't. Test file leak?"",
      fs.exists(workingDir));
    SnapshotDescription snapshot = SnapshotDescription.newBuilder().setName(""snapshot"").build();
    Path finishedDir = SnapshotDescriptionUtils.getCompletedSnapshotDir(snapshot, snapshotDir);

    try {
      SnapshotDescriptionUtils.completeSnapshot(finishedDir, workingDir, fs, workingFs, conf);
      fail(""Shouldn't successfully complete move of a non-existent directory."");
    }",1
"@Test
  public void testStart() throws Exception {
    JMXConnector connector =
      JMXConnectorFactory.connect(JMXListener.buildJMXServiceURL(CONNECTOR_PORT, CONNECTOR_PORT));

    MBeanServerConnection mb = connector.getMBeanServerConnection();
    String domain = mb.getDefaultDomain();
    Assert.assertTrue(""default domain is not correct"", !domain.isEmpty());
    connector.close();

  }",1
"@Test
  public void testMatchingTail() throws IOException {
    Path rootdir = htu.getDataTestDir();
    final FileSystem fs = rootdir.getFileSystem(conf);
    assertTrue(rootdir.depth() > 1);
    Path partPath = new Path(""a"", ""b"");
    Path fullPath = new Path(rootdir, partPath);
    Path fullyQualifiedPath = fs.makeQualified(fullPath);
    assertFalse(CommonFSUtils.isMatchingTail(fullPath, partPath));
    assertFalse(CommonFSUtils.isMatchingTail(fullPath, partPath.toString()));
    assertTrue(CommonFSUtils.isStartingWithPath(rootdir, fullPath.toString()));
    assertTrue(CommonFSUtils.isStartingWithPath(fullyQualifiedPath, fullPath.toString()));
    assertFalse(CommonFSUtils.isStartingWithPath(rootdir, partPath.toString()));
    assertFalse(CommonFSUtils.isMatchingTail(fullyQualifiedPath, partPath));
    assertTrue(CommonFSUtils.isMatchingTail(fullyQualifiedPath, fullPath));
    assertTrue(CommonFSUtils.isMatchingTail(fullyQualifiedPath, fullPath.toString()));
    assertTrue(CommonFSUtils.isMatchingTail(fullyQualifiedPath, fs.makeQualified(fullPath)));
    assertTrue(CommonFSUtils.isStartingWithPath(rootdir, fullyQualifiedPath.toString()));
    assertFalse(CommonFSUtils.isMatchingTail(fullPath, new Path(""x"")));
    assertFalse(CommonFSUtils.isMatchingTail(new Path(""x""), fullPath));
  }",1
"@Test
  public void testTestCompression() {
    assertTrue(CompressionTest.testCompression(""NONE""));
    assertTrue(CompressionTest.testCompression(""GZ""));

    if (NativeCodeLoader.isNativeCodeLoaded()) {
      // LZO is GPL so not included in hadoop install. You need to do an extra install to pick
      // up the needed support. This article is good on the steps needed to add LZO support:
      // https://stackoverflow.com/questions/23441142/class-com-hadoop-compression-lzo-lzocodec-not-found-for-spark-on-cdh-5
      // Its unlikely at test time that the extras are installed so this test is useless.
      // nativeCodecTest(""LZO"", ""lzo2"", ""com.hadoop.compression.lzo.LzoCodec"");
      nativeCodecTest(""LZ4"", null, ""org.apache.hadoop.io.compress.Lz4Codec"");
      nativeCodecTest(""SNAPPY"", ""snappy"", ""org.apache.hadoop.io.compress.SnappyCodec"");
      nativeCodecTest(""BZIP2"", ""bzip2"", ""org.apache.hadoop.io.compress.BZip2Codec"");
      nativeCodecTest(""ZSTD"", ""zstd"", ""org.apache.hadoop.io.compress.ZStandardCodec"");
    }",1
"@Test
  public void testReadingHTDFromFS() throws IOException {
    FileSystem fs = FileSystem.get(UTIL.getConfiguration());
    TableDescriptor htd =
      TableDescriptorBuilder.newBuilder(TableName.valueOf(name.getMethodName())).build();
    FSTableDescriptors fstd = new FSTableDescriptors(fs, testDir);
    fstd.createTableDescriptor(htd);
    TableDescriptor td2 =
      FSTableDescriptors.getTableDescriptorFromFs(fs, testDir, htd.getTableName());
    assertTrue(htd.equals(td2));
  }",1
"@Test
  public void testTableInfoFileStatusComparator() {
    FileStatus bare = new FileStatus(0, false, 0, 0, -1,
      new Path(""/tmp"", FSTableDescriptors.TABLEINFO_FILE_PREFIX));
    FileStatus future = new FileStatus(0, false, 0, 0, -1,
      new Path(""/tmp/tablinfo."" + EnvironmentEdgeManager.currentTime()));
    FileStatus farFuture = new FileStatus(0, false, 0, 0, -1,
      new Path(""/tmp/tablinfo."" + EnvironmentEdgeManager.currentTime() + 1000));
    FileStatus[] alist = { bare, future, farFuture }",1
"@Test
  public void testCopyFilesParallel() throws Exception {
    MiniDFSCluster cluster = htu.startMiniDFSCluster(1);
    cluster.waitActive();
    FileSystem fs = cluster.getFileSystem();
    Path src = new Path(""/src"");
    fs.mkdirs(src);
    for (int i = 0; i < 50; i++) {
      WriteDataToHDFS(fs, new Path(src, String.valueOf(i)), 1024);
    }",1
"@Test
  public void testGetArchivePath() throws Exception {
    Configuration conf = new Configuration();
    CommonFSUtils.setRootDir(conf, new Path(""root""));
    assertNotNull(HFileArchiveUtil.getArchivePath(conf));
  }",1
"@Test
  public void testSplit() throws IOException {
    final TableName tableName = TableName.valueOf(currentTest.getMethodName());
    final byte[] rowName = tableName.getName();
    final MultiVersionConcurrencyControl mvcc = new MultiVersionConcurrencyControl(1);
    final int howmany = 3;
    RegionInfo[] infos = new RegionInfo[3];
    Path tableDataDir = CommonFSUtils.getTableDir(hbaseDir, tableName);
    fs.mkdirs(tableDataDir);
    Path tabledir = CommonFSUtils.getWALTableDir(conf, tableName);
    fs.mkdirs(tabledir);
    for (int i = 0; i < howmany; i++) {
      infos[i] = RegionInfoBuilder.newBuilder(tableName).setStartKey(Bytes.toBytes("""" + i))
        .setEndKey(Bytes.toBytes("""" + (i + 1))).build();
      fs.mkdirs(new Path(tabledir, infos[i].getEncodedName()));
      fs.mkdirs(new Path(tableDataDir, infos[i].getEncodedName()));
      LOG.info(""allo "" + new Path(tabledir, infos[i].getEncodedName()).toString());
    }",1
"@Test
  public void testSetDataVersionMismatchInLoop() throws Exception {
    String znode = ""/hbase/splitWAL/9af7cfc9b15910a0b3d714bf40a3248f"";
    Configuration conf = TEST_UTIL.getConfiguration();
    ZKWatcher zkw = new ZKWatcher(conf, ""testSetDataVersionMismatchInLoop"", abortable, true);
    String ensemble = ZKConfig.getZKQuorumServersString(conf);
    RecoverableZooKeeper rzk = RecoverableZooKeeper.connect(conf, ensemble, zkw, null, null);
    rzk.create(znode, new byte[0], Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);
    rzk.setData(znode, Bytes.toBytes(""OPENING""), 0);
    Field zkField = RecoverableZooKeeper.class.getDeclaredField(""zk"");
    zkField.setAccessible(true);
    int timeout = conf.getInt(HConstants.ZK_SESSION_TIMEOUT, HConstants.DEFAULT_ZK_SESSION_TIMEOUT);
    ZookeeperStub zkStub = new ZookeeperStub(ensemble, timeout, zkw);
    zkStub.setThrowExceptionInNumOperations(1);
    zkField.set(rzk, zkStub);
    byte[] opened = Bytes.toBytes(""OPENED"");
    rzk.setData(znode, opened, 1);
    byte[] data = rzk.getData(znode, false, new Stat());
    assertTrue(Bytes.equals(opened, data));
  }",1
"@Test
  public void testClusterKeyWithMultiplePorts() throws Exception {
    // server has different port than the default port
    testKey(""server1:2182"", 2181, ""/hbase"", true);
    // multiple servers have their own port
    testKey(""server1:2182,server2:2183,server3:2184"", 2181, ""/hbase"", true);
    // one server has no specified port, should use default port
    testKey(""server1:2182,server2,server3:2184"", 2181, ""/hbase"", true);
    // the last server has no specified port, should use default port
    testKey(""server1:2182,server2:2183,server3"", 2181, ""/hbase"", true);
    // multiple servers have no specified port, should use default port for those servers
    testKey(""server1:2182,server2,server3:2184,server4"", 2181, ""/hbase"", true);
    // same server, different ports
    testKey(""server1:2182,server1:2183,server1"", 2181, ""/hbase"", true);
    // mix of same server/different port and different server
    testKey(""server1:2182,server2:2183,server1"", 2181, ""/hbase"", true);
  }",1
"@Test
  public void testHostPortParse() {
    ZKMainServer parser = new ZKMainServer();
    Configuration c = HBaseConfiguration.create();
    assertEquals(""127.0.0.1:"" + c.get(HConstants.ZOOKEEPER_CLIENT_PORT), parser.parse(c));
    final String port = ""1234"";
    c.set(HConstants.ZOOKEEPER_CLIENT_PORT, port);
    c.set(""hbase.zookeeper.quorum"", ""example.com"");
    assertEquals(""example.com:"" + port, parser.parse(c));
    c.set(""hbase.zookeeper.quorum"", ""example1.com,example2.com,example3.com"");
    String ensemble = parser.parse(c);
    assertTrue(port, ensemble.matches(""(example[1-3]\\.com:1234,){2}",1
"@Test
	public void testOptionSetDefinition() throws Exception {
		final Object ls = new OptionSetDefinitionDataLoader().load(null, Collections.singletonList(""/ls.xml""));
		System.out.println(ls);
		final Object uniq = new OptionSetDefinitionDataLoader().load(null, Collections.singletonList(""/uniq.xml""));
		System.out.println(uniq);
		final Object cut = new OptionSetDefinitionDataLoader().load(null, Collections.singletonList(""/cut.xml""));
		System.out.println(cut);
		final Object sort = new OptionSetDefinitionDataLoader().load(null, Collections.singletonList(""/sort.xml""));
		System.out.println(sort);
	}",1
"@Test
    public void testGrepFileThenDelete() throws IOException {
        //given
        final String[] lines = {""Hello"", ""World"", ""These are 3 lines""}",1
"@Test
    public void testGrepWholeLine(){
        final String lineRegex = ""the cistern .* the dishwasher hot-rinsing, and the kettle being"";
        final String actualLine = ""the cistern refilling, the dishwasher hot-rinsing, and the kettle being"";

        final File testFile = new File(outputDir.getPath() + ""/commuting.txt"" );
        assertEquals(actualLine, Unix4j.use(contextFactory).grep(Grep.Options.wholeLine, lineRegex, testFile).toStringResult());
    }",1
"@Test
	public void testRelativePath_5_noCommonAncestor() {
		final String actual = FileUtil.getRelativePath(new File(FileUtil.ROOT + ""home/john""), new File(FileUtil.ROOT + ""var/tmp/test.out""));
		Assert.assertEquals(FileUtil.ROOT.replace('\\', '/') + ""var/tmp/test.out"", actual);
	}",1
"@Test
	public void testRelativePath_3_indirectParent() {
		final String actual = new RelativePathBase(""/home/john"").getRelativePathFor(""/home/john/documents/important"");
		Assert.assertEquals(""./documents/important"", actual);
	}",1
"@Test
    public void testBytesFromFileLowBufferSize() throws IOException {
        File file = new File(""target/testFromFile"");
        file.delete();
        FileOutputStream out = new FileOutputStream(file);
        out.write(""abcdefg"".getBytes());
        out.close();
        final ByteArrayOutputStream bytes = new ByteArrayOutputStream();
        Bytes //
                .from(file, 4) //
                .doOnNext(new Consumer<byte[]>() {
                    @Override
                    public void accept(byte[] b) {
                        try {
                            bytes.write(b);
                        }",1
"@Test
    public void testUnzipExtractSpecificFile() {
        List<String> list = Bytes.unzip(new File(""src/test/resources/test.zip"")).filter(new Predicate<ZippedEntry>() {

            @Override
            public boolean test(ZippedEntry entry) {
                return entry.getName().equals(""document2.txt"");
            }",1
"@Test
    public void testPageSize() {
        Page page = new Page(new File(""target/p1""), 100);
        assertEquals(100, page.length());
        page.close();
    }",1
"@Test
    public void testGenerateURLFragment() {
        StandardURLTagFragmentGenerator g
                = new StandardURLTagFragmentGenerator();
        assertEquals("" href=\""abc\"""", g.generateURLFragment(""abc""));
        assertEquals("" href=\""images/abc.png\"""",
                g.generateURLFragment(""images/abc.png""));
        assertEquals("" href=\""http://www.jfree.org/images/abc.png\"""",
                g.generateURLFragment(""http://www.jfree.org/images/abc.png""));
    }",1
"@Test
    public void read() throws Exception {
        Map<String, Attributes> entities = LDIFUtils.read(getClass().getResourceAsStream(""/example.ldif""));
        assertThat(entities.size(), is(4));
    }",1
"@Test
    public void readMultipleEntities() throws Exception {
        String ldif = ""dn: dc=example,dc=com\n"" +
            ""objectClass: domain\n"" +
            ""objectClass: top\n"" +
            ""dc: example\n"" +
            ""\n"" +
            ""dn: ou=Users,dc=example,dc=com\n"" +
            ""objectClass: organizationalUnit\n"" +
            ""objectClass: top\n"" +
            ""ou: Users\n"";

        Map<String, Attributes> entities = LDIFUtils.read(new ByteArrayInputStream(ldif.getBytes()));
        assertThat(entities.size(), is(2));
        assertThat(entities.containsKey(""dc=example,dc=com""), is(true));
        assertThat(entities.containsKey(""ou=Users,dc=example,dc=com""), is(true));
    }",1
"@Test
  public void conversionProblemOutgoingSync() throws IOException {
    Retrofit retrofit =
        new Retrofit.Builder()
            .baseUrl(server.url(""/""))
            .addConverterFactory(
                new ToStringConverterFactory() {
                  @Override
                  public Converter<String, RequestBody> requestBodyConverter(
                      Type type,
                      Annotation[] parameterAnnotations,
                      Annotation[] methodAnnotations,
                      Retrofit retrofit) {
                    return value -> {
                      throw new UnsupportedOperationException(""I am broken!"");
                    }",1
"@Test
  public void responseBodyStreams() throws IOException {
    Retrofit retrofit =
        new Retrofit.Builder()
            .baseUrl(server.url(""/""))
            .addConverterFactory(new ToStringConverterFactory())
            .build();
    Service example = retrofit.create(Service.class);

    server.enqueue(
        new MockResponse().setBody(""1234"").setSocketPolicy(DISCONNECT_DURING_RESPONSE_BODY));

    Response<ResponseBody> response = example.getStreamingBody().execute();

    ResponseBody streamedBody = response.body();
    // When streaming we only detect socket problems as the ResponseBody is read.
    try {
      streamedBody.string();
      fail();
    }",1
"@Test
  public void transportProblemSync() {
    Retrofit retrofit =
        new Retrofit.Builder()
            .baseUrl(server.url(""/""))
            .addConverterFactory(new ToStringConverterFactory())
            .build();
    Service example = retrofit.create(Service.class);

    server.enqueue(new MockResponse().setSocketPolicy(SocketPolicy.DISCONNECT_AT_START));

    Call<String> call = example.getString();
    try {
      call.execute();
      fail();
    }",1
"@Test
  public void xmlRequestBody() throws Exception {
    server.enqueue(new MockResponse());

    Call<Void> call = service.postXml(SAMPLE_CONTACT);
    call.execute();

    RecordedRequest request = server.takeRequest();
    assertThat(request.getHeader(""Content-Type"")).isEqualTo(""application/xml; charset=utf-8"");
    assertThat(request.getBody().readUtf8()).isEqualTo(SAMPLE_CONTACT_XML);
  }",1
"@Test
  public void responseTypeCannotBeOkHttpResponse() {
    Retrofit retrofit = new Retrofit.Builder().baseUrl(server.url(""/"")).build();
    CallMethod service = retrofit.create(CallMethod.class);
    try {
      service.badType2();
      fail();
    }",1
"@Test
  public void testRegularAsyncExecution() {

    ProcessEngine processEngine = null;

    try {
      // Deploy
      processEngine = createProcessEngine(true);
      setClockToCurrentTime(processEngine);
      deploy(processEngine, ""AsyncExecutorTest.testRegularAsyncExecution.bpmn20.xml"");

      // Start process instance. Wait for all jobs to be done
      processEngine.getRuntimeService().startProcessInstanceByKey(""asyncExecutor"");

      // Move clock 3 minutes. Nothing should happen
      addSecondsToCurrentTime(processEngine, 180L);
      ProcessEngine processEngineForException = processEngine;
      assertThatExceptionOfType(ActivitiException.class)
        .isThrownBy(() -> waitForAllJobsBeingExecuted(processEngineForException, 500L));
      assertThat(processEngine.getTaskService().createTaskQuery().taskName(""The Task"").count()).isEqualTo(1);
      assertThat(processEngine.getTaskService().createTaskQuery().taskName(""Task after timer"").count()).isEqualTo(0);
      assertThat(processEngine.getManagementService().createTimerJobQuery().count()).isEqualTo(1);
      assertThat(getAsyncExecutorJobCount(processEngine)).isEqualTo(0);

      // Move clock 3 minutes and 1 second. Triggers the timer
      addSecondsToCurrentTime(processEngine, 181);
      waitForAllJobsBeingExecuted(processEngine);

      // Verify if all is as expected
      assertThat(processEngine.getTaskService().createTaskQuery().taskName(""The Task"").count()).isEqualTo(0);
      assertThat(processEngine.getTaskService().createTaskQuery().taskName(""Task after timer"").count()).isEqualTo(1);
      assertThat(processEngine.getManagementService().createTimerJobQuery().count()).isEqualTo(0);
      assertThat(processEngine.getManagementService().createJobQuery().count()).isEqualTo(0);

      assertThat(getAsyncExecutorJobCount(processEngine)).isEqualTo(1);
    }",1
"@Test
    public void noDefaultProfile() throws IOException {
        final ConfigFile configFile =
                ConfigFileReader.parse(""src/test/resources/unit_test_no_default_config"", ""USER"");
        assertEquals(""default_user"", configFile.get(""user""));
        assertEquals(""default_tenancy"", configFile.get(""tenancy""));
        assertNull(configFile.get(""region""));
    }",1
"@Test
    public void allVariables_allFilledIn() {
        String endpoint =
                DefaultEndpointConfiguration.builder(
                                ""https://{serviceEndpointPrefix}",1
"@Test
    public void someVariables_noSecondLevelDomain() {
        String endpoint =
                DefaultEndpointConfiguration.builder(
                                ""https://{serviceEndpointPrefix}",1
"@Test
    public void someVariables_noServiceName_noSecondLevelDomain() {
        String endpoint =
                DefaultEndpointConfiguration.builder(""https://foobar.{region}",1
"@Test
    public void createEndpoint_useCustomTemplate() {
        Service testService =
                Services.serviceBuilder()
                        .serviceEndpointPrefix(""foobar"")
                        .serviceName(""EndpointBuilderTest2"")
                        .serviceEndpointTemplate(""https://foobar2.{region}",1
"@Test
    public void createEndpoint_useCustomTemplate_allTemplatesUsed() {
        Service testService =
                Services.serviceBuilder()
                        .serviceEndpointPrefix(""foobar"")
                        .serviceName(""EndpointBuilderTest3"")
                        .serviceEndpointTemplate(
                                ""http://{serviceEndpointPrefix}",1
"@Test
    public void testText() throws Exception {


        final AtomicBoolean connected = new AtomicBoolean(false);

        final ServletContainer container = ServletContainer.Factory.newInstance();

        DeploymentUtils.setupServlet(new ServletInfo(""websocket"", WebSocketServlet.class,
                new ImmediateInstanceFactory<Servlet>(new WebSocketServlet(new WebSocketConnectionCallback() {
                    @Override
                    public void onConnect(final WebSocketHttpExchange exchange, final WebSocketChannel channel) {
                        connected.set(true);
                        channel.getReceiveSetter().set(new AbstractReceiveListener() {

                            @Override
                            protected void onFullTextMessage(WebSocketChannel channel, BufferedTextMessage message) throws IOException {
                                final String string = message.getData();
                                if(string.equals(""hello"")) {
                                    WebSockets.sendText(""world"", channel, null);
                                }",1
"@Test
    public void testGte() {
        String field = ""field1"";
        Long gte = 10l;


        Map<String, Object> config = createConfig(""field"", field,
                ""gte"", gte);
        MathComparatorCondition mathComparatorCondition = new MathComparatorCondition.Factory().create(config, conditionParser);

        Doc doc = createDoc(""field1"", 15l);
        assertThat(mathComparatorCondition.evaluate(doc)).isTrue();

        doc = createDoc(""field1"", 10);
        assertThat(mathComparatorCondition.evaluate(doc)).isTrue();

        doc = createDoc(""field1"", 5l);
        assertThat(mathComparatorCondition.evaluate(doc)).isFalse();
    }",1
"@Test
    public void testGtAndLt() {
        String field = ""field1"";
        Long lt = 20l;
        Long gt = 10l;


        Map<String, Object> config = createConfig(""field"", field,
                ""lt"", lt,
                ""gt"", gt);
        MathComparatorCondition mathComparatorCondition = new MathComparatorCondition.Factory().create(config, conditionParser);

        Doc doc = createDoc(""field1"", 15l);
        assertThat(mathComparatorCondition.evaluate(doc)).isTrue();

        doc = createDoc(""field1"", 10);
        assertThat(mathComparatorCondition.evaluate(doc)).isFalse();

        doc = createDoc(""field1"", 20);
        assertThat(mathComparatorCondition.evaluate(doc)).isFalse();

        doc = createDoc(""field1"", 5l);
        assertThat(mathComparatorCondition.evaluate(doc)).isFalse();

        doc = createDoc(""field1"", 25l);
        assertThat(mathComparatorCondition.evaluate(doc)).isFalse();
    }",1
"@Test
    public void testLte() {
        String field = ""field1"";
        Long lte = 10l;


        Map<String, Object> config = createConfig(""field"", field,
                ""lte"", lte);
        MathComparatorCondition mathComparatorCondition = new MathComparatorCondition.Factory().create(config, conditionParser);

        Doc doc = createDoc(""field1"", 15l);
        assertThat(mathComparatorCondition.evaluate(doc)).isFalse();

        doc = createDoc(""field1"", 10);
        assertThat(mathComparatorCondition.evaluate(doc)).isTrue();

        doc = createDoc(""field1"", 5l);
        assertThat(mathComparatorCondition.evaluate(doc)).isTrue();
    }",1
"@Test
    public void testConditionalExecutionStep() {
        String fieldExists1 = ""fieldExists1"";
        String fieldExists2 = ""fieldExists2"";
        String fieldToAdd = ""fieldToAdd"";
        String valueOnTrue = ""valueOnTrue"";
        String valueOnFalse = ""valueOnFalse"";

        Pipeline pipeline = createPipeline(createConditionalExecutionStep(
                createExistsCondition(fieldExists1, fieldExists2),
                createExecutionSteps(
                        createAddFieldExecutionStep(""newField1"", ""value1""),
                        createAddFieldExecutionStep(fieldToAdd, valueOnTrue)
                ),
                createExecutionSteps(
                        createAddFieldExecutionStep(""newField1"", ""value1""),
                        createAddFieldExecutionStep(fieldToAdd, valueOnFalse))));

        Doc doc1 = createDoc(fieldExists1, ""value1"", fieldExists2, ""value2"");
        ExecutionResult executionResult = pipelineExecutor.execute(pipeline, doc1);
        assertThat(executionResult.isSucceeded()).isTrue();
        assertThat(executionResult.isOvertime()).isFalse();
        assertThat(doc1.getSource().get(""newField1"")).isEqualTo(""value1"");
        String value1 = doc1.getField(fieldToAdd);
        assertThat(value1).isEqualTo(valueOnTrue);

        Doc doc2 = createDoc(fieldExists1, ""value3"");
        ExecutionResult executionResult2 = pipelineExecutor.execute(pipeline, doc2);
        assertThat(executionResult2.isSucceeded()).isTrue();
        assertThat(executionResult2.isOvertime()).isFalse();
        assertThat(doc2.getSource().get(""newField1"")).isEqualTo(""value1"");
        String value2 = doc2.getField(fieldToAdd);
        assertThat(value2).isEqualTo(valueOnFalse);
        assertThat(pipelineExecutorMetrics.getTotalDocsSucceededProcessing()).isEqualTo(2);
    }",1
"@Test
    public void testFailOnFailureExecutionStep() {
        Pipeline pipeline = createStopOnFailurePipeline(
                createAddFieldExecutionStep(""newField1"", ""value1""),
                createFailAlwaysExecutionStep(
                        createAddFieldExecutionStep(""newField2"", ""value2"")),
                        createFailAlwaysExecutionStep()
                );

        Doc doc = createDoc(""id"", ""testFailOnFailureExecutionStep"", ""message"", ""hola"");

        ExecutionResult executionResult = pipelineExecutor.execute(pipeline, doc);
        assertThat(executionResult.isSucceeded()).isFalse();

        assertThat(doc.getSource().get(""newField1"")).isEqualTo(""value1"");
        assertThat(doc.getSource().get(""newField2"")).isEqualTo(""value2"");
        assertThat(overtimeProcessingDocs.contains(doc)).isFalse();
        assertThat(executionResult.isOvertime()).isFalse();
        assertThat(pipelineExecutorMetrics.getTotalDocsFailedProcessing()).isEqualTo(1);
        assertThat(pipelineExecutorMetrics.getTotalDocsSucceededProcessing()).isEqualTo(0);
    }",1
"@Test
    public void testFailureWithException() {
        Pipeline pipeline = createStopOnFailurePipeline(
                createAddFieldExecutionStep(""newField1"", ""value1""),
                createFailAlwaysExecutionStep(new ProcessorExecutionException(""failProcessor"", new RuntimeException(""fail message"")))
        );
        Doc doc = createDoc(""id"", ""testFailureWithException"", ""message"", ""hola"",
                ""type"", ""test"");

        ExecutionResult result = pipelineExecutor.execute(pipeline, doc);
        assertThat(result.isSucceeded()).isFalse();
        assertThat(result.isOvertime()).isFalse();
        assertThat(result.getError().get().getException().isPresent()).isTrue();

        assertThat(doc.getSource().get(""newField1"")).isEqualTo(""value1"");
        assertThat(overtimeProcessingDocs.contains(doc)).isFalse();
        assertThat(pipelineExecutorMetrics.getTotalDocsFailedProcessing()).isEqualTo(1);
        assertThat(pipelineExecutorMetrics.getTotalDocsSucceededProcessing()).isEqualTo(0);
    }",1
"@Test
    public void testKillLongProcessingExecution() {
        Pipeline pipeline = createPipeline(
                createAddFieldExecutionStep(""newField1"", ""value1""),
                createSleepExecutionStep(EXPIRED_THRESHOLD_TIME_MS + 300)
        );
        Doc doc = createDoc(""id"", ""testKillLongProcessingExecution"", ""message"", ""hola"",
                ""type"", ""test"");

        ExecutionResult executionResult = pipelineExecutor.execute(pipeline, doc);
        assertThat(executionResult.isExpired()).isTrue();
        assertThat(Thread.currentThread().isInterrupted()).isFalse();

        assertThat(doc.getSource().get(""newField1"")).isEqualTo(""value1"");
        assertThat(overtimeProcessingDocs.contains(doc)).isTrue();
        assertThat(pipelineExecutorMetrics.getTotalDocsOvertimeProcessing()).isEqualTo(1);
        assertThat(pipelineExecutorMetrics.getTotalDocsProcessingExpired()).isEqualTo(1);
    }",1
"@Test
    public void testAppendValuesWhileFieldExist() {
        List<String> existingList = new ArrayList<>();
        existingList.add(EXISTING_VALUE);

        List<String> values = Arrays.asList(APPENDED_VALUE, ANOTHER_VALUE);
        AppendListProcessor appendListProcessor = createProcessor(AppendListProcessor.class, ""path"", FIELD_NAME, ""values"", values);
        Doc doc = createDoc(FIELD_NAME, existingList);

        assertThat(appendListProcessor.process(doc).isSucceeded()).isTrue();
        assertThat((List) doc.getField(FIELD_NAME)).isEqualTo(Arrays.asList(EXISTING_VALUE, APPENDED_VALUE, ANOTHER_VALUE));
    }",1
"@Test
    public void testAppendValuesWithTemplate() {
        List<String> values = Arrays.asList(ANOTHER_VALUE, ""{{"" + TEMPLATE_FIELD + ""}",1
"@Test
    public void testWithPartialColumnsNames() {
        String field = ""message"";
        String csv = ""1,\""this\"",is,an,ip,\""192.168.1.1\"",true"";

        Doc doc = createDoc(field, csv);

        Map<String,Object> config = new HashMap<>();
        config.put(""field"", field);
        config.put(""columns"", Arrays.asList(""id"", ""field1"", ""field2"", ""field3"", ""field4""));

        CsvProcessor csvProcessor = new CsvProcessor.Factory().create(config);

        ProcessResult processResult = csvProcessor.process(doc);

        assertThat(processResult.isSucceeded()).isTrue();
        assertThat((String) doc.getField(""id"")).isEqualTo(""1"");
        assertThat((String) doc.getField(""field1"")).isEqualTo(""this"");
        assertThat((String) doc.getField(""field2"")).isEqualTo(""is"");
        assertThat((String) doc.getField(""field3"")).isEqualTo(""an"");
        assertThat((String) doc.getField(""field4"")).isEqualTo(""ip"");
        assertThat((String) doc.getField(""column6"")).isEqualTo(""192.168.1.1"");
        assertThat((String) doc.getField(""column7"")).isEqualTo(""true"");
    }",1
"@Test
    public void testWithSeparatorAndQuoteChar() {
        String field = ""message"";
        String csv = ""1\t\\this\\\tis\tan\tip\t\\192.168.1.1\\"";

        Doc doc = createDoc(field, csv);

        Map<String,Object> config = new HashMap<>();
        config.put(""field"", field);
        config.put(""separator"", ""\t"");
        config.put(""quoteChar"", ""\\"");

        CsvProcessor csvProcessor = new CsvProcessor.Factory().create(config);

        ProcessResult processResult = csvProcessor.process(doc);

        assertThat(processResult.isSucceeded()).isTrue();
        assertThat((String) doc.getField(""column1"")).isEqualTo(""1"");
        assertThat((String) doc.getField(""column2"")).isEqualTo(""this"");
        assertThat((String) doc.getField(""column3"")).isEqualTo(""is"");
        assertThat((String) doc.getField(""column4"")).isEqualTo(""an"");
        assertThat((String) doc.getField(""column5"")).isEqualTo(""ip"");
        assertThat((String) doc.getField(""column6"")).isEqualTo(""192.168.1.1"");
    }",1
"@Test
    public void testIso8601Format() {
        String field = ""datetime"";
        String targetField = ""@timestamp"";
        ZoneId zoneId = ZoneId.of(""UTC"");
        ZonedDateTime zonedDateTime = LocalDateTime.now().atZone(zoneId);
        String iso8601Format1 = zonedDateTime.format(DateTimeFormatter.ofPattern(""yyyy-MM-dd'T'HH:mm:ss,SSS""));
        String iso8601Format2 = zonedDateTime.format(DateTimeFormatter.ofPattern(""yyyy-MM-dd'T'HH:mm:ss.SSSSSSxxx""));
        Doc doc = createDoc(field, iso8601Format1);

        Map<String,Object> config = createConfig(""field"", field,
                ""targetField"", targetField,
                ""formats"", Arrays.asList(""ISO8601""),
                ""timeZone"", zoneId.toString());

        DateProcessor dateProcessor = createProcessor(DateProcessor.class, config);

        assertThat(dateProcessor.process(doc).isSucceeded()).isTrue();
        assertThat((String) doc.getField(targetField)).isEqualTo(zonedDateTime.format(DateProcessor.ELASTIC));

        doc = createDoc(field, iso8601Format2);

        assertThat(dateProcessor.process(doc).isSucceeded()).isTrue();
        assertThat((String) doc.getField(targetField)).isEqualTo(zonedDateTime.format(DateProcessor.ELASTIC));
    }",1
"@Test
    public void testISOFormatWithNumberValueInField() {
        String field = ""datetime"";
        String targetField = ""timestamp"";

        ZoneId zoneId = ZoneId.of(""UTC"");
        ZonedDateTime zonedDateTime = LocalDateTime.now().atZone(zoneId);
        Doc doc = createDoc(field, zonedDateTime.toInstant().toEpochMilli() / 1000);

        Map<String,Object> config = createConfig(""field"", field,
                ""targetField"", targetField,
                ""formats"", Arrays.asList(""dd/MMM/yyyy:HH:mm:ss""));

        DateProcessor dateProcessor = createProcessor(DateProcessor.class, config);

        assertThat(dateProcessor.process(doc).isSucceeded()).isFalse();
        assertThat(doc.hasField(targetField)).isFalse();
    }",1
"@Test
    public void testOutputForamtUnix() {
        String field = ""datetime"";
        String targetField = ""@timestamp"";
        ZoneId zoneId = ZoneId.of(""Europe/Paris"");
        String outputFormat = ""UNIX"";
        ZonedDateTime zonedDateTime = LocalDateTime.now().atZone(zoneId);
        String iso8601Format = zonedDateTime.format(DateTimeFormatter.ofPattern(""yyyy-MM-dd'T'HH:mm:ss,SSS""));
        Doc doc = createDoc(field, iso8601Format);

        Map<String,Object> config = createConfig(""field"", field,
                ""targetField"", targetField,
                ""formats"", Arrays.asList(""ISO8601""),
                ""outputFormat"", outputFormat,
                ""timeZone"", zoneId.toString());

        DateProcessor dateProcessor = createProcessor(DateProcessor.class, config);

        assertThat(dateProcessor.process(doc).isSucceeded()).isTrue();
        assertThat((String)doc.getField(targetField)).isEqualTo(zonedDateTime.format(DateProcessor.UNIX));
    }",1
"@Test
    public void testParseInvalidObjects() {
        String field = ""datetime"";
        String targetField = ""@timestamp"";
        ZoneId zoneId = ZoneId.of(""UTC"");
        Doc docWithMap = createDoc(field, ImmutableMap.of(""its"", ""a"", ""map"", ""should"", ""not"", ""work""));

        Map<String,Object> config = createConfig(""field"", field,
                ""targetField"", targetField,
                ""formats"", Arrays.asList(""UNIX_MS""),
                ""timeZone"", zoneId.toString());

        DateProcessor dateProcessor = createProcessor(DateProcessor.class, config);

        assertThat(dateProcessor.process(docWithMap).isSucceeded()).isFalse();

        Doc docWithList = createDoc(field, Arrays.asList(""its"", ""a"", ""list"", ""should"", ""not"", ""work""));

        assertThat(dateProcessor.process(docWithList).isSucceeded()).isFalse();
    }",1
"@Test
    public void testSeveralISOPatterns() {
        String field = ""datetime"";
        String targetField = ""@timestamp"";
        ZoneId zoneId = ZoneId.of(""UTC"");
        ZonedDateTime zonedDateTime = LocalDateTime.now().atZone(zoneId);
        List<String> formats = Arrays.asList(""dd/MM/yyyy HH:mm:ss"", ""yyyy-MM-dd'T'HH:mm:ssZ"", ""yyyy-MM-dd HH:mm:ss.SSSSS"", ""ddMMyyyy HHmmssSSS"");

        Map<String,Object> config = createConfig(""field"", field,
                ""targetField"", targetField,
                ""formats"", formats,
                ""timeZone"", zoneId.toString());

        DateProcessor dateProcessor = createProcessor(DateProcessor.class, config);

        formats.forEach(format -> {
            DateTimeFormatter formatter = DateTimeFormatter.ofPattern(format);
            String dateString = zonedDateTime.format(formatter);
            Doc doc = createDoc(field, dateString);

            ZonedDateTime expectedDateTime = LocalDateTime.parse(dateString, formatter).atZone(zoneId);

            assertThat(dateProcessor.process(doc).isSucceeded()).isTrue();
            assertThat((String) doc.getField(targetField)).isEqualTo(expectedDateTime.format(DateProcessor.ELASTIC));
        }",1
"@Test
    public void testUnixFormatString() {
        String field = ""datetime"";
        String targetField = ""@timestamp"";
        ZoneId zoneId = ZoneId.of(""Europe/Paris"");
        ZonedDateTime zonedDateTime = LocalDateTime.now().atZone(zoneId).truncatedTo(ChronoUnit.SECONDS);
        Doc doc = createDoc(field, String.valueOf(zonedDateTime.toInstant().toEpochMilli() / 1000));

        Map<String,Object> config = createConfig(""field"", field,
                ""targetField"", targetField,
                ""formats"", Arrays.asList(""UNIX""),
                ""timeZone"", zoneId.toString());

        DateProcessor dateProcessor = createProcessor(DateProcessor.class, config);

        assertThat(dateProcessor.process(doc).isSucceeded()).isTrue();
        assertThat((String) doc.getField(targetField)).isEqualTo(zonedDateTime.format(DateProcessor.ELASTIC));
    }",1
"@Test
    public void testUnixMsFormat() {
        String field = ""datetime"";
        String targetField = ""@timestamp"";
        ZoneId zoneId = ZoneId.of(""Europe/Paris"");
        ZonedDateTime zonedDateTime = LocalDateTime.now().atZone(zoneId);
        Doc doc = createDoc(field, zonedDateTime.toInstant().toEpochMilli());

        Map<String,Object> config = createConfig(""field"", field,
                ""targetField"", targetField,
                ""formats"", Arrays.asList(""UNIX_MS""),
                ""timeZone"", zoneId.toString());

        DateProcessor dateProcessor = createProcessor(DateProcessor.class, config);

        assertThat(dateProcessor.process(doc).isSucceeded()).isTrue();
        assertThat((String)doc.getField(targetField)).isEqualTo(zonedDateTime.format(DateProcessor.ELASTIC));
    }",1
"@Test
    public void testUnixMsFormatString() {
        String field = ""datetime"";
        String targetField = ""@timestamp"";
        ZoneId zoneId = ZoneId.of(""Europe/Paris"");
        ZonedDateTime zonedDateTime = LocalDateTime.now().atZone(zoneId);
        Doc doc = createDoc(field, String.valueOf(zonedDateTime.toInstant().toEpochMilli()));

        Map<String,Object> config = createConfig(""field"", field,
                ""targetField"", targetField,
                ""formats"", Arrays.asList(""UNIX_MS""),
                ""timeZone"", zoneId.toString());

        DateProcessor dateProcessor = createProcessor(DateProcessor.class, config);

        assertThat(dateProcessor.process(doc).isSucceeded()).isTrue();
        assertThat((String)doc.getField(targetField)).isEqualTo(zonedDateTime.format(DateProcessor.ELASTIC));
    }",1
"@Test
    public void testInternalIp() {
        String ip = ""192.168.1.1"";
        String source = ""ipString"";
        String target = ""geoip"";

        Map<String, Object> config = createConfig(""sourceField"", source,
                ""tagsOnSuccess"", Arrays.asList(""geoip""));

        GeoIpProcessor geoIpProcessor = createProcessor(GeoIpProcessor.class, config);

        Doc doc = createDoc(source, ip);

        assertThat(geoIpProcessor.process(doc).isSucceeded()).isTrue();
        assertThat(doc.hasField(target)).isFalse();
    }",1
"@Test
    public void testValidIpWithSpecificProperties() {
        String ip = ""81.2.69.144"";
        String source = ""ipString"";

        Map<String,Object> config = createConfig(""sourceField"", source,
                ""properties"", Arrays.asList(""ip"", ""country_name"", ""country_code2"", ""city_name""));
        GeoIpProcessor geoIpProcessor = createProcessor(GeoIpProcessor.class, config);

        Doc doc = createDoc(source, ip);

        ProcessResult processResult = geoIpProcessor.process(doc);

        assertThat(processResult.isSucceeded()).isTrue();
        assertThat(doc.hasField(""geoip"")).isTrue();
        Map<String, Object> geoIp = doc.getField(""geoip"");
        assertThat(geoIp.size()).isGreaterThanOrEqualTo(3);
        assertThat(geoIp.get(""country_name"")).isEqualTo(""United Kingdom"");
        assertThat(geoIp.get(""ip"")).isEqualTo(ip);
    }",1
"@Test
    public void testConverters() throws InterruptedException {
        String field = ""message"";
        List<String> patterns = Arrays.asList(""%{NUMBER:int:int}",1
"@Test
    public void testGrokParseFailure() throws InterruptedException {
        String field = ""message"";
        List<String> patterns = Arrays.asList(""%{COMBINEDAPACHELOG}",1
"@Test
    public void testPatternsPriority() throws InterruptedException {
        String field = ""message"";
        List<String> patterns = Arrays.asList(
                ""%{COMBINEDAPACHELOG}",1
"@Test
    public void testAllowDuplicateValues() throws InterruptedException {
        String field = ""message"";
        Doc doc = createDoc(field, KEY_VALUE_MESSAGE_WITH_DUPLICATE_KEYS);

        Map<String,Object> config = createConfig(""field"", field);

        KeyValueProcessor kvProcessor = createProcessor(KeyValueProcessor.class, config);

        ProcessResult processResult = kvProcessor.process(doc);

        assertThat(processResult.isSucceeded()).isTrue();
        assertThat((List) doc.getField(""sameKey"")).isEqualTo(Arrays.asList(""value1"", ""value2"", ""value3"", ""value4""));
    }",1
"@Test
    public void testDontAllowDuplicateValues() throws InterruptedException {
        String field = ""message"";
        Doc doc = createDoc(field, KEY_VALUE_MESSAGE_WITH_DUPLICATE_KEYS);

        Map<String,Object> config = createConfig(""field"", field,
                ""allowDuplicateValues"", false);

        KeyValueProcessor kvProcessor = createProcessor(KeyValueProcessor.class, config);

        ProcessResult processResult = kvProcessor.process(doc);

        assertThat(processResult.isSucceeded()).isTrue();
        assertThat((String) doc.getField(""sameKey"")).isEqualTo(""value1"");
    }",1
"@Test
    public void testNullField() throws InterruptedException {
        String field = ""message"";
        Doc doc = createDoc(field, null);

        Map<String,Object> config = createConfig(""field"", field);

        KeyValueProcessor kvProcessor = createProcessor(KeyValueProcessor.class, config);

        ProcessResult processResult = kvProcessor.process(doc);

        assertThat(processResult.isSucceeded()).isFalse();
    }",1
"@Test
    public void testWithTargetField() throws InterruptedException {
        String field = ""message"";
        String targetField = ""kv"";
        Doc doc = createDoc(field, getDefaultMessage());

        Map<String,Object> config = createConfig(""field"", field,
                ""targetField"", targetField);

        KeyValueProcessor kvProcessor = createProcessor(KeyValueProcessor.class, config);

        ProcessResult processResult = kvProcessor.process(doc);

        assertThat(processResult.isSucceeded()).isTrue();
        assertThat(doc.hasField(targetField)).isTrue();
        Map<Object,String> kv = doc.getField(targetField);
        assertThat(kv.get(""simple"")).isEqualTo(""value"");
        assertThat(kv.get(""brackets"")).isEqualTo(""with space"");
        assertThat(kv.get(""roundBrackets"")).isEqualTo(""with two spaces"");
        assertThat(kv.get(""angleBrackets"")).isEqualTo(""without"");
        assertThat(kv.get(""%trim%"")).isEqualTo(""!value!"");
        assertThat(kv.get(""complex"")).isEqualTo(""innerKey=innerValue withBrackets=(another innerValue)"");
    }",1
"@Test
    public void testRenameField() {
        String fromField = RandomStringUtils.randomAlphanumeric(5);
        String nestedToField = RandomStringUtils.randomAlphanumeric(5) + ""."" + RandomStringUtils.randomAlphanumeric(5);
        Doc doc = createDoc(fromField, ""value"");

        Map<String, Object> config = createConfig(""from"", fromField,
                ""to"", nestedToField);
        RenameFieldProcessor renameFieldProcessor = createProcessor(RenameFieldProcessor.class, config);

        assertThat(renameFieldProcessor.process(doc).isSucceeded()).isTrue();

        assertThat((String)doc.getField(nestedToField)).isEqualTo(""value"");
        assertThatThrownBy(() -> doc.getField(fromField)).isInstanceOf(IllegalStateException.class);
    }",1
"@Test
    public void testRenameJsonWithTemplateInTo() {
        Map valueBeingRename = ImmutableMap.of(""x"", 5);

        Doc doc = createDoc(
                ""field-a"", ""field-b"",
                ""field-c"", valueBeingRename);

        Map<String, Object> config = createConfig(
                ""from"", ""field-c"",
                ""to"", ""{{field-a}",1
"@Test
    public void testRenameWithTemplateInTo() {
        Doc doc = createDoc(""field-a"", ""field-b"",
                            ""field-c"", ""value-of-c"");

        Map<String, Object> config = createConfig(
                ""from"", ""field-c"",
                ""to"", ""{{field-a}",1
"@Test
    public void testStringFieldNotContainedSeparator() {
        String field = ""fieldName"";
        String value = ""this string is gonna be without any commas"";

        Doc doc = createDoc(field, value);

        SplitProcessor splitProcessor = createProcessor(SplitProcessor.class, createConfig(""field"", field, ""separator"", "",""));

        ProcessResult processResult = splitProcessor.process(doc);

        assertThat(processResult.isSucceeded()).isTrue();
        assertThat((String) doc.getField(field)).isEqualTo(value);
    }",1
"@Test
    public void testSubstitute() {
        String field = ""message"";
        String message = ""I'm g@nna \""remove\"" $ome spec!al characters"";

        String pattern = ""\\$|@|!|\\\""|'"";
        String replacement = ""."";
        Map<String, Object> config = createConfig(""field"", field,
                ""pattern"", pattern,
                ""replacement"", replacement);

        Doc doc = createDoc(field, message);

        SubstituteProcessor substituteProcessor = createProcessor(SubstituteProcessor.class, config);

        ProcessResult processResult = substituteProcessor.process(doc);

        assertThat(processResult.isSucceeded()).isTrue();
        assertThat((String) doc.getField(field)).isEqualTo(""I.m g.nna .remove. .ome spec.al characters"");
    }",1
"@Test
    public void testListOfStringFields() {
        List<String> fields = Arrays.asList(""field1"", ""field2"", ""field3"");

        Doc doc = createDoc(""field1"", ""lower case"",
                ""field2"", ""camelCase"",
                ""field3"", ""UPPER CASE"");

        UpperCaseProcessor upperCaseProcessor = createProcessor(UpperCaseProcessor.class, ""fields"", fields);

        ProcessResult processResult = upperCaseProcessor.process(doc);

        assertThat(processResult.isSucceeded()).isTrue();
        assertThat((String) doc.getField(""field1"")).isEqualTo(""LOWER CASE"");
        assertThat((String) doc.getField(""field2"")).isEqualTo(""CAMELCASE"");
        assertThat((String) doc.getField(""field3"")).isEqualTo(""UPPER CASE"");
    }",1
"@Test
    public void testLongInvalidUserAgent() {
        String field = ""agent"";
        String targetField = ""user_agent"";
        String uaString = ""CuYDtymfoAScnOxlaYbvTZiOZEWVJsbLZIZBGPvDVjuqcxytUchOaksgiArcMcBhbmfynRsdFMpgSpUrbLqPncRqpMLuvlNvAkEhllRxWcTQSTRqVZVBfIYDVaCdZSd"" +
                ""nGivuEqYjSvNrywusJkRPyduDWrLVulUhjwcjIdBcdGscnJbSMdnHAhynpMrkKCGZbObUUHVrnDQKahvIgWImBAjfmGiFiRkqMwbarGNyZKyAARSairELcVQjJUntvraJvwjaLERayVhdA"" +
                ""ChAQPQtxDLJWKFgrPsmopUOGIHDsvWfesGmBMlmtBuqVZYLmhdntvtMQIoWSETeqMWbvZhodEqewrrZhIepPffvrQCYaNtVLQDHtRxLtPkTotNypeNFrWpUDybqFGhXXMoGZmLjQIsEMYSNpAbx"" +
                ""GjwelEhCGtUWVpLQpqRjdImPEPJoWFjqMDYfmgXHiLXAZTxUWXIolDvPuAWQMMSdMWGoqunKcscNUgfLpepUUKcQInfkNtbfVgbKHQVnIQpxxsClYbdfUmtclWtEPlnacpqBmmTvqAuqFnETBCMoiw"" +
                ""MTNGYMkcnpnATSkFIiiBdfNDRxqFSVLjyJvErEUaNLwjZBjZQaTlZRqcdJLnNyncDiJnuTjYeBNbjesLxPHuhnmWQMyAqSUXQNnlAQEyIuMeMgUDOopchnbywdvZPAFeFqZiRZHvcyOuPfrrHnyTDXhvrb"" +
                ""PvsjXhGORhwKVxqeFTekIhaXJVRMyosFcixPIrMEJWQTWlrcBUkCoNFNkoAFgNgLnZKLNhqKGKMXAPydwaCDWnMpfSuLVrKtJItkPQlorTRZFumHILjeTXYWBQbFhsZQXZCgGKfFFVRmODbjNGuTGFBRNTkF"" +
                ""nQkyORSPrTBRYYoCnPrJISdVxKBDaNDObbUUdMruDDoZRHxiRmOLWoYXwJvYblLNTVPAWkuHjhaETjVrJIyjmDJJcwpGhjFsHbNkUviOonqgssqjRidcrOibeXmmZxopIPrJIQyFOpEOpqCxWXotAefjlUtraVxC"" +
                ""iVXZAnwbvAASmAZnVWEMfeyDnZsyAYkKkHVgfRZMNfvBTRHpOiljWfPxlfuGwEIOHDPjJrqcvBidPoGASKqGucXnHrCApdJLVsIfVgQvNdiipSULVHhTvCcYMvMbPEOLiLtxHxwSmmGNNyGHlPJHixdBnVecmlwDggRArN"" +
                ""YFThgTPZnhJLOVdnAvAqoLkNmnVZtqsycjxaOWgQHdZRGFLePqaoibfSYjZERticqPpDymJgQDgMLTbpwIbqHggWaNEcAeUKlRjImqLjeJDBkELMIvhPSWDWsoZAcpBNnFYKtSbMEGtTWUjNQAJUOLUVqFFWQeS"" +
                ""LcuegjfsKxRyLSLOSbhLimlZSbBTWvZOqHuSaWGGnHtEQODgmlovxEOLeVxromEJDjXOPKSxFTaoNbFBGdvaPyIhBiDugAUEptbgQDIBKWXAVBCBiTjNolVjpTbMqYyucMfTVuCVqOHqxOXamDEjlUgXKBfCdwuFiKFsFfut"" +
                ""pcBMdYntTglEUMDigTQeDmfRErWVxJDNJhgFyCwtxcXqCtStUqHLPiwKZMrwkqDyexijsBpSdtlKBRXWCXMjrXYQmcsqgLThYWPvmrNTbyfpNEQUCimIGryiHnyCLkIxwqeZCsGpbeenALhrCfNcNCumNqwoa"" +
                ""gPXIXySbhpLkkqPZZqIWAqeaufviIYnLKswSxoLQpMOlMBhmkkBPjejWHjflHJtFZUSZWoytPnpjGYOOBFdqDMpDLOwoZsnuAIRwdepUreybIyQIafxigaLrayUNisocGYdlJJWagcoNPjTUtdjWiWwwqeLXh"" +
                ""KvJGGVYgBSMYGeeLKsuEliUYQNZVAurpNmrlHCYrIpTdURiPRTWACViTHnUUvHZYWOsgUUWgVKGBQQfuMOVoMWuoIpfaVoNoVvUnlbdoTvpCcbyGOyEdInlatGCxgwAKYKDlyUbgcwDNoEtotgLJOYXYxoxh"" +
                ""rrXtnMCXjsJtWfdBUfHtTZXvibUxVrqcCxjpQFVhCCNrtvmxKyPPAEMmKYaYbbWgXCiXBHgUJKprxlaLHbgPlfAnNggqUHhkuDXQR"";
        Doc doc = createDoc(field, uaString);

        Map<String,Object> config = createConfig(""field"", field,
                ""targetField"", targetField);
        UserAgentProcessor uaProceesor = createProcessor(UserAgentProcessor.class, config);
        ProcessResult processResult = uaProceesor.process(doc);

        assertThat(processResult.isSucceeded()).isTrue();
        assertThat(doc.hasField(targetField)).isTrue();

        Map<String,String> userAgent = doc.getField(targetField);

        assertThat(userAgent.get(""name"")).isEqualTo(""Other"");
        assertThat(userAgent.get(""os"")).isEqualTo(""Other"");
        assertThat(userAgent.get(""os_name"")).isEqualTo(""Other"");
        assertThat(userAgent.get(""device"")).isEqualTo(""Other"");

        assertThat(doc.hasField(""tags"")).isTrue();
        assertThat((List)doc.getField(""tags"")).isNotEmpty().contains(""_user_agent_truncated"");

    }",1
"@Test
    public void testUserAgentWithPrefix() {
        String field = ""agent"";
        String prefix = ""UA-"";
        String uaString = ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.98 Safari/537.36"";
        Doc doc = createDoc(field, uaString);

        Map<String,Object> config = createConfig(""field"", field,
                ""prefix"", prefix);
        UserAgentProcessor uaProceesor = createProcessor(UserAgentProcessor.class, config);

        ProcessResult processResult = uaProceesor.process(doc);

        assertThat(processResult.isSucceeded()).isTrue();
        assertThat((String)doc.getField(prefix + ""name"")).isEqualTo(""Chrome"");
        assertThat((String)doc.getField(prefix + ""major"")).isEqualTo(""54"");
        assertThat((String)doc.getField(prefix + ""minor"")).isEqualTo(""0"");
        assertThat((String)doc.getField(prefix + ""patch"")).isEqualTo(""2840"");

        assertThat((String)doc.getField(prefix + ""os"")).isEqualTo(""Mac OS X 10.10.5"");
        assertThat((String)doc.getField(prefix + ""os_name"")).isEqualTo(""Mac OS X"");
        assertThat((String)doc.getField(prefix + ""os_major"")).isEqualTo(""10"");
        assertThat((String)doc.getField(prefix + ""os_minor"")).isEqualTo(""10"");
        assertThat((String)doc.getField(prefix + ""os_patch"")).isEqualTo(""5"");

        assertThat((String)doc.getField(prefix + ""device"")).isEqualTo(""Other"");
    }",1
"@Test
    public void testInvalidXml() {
        String field = ""xml"";

        Doc doc = createDoc(field, INVALID_XML);

        XmlProcessor xmlProcessor = createProcessor(XmlProcessor.class, ""field"", field);

        ProcessResult processResult = xmlProcessor.process(doc);

        assertThat(processResult.isSucceeded()).isFalse();
    }",1
"@Test
    public void testValidXml() {
        String field = ""xml"";

        Doc doc = createDoc(field, VALID_XML);

        XmlProcessor xmlProcessor = createProcessor(XmlProcessor.class, ""field"", field);

        ProcessResult processResult = xmlProcessor.process(doc);

        assertThat(processResult.isSucceeded()).isTrue();
        assertThat((String) doc.getField(""country.id"")).isEqualTo(""1"");
        assertThat((String) doc.getField(""country.name"")).isEqualTo(""Israel"");
        assertThat(doc.hasField(""country.TestEmptyField"")).isFalse();
        assertThatThrownBy(() -> doc.getField(""country.TestEmptyField"")).isInstanceOf(IllegalStateException.class);
        assertThat((List) doc.getField(""country.cities.city""))
                .isEqualTo(Arrays.asList(ImmutableMap.of(""name"", ""Jerusalem""),
                        ImmutableMap.of(""name"", ""Tel Aviv"")));
        assertThat((String) doc.getField(""country.lat"")).isEqualTo(""31.0461"");
        assertThat((String) doc.getField(""country.long"")).isEqualTo(""34.8516"");
        assertThat((String) doc.getField(""country.continent"")).isEqualTo(""Asia"");
        assertThat((String) doc.getField(""country.currency"")).isEqualTo(""New Shekel"");
        assertThat((List) doc.getField(""country.languages.language"")).isEqualTo(Arrays.asList(""Hebrew"", ""Arabic"", ""English""));
    }",1
"@Test
    public void testDocWithListField() {
        Template listTemplate = templateService.createTemplate(""this is {{list}",1
"@Test
    public void test_v4_1_16() throws Exception {
        NavMesh mesh = loadNavMesh(""graph_v4_1_16.zip"");
        float[] startPos = new float[] { 22.93f, -2.37f, -5.11f }",1
"@Test
    public void testDungeon32Bit() throws IOException {
        InputStream is = getClass().getClassLoader().getResourceAsStream(""dungeon_all_tiles_navmesh_32bit.bin"");
        NavMesh mesh = reader.read32Bit(is, 6);
        assertThat(mesh.getMaxTiles()).isEqualTo(128);
        assertThat(mesh.getParams().maxPolys).isEqualTo(0x8000);
        assertThat(mesh.getParams().tileWidth).isEqualTo(9.6f, offset(0.001f));
        List<MeshTile> tiles = mesh.getTilesAt(6, 9);
        assertThat(tiles).hasSize(1);
        assertThat(tiles.get(0).data.polys).hasSize(2);
        assertThat(tiles.get(0).data.verts).hasSize(7 * 3);
        tiles = mesh.getTilesAt(2, 9);
        assertThat(tiles).hasSize(1);
        assertThat(tiles.get(0).data.polys).hasSize(2);
        assertThat(tiles.get(0).data.verts).hasSize(9 * 3);
        tiles = mesh.getTilesAt(4, 3);
        assertThat(tiles).hasSize(1);
        assertThat(tiles.get(0).data.polys).hasSize(3);
        assertThat(tiles.get(0).data.verts).hasSize(6 * 3);
        tiles = mesh.getTilesAt(2, 8);
        assertThat(tiles).hasSize(1);
        assertThat(tiles.get(0).data.polys).hasSize(5);
        assertThat(tiles.get(0).data.verts).hasSize(17 * 3);
    }",1
"@Test
	public void testMarshalAndSendNoUnmarshallerSet() throws Exception {

		connectionMock.close();

		template.setUnmarshaller(null);
		assertThatIllegalStateException().isThrownBy(() -> template.marshalSendAndReceive(new Object()));
	}",1
"@Test
	public void testSendAndReceiveMarshalNoResponse() throws Exception {

		Marshaller marshallerMock = mock(Marshaller.class);
		template.setMarshaller(marshallerMock);
		marshallerMock.marshal(isA(Object.class), isA(Result.class));

		connectionMock.send(isA(WebServiceMessage.class));
		when(connectionMock.hasError()).thenReturn(false);
		when(connectionMock.receive(messageFactory)).thenReturn(null);
		connectionMock.close();

		Object result = template.marshalSendAndReceive(new Object());

		assertThat(result).isNull();
	}",1
"@Test
	public void testSendAndReceiveMessageNoResponse() throws Exception {

		WebServiceMessageExtractor extractorMock = mock(WebServiceMessageExtractor.class);

		connectionMock.send(isA(WebServiceMessage.class));
		when(connectionMock.hasError()).thenReturn(false);
		when(connectionMock.receive(messageFactory)).thenReturn(null);
		connectionMock.close();

		Object result = template.sendAndReceive(null, extractorMock);

		assertThat(result).isNull();
	}",1
"@Test
	public void testSendAndReceiveResultNoResponse() throws Exception {

		connectionMock.send(isA(WebServiceMessage.class));
		when(connectionMock.hasError()).thenReturn(false);
		when(connectionMock.receive(messageFactory)).thenReturn(null);
		connectionMock.close();

		StringResult result = new StringResult();
		boolean b = template.sendSourceAndReceiveToResult(new StringSource(""<request />""), result);

		assertThat(b).isFalse();
	}",1
"@Test
	public void ordering() {

		ApplicationContext applicationContext = new ClassPathXmlApplicationContext(
				""interceptorsBeanDefinitionParserOrderTest.xml"", getClass());

		List<DelegatingSmartEndpointInterceptor> interceptors = new ArrayList<DelegatingSmartEndpointInterceptor>(
				applicationContext.getBeansOfType(DelegatingSmartEndpointInterceptor.class).values());

		assertThat(interceptors).hasSize(6);

		for (int i = 0; i < interceptors.size(); i++) {

			DelegatingSmartEndpointInterceptor delegatingInterceptor = interceptors.get(i);
			MyInterceptor interceptor = (MyInterceptor) delegatingInterceptor.getDelegate();

			assertThat(interceptor.getOrder()).isEqualTo(i);
		}",1
"@Test
	public void invoke() throws Exception {

		MessageFactory messageFactory = MessageFactory.newInstance();
		SOAPMessage request = messageFactory.createMessage();
		request.getSOAPBody().addBodyElement(QName.valueOf(""{http://springframework.org/spring-ws}",1
"@Test
	public void registrationSingle() throws NoSuchMethodException {

		MethodEndpoint endpoint = mapping.lookupEndpoint(new QName(""http://springframework.org/spring-ws"", ""Request""));

		assertThat(endpoint).isNotNull();

		Method doIt = MyEndpoint.class.getMethod(""doIt"", Source.class);
		MethodEndpoint expected = new MethodEndpoint(""endpoint"", applicationContext, doIt);

		assertThat(endpoint).isEqualTo(expected);
	}",1
"@Test
	public void testNoMatch() throws Exception {

		SaajSoapMessage message = loadSaajMessage(""200408/response-no-message-id.xml"");
		MessageContext messageContext = new DefaultMessageContext(message, new SaajSoapMessageFactory(messageFactory));

		EndpointInvocationChain endpoint = mapping.getEndpoint(messageContext);

		assertThat(endpoint).isNull();
	}",1
"@Test
	public void testProcessMustUnderstandHeadersForActorSoap11() throws Exception {

		MessageFactory messageFactory = MessageFactory.newInstance(SOAPConstants.SOAP_1_1_PROTOCOL);
		SOAPMessage request = messageFactory.createMessage();
		SOAPHeaderElement header = request.getSOAPHeader()
				.addHeaderElement(new QName(""http://www.springframework.org"", ""Header"", ""spring-ws""));
		String headerActor = ""http://www/springframework.org/role"";
		header.setActor(headerActor);
		header.setMustUnderstand(true);
		SoapMessageFactory factory = new SaajSoapMessageFactory(messageFactory);
		MessageContext context = new DefaultMessageContext(new SaajSoapMessage(request), factory);
		expect(interceptorMock.understands(isA(SoapHeaderElement.class))).andReturn(true);

		replay(interceptorMock);

		SoapEndpointInvocationChain chain = new SoapEndpointInvocationChain(new Object(),
				new SoapEndpointInterceptor[] { interceptorMock }",1
"@Test
	public void testProcessMustUnderstandHeadersNotUnderstoodSoap12() throws Exception {

		MessageFactory messageFactory = MessageFactory.newInstance(SOAPConstants.SOAP_1_2_PROTOCOL);
		SOAPMessage request = messageFactory.createMessage();
		SOAPHeaderElement header = request.getSOAPHeader()
				.addHeaderElement(new QName(""http://www.springframework.org"", ""Header"", ""spring-ws""));
		header.setMustUnderstand(true);
		header.setRole(SOAPConstants.URI_SOAP_1_2_ROLE_NEXT);
		SoapMessageFactory factory = new SaajSoapMessageFactory(messageFactory);
		MessageContext context = new DefaultMessageContext(new SaajSoapMessage(request), factory);
		expect(interceptorMock.understands(isA(SoapHeaderElement.class))).andReturn(false);

		replay(interceptorMock);

		SoapEndpointInvocationChain chain = new SoapEndpointInvocationChain(new Object(),
				new SoapEndpointInterceptor[] { interceptorMock }",1
"@Test
	public void testDetectWsdlDefinitions() throws Exception {

		servlet.setContextClass(WsdlDefinitionWebApplicationContext.class);
		servlet.init(config);
		MockHttpServletRequest request = new MockHttpServletRequest(HttpTransportConstants.METHOD_GET, ""/definition.wsdl"");
		MockHttpServletResponse response = new MockHttpServletResponse();
		servlet.service(request, response);
		DocumentBuilderFactory documentBuilderFactory = DocumentBuilderFactoryUtils.newInstance();
		documentBuilderFactory.setNamespaceAware(true);
		DocumentBuilder documentBuilder = documentBuilderFactory.newDocumentBuilder();
		Document result = documentBuilder.parse(new ByteArrayInputStream(response.getContentAsByteArray()));
		Document expected = documentBuilder.parse(getClass().getResourceAsStream(""wsdl11-input.wsdl""));

		XmlAssert.assertThat(result).and(expected).ignoreWhitespace().areIdentical();
	}",1
"@Test
	public void testPopulateBinding() throws Exception {

		String namespace = ""http://springframework.org/spring-ws"";
		definition.addNamespace(""tns"", namespace);
		definition.setTargetNamespace(namespace);

		PortType portType = definition.createPortType();
		portType.setQName(new QName(namespace, ""PortType""));
		portType.setUndefined(false);
		definition.addPortType(portType);
		Operation operation = definition.createOperation();
		operation.setName(""Operation"");
		operation.setUndefined(false);
		operation.setStyle(OperationType.REQUEST_RESPONSE);
		portType.addOperation(operation);
		Input input = definition.createInput();
		input.setName(""Input"");
		operation.setInput(input);
		Output output = definition.createOutput();
		output.setName(""Output"");
		operation.setOutput(output);
		Fault fault = definition.createFault();
		fault.setName(""Fault"");
		operation.addFault(fault);

		Properties soapActions = new Properties();
		soapActions.setProperty(""Operation"", namespace + ""/Action"");
		provider.setSoapActions(soapActions);

		provider.setServiceName(""Service"");

		String locationUri = ""http://localhost:8080/services"";
		provider.setLocationUri(locationUri);

		provider.setCreateSoap11Binding(true);
		provider.setCreateSoap12Binding(true);

		provider.addBindings(definition);
		provider.addServices(definition);

		Binding binding = definition.getBinding(new QName(namespace, ""PortTypeSoap11""));

		assertThat(binding).isNotNull();

		binding = definition.getBinding(new QName(namespace, ""PortTypeSoap12""));

		assertThat(binding).isNotNull();

		Service service = definition.getService(new QName(namespace, ""Service""));

		assertThat(service).isNotNull();
		assertThat(service.getPorts()).hasSize(2);

		Port port = service.getPort(""PortTypeSoap11"");

		assertThat(port).isNotNull();

		port = service.getPort(""PortTypeSoap12"");

		assertThat(port).isNotNull();
	}",1
"@Test
	public void testAddMessages() throws Exception {

		String definitionNamespace = ""http://springframework.org/spring-ws"";
		definition.addNamespace(""tns"", definitionNamespace);
		definition.setTargetNamespace(definitionNamespace);
		String schemaNamespace = ""http://www.springframework.org/spring-ws/schema"";
		definition.addNamespace(""schema"", schemaNamespace);

		Resource resource = new ClassPathResource(""schema.xsd"", getClass());
		Document schemaDocument = documentBuilder.parse(SaxUtils.createInputSource(resource));
		Types types = definition.createTypes();
		definition.setTypes(types);
		Schema schema = (Schema) definition.getExtensionRegistry().createExtension(Types.class,
				new QName(""http://www.w3.org/2001/XMLSchema"", ""schema""));
		types.addExtensibilityElement(schema);
		schema.setElement(schemaDocument.getDocumentElement());

		provider.addMessages(definition);

		assertThat(definition.getMessages()).hasSize(2);

		Message message = definition.getMessage(new QName(definitionNamespace, ""GetOrderRequest""));

		assertThat(message).isNotNull();

		Part part = message.getPart(""GetOrderRequest"");

		assertThat(part).isNotNull();
		assertThat(part.getElementName()).isEqualTo(new QName(schemaNamespace, ""GetOrderRequest""));

		message = definition.getMessage(new QName(definitionNamespace, ""GetOrderResponse""));

		assertThat(message).isNotNull();

		part = message.getPart(""GetOrderResponse"");

		assertThat(part).isNotNull();
		assertThat(part.getElementName()).isEqualTo(new QName(schemaNamespace, ""GetOrderResponse""));
	}",1
"@Test
	public void testStringResult() throws Exception {

		Document document = DocumentBuilderFactoryUtils.newInstance().newDocumentBuilder().newDocument();
		Element element = document.createElementNS(""namespace"", ""prefix:localName"");
		document.appendChild(element);

		Transformer transformer = TransformerFactoryUtils.newInstance().newTransformer();
		StringResult result = new StringResult();
		transformer.transform(new DOMSource(document), result);

		assertThat(result.toString()).and(""<prefix:localName xmlns:prefix='namespace'/>"").ignoreWhitespace().areIdentical();
	}",1
"@Test
	public void testLoadSchema() throws Exception {

		Resource resource = new ClassPathResource(""schema.xsd"", getClass());
		Schema schema = SchemaLoaderUtils.loadSchema(resource, XMLConstants.W3C_XML_SCHEMA_NS_URI);

		assertThat(schema).isNotNull();
		assertThat(resource.isOpen()).isFalse();
	}",1
"@Test
    public void should_generate_manager_class_for_index_dsl() throws Exception {
        setExec(aptUtils -> {

            final GlobalParsingContext globalContext = new GlobalParsingContext(
                    V3_7.INSTANCE,
                    InsertStrategy.ALL_FIELDS,
                    new LowerCaseNaming(),
                    FieldFilter.EXPLICIT_ENTITY_FIELD_FILTER,
                    FieldFilter.EXPLICIT_UDT_FIELD_FILTER,
                    Optional.empty());

            final String className = TestEntityWithSASI.class.getCanonicalName();
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

            final EntityParser entityParser = new EntityParser(aptUtils);

            final EntityMetaSignature entityMetaSignature = entityParser.parseEntity(typeElement, globalContext);

            final ManagerAndDSLClasses managerAndDSLClasses = ManagerCodeGen.buildManager(globalContext, aptUtils, entityMetaSignature);

            final StringBuilder builder = new StringBuilder();
            try {
                JavaFile.builder(TypeUtils.GENERATED_PACKAGE, managerAndDSLClasses.managerClass)
                        .build()
                        .writeTo(builder);
            }",1
"@Test
    public void should_build_entity_with_static_counter_column() throws Exception {
        setExec(aptUtils -> {
            final String className = TestEntityWithStaticCounterColumn.class.getCanonicalName();
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

            final EntityMetaCodeGen builder = new EntityMetaCodeGen(aptUtils);
            final List<FieldParser.FieldMetaSignature> parsingResults = getTypeParsingResults(aptUtils, typeElement, context);
            final TypeSpec typeSpec = builder.buildEntityMeta(EntityType.TABLE, typeElement, context, parsingResults, emptyList()).sourceCode;

            assertThat(buildSource(typeSpec)).isEqualTo(
                    readCodeBlockFromFile(""expected_code/entity_meta_builder/should_build_entity_with_static_counter_column.txt""));
        }",1
"@Test
    public void should_build_view_meta() throws Exception {
        setExec(aptUtils -> {
            final String className = TestViewSensorByType.class.getCanonicalName();
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

            final EntityMetaCodeGen builder = new EntityMetaCodeGen(aptUtils);
            final List<FieldParser.FieldMetaSignature> parsingResults = getTypeParsingResults(aptUtils, typeElement, context);
            final TypeSpec typeSpec = builder.buildEntityMeta(EntityType.VIEW, typeElement, context, parsingResults, emptyList()).sourceCode;

            assertThat(buildSource(typeSpec)).isEqualTo(
                    readCodeBlockFromFile(""expected_code/entity_meta_builder/should_build_view_meta.txt""));
        }",1
"@Test
    public void should_generate_udt_property_class() throws Exception {
        setExec(aptUtils -> {
            final String className = TestUDT.class.getCanonicalName();
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

            final UDTMetaCodeGen builder = new UDTMetaCodeGen(aptUtils);

            final GlobalParsingContext globalContext = GlobalParsingContext.defaultContext();
            final EntityParsingContext context = new EntityParsingContext(typeElement,
                    ClassName.get(TestUDT.class), new LowerCaseNaming(), globalContext);
            final List<FieldMetaSignature> parsingResults = getTypeParsingResults(aptUtils, typeElement, globalContext);

            final TypeSpec typeSpec = builder.buildUDTClassProperty(typeElement, context, parsingResults, Collections.emptyList());

            assertThat(typeSpec.toString().trim()).isEqualTo(
                    readCodeBlockFromFile(""expected_code/udt_meta_builder/should_generate_udt_property_class.txt""));

        }",1
"@Test
    public void should_generate_udt_with_custom_constructor_property_class() throws Exception {
        setExec(aptUtils -> {
            final String className = TestUDTWithCustomConstructor.class.getCanonicalName();
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);

            final UDTMetaCodeGen builder = new UDTMetaCodeGen(aptUtils);

            final GlobalParsingContext globalContext = GlobalParsingContext.defaultContext();
            final EntityParsingContext context = new EntityParsingContext(typeElement,
                    ClassName.get(TestUDT.class), new LowerCaseNaming(), globalContext);
            final List<AccessorsExclusionContext> exclusionContexts = Arrays.asList(
                    new AccessorsExclusionContext(""name"", false, true),
                    new AccessorsExclusionContext(""list"", false, true));
            final List<FieldMetaSignature> fieldMetaSignatures = getTypeParsingResults(aptUtils, typeElement, exclusionContexts, globalContext);

            final List<FieldMetaSignature> constructorInjectedFieldMetaSignatures = fieldMetaSignatures
                    .stream()
                    .filter(fieldMeta -> !fieldMeta.context.fieldName.equals(""date""))
                    .collect(Collectors.toList());

            final TypeSpec typeSpec = builder.buildUDTClassProperty(typeElement, context, fieldMetaSignatures, constructorInjectedFieldMetaSignatures);

            assertThat(typeSpec.toString().trim()).isEqualTo(
                    readCodeBlockFromFile(""expected_code/udt_meta_builder/should_generate_udt_with_custom_constructor_property_class.txt""));

        }",1
"@Test
    public void should_generate_field_info_for_primitiveBoolean() throws Exception {
        setExec(aptUtils -> {
            FieldInfoParser parser = new FieldInfoParser(aptUtils);
            final String className = TestEntityForFieldInfo.class.getCanonicalName();
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);
            final EntityParsingContext context = new EntityParsingContext(typeElement, ClassName.get(TestEntityForFieldInfo.class), strategy, globalParsingContext);

            // private boolean primitiveBoolean;
            VariableElement elm = findFieldInType(typeElement, ""primitiveBoolean"");
            final AnnotationTree annotationTree = AnnotationTree.buildFrom(aptUtils,  globalParsingContext, elm);

            FieldInfoContext fieldInfo = parser.buildFieldInfo(elm, annotationTree, context);

            assertThat(fieldInfo.codeBlock.toString().trim().replaceAll(""\n"", """"))
                    .isEqualTo(readCodeLineFromFile(""expected_code/method_parser/should_generate_field_info_for_primitiveBoolean.txt""));
        }",1
"@Test
    public void should_generate_field_info_for_public_final_columns() throws Exception {
        setExec(aptUtils -> {
            final FieldInfoParser parser = new FieldInfoParser(aptUtils);
            final String className = TestEntityForFieldInfo.class.getCanonicalName();
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);
            final List<AccessorsExclusionContext> exclusionContexts = Arrays.asList(new AccessorsExclusionContext(""immutableColumn"", true, true));
            final EntityParsingContext context = new EntityParsingContext(typeElement,
                    ClassName.get(TestEntityForFieldInfo.class), strategy, exclusionContexts,
                    globalParsingContext);

            VariableElement elm = findFieldInType(typeElement, ""immutableColumn"");

            final AnnotationTree annotationTree = AnnotationTree.buildFrom(aptUtils,  globalParsingContext, elm);

            FieldInfoContext fieldInfo = parser.buildFieldInfo(elm, annotationTree, context);

            assertThat(fieldInfo.codeBlock.toString().trim().replaceAll(""\n"", """"))
                    .isEqualTo(readCodeLineFromFile(""expected_code/method_parser/should_generate_field_info_for_public_final_columns.txt""));
        }",1
"@Test
    public void should_parse_computed_field_with_codec() throws Exception {

        setExec(aptUtils -> {
            final FieldParser fieldParser = new FieldParser(aptUtils);
            final String className = TestEntityForCodecs.class.getCanonicalName();
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);
            final EntityParsingContext entityContext = new EntityParsingContext(typeElement, ClassName.get(TestEntityForCodecs.class), strategy, globalParsingContext);

            // @Computed(function = ""writetime"",  alias = ""writetime"", targettargetColumnsap""}",1
"@Test
    public void should_parse_field_with_case_sensitive_overriden_name() throws Exception {
        setExec(aptUtils -> {
            final FieldParser fieldParser = new FieldParser(aptUtils);
            final String className = TestEntityForCodecs.class.getCanonicalName();
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);
            final EntityParsingContext entityContext = new EntityParsingContext(typeElement, ClassName.get(TestEntityForCodecs.class), strategy, globalParsingContext);

            // @Column(""\""overRiden\"""")
            // private String overridenName;
            VariableElement elm = findFieldInType(typeElement, ""overridenName"");

            FieldMetaSignature parsingResult = fieldParser.parse(elm, entityContext);

            assertThat(parsingResult.targetType.toString()).isEqualTo(String.class.getCanonicalName());
            assertThat(parsingResult.buildPropertyAsField().toString().trim().replaceAll(""\n"", """"))
                    .isEqualTo(readCodeLineFromFile(""expected_code/field_parser/should_parse_field_with_case_sensitive_overriden_name.txt""));
        }",1
"@Test
    public void should_parse_list_udt() throws Exception {

        setExec(aptUtils -> {
            final FieldParser fieldParser = new FieldParser(aptUtils);
            final String className = TestEntityForCodecs.class.getCanonicalName();
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);
            final EntityParsingContext entityContext = new EntityParsingContext(typeElement, ClassName.get(TestEntityForCodecs.class), strategy, globalParsingContext);

            // private List<TestUDT> listUdt;
            VariableElement elm = findFieldInType(typeElement, ""listUdt"");
            FieldMetaSignature parsingResult = fieldParser.parse(elm, entityContext);

            assertThat(parsingResult.targetType.toString()).isEqualTo(""java.util.List<com.datastax.driver.core.UDTValue>"");
            assertThat(parsingResult.buildPropertyAsField().toString().trim().replaceAll(""\n"", """"))
                    .isEqualTo(readCodeLineFromFile(""expected_code/field_parser/should_parse_list_udt.txt""));
        }",1
"@Test
    public void should_parse_map_udt() throws Exception {

        setExec(aptUtils -> {
            final FieldParser fieldParser = new FieldParser(aptUtils);
            final String className = TestEntityForCodecs.class.getCanonicalName();
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);
            final EntityParsingContext entityContext = new EntityParsingContext(typeElement, ClassName.get(TestEntityForCodecs.class), strategy, globalParsingContext);

            // private Map<Integer, TestUDT> mapUdt;
            VariableElement elm = findFieldInType(typeElement, ""mapUdt"");
            FieldMetaSignature parsingResult = fieldParser.parse(elm, entityContext);

            assertThat(parsingResult.targetType.toString()).isEqualTo(""java.util.Map<java.lang.Integer, com.datastax.driver.core.UDTValue>"");
            assertThat(parsingResult.buildPropertyAsField().toString().trim().replaceAll(""\n"", """"))
                    .isEqualTo(readCodeLineFromFile(""expected_code/field_parser/should_parse_map_udt.txt""));
        }",1
"@Test
    public void should_parse_map_with_nested_json() throws Exception {

        setExec(aptUtils -> {
            final FieldParser fieldParser = new FieldParser(aptUtils);
            final String className = TestEntityForCodecs.class.getCanonicalName();
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);
            final EntityParsingContext entityContext = new EntityParsingContext(typeElement, ClassName.get(TestEntityForCodecs.class), strategy, globalParsingContext);

            // private Map<Integer, @JSON List<Map<Integer, String>>> mapWithNestedJson;
            VariableElement elm = findFieldInType(typeElement, ""mapWithNestedJson"");
            FieldMetaSignature parsingResult = fieldParser.parse(elm, entityContext);

            assertThat(parsingResult.targetType.toString()).isEqualTo(""java.util.Map<java.lang.Integer, java.lang.String>"");
            assertThat(parsingResult.buildPropertyAsField().toString().trim().replaceAll(""\n"", """"))
                    .isEqualTo(readCodeLineFromFile(""expected_code/field_parser/should_parse_map_with_nested_json.txt""));
        }",1
"@Test
    public void should_parse_nested_int_array() throws Exception {

        setExec(aptUtils -> {
            final FieldParser fieldParser = new FieldParser(aptUtils);
            final String className = TestEntityForCodecs.class.getCanonicalName();
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);
            final EntityParsingContext entityContext = new EntityParsingContext(typeElement, ClassName.get(TestEntityForCodecs.class), strategy, globalParsingContext);

            //  private List<@Frozen Map<@Enumerated ProtocolVersion, List<int[]>>> nestedArrays;
            VariableElement elm = findFieldInType(typeElement, ""nestedArrays"");
            FieldMetaSignature parsingResult = fieldParser.parse(elm, entityContext);

            assertThat(parsingResult.targetType.toString()).isEqualTo(""java.util.List<java.util.Map<java.lang.String, java.util.List<int[]>>>"");
            assertThat(parsingResult.buildPropertyAsField().toString().trim().replaceAll(""\n"", """"))
                    .isEqualTo(readCodeLineFromFile(""expected_code/field_parser/should_parse_nested_int_array.txt""));
        }",1
"@Test
    public void should_parse_nested_tuple() throws Exception {

        setExec(aptUtils -> {
            final FieldParser fieldParser = new FieldParser(aptUtils);
            final String className = TestEntityForCodecs.class.getCanonicalName();
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);
            final EntityParsingContext entityContext = new EntityParsingContext(typeElement, ClassName.get(TestEntityForCodecs.class), strategy, globalParsingContext);

            // private Map<Integer, Tuple2<Integer, String>> nestedTuple;
            VariableElement elm = findFieldInType(typeElement, ""nestedTuple"");
            FieldMetaSignature parsingResult = fieldParser.parse(elm, entityContext);

            assertThat(parsingResult.targetType.toString()).isEqualTo(""java.util.Map<java.lang.Integer, com.datastax.driver.core.TupleValue>"");
            assertThat(parsingResult.buildPropertyAsField().toString().trim().replaceAll(""\n"", """"))
                    .isEqualTo(readCodeLineFromFile(""expected_code/field_parser/should_parse_nested_tuple.txt""));
        }",1
"@Test
    public void should_parse_nested_udt() throws Exception {

        setExec(aptUtils -> {
            final FieldParser fieldParser = new FieldParser(aptUtils);
            final String className = TestEntityForCodecs.class.getCanonicalName();
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);
            final EntityParsingContext entityContext = new EntityParsingContext(typeElement, ClassName.get(TestEntityForCodecs.class), strategy, globalParsingContext);

            // private TestNestedUDT nestedUDT;
            VariableElement elm = findFieldInType(typeElement, ""nestedUDT"");
            FieldMetaSignature parsingResult = fieldParser.parse(elm, entityContext);

            assertThat(parsingResult.targetType.toString()).isEqualTo(UDTValue.class.getCanonicalName());
            assertThat(parsingResult.udtMetaSignature.isPresent()).isTrue();
            assertThat(parsingResult.buildPropertyAsField().toString().trim().replaceAll(""\n"", """"))
                    .isEqualTo(readCodeLineFromFile(""expected_code/field_parser/should_parse_nested_udt.txt""));
        }",1
"@Test
    public void should_parse_non_frozen_udt() throws Exception {
        setExec(aptUtils -> {
            final GlobalParsingContext globalContext = new GlobalParsingContext(V3_6.INSTANCE, InsertStrategy.ALL_FIELDS, new LowerCaseNaming(),
                    EXPLICIT_ENTITY_FIELD_FILTER, EXPLICIT_UDT_FIELD_FILTER, Optional.empty());
            final FieldParser fieldParser = new FieldParser(aptUtils);
            final String className = TestEntityForCodecs.class.getCanonicalName();
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);
            final EntityParsingContext entityContext = new EntityParsingContext(typeElement, ClassName.get(TestEntityForCodecs.class), strategy, globalContext);

            // @Column
            // private TestUDT nonFrozenUDT;
            VariableElement elm = findFieldInType(typeElement, ""nonFrozenUDT"");

            FieldMetaSignature parsingResult = fieldParser.parse(elm, entityContext);

            assertThat(parsingResult.targetType.toString()).isEqualTo(UDTValue.class.getCanonicalName());
            assertThat(parsingResult.buildPropertyAsField().toString().trim().replaceAll(""\n"", """"))
                    .isEqualTo(readCodeLineFromFile(""expected_code/field_parser/should_parse_non_frozen_udt.txt""));
        }",1
"@Test
    public void should_parse_optional_protocol_version_from_inline_codec() throws Exception {

        setExec(aptUtils -> {
            final GlobalParsingContext globalContext = globalParsingContext;
            new CodecRegistryParser(aptUtils).parseCodecs(env, globalContext);
            final FieldParser fieldParser = new FieldParser(aptUtils);
            final String className = TestEntityForCodecs.class.getCanonicalName();
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);
            final EntityParsingContext entityContext = new EntityParsingContext(typeElement, ClassName.get(TestEntityForCodecs.class), strategy,
                    globalContext);

            // @Column
            // private Optional<@Enumerated(Encoding.ORDINAL) ProtocolVersion> optionalEncodingAsOrdinal;
            VariableElement elm = findFieldInType(typeElement, ""optionalEncodingAsOrdinal"");
            FieldMetaSignature parsingResult = fieldParser.parse(elm, entityContext);

            assertThat(parsingResult.targetType.toString()).isEqualTo(""java.lang.Integer"");
            assertThat(parsingResult.buildPropertyAsField().toString().trim().replaceAll(""\n"", """"))
                    .isEqualTo(readCodeLineFromFile(""expected_code/field_parser/should_parse_optional_protocol_version_from_inline_codec.txt""));
        }",1
"@Test
    public void should_parse_optional_string() throws Exception {

        setExec(aptUtils -> {
            final GlobalParsingContext globalContext = globalParsingContext;
            new CodecRegistryParser(aptUtils).parseCodecs(env, globalContext);
            final FieldParser fieldParser = new FieldParser(aptUtils);
            final String className = TestEntityForCodecs.class.getCanonicalName();
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);
            final EntityParsingContext entityContext = new EntityParsingContext(typeElement, ClassName.get(TestEntityForCodecs.class), strategy,
                    globalContext);

            // @Column
            // private Optional<String> optionalString;
            VariableElement elm = findFieldInType(typeElement, ""optionalString"");
            FieldMetaSignature parsingResult = fieldParser.parse(elm, entityContext);

            assertThat(parsingResult.targetType.toString()).isEqualTo(""java.lang.String"");
            assertThat(parsingResult.buildPropertyAsField().toString().trim().replaceAll(""\n"", """"))
                    .isEqualTo(readCodeLineFromFile(""expected_code/field_parser/should_parse_optional_string.txt""));
        }",1
"@Test
    public void should_parse_set_nesting() throws Exception {

        setExec(aptUtils -> {
            final FieldParser fieldParser = new FieldParser(aptUtils);
            final String className = TestEntityForCodecs.class.getCanonicalName();
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);
            final EntityParsingContext entityContext = new EntityParsingContext(typeElement, ClassName.get(TestEntityForCodecs.class), strategy, globalParsingContext);

            // private Set<Map<Integer,String>> setNesting;
            VariableElement elm = findFieldInType(typeElement, ""setNesting"");
            FieldMetaSignature parsingResult = fieldParser.parse(elm, entityContext);

            assertThat(parsingResult.targetType.toString()).isEqualTo(""java.util.Set<java.util.Map<java.lang.Integer, java.lang.String>>"");
            assertThat(parsingResult.buildPropertyAsField().toString().trim().replaceAll(""\n"", """"))
                    .isEqualTo(readCodeLineFromFile(""expected_code/field_parser/should_parse_set_nesting.txt""));
        }",1
"@Test
    public void should_parse_set_udt() throws Exception {

        setExec(aptUtils -> {
            final FieldParser fieldParser = new FieldParser(aptUtils);
            final String className = TestEntityForCodecs.class.getCanonicalName();
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);
            final EntityParsingContext entityContext = new EntityParsingContext(typeElement, ClassName.get(TestEntityForCodecs.class), strategy, globalParsingContext);

            // private Set<TestUDT> setUdt;
            VariableElement elm = findFieldInType(typeElement, ""setUdt"");
            FieldMetaSignature parsingResult = fieldParser.parse(elm, entityContext);

            assertThat(parsingResult.targetType.toString()).isEqualTo(""java.util.Set<com.datastax.driver.core.UDTValue>"");
            assertThat(parsingResult.buildPropertyAsField().toString().trim().replaceAll(""\n"", """"))
                    .isEqualTo(readCodeLineFromFile(""expected_code/field_parser/should_parse_set_udt.txt""));
        }",1
"@Test
    public void should_parse_tuple_nesting() throws Exception {

        setExec(aptUtils -> {
            final FieldParser fieldParser = new FieldParser(aptUtils);
            final String className = TestEntityForCodecs.class.getCanonicalName();
            final TypeElement typeElement = aptUtils.elementUtils.getTypeElement(className);
            final EntityParsingContext entityContext = new EntityParsingContext(typeElement, ClassName.get(TestEntityForCodecs.class), strategy, globalParsingContext);

            // private Tuple2<Integer, List<String>> tupleNesting;
            VariableElement elm = findFieldInType(typeElement, ""tupleNesting"");
            FieldMetaSignature parsingResult = fieldParser.parse(elm, entityContext);

            assertThat(parsingResult.targetType.toString()).isEqualTo(TUPLE_VALUE_CLASSNAME);
            assertThat(parsingResult.buildPropertyAsField().toString().trim().replaceAll(""\n"", """"))
                    .isEqualTo(readCodeLineFromFile(""expected_code/field_parser/should_parse_tuple_nesting.txt""));
        }",1
"@Test
    public void should_fail_validating_schema_when_partition_key_missing() throws Exception {
        //Given
        final Cluster cluster = CassandraEmbeddedServerBuilder.builder()
                .withScript(""EntityWithMissingPartitionKey/schema.cql"")
                .buildNativeCluster();
        cluster.init();

        //When
        exception.expect(AchillesBeanMappingException.class);
        exception.expectMessage(""The mapped partition key(s) [id] for entity "" +
                ""info.archinnov.achilles.internals.entities.EntityWithMissingPartitionKey "" +
                ""do not correspond to live schema partition key(s) [id, bucket]"");

        //Then
        ManagerFactoryBuilder
                .builder(cluster)
                .withManagedEntityClasses(EntityWithMissingPartitionKey.class)
                .build();
    }",1
"@Test
    public void should_query_using_collection_index_fromJSON() throws Exception {
        //Given
        final Long id = RandomUtils.nextLong(0L, Long.MAX_VALUE);
        scriptExecutor.executeScriptTemplate(""EntityWithIndicesForJSON/insertRows.cql"", ImmutableMap.of(""id"", id));

        //When
        final List<EntityWithIndicesForJSON> actual = manager
                .indexed()
                .select()
                .allColumns_FromBaseTable()
                .where()
                .indexed_collectionIndex().Contains_FromJson(""\""4\"""")
                .getList();

        //Then
        assertThat(actual).hasSize(1);
        final EntityWithIndicesForJSON entity = actual.get(0);
        assertThat(entity.getSimpleIndex()).isEqualTo(""411"");
    }",1
"@Test
    public void should_query_using_index_and_clustering_column_fromJSON() throws Exception {
        //Given
        final Long id = RandomUtils.nextLong(0L, Long.MAX_VALUE);
        scriptExecutor.executeScriptTemplate(""EntityWithIndicesForJSON/insertRows.cql"", ImmutableMap.of(""id"", id));

        //When
        final List<EntityWithIndicesForJSON> actual = manager
                .indexed()
                .select()
                .allColumns_FromBaseTable()
                .where()
                .indexed_simpleIndex().Eq(""312"")
                .clust1().Eq_FromJson(""3"")
                .clust3().Eq_FromJson(""\""2\"""")
                .getList();

        //Then
        assertThat(actual).hasSize(1);
        final EntityWithIndicesForJSON entity = actual.get(0);
        assertThat(entity.getSimpleIndex()).isEqualTo(""312"");
    }",1
"@Test
    public void should_build_schema_for_complex_types() throws Exception {
        //Given
        final EntityWithComplexTypes_AchillesMeta meta = new EntityWithComplexTypes_AchillesMeta();
        final CodecRegistry codecRegistry = new CodecRegistry();
        TupleTypeFactory tupleTypeFactory = new TupleTypeFactory(ProtocolVersion.NEWEST_SUPPORTED, codecRegistry);
        UserTypeFactory userTypeFactory = new UserTypeFactory(ProtocolVersion.NEWEST_SUPPORTED, codecRegistry);

        meta.inject(userTypeFactory, tupleTypeFactory);

        //When
        final String actual = meta.generateSchema(context);

        //Then
        assertThat(actual.trim()).isEqualTo(readCodeBlockFromFile(
                ""schema/should_build_schema_for_complex_types.cql""));
    }",1
"@Test
    public void should_build_schema_for_entity_with_composite_partition_key() throws Exception {
        //Given
        final EntityWithCompositePartitionKey_AchillesMeta meta = new EntityWithCompositePartitionKey_AchillesMeta();

        //When
        final String actual = meta.generateSchema(context);

        //Then
        assertThat(actual.trim()).isEqualTo(readCodeBlockFromFile(
                ""schema/should_build_schema_for_entity_with_composite_partition_key.cql""));
    }",1
"@Test
    public void should_build_schema_for_entity_with_simple_partition_key() throws Exception {
        //Given
        final EntityWithSimplePartitionKey_AchillesMeta meta = new EntityWithSimplePartitionKey_AchillesMeta();

        //When
        final String actual = meta.generateSchema(context);

        //Then
        assertThat(actual.trim()).isEqualTo(readCodeBlockFromFile(
                ""schema/should_build_schema_for_entity_with_simple_partition_key.cql""));
    }",1
"@Test
    public void should_build_schema_for_entity_with_static_annotations() throws Exception {
        //Given
        final EntityWithStaticAnnotations_AchillesMeta meta = new EntityWithStaticAnnotations_AchillesMeta();

        //When
        final String actual = meta.generateSchema(context);

        //Then
        assertThat(actual.trim()).isEqualTo(readCodeBlockFromFile(
                ""schema/should_build_schema_for_entity_with_static_annotations.cql""));
    }",1
"@Test
    public void should_build_schema_for_entity_with_static_column() throws Exception {
        //Given
        final EntityWithStaticColumn_AchillesMeta meta = new EntityWithStaticColumn_AchillesMeta();

        //When
        final String actual = meta.generateSchema(context);

        //Then
        assertThat(actual.trim()).isEqualTo(readCodeBlockFromFile(
                ""schema/should_build_schema_for_entity_with_static_column.cql""));
    }",1
"@Test
    public void should_build_schema_for_simple_entity() throws Exception {
        //Given
        final SimpleEntity_AchillesMeta meta = new SimpleEntity_AchillesMeta();

        //When
        final String actual = meta.generateSchema(context);

        //Then
        assertThat(actual.trim()).isEqualTo(readCodeBlockFromFile(
                ""schema/should_build_schema_for_simple_entity.cql""));
    }",1
"@Test
    public void testGetPackageFromModule() {
        Assert.assertEquals(CodeGenUtil.getPackageFromModule(""one-sys""), ""com.lcw.one.sys"");
        Assert.assertEquals(CodeGenUtil.getPackageFromModule(""one-code-gen""), ""com.lcw.one.codegen"");
        Assert.assertEquals(CodeGenUtil.getPackageFromModule(""one-test-camel""), ""com.lcw.one.testCamel"");
    }",1
"@Test
  public void shouldNotFailForEmptyPaths() throws MalformedURLException {
    URL url = new URL(""file://"" + file.getAbsolutePath());
    FileSystemReader fileSystemReader = FileSystemReader.getInstance(url);
    assertThat(fileSystemReader, notNullValue());
    assertThat(fileSystemReader.getSubPackagesOfPackage("""").size(), is(0));
    assertThat(fileSystemReader.getTypesInPackage("""").size(), is(0));
  }",1
"@Test
  public void throwsProperException() throws MalformedURLException {
    String anyUrl = anyUrl();
    try {
      FileSystemReader.getInstance(new URL(anyUrl));
      fail(""Expected exception not thrown!"");
    }",1
"@Test
    public void testWrite() throws Exception {
        Bean bean1 = new Bean();
        bean1.setStringField(""str"");
        bean1.setBooleanField(true);
        bean1.setCharField('s');
        bean1.setDoubleField(10.1);
        bean1.setFloatField(1.5f);
        bean1.setIntField(10);
        bean1.setLongField(100);
        bean1.setEnumField(AnEnum.ValueA);
        bean1.setEnumBean(AnEnumBean.Two);

        JSONWriter jsonWriter = new DefaultJSONWriter();
        jsonWriter.setEnumAsBean(false);
        String json = jsonWriter.write(bean1);
        TestUtils.assertEquals(DefaultJSONWriter.class.getResource(""jsonwriter-write-bean-01.txt""), json);
    }",1
"@Test
    public void testWriteBeanWithList() throws Exception {
        BeanWithList bean1 = new BeanWithList();
        bean1.setStringField(""str"");
        bean1.setBooleanField(true);
        bean1.setCharField('s');
        bean1.setDoubleField(10.1);
        bean1.setFloatField(1.5f);
        bean1.setIntField(10);
        bean1.setLongField(100);
        bean1.setEnumField(AnEnum.ValueA);
        bean1.setEnumBean(AnEnumBean.Two);
        List<String> errors = new ArrayList<String>();
        errors.add(""Field is required"");
        bean1.setErrors(errors);

        JSONWriter jsonWriter = new DefaultJSONWriter();
        jsonWriter.setEnumAsBean(false);
        jsonWriter.setIgnoreHierarchy(false);
        String json = jsonWriter.write(bean1);
        TestUtils.assertEquals(DefaultJSONWriter.class.getResource(""jsonwriter-write-bean-04.txt""), json);
    }",1
"@Test
  public void getYamlFromTgzTestCustomArchive() throws Exception {
    InputStream is = getClass().getResourceAsStream(""mongodb-4.0.4.tgz"");
    InputStream chartFromInputStream = underTest.getChartFromInputStream(is);
    String fileContent = IOUtils.toString(new InputStreamReader(chartFromInputStream));

    String expected = ""appVersion: 3.6.6\n""
        + ""description: NoSQL document-oriented database that stores JSON-like documents with\n""
        + ""  dynamic schemas, simplifying the integration of data in content-driven applications.\n""
        + ""engine: gotpl\n""
        + ""home: https://mongodb.org\n""
        + ""icon: https://bitnami.com/assets/stacks/mongodb/img/mongodb-stack-220x234.png\n""
        + ""keywords:\n""
        + ""- mongodb\n""
        + ""- database\n""
        + ""- nosql\n""
        + ""- cluster\n""
        + ""- replicaset\n""
        + ""- replication\n""
        + ""maintainers:\n""
        + ""- email: containers@bitnami.com\n""
        + ""  name: Bitnami\n""
        + ""name: mongodb\n""
        + ""sources:\n""
        + ""- https://github.com/bitnami/bitnami-docker-mongodb\n""
        + ""version: 4.0.4\n"";
    assertThat(fileContent, is(expected));
  }",1
"@Test
	public void getAllFullDumps() throws IOException {
		wrf.setWebResourceContentsFromResource(
				""https://dumps.wikimedia.org/wikidatawiki/"",
				""/wikidatawiki-index-old.html"", this.getClass());

		setLocalDump(""20140210"", DumpContentType.FULL, false);
		setLocalDump(""20140123"", DumpContentType.FULL, true);
		setLocalDump(""20140106"", DumpContentType.CURRENT, true);
		setLocalDump(""20131201"", DumpContentType.FULL, true);
		setLocalDump(""nodate"", DumpContentType.FULL, true);

		WmfDumpFileManager dumpFileManager = new WmfDumpFileManager(
				""wikidatawiki"", dm, wrf);

		List<? extends MwDumpFile> dumpFiles = dumpFileManager
				.findAllDumps(DumpContentType.FULL);

		String[] dumpDates = { ""20140210"", ""20140123"", ""20140106"", ""20131221"",
				""20131201"" }",1
"@Test
  public void createValidModel() {
    factory = new GroovyModelFactory() {
      @Override
      protected InputStream getModelResourceAsStream()
          throws IOException {
        return TestGroovyModelFactory.class.getResourceAsStream(""wro.groovy"");
      }",1
"@Test
  public void createValidModelContainingHiphen() {
    factory = new GroovyModelFactory() {
      @Override
      protected InputStream getModelResourceAsStream()
          throws IOException {
        return getClass().getResourceAsStream(""wroWithHiphen.groovy"");
      }",1
"@Test
  public void createIncompleteModel() {
    factory = new JsonModelFactory() {
      @Override
      protected InputStream getModelResourceAsStream()
          throws IOException {
        return getClass().getResourceAsStream(""incomplete-wro.json"");
      }",1
"@Test
  public void testFromFolder()
      throws Exception {
    final URL url = getClass().getResource(""../ngannotate"");

    final File testFolder = new File(url.getFile(), ""test"");
    final File expectedFolder = new File(url.getFile(), ""expected"");
    WroTestUtils.compareFromDifferentFoldersByExtension(testFolder, expectedFolder, ""js"", victim);
  }",1
"@Test
  public void testFromFolder()
      throws Exception {
    final URL url = getClass().getResource(""../ngmin"");

    final File testFolder = new File(url.getFile(), ""test"");
    final File expectedFolder = new File(url.getFile(), ""expected"");
    WroTestUtils.compareFromDifferentFoldersByExtension(testFolder, expectedFolder, ""js"", victim);
  }",1
"@Test
  public void testFromFolder()
      throws IOException {
    final URL url = getClass().getResource(""csslint"");

    final File testFolder = new File(url.getFile());
    WroTestUtils.compareFromSameFolder(testFolder, new WildcardFileFilter(""*.css""), Transformers.noOpTransformer(),
        victim);
  }",1
"@Test
  public void testAdvancedOptimization()
      throws IOException {
    victim.setCompilationLevel(CompilationLevel.ADVANCED_OPTIMIZATIONS);
    final URL url = getClass().getResource(""google"");

    final File expectedFolder = new File(url.getFile(), ""expectedAdvanced"");
    WroTestUtils.compareFromDifferentFoldersByExtension(testFolder, expectedFolder, ""js"", (ResourcePreProcessor) victim);
  }",1
"@Test
  public void testPackFromFolder()
      throws Exception {
    final ResourcePostProcessor processor = JsonHPackProcessor.packProcessor();
    final URL url = getClass().getResource(""jsonhpack"");

    final File testFolder = new File(url.getFile(), ""test"");
    final File expectedFolder = new File(url.getFile(), ""pack"");
    WroTestUtils.compareFromDifferentFoldersByExtension(testFolder, expectedFolder, ""js"", processor);
  }",1
"@Test
  public void testFromFolder()
      throws Exception {
    final ResourcePostProcessor processor = new NodeTypeScriptProcessor();
    final URL url = getClass().getResource(""typescript"");

    final File testFolder = new File(url.getFile(), ""test"");
    final File expectedFolder = new File(url.getFile(), ""expected"");
    WroTestUtils.compareFromDifferentFoldersByExtension(testFolder, expectedFolder, ""js"", processor);
  }",1
"@Test
  public void shouldUglifyFiles()
      throws IOException {
    final ResourcePostProcessor processor = new UglifyJsProcessor();
    final URL url = getClass().getResource(""uglify"");

    final File expectedFolder = new File(url.getFile(), ""expected"");
    WroTestUtils.compareFromDifferentFoldersByExtension(testFolder, expectedFolder, ""js"", processor);
  }",1
"@Test
  public void shouldMininimizeCss()
      throws IOException {
    final URL url = getClass().getResource(""yui"");

    final File testFolder = new File(url.getFile(), ""test"");
    final File expectedFolder = new File(url.getFile(), ""expected"");
    WroTestUtils.compareFromDifferentFoldersByExtension(testFolder, expectedFolder, ""css"", victim);
  }",1
"@Test
  public void shouldSetResponseLength()
      throws IOException {
    final String resourceUri = ""classpath:"" + packagePath + ""/"" + ""test.css"";
    when(mockAuthorizationManager.isAuthorized(resourceUri)).thenReturn(true);
    when(request.getParameter(ResourceProxyRequestHandler.PARAM_RESOURCE_ID)).thenReturn(resourceUri);
    when(mockUriLocator.locate(anyString())).thenReturn(new ClasspathUriLocator().locate(resourceUri));

    victim.handle(request, response);
    final int expectedLength = IOUtils.toString(getInputStream(""test.css""), Charset.defaultCharset()).length();

    verify(response, times(1)).setContentLength(expectedLength);
  }",1
"@Test
  public void testDuplicatedResourcesShouldBeSkipped()
      throws Exception {
    new GenericTestBuilder().processAndCompare(""/repeatedResources.js"", ""classpath:ro/isdc/wro/manager/repeated-out.js"");
  }",1
"@Test
  public void testFromFolder()
      throws Exception {
    final ResourcePreProcessor processor = new WroManagerProcessor();
    final URL url = getClass().getResource(""wroManager"");

    final File testFolder = new File(url.getFile(), ""test"");
    final File expectedFolder = new File(url.getFile(), ""expected"");
    WroTestUtils.compareFromDifferentFoldersByExtension(testFolder, expectedFolder, ""js"", processor);
  }",1
"@Test
  public void testMinimizeAttributeIsTrueOnResource()
      throws Exception {
    new GenericTestBuilder().processAndCompare(""/resourceMinimizeTrue.js"",
        ""classpath:ro/isdc/wro/manager/sample.min.js"");
  }",1
"@Test
  public void shouldBeNullWhenDestinationFolderIsUnrelatedToBuildDirectory() {
    final File unrelatedFolder = new File(getClass().getResource("""").getFile()).getParentFile();
    victim.setDestinationFolder(unrelatedFolder);
    assertEquals(null, victim.resolve());
  }",1
"@Test
  public void shouldGenerateXmlReportFileWithCheckstyleFormat()
      throws Exception {
    generateAndCompareReportUsingFormat(FormatterType.CHECKSTYLE.getFormat(), ""csslint-checkstyle.xml"");
  }",1
"@Test
  public void shouldGenerateXmlReportFileWithDefaultFormat()
      throws Exception {
    generateAndCompareReportUsingFormat(null, ""csslint-default.xml"");
  }",1
"@Test
  public void shouldNotFailWhenThresholdIsGreaterThanNumberOfErrors()
      throws Exception {
    mojo.setFailThreshold(5);
    setWroWithValidResources();
    mojo.execute();
  }",1
"@Test
  public void shouldGenerateReportWithDefaultFormat()
      throws Exception {
    generateAndCompareReportFile(null, ""jshint-default.xml"");
  }",1
"@Test
  public void shouldGenerateReportWithDefaultFormat() throws Exception {
    generateAndCompareReportFile(null, ""jslint-default.xml"");
  }",1
"@Test
  public void testWroXmlWithInvalidResourcesAndIgnoreMissingResourcesTrue()
      throws Exception {
    setWroWithInvalidResources();
    victim.setIgnoreMissingResources(true);
    victim.execute();
  }",1
"@Test
  public void testMojoWithConfigurableWroManagerFactoryWithValidAndEmptyConfigFileSet()
      throws Exception {
    setWroWithValidResources();
    victim.setIgnoreMissingResources(true);
    victim.setWroManagerFactory(ConfigurableWroManagerFactory.class.getName());
    victim.execute();
  }",1
"@Test
  public void testMojoWithCustomManagerFactoryWithInvalidResourceAndIgnoreMissingResources()
      throws Exception {
    setWroWithInvalidResources();
    victim.setIgnoreMissingResources(true);
    victim.setWroManagerFactory(CustomManagerFactory.class.getName());
    victim.execute();
  }",1
"@Test
  public void shouldDetectGroupReferenceFromImportedModel() {
    final WroModel model = loadModelFromLocation(""shouldDetectGroupReferenceFromImportedModel.xml"");
    assertEquals(2, model.getGroups().size());
  }",1
"@Test
  public void testTwoConcurrentCreationCalls() {
    testSuccessfulCreation();
    factory.create();
  }",1
"@Test
  public void shouldFindWildcardResourcesForFolderContainingSpaces()
      throws IOException {
    victim.locate(createUri(""/folder with spaces/**.css"", ""test""));
  }",1
"@Test
  public void testWildcard2Resources()
      throws IOException {
    victim.locate(createUri(""/css/*.cs?""));
  }",1
"@Test
  public void shouldLocateWildcard3Resources()
      throws IOException {
    victim.locate(createUri(""*.???""));
  }",1
"@Test
  public void shouldFindAllChildFoldersAndFiles()
      throws IOException {
    final ThreadLocal<Collection<String>> filenameListHolder = new ThreadLocal<Collection<String>>();
    final UriLocator uriLocator = createJarLocator(filenameListHolder);
    uriLocator.locate(""classpath:com/app/**"");
    final Collection<String> filenameList = filenameListHolder.get();
    assertNotNull(filenameList);
    assertEquals(
            Arrays.toString(new String[]{
                    ""com/app/level1"", ""com/app/level1/level2"", ""com/app/level1/level2/styles"",
                    ""com/app/level1/level2/styles/style.css"", ""com/app/level1/level2/level2.css"", ""com/app/level1/level1.css""
            }",1
"@Test
  public void shouldGetJarFileFromFile() {
    final String actual = jarStreamLocator.getJarFile(new File(""file:path/to/file!one/two/three.class"")).getPath();
    final String expected = FilenameUtils.separatorsToSystem(""path/to/file"");
    assertEquals(expected, actual);
  }",1
"@Test
  public void testLocateJarStreamDelegate()
      throws IOException {
    final InputStream is = jarStreamLocator.locateStream(""classpath:com/test/app/*.js"", new File(""src/test/resources/""));
    final String content = readLines(is).get(0);
    assertTrue(content.contains(""1.js""));
    assertTrue(content.contains(""2.js""));
    assertTrue(content.contains(""3.js""));
    assertTrue(!content.contains(""1.css""));
    
    closeQuietly(is);
  }",1
"@Test
  public void testCopyrightStripperProcessor()
      throws Exception {
    final ResourcePreProcessor decoratedProcessor = new CssMinProcessor();
    final ResourcePreProcessor processor = CopyrightKeeperProcessorDecorator.decorate(decoratedProcessor);
    final URL url = ResourcePreProcessor.class.getResource(""copyright"");

    final File testFolder = new File(url.getFile(), ""test"");
    final File expectedFolder = new File(url.getFile(), ""expected"");
    WroTestUtils.compareFromDifferentFoldersByExtension(testFolder, expectedFolder, ""css"", processor);
  }",1
"@Test
  public void shouldRemoveOriginalUrl()
      throws Exception {
    compareResultsFromFolderUsingProcessor(""expectedEmptyReplace"", createProcessorWithHandler(new ItemHandler() {
      public String replace(final String originalDeclaration, final String originalUrl) {
        return originalDeclaration.replace(originalUrl, """");
      }",1
"@Test
  public void shouldGenerateCorrectDataURIForCSSWithCharset()
      throws IOException {
    final String expected = ""data:text/css;charset=UTF-8;base64,aW5wdXQuYnV0dG9uIHsKCWJhY2tncm91bmQ6IHVybChodHRwOi8vd3JvNGouZ29vZ2xlY29kZS5jb20vc3ZuL3dpa2kvaW1nL2ZvbGRlclN0cnVjdHVyZS5wbmcpOwoJYmFja2dyb3VuZC1pbWFnZTogdXJsKCJodHRwOi8vd3JvNGouZ29vZ2xlY29kZS5jb20vc3ZuL3dpa2kvaW1nL2ZvbGRlclN0cnVjdHVyZS5wbmciKTsKCWZpbHRlcjogcHJvZ2lkOkRYSW1hZ2VUcmFuc2Zvcm0uTWljcm9zb2Z0LkFscGhhSW1hZ2VMb2FkZXIoc3JjPSdodHRwOi8vd3JvNGouZ29vZ2xlY29kZS5jb20vc3ZuL3dpa2kvaW1nL2ZvbGRlclN0cnVjdHVyZS5wbmcnLCBzaXppbmdNZXRob2Q9J3NjYWxlJyk7Cn0KLm11bHRpbGluZUFuZFNwYWNlcyB7CiAgYmFja2dyb3VuZDogCiAgICAgdXJsKCAiaHR0cDovL3dybzRqLmdvb2dsZWNvZGUuY29tL3N2bi93aWtpL2ltZy9mb2xkZXJTdHJ1Y3R1cmUucG5nICIgKTsKICBiYWNrZ3JvdW5kLWltYWdlIDogCiAgICAgdXJsKCAiaHR0cDovL3dybzRqLmdvb2dsZWNvZGUuY29tL3N2bi93aWtpL2ltZy9mb2xkZXJTdHJ1Y3R1cmUucG5nICIgKSAgICAgCn0KQGZvbnQtZmFjZSB7CiAgICBzcmM6IHVybChodHRwOi8vd3JvNGouZ29vZ2xlY29kZS5jb20vc3ZuL3dpa2kvaW1nL2ZvbGRlclN0cnVjdHVyZS5wbmcpOwp9"";
    final String actual = dataUriGenerator.generateDataURI(getInputStream(""dataUri.css""), ""dataUri.css"");
    WroTestUtils.compare(expected, actual);
  }",1
"@Test
  public void shouldGenerateCorrectDataUriForPng()
      throws Exception {
    final String expected = ""data:image/png;base64,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"";
    final String actual = dataUriGenerator.generateDataURI(getInputStream(""dataUri.png""), ""dataUri.png"");
    
    assertEquals(expected, actual);
  }",1
"@Test
  public void testFromFolder()
      throws Exception {
    final URL url = getClass().getResource(""conformColors"");
    
    final File testFolder = new File(url.getFile(), ""test"");
    final File expectedFolder = new File(url.getFile(), ""expected"");
    WroTestUtils.compareFromDifferentFoldersByExtension(testFolder, expectedFolder, ""css"", processor);
  }",1
"@Test
  public void testFromFolder()
      throws Exception {
    final URL url = getClass().getResource(""consoleStripper"");
    final File testFolder = new File(url.getFile(), ""test"");
    final File expectedFolder = new File(url.getFile(), ""expected"");
    WroTestUtils.compareFromDifferentFoldersByExtension(testFolder, expectedFolder, ""js"", processor);
  }",1
"@Test
  public void shouldHandleWrongCss()
      throws Exception {
    final ResourcePostProcessor processor = new ExceptionHandlingProcessorDecorator(new CssMinProcessor());
    
    final URL url = getClass().getResource(""cssmin"");
    
    final File testFolder = new File(url.getFile(), ""test"");
    final File expectedFolder = new File(url.getFile(), ""expectedInvalid"");
    WroTestUtils.compareFromDifferentFoldersByExtension(testFolder, expectedFolder, ""css"", processor);
  }",1
"@Test
  public void testFromFolder()
      throws Exception {
    Context.get().getConfig().setIgnoreMissingResources(false);
    final URL url = getClass().getResource(""cssImport"");
    
    final File testFolder = new File(url.getFile(), ""test"");
    final File expectedFolder = new File(url.getFile(), ""expectedLess"");
    WroTestUtils.compareFromDifferentFoldersByExtension(testFolder, expectedFolder, ""css"", victim);
  }",1
"@Test
  public void testFromFolder()
      throws IOException {
    final ResourcePostProcessor processor = new MultiLineCommentStripperProcessor();
    
    final URL url = getClass().getResource(""multiline"");
    
    final File testFolder = new File(url.getFile(), ""test"");
    final File expectedFolder = new File(url.getFile(), ""expected"");
    WroTestUtils.compareFromDifferentFoldersByExtension(testFolder, expectedFolder, ""*"", processor);
  }",1
"@Test
  public void testProcessor()
      throws Exception {
    final Properties properties = new Properties();
    properties.setProperty(""prop1"", ""value1"");
    properties.setProperty(""prop2"", ""value2"");
    properties.setProperty(""prop3"", ""value3"");
    properties.setProperty(""prop4"", ""value4"");
    final ResourcePreProcessor processor = new PlaceholderProcessor().setPropertiesFactory(WroUtil.simpleObjectFactory(properties));
    final URL url = getClass().getResource(""placeholder"");

    final File testFolder = new File(url.getFile(), ""test"");
    final File expectedFolder = new File(url.getFile(), ""expected"");
    WroTestUtils.compareFromDifferentFoldersByExtension(testFolder, expectedFolder, ""css"", processor);
  }",1
"@Test
  public void testFromFolder()
      throws IOException {
    final ResourcePreProcessor processor = new SemicolonAppenderPreProcessor();
    
    final URL url = getClass().getResource(""semicolonAppender"");
    
    final File testFolder = new File(url.getFile(), ""test"");
    final File expectedFolder = new File(url.getFile(), ""expected"");
    WroTestUtils.compareFromDifferentFoldersByExtension(testFolder, expectedFolder, ""js"", processor);
  }",1
"@Test
  public void testFromFolder()
      throws Exception {
    final URL url = getClass().getResource(""variablizeColors"");
    
    final File testFolder = new File(url.getFile(), ""test"");
    final File expectedFolder = new File(url.getFile(), ""expected"");
    WroTestUtils.compareFromDifferentFoldersByExtension(testFolder, expectedFolder, ""css"", processor);
  }",1
"@Test
	public void testGuessCurrentDump() throws IOException {
		this.dm.setFileContents(
				this.dmPath.resolve(""current-dump.xml.bz2""), """");
		MwLocalDumpFile df = new MwLocalDumpFile(
				""/current-dump.xml.bz2"");
		assertTrue(df.isAvailable());
		assertEquals(df.getDumpContentType(), DumpContentType.CURRENT);
	}",1
"@Test
	public void shouldFindValidWebjar() throws Exception {
		assertNotEmpty(victim.locate(""webjars:jquery.js""));
		assertNotEmpty(victim.locate(""webjars:jquery/"" + ExternalLibrary.JQUERY.version() + ""/jquery.js""));
		assertNotEmpty(victim.locate(""webjars:/jquery/"" + ExternalLibrary.JQUERY.version() + ""/jquery.js""));
	}",1
"@Test
	public void shouldLocateWebjarResourceContainingQuestionMarkInUri() throws Exception {
		victim.locate(""webjars:font-awesome/"" + ExternalLibrary.FONT_AWESOME.version() + ""/webfonts/fa-regular-400.woff?v=""
				+ ExternalLibrary.FONT_AWESOME.version());
	}",1
"@Test
	public void shouldNotFailWhenThereIsAWebjarResourceOutsideOfJar() throws IOException {
		assertNotEmpty(victim.locate(""webjars:webjarFail.js""));
	}",1
"@Test
  public void shouldInvokeRegisteredCallbacks() {
    final LifecycleCallback callback = Mockito.mock(LifecycleCallback.class);
    final Resource changedResource = Resource.create(""test.js"");
    registry.registerCallback(factoryFor(callback));
    
    registry.onBeforeModelCreated();
    Mockito.verify(callback).onBeforeModelCreated();
    
    registry.onAfterModelCreated();
    Mockito.verify(callback).onAfterModelCreated();
    
    registry.onBeforePreProcess();
    Mockito.verify(callback).onBeforePreProcess();
    
    registry.onAfterPreProcess();
    Mockito.verify(callback).onAfterPreProcess();
    
    registry.onBeforePostProcess();
    Mockito.verify(callback).onBeforePostProcess();
    
    registry.onAfterPostProcess();
    Mockito.verify(callback).onAfterPostProcess();
    
    registry.onBeforeMerge();
    Mockito.verify(callback).onBeforeMerge();
    
    registry.onAfterMerge();
    Mockito.verify(callback).onAfterMerge();
    
    registry.onProcessingComplete();
    Mockito.verify(callback).onProcessingComplete();
    
    registry.onResourceChanged(changedResource);
    Mockito.verify(callback).onResourceChanged(Mockito.eq(changedResource));
  }",1
"@Test
  public void testWildcardLocator()
      throws IOException {
    locator = new DefaultWildcardStreamLocator() {
      @Override
      void triggerWildcardExpander(final Collection<File> allFiles, final WildcardContext wildcardContext)
          throws IOException {
        assertEquals(2, allFiles.size());
      }",1
"@Test
  public void shouldRemoveImportsFromComments()
      throws Exception {
    compareResultsFromFolderUsingProcessor(""expectedRemoveImportsFromComments"", createImportsRemovalProcessor());
  }",1
"@Test
    public void testIsDevOpsAdmin() throws Throwable {
        Method setAuthCtxMethod = Operation.class.getDeclaredMethod(""setAuthorizationContext"",
                AuthorizationContext.class);

        Claims guestClaims = new Claims.Builder().setSubject(GuestUserService.SELF_LINK)
                .getResult();
        AuthorizationContext guestContext = AuthorizationContext.Builder.create()
                .setClaims(guestClaims).getResult();

        // TODO Currently all authorized non-guest users are devOpsAdmins. Needs to be changed after
        // roles are introduced. Also, a case for developer authorization context and cloud admin
        // need to be added.
        Claims devOpsClaims = new Claims.Builder()
                .setSubject(AuthUtil.buildUserServicePathFromPrincipalId(encode(""some-user@local"")))
                .getResult();
        AuthorizationContext devOpsContext = AuthorizationContext.Builder.create()
                .setClaims(devOpsClaims).getResult();

        setAuthCtxMethod.setAccessible(true);

        Operation op = new Operation();
        setAuthCtxMethod.invoke(op, (AuthorizationContext) null);
        assertEquals(null, op.getAuthorizationContext());
        assertFalse(""<null> authorization context should not be treated as devOps admin context"",
                AuthUtil.isDevOpsAdmin(op));

        setAuthCtxMethod.invoke(op, guestContext);
        assertFalse(""Guest authorization context should not be trated as devOps admin context"",
                AuthUtil.isDevOpsAdmin(op));

        setAuthCtxMethod.invoke(op, devOpsContext);
        assertTrue(""Any non-guest authorized user should be a devOps admin"",
                AuthUtil.isDevOpsAdmin(op));

        setAuthCtxMethod.setAccessible(false);
    }",1
"@Test
    public void testExportService() throws InterruptedException {
        int port = NetUtils.getAvailablePort();
        URL serviceurl = URL.valueOf(""dubbo://127.0.0.1:"" + port + ""/test?proxy=jdk&timeout="" + Integer.MAX_VALUE);
        DemoService demo = new DemoServiceImpl();
        Invoker<DemoService> invoker = proxy.getInvoker(demo, DemoService.class, serviceurl);
        protocol.export(invoker);
        synchronized (EnumBak.class) {
            EnumBak.class.wait();
        }",1
"@Test
  public void testCompactFormat() throws IOException {
    out=new BufferedWriter(new FileWriter(CONFIG));
    startConfig();
    appendCompactFormatProperty(""a"", ""b"");
    appendCompactFormatProperty(""c"", ""d"", true);
    appendCompactFormatProperty(""e"", ""f"", false, ""g"");
    endConfig();
    Path fileResource = new Path(CONFIG);
    Configuration conf = new Configuration(false);
    conf.addResource(fileResource);

    assertEquals(""b"", conf.get(""a""));

    assertEquals(""d"", conf.get(""c""));
    Set<String> s = conf.getFinalParameters();
    assertEquals(1, s.size());
    assertTrue(s.contains(""c""));

    assertEquals(""f"", conf.get(""e""));
    String[] sources = conf.getPropertySources(""e"");
    assertEquals(2, sources.length);
    assertEquals(""g"", sources[0]);
    assertEquals(fileResource.toString(), sources[1]);
  }",1
"@Test
  public void testConcurrentAccesses() throws Exception {
    out = new BufferedWriter(new FileWriter(CONFIG));
    startConfig();
    declareProperty(""some.config"", ""xyz"", ""xyz"", false);
    endConfig();
    Path fileResource = new Path(CONFIG);
    Configuration conf = new Configuration();
    conf.addResource(fileResource);

    class ConfigModifyThread extends Thread {
      final private Configuration config;
      final private String prefix;

      public ConfigModifyThread(Configuration conf, String prefix) {
        config = conf;
        this.prefix = prefix;
      }",1
"@Test
  public void testDoubleValues() throws IOException {
    out=new BufferedWriter(new FileWriter(CONFIG));
    startConfig();
    appendProperty(""test.double1"", ""3.1415"");
    appendProperty(""test.double2"", ""003.1415"");
    appendProperty(""test.double3"", ""-3.1415"");
    appendProperty(""test.double4"", "" -3.1415 "");
    appendProperty(""test.double5"", ""xyz-3.1415xyz"");
    endConfig();
    Path fileResource = new Path(CONFIG);
    conf.addResource(fileResource);
    assertEquals(3.1415, conf.getDouble(""test.double1"", 0.0), DOUBLE_DELTA);
    assertEquals(3.1415, conf.getDouble(""test.double2"", 0.0), DOUBLE_DELTA);
    assertEquals(-3.1415, conf.getDouble(""test.double3"", 0.0), DOUBLE_DELTA);
    assertEquals(-3.1415, conf.getDouble(""test.double4"", 0.0), DOUBLE_DELTA);
    try {
      conf.getDouble(""test.double5"", 0.0);
      fail(""Property had invalid double value, but was read successfully."");
    }",1
"@Test
  public void testDumpProperty() throws IOException {
    StringWriter outWriter = new StringWriter();
    ObjectMapper mapper = new ObjectMapper();
    String jsonStr = null;
    String xmlStr = null;
    try {
      Configuration testConf = new Configuration(false);
      out = new BufferedWriter(new FileWriter(CONFIG));
      startConfig();
      appendProperty(""test.key1"", ""value1"");
      appendProperty(""test.key2"", ""value2"", true);
      appendProperty(""test.key3"", ""value3"");
      endConfig();
      Path fileResource = new Path(CONFIG);
      testConf.addResource(fileResource);
      out.close();

      // case 1: dump an existing property
      // test json format
      outWriter = new StringWriter();
      Configuration.dumpConfiguration(testConf, ""test.key2"", outWriter);
      jsonStr = outWriter.toString();
      outWriter.close();
      mapper = new ObjectMapper();
      SingleJsonConfiguration jconf1 =
          mapper.readValue(jsonStr, SingleJsonConfiguration.class);
      JsonProperty jp1 = jconf1.getProperty();
      assertEquals(""test.key2"", jp1.getKey());
      assertEquals(""value2"", jp1.getValue());
      assertEquals(true, jp1.isFinal);
      assertEquals(fileResource.toString(), jp1.getResource());

      // test xml format
      outWriter = new StringWriter();
      testConf.writeXml(""test.key2"", outWriter);
      xmlStr = outWriter.toString();
      outWriter.close();
      Configuration actualConf1 = getActualConf(xmlStr);
      assertEquals(1, actualConf1.size());
      assertEquals(""value2"", actualConf1.get(""test.key2""));
      assertTrue(actualConf1.getFinalParameters().contains(""test.key2""));
      assertEquals(fileResource.toString(),
          actualConf1.getPropertySources(""test.key2"")[0]);

      // case 2: dump an non existing property
      // test json format
      try {
        outWriter = new StringWriter();
        Configuration.dumpConfiguration(testConf,
            ""test.unknown.key"", outWriter);
        outWriter.close();
      }",1
"@Test
  public void testEntityReference() throws Exception {
    tearDown();
    out=new BufferedWriter(new FileWriter(CONFIG));
    writeHeader();
    declareEntity(""configuration"", ""d"", ""d"");
    writeConfiguration();
    appendProperty(""a"", ""b"");
    appendProperty(""c"", ""&d;"");
    endConfig();

    // verify that the includes file contains all properties
    Path fileResource = new Path(CONFIG);
    conf.addResource(fileResource);
    assertEquals(conf.get(""a""), ""b"");
    assertEquals(conf.get(""c""), ""d"");
    tearDown();
  }",1
"@Test
  public void testEnumFromXml() throws IOException {
    out=new BufferedWriter(new FileWriter(CONFIG_FOR_ENUM));
    startConfig();
    appendProperty(""test.enum"","" \t \n   FOO \t \n"");
    appendProperty(""test.enum2"","" \t \n   Yak.FOO \t \n"");
    endConfig();

    Configuration conf = new Configuration();
    Path fileResource = new Path(CONFIG_FOR_ENUM);
    conf.addResource(fileResource);
    assertSame(Yak.FOO, conf.getEnum(""test.enum"", Yak.FOO));
    boolean fail = false;
    try {
      conf.getEnum(""test.enum2"", Yak.FOO);
    }",1
"@Test
  public void testGetClassByNameOrNull() throws Exception {
    Configuration config = new Configuration();
    Class<?> clazz = config.getClassByNameOrNull(""java.lang.Object"");
    assertNotNull(clazz);
  }",1
"@Test
  public void testHexValues() throws IOException{
    out=new BufferedWriter(new FileWriter(CONFIG));
    startConfig();
    appendProperty(""test.hex1"", ""0x10"");
    appendProperty(""test.hex2"", ""0xF"");
    appendProperty(""test.hex3"", ""-0x10"");
    // Invalid?
    appendProperty(""test.hex4"", ""-0x10xyz"");
    endConfig();
    Path fileResource = new Path(CONFIG);
    conf.addResource(fileResource);
    assertEquals(16, conf.getInt(""test.hex1"", 0));
    assertEquals(16, conf.getLong(""test.hex1"", 0));
    assertEquals(15, conf.getInt(""test.hex2"", 0));
    assertEquals(15, conf.getLong(""test.hex2"", 0));
    assertEquals(-16, conf.getInt(""test.hex3"", 0));
    assertEquals(-16, conf.getLong(""test.hex3"", 0));
    try {
      conf.getLong(""test.hex4"", 0);
      fail(""Property had invalid long value, but was read successfully."");
    }",1
"@Test
  public void testHumanReadableValues() throws IOException {
    out = new BufferedWriter(new FileWriter(CONFIG));
    startConfig();
    appendProperty(""test.humanReadableValue1"", ""1m"");
    appendProperty(""test.humanReadableValue2"", ""1M"");
    appendProperty(""test.humanReadableValue5"", ""1MBCDE"");

    endConfig();
    Path fileResource = new Path(CONFIG);
    conf.addResource(fileResource);
    assertEquals(1048576, conf.getLongBytes(""test.humanReadableValue1"", 0));
    assertEquals(1048576, conf.getLongBytes(""test.humanReadableValue2"", 0));
    try {
      conf.getLongBytes(""test.humanReadableValue5"", 0);
      fail(""Property had invalid human readable value, but was read successfully."");
    }",1
"@Test
  public void testIncludesWithFallback() throws Exception {
    tearDown();
    out=new BufferedWriter(new FileWriter(CONFIG2));
    startConfig();
    appendProperty(""a"",""b"");
    appendProperty(""c"",""d"");
    endConfig();

    out=new BufferedWriter(new FileWriter(CONFIG));
    startConfig();
    startInclude(CONFIG2);
    startFallback();
    appendProperty(""a"", ""b.fallback"");
    appendProperty(""c"", ""d.fallback"", true);
    endFallback();
    endInclude();
    appendProperty(""e"",""f"");
    appendProperty(""g"",""h"");
    startInclude(""MissingConfig.xml"");
    startFallback();
    appendProperty(""i"", ""j.fallback"");
    appendProperty(""k"", ""l.fallback"", true);
    endFallback();
    endInclude();
    endConfig();

    // verify that the includes file contains all properties
    Path fileResource = new Path(CONFIG);
    conf.addResource(fileResource);
    assertEquals(""b"", conf.get(""a""));
    assertEquals(""d"", conf.get(""c""));
    assertEquals(""f"", conf.get(""e""));
    assertEquals(""h"", conf.get(""g""));
    assertEquals(""j.fallback"", conf.get(""i""));
    assertEquals(""l.fallback"", conf.get(""k""));
    tearDown();
  }",1
"@Test
  public void testIntegerValues() throws IOException{
    out=new BufferedWriter(new FileWriter(CONFIG));
    startConfig();
    appendProperty(""test.int1"", ""20"");
    appendProperty(""test.int2"", ""020"");
    appendProperty(""test.int3"", ""-20"");
    appendProperty(""test.int4"", "" -20 "");
    appendProperty(""test.int5"", "" -20xyz "");
    endConfig();
    Path fileResource = new Path(CONFIG);
    conf.addResource(fileResource);
    assertEquals(20, conf.getInt(""test.int1"", 0));
    assertEquals(20, conf.getLong(""test.int1"", 0));
    assertEquals(20, conf.getLongBytes(""test.int1"", 0));
    assertEquals(20, conf.getInt(""test.int2"", 0));
    assertEquals(20, conf.getLong(""test.int2"", 0));
    assertEquals(20, conf.getLongBytes(""test.int2"", 0));
    assertEquals(-20, conf.getInt(""test.int3"", 0));
    assertEquals(-20, conf.getLong(""test.int3"", 0));
    assertEquals(-20, conf.getLongBytes(""test.int3"", 0));
    assertEquals(-20, conf.getInt(""test.int4"", 0));
    assertEquals(-20, conf.getLong(""test.int4"", 0));
    assertEquals(-20, conf.getLongBytes(""test.int4"", 0));
    try {
      conf.getInt(""test.int5"", 0);
      fail(""Property had invalid int value, but was read successfully."");
    }",1
"@Test
  public void testOverlay() throws IOException{
    out=new BufferedWriter(new FileWriter(CONFIG));
    startConfig();
    appendProperty(""a"",""b"");
    appendProperty(""b"",""c"");
    appendProperty(""d"",""e"");
    appendProperty(""e"",""f"", true);
    endConfig();

    out=new BufferedWriter(new FileWriter(CONFIG2));
    startConfig();
    appendProperty(""a"",""b"");
    appendProperty(""b"",""d"");
    appendProperty(""e"",""e"");
    endConfig();

    Path fileResource = new Path(CONFIG);
    conf.addResource(fileResource);

    //set dynamically something
    conf.set(""c"",""d"");
    conf.set(""a"",""d"");

    Configuration clone=new Configuration(conf);
    clone.addResource(new Path(CONFIG2));

    assertEquals(clone.get(""a""), ""d"");
    assertEquals(clone.get(""b""), ""d"");
    assertEquals(clone.get(""c""), ""d"");
    assertEquals(clone.get(""d""), ""e"");
    assertEquals(clone.get(""e""), ""f"");

  }",1
"@Test
  public void testVariableSubstitution() throws IOException {
    // stubbing only environment dependent functions
    Configuration mock = Mockito.spy(conf);
    Mockito.when(mock.getProperty(""user.name"")).thenReturn(""hadoop_user"");
    Mockito.when(mock.getenv(""FILE_NAME"")).thenReturn(""hello"");

    out=new BufferedWriter(new FileWriter(CONFIG));
    startConfig();
    declareProperty(""my.int"", ""${intvar}",1
"@Test
  public void testDelegationTokensOpsHttpKerberized() throws Exception {
    testDelegationTokensOps(false, true);
  }",1
"@Test
  public void testDelegationTokensOpsHttpPseudo() throws Exception {
    testDelegationTokensOps(false, false);
  }",1
"@Test
  public void testKMSJMX() throws Exception {
    Configuration conf = new Configuration();
    final File confDir = getTestDir();
    conf = createBaseKMSConf(confDir, conf);
    final String processName = ""testkmsjmx"";
    conf.set(KMSConfiguration.METRICS_PROCESS_NAME_KEY, processName);
    writeConf(confDir, conf);

    runServer(null, null, confDir, new KMSCallable<Void>() {
      @Override
      public Void call() throws Exception {
        final URL jmxUrl = new URL(
            getKMSUrl() + ""/jmx?user.name=whatever&qry=Hadoop:service=""
                + processName + "",name=JvmMetrics"");
        LOG.info(""Requesting jmx from "" + jmxUrl);
        final StringBuilder sb = new StringBuilder();
        final InputStream in = jmxUrl.openConnection().getInputStream();
        final byte[] buffer = new byte[64 * 1024];
        int len;
        while ((len = in.read(buffer)) > 0) {
          sb.append(new String(buffer, 0, len));
        }",1
"@Test
  public void testKMSTimeout() throws Exception {
    File confDir = getTestDir();
    Configuration conf = createBaseKMSConf(confDir);
    conf.setInt(CommonConfigurationKeysPublic.KMS_CLIENT_TIMEOUT_SECONDS, 1);
    writeConf(confDir, conf);

    ServerSocket sock;
    int port;
    try {
      sock = new ServerSocket(0, 50, InetAddress.getByName(""localhost""));
      port = sock.getLocalPort();
    }",1
"@Test
  public void testKMSWithZKDTSM() throws Exception {
    doKMSWithZK(false, true);
  }",1
"@Test
  public void testMultiPartUploadConcurrent() throws IOException {
    FileSystem.clearStatistics();
    long size = 50 * 1024 * 1024 - 1;
    ContractTestUtils.createAndVerifyFile(fs, getTestPath(), size);
    FileSystem.Statistics statistics =
        FileSystem.getStatistics(""oss"", AliyunOSSFileSystem.class);
    assertEquals(105, statistics.getReadOps());
    assertEquals(size, statistics.getBytesRead());
    assertEquals(52, statistics.getWriteOps());
    assertEquals(size, statistics.getBytesWritten());
    bufferShouldReleased();
  }",1
"@Test
  public void testZeroByteUpload() throws IOException {
    ContractTestUtils.createAndVerifyFile(fs, getTestPath(), 0);
    bufferShouldReleased(true);
  }",1
"@Test
  public void testOSSFileReaderTask() throws Exception {
    Path smallSeekFile = setPath(""/test/smallSeekFileOSSFileReader.txt"");
    long size = 5 * 1024 * 1024;

    ContractTestUtils.generateTestFile(this.fs, smallSeekFile, size, 256, 255);
    LOG.info(""5MB file created: smallSeekFileOSSFileReader.txt"");
    ReadBuffer readBuffer = new ReadBuffer(12, 24);
    AliyunOSSFileReaderTask task = new AliyunOSSFileReaderTask(""1"",
        ((AliyunOSSFileSystem)this.fs).getStore(), readBuffer);
    //NullPointerException, fail
    task.run();
    assertEquals(readBuffer.getStatus(), ReadBuffer.STATUS.ERROR);
    //OK
    task = new AliyunOSSFileReaderTask(
        ""test/test/smallSeekFileOSSFileReader.txt"",
        ((AliyunOSSFileSystem)this.fs).getStore(), readBuffer);
    task.run();
    assertEquals(readBuffer.getStatus(), ReadBuffer.STATUS.SUCCESS);
  }",1
"@Test
  public void testSeekFile() throws Exception {
    Path smallSeekFile = setPath(""/test/smallSeekFile.txt"");
    long size = 5 * 1024 * 1024;

    ContractTestUtils.generateTestFile(this.fs, smallSeekFile, size, 256, 255);
    LOG.info(""5MB file created: smallSeekFile.txt"");

    FSDataInputStream instream = this.fs.open(smallSeekFile);
    int seekTimes = 5;
    LOG.info(""multiple fold position seeking test...:"");
    for (int i = 0; i < seekTimes; i++) {
      long pos = size / (seekTimes - i) - 1;
      LOG.info(""begin seeking for pos: "" + pos);
      instream.seek(pos);
      assertTrue(""expected position at:"" + pos + "", but got:""
          + instream.getPos(), instream.getPos() == pos);
      LOG.info(""completed seeking at pos: "" + instream.getPos());
    }",1
"@Test
  public void testDelete() throws Exception {
    Path danglingFile = new Path(""/crashedInTheMiddle"");

    // Create a file and leave it dangling and try to delete it.
    FSDataOutputStream stream = fs.create(danglingFile);
    stream.write(new byte[] { 1, 2, 3 }",1
"@Test
  public void testCreatePermission() throws IOException {
    assumeNotWindows();
    String filename = ""foo"";
    Path f = fileContextTestHelper.getTestRootPath(fc, filename);
    fileContextTestHelper.createFile(fc, filename);
    doFilePermissionCheck(FileContext.FILE_DEFAULT_PERM.applyUMask(fc.getUMask()),
                        fc.getFileStatus(f).getPermission());
  }",1
"@Test
  public void testCreateFileWithNullName() throws IOException {
    String fileName = null;

    try {

      Path testPath = qualifiedPath(fileName, fc2);
      // Ensure file does not exist
      Assert.assertFalse(exists(fc2, testPath));

      // Create a file on fc2's file system using fc1
      createFile(fc1, testPath);
      Assert.fail(""Create file with null name should throw IllegalArgumentException."");
    }",1
"@Test
  public void testListStatusThrowsExceptionForNonExistentFile()
      throws Exception {
    String testFile = ""test/hadoop/file"";
    Path testPath = qualifiedPath(testFile, fc2);
    try {
      fc1.listStatus(testPath);
      Assert.fail(""Should throw FileNotFoundException"");
    }",1
"@Test
  public void testStructureGenerator() throws Exception {
    StructureGenerator sg = new StructureGenerator();
    String[] args = new String[]{""-maxDepth"", ""2"", ""-minWidth"", ""1"",
        ""-maxWidth"", ""2"", ""-numOfFiles"", ""2"",
        ""-avgFileSize"", ""1"", ""-outDir"", OUT_DIR.getAbsolutePath(), ""-seed"", ""1""}",1
"@Test
  public void testAttemptPathConstructionWrongSchema() throws Exception {
    Configuration config = newConfig();
    final String jobUUID = addUUID(config);
    config.set(BUFFER_DIR,
        ""hdfs://nn:8020/tmp/mr-local-0,hdfs://nn:8020/tmp/mr-local-1"");
    intercept(IllegalArgumentException.class, ""Wrong FS"",
        () -> getLocalTaskAttemptTempDir(config, jobUUID,
                tac.getTaskAttemptID()));
  }",1
"@Test
  public void testJobCommitFailure() throws Exception {
    Path jobAttemptPath = jobCommitter.getJobAttemptPath(job);
    FileSystem fs = jobAttemptPath.getFileSystem(conf);

    Set<String> uploads = runTasks(job, 4, 3);

    assertPathExists(fs, ""No job attempt path"", jobAttemptPath);

    errors.failOnCommit(5);
    setMockLogLevel(MockS3AFileSystem.LOG_NAME);

    intercept(IOException.class,
        ""Fail on commit 5"",
        ""Should propagate the commit failure"",
        () -> {
          jobCommitter.commitJob(job);
          return jobCommitter.toString();
        }",1
"@Test
  public void testFinalDestinationBaseDirectChild() {
    finalDestination(l(MAGIC_PATH_PREFIX, BASE, ""3.txt""));
  }",1
"@Test
  public void testFinalDestinationMagic1() {
    assertEquals(l(""first"", ""2""),
        finalDestination(l(""first"", MAGIC_PATH_PREFIX, ""2"")));
  }",1
"@Test
  public void testLastElementSingle() {
    assertEquals(""first"", lastElement(l(""first"")));
  }",1
"@Test
  public void testParentDeepMagic() {
    assertParents(a(""parent1"", ""parent2""), DEEP_MAGIC);
  }",1
"@Test
  public void testCreateFile() throws Exception {
    Path file = touch(sftpFs, name.getMethodName().toLowerCase());
    assertTrue(localFs.exists(file));
    assertTrue(sftpFs.delete(file, false));
    assertFalse(localFs.exists(file));
    assertThat(
        ((SFTPFileSystem) sftpFs).getConnectionPool().getLiveConnCount())
        .isEqualTo(1);
  }",1
"@Test
  public void testReadFile() throws Exception {
    byte[] data = ""yaks"".getBytes();
    Path file = touch(localFs, name.getMethodName().toLowerCase(), data);
    FSDataInputStream is = null;
    try {
      is = sftpFs.open(file);
      byte[] b = new byte[data.length];
      is.read(b);
      assertArrayEquals(data, b);
    }",1
"@Test
  public void testRenameFile() throws Exception {
    byte[] data = ""dingos"".getBytes();
    Path file1 = touch(localFs, name.getMethodName().toLowerCase() + ""1"");
    Path file2 = new Path(localDir, name.getMethodName().toLowerCase() + ""2"");

    assertTrue(sftpFs.rename(file1, file2));

    assertTrue(sftpFs.exists(file2));
    assertFalse(sftpFs.exists(file1));

    assertTrue(localFs.exists(file2));
    assertFalse(localFs.exists(file1));

    assertTrue(sftpFs.delete(file2, false));
    assertThat(
        ((SFTPFileSystem) sftpFs).getConnectionPool().getLiveConnCount())
        .isEqualTo(1);
  }",1
"@Test
  public void testStatFile() throws Exception {
    byte[] data = ""yaks"".getBytes();
    Path file = touch(localFs, name.getMethodName().toLowerCase(), data);

    FileStatus lstat = localFs.getFileStatus(file);
    FileStatus sstat = sftpFs.getFileStatus(file);
    assertNotNull(sstat);

    assertEquals(lstat.getPath().toUri().getPath(),
                 sstat.getPath().toUri().getPath());
    assertEquals(data.length, sstat.getLen());
    assertEquals(lstat.getLen(), sstat.getLen());
    assertTrue(sftpFs.delete(file, false));
    assertThat(
        ((SFTPFileSystem) sftpFs).getConnectionPool().getLiveConnCount())
        .isEqualTo(1);
  }",1
"@Test
  public void testRenameFileIntoDir() throws Exception {
    Path srcPath = new Path(TEST_ROOT_DIR, ""testRenameSrc"");
    Path dstPath = new Path(TEST_ROOT_DIR, ""testRenameDir"");
    localFs.mkdirs(dstPath);
    verifyRename(srcPath, dstPath, true);
  }",1
"@Test
  public void testRenameFileToFile() throws Exception {
    Path srcPath = new Path(TEST_ROOT_DIR, ""testRenameSrc"");
    Path dstPath = new Path(TEST_ROOT_DIR, ""testRenameDst"");
    verifyRename(srcPath, dstPath, false);
  }",1
"@Test
  public void testSetPermissionCrc() throws Exception {
    FileSystem rawFs = localFs.getRawFileSystem();
    Path p = new Path(TEST_ROOT_DIR, ""testCrcPermissions"");
    localFs.createNewFile(p);
    Path crc = localFs.getChecksumFile(p);
    assert(rawFs.exists(crc));

    for (short mode : Arrays.asList((short)0666, (short)0660, (short)0600)) {
      FsPermission perm = new FsPermission(mode);
      localFs.setPermission(p, perm);
      assertEquals(perm, localFs.getFileStatus(p).getPermission());
      assertEquals(perm, rawFs.getFileStatus(crc).getPermission());
    }",1
"@Test
  public void testFsClose() throws Exception {
    {
      Configuration conf = new Configuration();
      new Path(""file:///"").getFileSystem(conf);
      FileSystem.closeAll();
    }",1
"@Test
  public void testReadSymlinkWithNullInput() {
    String result = FileUtil.readLink(null);
    Assert.assertEquals("""", result);
  }",1
"@Test
  public void testReadSymlinkWithAFileAsInput() throws IOException {
    File file = new File(del, FILE);

    String result = FileUtil.readLink(file);
    Assert.assertEquals("""", result);

    Verify.delete(file);
  }",1
"@Test
  public void testSymlink2DifferentFile() throws IOException {
    File file = new File(del, FILE);
    File fileSecond = new File(del, FILE + ""_1"");
    File link = new File(del, ""_link"");

    // Create a symbolic link
    // The operation should succeed
    int result =
        FileUtil.symLink(file.getAbsolutePath(), link.getAbsolutePath());

    Assert.assertEquals(0, result);

    // The operation should fail and returns 1
    result =
        FileUtil.symLink(fileSecond.getAbsolutePath(), link.getAbsolutePath());

    Assert.assertEquals(1, result);
  }",1
"@Test
  public void testPathEscapes() throws IOException {
    Path path = new Path(TEST_ROOT_DIR, ""foo%bar"");
    writeFile(fileSys, path, 1);
    FileStatus status = fileSys.getFileStatus(path);
    assertEquals(fileSys.makeQualified(path), status.getPath());
    cleanupFile(fileSys, path);
  }",1
"@Test
  public void testSyncable() throws IOException {
    FileSystem fs = fileSys.getRawFileSystem();
    Path file = new Path(TEST_ROOT_DIR, ""syncable"");
    FSDataOutputStream out = fs.create(file);
    final int bytesWritten = 1;
    byte[] expectedBuf = new byte[] {'0', '1', '2', '3'}",1
"@Test
  public void testBasicPaths() {
    URI uri = fSys.getUri();
    Assert.assertEquals(chrootedTo.toUri(), uri);
    Assert.assertEquals(fSys.makeQualified(
        new Path(System.getProperty(""user.home""))),
        fSys.getWorkingDirectory());
    Assert.assertEquals(fSys.makeQualified(
        new Path(System.getProperty(""user.home""))),
        fSys.getHomeDirectory());
    /*
     * ChRootedFs as its uri like file:///chrootRoot.
     * This is questionable since path.makequalified(uri, path) ignores
     * the pathPart of a uri. So our notion of chrooted URI is questionable.
     * But if we were to fix Path#makeQualified() then  the next test should
     *  have been:

    Assert.assertEquals(
        new Path(chrootedTo + ""/foo/bar"").makeQualified(
            FsConstants.LOCAL_FS_URI, null),
        fSys.makeQualified(new Path( ""/foo/bar"")));
    */
    
    Assert.assertEquals(
        new Path(""/foo/bar"").makeQualified(FsConstants.LOCAL_FS_URI, null),
        fSys.makeQualified(new Path(""/foo/bar"")));
  }",1
"@Test
  public void testCreateDelete() throws IOException {
    

    // Create file 
    fileSystemTestHelper.createFile(fSys, ""/foo"");
    Assert.assertTrue(fSys.isFile(new Path(""/foo"")));
    Assert.assertTrue(fSysTarget.isFile(new Path(chrootedTo, ""foo"")));
    
    // Create file with recursive dir
    fileSystemTestHelper.createFile(fSys, ""/newDir/foo"");
    Assert.assertTrue(fSys.isFile(new Path(""/newDir/foo"")));
    Assert.assertTrue(fSysTarget.isFile(new Path(chrootedTo,""newDir/foo"")));
    
    // Delete the created file
    Assert.assertTrue(fSys.delete(new Path(""/newDir/foo""), false));
    Assert.assertFalse(fSys.exists(new Path(""/newDir/foo"")));
    Assert.assertFalse(fSysTarget.exists(new Path(chrootedTo, ""newDir/foo"")));
    
    // Create file with a 2 component dirs recursively
    fileSystemTestHelper.createFile(fSys, ""/newDir/newDir2/foo"");
    Assert.assertTrue(fSys.isFile(new Path(""/newDir/newDir2/foo"")));
    Assert.assertTrue(fSysTarget.isFile(new Path(chrootedTo,""newDir/newDir2/foo"")));
    
    // Delete the created file
    Assert.assertTrue(fSys.delete(new Path(""/newDir/newDir2/foo""), false));
    Assert.assertFalse(fSys.exists(new Path(""/newDir/newDir2/foo"")));
    Assert.assertFalse(fSysTarget.exists(new Path(chrootedTo,""newDir/newDir2/foo"")));
  }",1
"@Test
  public void testListLocatedFileStatus() throws Exception {
    final Path mockMount = new Path(""mockfs://foo/user"");
    final Path mockPath = new Path(""/usermock"");
    final Configuration conf = new Configuration();
    conf.setClass(""fs.mockfs.impl"", MockFileSystem.class, FileSystem.class);
    ConfigUtil.addLink(conf, mockPath.toString(), mockMount.toUri());
    FileSystem vfs = FileSystem.get(URI.create(""viewfs:///""), conf);
    vfs.listLocatedStatus(mockPath);
    final FileSystem mockFs =
        ((MockFileSystem) getChildFileSystem((ViewFileSystem) vfs,
            new URI(""mockfs://foo/""))).getRawFileSystem();
    verify(mockFs).listLocatedStatus(new Path(mockMount.toUri().getPath()));
  }",1
"@Test
  public void testResolvePath() throws IOException {
    Assert.assertEquals(chrootedTo, fSys.resolvePath(new Path(""/""))); 
    fileSystemTestHelper.createFile(fSys, ""/foo"");
    Assert.assertEquals(new Path(chrootedTo, ""foo""),
        fSys.resolvePath(new Path(""/foo""))); 
  }",1
"@Test
  public void testCreateDelete() throws IOException {
    

    // Create file 
    fileContextTestHelper.createFileNonRecursive(fc, ""/foo"");
    Assert.assertTrue(isFile(fc, new Path(""/foo"")));
    Assert.assertTrue(isFile(fcTarget, new Path(chrootedTo, ""foo"")));
    
    // Create file with recursive dir
    fileContextTestHelper.createFile(fc, ""/newDir/foo"");
    Assert.assertTrue(isFile(fc, new Path(""/newDir/foo"")));
    Assert.assertTrue(isFile(fcTarget, new Path(chrootedTo,""newDir/foo"")));
    
    // Delete the created file
    Assert.assertTrue(fc.delete(new Path(""/newDir/foo""), false));
    Assert.assertFalse(exists(fc, new Path(""/newDir/foo"")));
    Assert.assertFalse(exists(fcTarget, new Path(chrootedTo,""newDir/foo"")));
    
    // Create file with a 2 component dirs recursively
    fileContextTestHelper.createFile(fc, ""/newDir/newDir2/foo"");
    Assert.assertTrue(isFile(fc, new Path(""/newDir/newDir2/foo"")));
    Assert.assertTrue(isFile(fcTarget, new Path(chrootedTo,""newDir/newDir2/foo"")));
    
    // Delete the created file
    Assert.assertTrue(fc.delete(new Path(""/newDir/newDir2/foo""), false));
    Assert.assertFalse(exists(fc, new Path(""/newDir/newDir2/foo"")));
    Assert.assertFalse(exists(fcTarget, new Path(chrootedTo,""newDir/newDir2/foo"")));
  }",1
"@Test
  public void testResolvePath() throws IOException {
    Assert.assertEquals(chrootedTo, fc.getDefaultFileSystem().resolvePath(new Path(""/""))); 
    fileContextTestHelper.createFile(fc, ""/foo"");
    Assert.assertEquals(new Path(chrootedTo, ""foo""),
        fc.getDefaultFileSystem().resolvePath(new Path(""/foo""))); 
  }",1
"@Test
  public void testWorkingDirectory() throws Exception {

    // First we cd to our test root
    fc.mkdir(new Path(""/testWd""), FileContext.DEFAULT_PERM, false);
    Path workDir = new Path(""/testWd"");
    Path fqWd = fc.makeQualified(workDir);
    fc.setWorkingDirectory(workDir);
    Assert.assertEquals(fqWd, fc.getWorkingDirectory());

    fc.setWorkingDirectory(new Path("".""));
    Assert.assertEquals(fqWd, fc.getWorkingDirectory());

    fc.setWorkingDirectory(new Path(""..""));
    Assert.assertEquals(fqWd.getParent(), fc.getWorkingDirectory());
    
    // cd using a relative path

    // Go back to our test root
    workDir = new Path(""/testWd"");
    fqWd = fc.makeQualified(workDir);
    fc.setWorkingDirectory(workDir);
    Assert.assertEquals(fqWd, fc.getWorkingDirectory());
    
    Path relativeDir = new Path(""existingDir1"");
    Path absoluteDir = new Path(workDir,""existingDir1"");
    fc.mkdir(absoluteDir, FileContext.DEFAULT_PERM, true);
    Path fqAbsoluteDir = fc.makeQualified(absoluteDir);
    fc.setWorkingDirectory(relativeDir);
    Assert.assertEquals(fqAbsoluteDir, fc.getWorkingDirectory());
    // cd using a absolute path
    absoluteDir = new Path(""/test/existingDir2"");
    fqAbsoluteDir = fc.makeQualified(absoluteDir);
    fc.mkdir(absoluteDir, FileContext.DEFAULT_PERM, true);
    fc.setWorkingDirectory(absoluteDir);
    Assert.assertEquals(fqAbsoluteDir, fc.getWorkingDirectory());
    
    // Now open a file relative to the wd we just set above.
    Path absolutePath = new Path(absoluteDir, ""foo"");
    fc.create(absolutePath, EnumSet.of(CreateFlag.CREATE)).close();
    fc.open(new Path(""foo"")).close();
    
    // Now mkdir relative to the dir we cd'ed to
    fc.mkdir(new Path(""newDir""), FileContext.DEFAULT_PERM, true);
    Assert.assertTrue(isDir(fc, new Path(absoluteDir, ""newDir"")));

    absoluteDir = fileContextTestHelper.getTestRootPath(fc, ""nonexistingPath"");
    try {
      fc.setWorkingDirectory(absoluteDir);
      Assert.fail(""cd to non existing dir should have failed"");
    }",1
"@Test
  public void testBlockReaderLocalByteBufferFastLaneReadsNoChecksumNoReadahead()
      throws IOException {
    runBlockReaderLocalTest(new TestBlockReaderLocalByteBufferFastLaneReads(),
        false, 0);
  }",1
"@Test
  public void testBlockReaderLocalByteBufferReadsNoChecksum()
      throws IOException {
    runBlockReaderLocalTest(
        new TestBlockReaderLocalByteBufferReads(),
        false, HdfsClientConfigKeys.DFS_DATANODE_READAHEAD_BYTES_DEFAULT);
  }",1
"@Test
  public void testBlockReaderLocalOnFileWithoutChecksumNoReadahead()
      throws IOException {
    runBlockReaderLocalTest(new TestBlockReaderLocalOnFileWithoutChecksum(),
        true, 0);
  }",1
"@Test
  public void testBlockReaderLocalReadCorruptNoReadahead()
      throws IOException {
    runBlockReaderLocalTest(new TestBlockReaderLocalReadCorrupt(), true, 0);
  }",1
"@Test
  public void testBlockReaderLocalReadZeroBytesNoChecksum()
      throws IOException {
    runBlockReaderLocalTest(new TestBlockReaderLocalReadZeroBytes(),
        false, HdfsClientConfigKeys.DFS_DATANODE_READAHEAD_BYTES_DEFAULT);
  }",1
"@Test
  public void testBlackListIpClient() throws IOException {
    Configuration conf = new Configuration();
    FileUtils.write(blacklistFile,
        InetAddress.getLocalHost().getHostAddress(), true);
    conf.set(BlackListBasedTrustedChannelResolver
            .DFS_DATATRANSFER_CLIENT_FIXED_BLACK_LIST_FILE,
        blacklistFile.getAbsolutePath());

    resolver.setConf(conf);
    assertFalse(resolver.isTrusted());

  }",1
"@Test
  public void testStartStop() throws IOException {
    Configuration conf = new Configuration();
    MiniJournalCluster c = new MiniJournalCluster.Builder(conf)
      .build();
    c.waitActive();
    try {
      URI uri = c.getQuorumJournalURI(""myjournal"");
      String[] addrs = uri.getAuthority().split("";"");
      assertEquals(3, addrs.length);
      
      JournalNode node = c.getJournalNode(0);
      String dir = node.getConf().get(DFSConfigKeys.DFS_JOURNALNODE_EDITS_DIR_KEY);
      assertEquals(
          new File(MiniDFSCluster.getBaseDirectory() + ""journalnode-0"")
            .getAbsolutePath(),
          dir);
    }",1
"@Test
  public void testMetaSaveMissingReplicas() throws Exception {
    List<DatanodeStorageInfo> origStorages = getStorages(0, 1);
    List<DatanodeDescriptor> origNodes = getNodes(origStorages);
    BlockInfo block = makeBlockReplicasMissing(0, origNodes);
    File file = new File(""test.log"");
    PrintWriter out = new PrintWriter(file);
    bm.metaSave(out);
    out.flush();
    FileInputStream fstream = new FileInputStream(file);
    DataInputStream in = new DataInputStream(fstream);
    BufferedReader reader = new BufferedReader(new InputStreamReader(in));
    StringBuilder buffer = new StringBuilder();
    String line;
    try {
      while ((line = reader.readLine()) != null) {
        buffer.append(line);
      }",1
"@Test
  public void testStorageWithRemainingCapacity() throws Exception {
    final Configuration conf = new HdfsConfiguration();
    final MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).build();
    FileSystem fs = FileSystem.get(conf);
    Path file1 = null;
    try {
      cluster.waitActive();
      final FSNamesystem namesystem = cluster.getNamesystem();
      final String poolId = namesystem.getBlockPoolId();
      final DatanodeRegistration nodeReg =
        InternalDataNodeTestUtils.getDNRegistrationForBP(cluster.getDataNodes().
        		get(0), poolId);
      final DatanodeDescriptor dd = NameNodeAdapter.getDatanode(namesystem,
    		  nodeReg);
      // By default, MiniDFSCluster will create 1 datanode with 2 storages.
      // Assigning 64k for remaining storage capacity and will 
      //create a file with 100k.
      for(DatanodeStorageInfo storage:  dd.getStorageInfos()) { 
    	  storage.setUtilizationForTesting(65536, 0, 65536, 0);
      }",1
"@Test
  public void testBlocksCounter() throws Exception {
    DatanodeDescriptor dd = BlockManagerTestUtil.getLocalDatanodeDescriptor(true);
    assertEquals(0, dd.numBlocks());
    BlockInfo blk = new BlockInfoContiguous(new Block(1L), (short) 1);
    BlockInfo blk1 = new BlockInfoContiguous(new Block(2L), (short) 2);
    DatanodeStorageInfo[] storages = dd.getStorageInfos();
    assertTrue(storages.length > 0);
    // add first block
    assertEquals(AddBlockResult.ADDED, storages[0].addBlock(blk));
    assertEquals(1, dd.numBlocks());
    // remove a non-existent block
    assertFalse(BlocksMap.removeBlock(dd, blk1));
    assertEquals(1, dd.numBlocks());
    // add an existent block
    assertNotEquals(AddBlockResult.ADDED, storages[0].addBlock(blk));
    assertEquals(1, dd.numBlocks());
    // add second block
    assertEquals(AddBlockResult.ADDED, storages[0].addBlock(blk1));
    assertEquals(2, dd.numBlocks());
    // remove first block
    assertTrue(BlocksMap.removeBlock(dd, blk));
    assertEquals(1, dd.numBlocks());
    // remove second block
    assertTrue(BlocksMap.removeBlock(dd, blk1));
    assertEquals(0, dd.numBlocks());    
  }",1
"@Test
  public void testRemoveIncludedNode() throws IOException {
    FSNamesystem fsn = Mockito.mock(FSNamesystem.class);

    // Set the write lock so that the DatanodeManager can start
    Mockito.when(fsn.hasWriteLock()).thenReturn(true);

    DatanodeManager dm = mockDatanodeManager(fsn, new Configuration());
    HostFileManager hm = new HostFileManager();
    HostSet noNodes = new HostSet();
    HostSet oneNode = new HostSet();
    HostSet twoNodes = new HostSet();
    DatanodeRegistration dr1 = new DatanodeRegistration(
      new DatanodeID(""127.0.0.1"", ""127.0.0.1"", ""someStorageID-123"",
          12345, 12345, 12345, 12345),
      new StorageInfo(HdfsServerConstants.NodeType.DATA_NODE),
      new ExportedBlockKeys(), ""test"");
    DatanodeRegistration dr2 = new DatanodeRegistration(
      new DatanodeID(""127.0.0.1"", ""127.0.0.1"", ""someStorageID-234"",
          23456, 23456, 23456, 23456),
      new StorageInfo(HdfsServerConstants.NodeType.DATA_NODE),
      new ExportedBlockKeys(), ""test"");

    twoNodes.add(entry(""127.0.0.1:12345""));
    twoNodes.add(entry(""127.0.0.1:23456""));
    oneNode.add(entry(""127.0.0.1:23456""));

    hm.refresh(twoNodes, noNodes);
    Whitebox.setInternalState(dm, ""hostConfigManager"", hm);

    // Register two data nodes to simulate them coming up.
    // We need to add two nodes, because if we have only one node, removing it
    // will cause the includes list to be empty, which means all hosts will be
    // allowed.
    dm.registerDatanode(dr1);
    dm.registerDatanode(dr2);

    // Make sure that both nodes are reported
    List<DatanodeDescriptor> both =
        dm.getDatanodeListForReport(HdfsConstants.DatanodeReportType.ALL);

    // Sort the list so that we know which one is which
    Collections.sort(both);

    Assert.assertEquals(""Incorrect number of hosts reported"",
        2, both.size());
    Assert.assertEquals(""Unexpected host or host in unexpected position"",
        ""127.0.0.1:12345"", both.get(0).getInfoAddr());
    Assert.assertEquals(""Unexpected host or host in unexpected position"",
        ""127.0.0.1:23456"", both.get(1).getInfoAddr());

    // Remove one node from includes, but do not add it to excludes.
    hm.refresh(oneNode, noNodes);

    // Make sure that only one node is still reported
    List<DatanodeDescriptor> onlyOne =
        dm.getDatanodeListForReport(HdfsConstants.DatanodeReportType.ALL);

    Assert.assertEquals(""Incorrect number of hosts reported"",
        1, onlyOne.size());
    Assert.assertEquals(""Unexpected host reported"",
        ""127.0.0.1:23456"", onlyOne.get(0).getInfoAddr());

    // Remove all nodes from includes
    hm.refresh(noNodes, noNodes);

    // Check that both nodes are reported again
    List<DatanodeDescriptor> bothAgain =
        dm.getDatanodeListForReport(HdfsConstants.DatanodeReportType.ALL);

    // Sort the list so that we know which one is which
    Collections.sort(bothAgain);

    Assert.assertEquals(""Incorrect number of hosts reported"",
        2, bothAgain.size());
    Assert.assertEquals(""Unexpected host or host in unexpected position"",
        ""127.0.0.1:12345"", bothAgain.get(0).getInfoAddr());
    Assert.assertEquals(""Unexpected host or host in unexpected position"",
        ""127.0.0.1:23456"", bothAgain.get(1).getInfoAddr());
  }",1
"@Test
  public void testGetDatanodeByHost() throws Exception {
    assertEquals(map.getDatanodeByHost(""1.1.1.1""), dataNodes[0]);
    assertEquals(map.getDatanodeByHost(""2.2.2.2""), dataNodes[1]);
    DatanodeDescriptor node = map.getDatanodeByHost(""3.3.3.3"");
    assertTrue(node == dataNodes[2] || node == dataNodes[3]);
    assertNull(map.getDatanodeByHost(""4.4.4.4""));
  }",1
"@Test
  public void testDatanodeBlocksReplicatedMetric() throws Exception {
    Configuration conf = new HdfsConfiguration();
    MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).build();
    try {
      FileSystem fs = cluster.getFileSystem();
      List<DataNode> datanodes = cluster.getDataNodes();
      assertEquals(datanodes.size(), 1);
      DataNode datanode = datanodes.get(0);

      MetricsRecordBuilder rb = getMetrics(datanode.getMetrics().name());
      long blocksReplicated = getLongCounter(""BlocksReplicated"", rb);
      assertEquals(""No blocks replicated yet"", 0, blocksReplicated);

      Path path = new Path(""/counter.txt"");
      DFSTestUtil.createFile(fs, path, 1024, (short) 2, Time.monotonicNow());
      cluster.startDataNodes(conf, 1, true, StartupOption.REGULAR, null);
      ExtendedBlock firstBlock = DFSTestUtil.getFirstBlock(fs, path);
      DFSTestUtil.waitForReplication(cluster, firstBlock, 1, 2, 0);

      MetricsRecordBuilder rbNew = getMetrics(datanode.getMetrics().name());
      blocksReplicated = getLongCounter(""BlocksReplicated"", rbNew);
      assertEquals(""blocks replicated counter incremented"", 1, blocksReplicated);
    }",1
"@Test
  public void testInjectionEmpty() throws IOException {
    SimulatedFSDataset fsdataset = getSimulatedFSDataset(); 
    assertBlockReportCountAndSize(fsdataset, 0);
    int bytesAdded = addSomeBlocks(fsdataset);
    assertBlockReportCountAndSize(fsdataset, NUMBLOCKS);
    assertBlockLengthInBlockReports(fsdataset);
    
    // Inject blocks into an empty fsdataset
    //  - injecting the blocks we got above.
    SimulatedFSDataset sfsdataset = getSimulatedFSDataset();
    injectBlocksFromBlockReport(fsdataset, sfsdataset);
    assertBlockReportCountAndSize(fsdataset, NUMBLOCKS);
    assertBlockLengthInBlockReports(fsdataset, sfsdataset);

    assertEquals(bytesAdded, sfsdataset.getDfsUsed());
    assertEquals(sfsdataset.getCapacity()-bytesAdded, sfsdataset.getRemaining());
  }",1
"@Test
  public void testGreedyPlannerBalanceVolumeSet() throws Exception {
    URI clusterJson = getClass()
        .getResource(""/diskBalancer/data-cluster-3node-3disk.json"").toURI();
    ClusterConnector jsonConnector = ConnectorFactory.getCluster(clusterJson,
        null);
    DiskBalancerCluster cluster = new DiskBalancerCluster(jsonConnector);
    cluster.readClusterInfo();
    Assert.assertEquals(3, cluster.getNodes().size());
    cluster.setNodesToProcess(cluster.getNodes());
    DiskBalancerDataNode node = cluster.getNodes().get(0);
    GreedyPlanner planner = new GreedyPlanner(10.0f, node);
    NodePlan plan = new NodePlan(node.getDataNodeName(),
        node.getDataNodePort());
    planner.balanceVolumeSet(node, node.getVolumeSets().get(""SSD""), plan);
  }",1
"@Test
  public void testGreedyPlannerComputePlan() throws Exception {
    URI clusterJson = getClass()
        .getResource(""/diskBalancer/data-cluster-3node-3disk.json"").toURI();
    ClusterConnector jsonConnector = ConnectorFactory.getCluster(clusterJson,
        null);
    DiskBalancerCluster cluster = new DiskBalancerCluster(jsonConnector);
    cluster.readClusterInfo();
    Assert.assertEquals(3, cluster.getNodes().size());
    cluster.setNodesToProcess(cluster.getNodes());
    List<NodePlan> plan = cluster.computePlan(10.0f);
    Assert.assertNotNull(plan);
  }",1
"@Test
  public void testLoadsCorrectClusterConnector() throws Exception {
    ClusterConnector connector = ConnectorFactory.getCluster(getClass()
            .getResource(""/diskBalancer/data-cluster-3node-3disk.json"").toURI()
        , null);
    assertEquals(connector.getClass().toString(),
        ""class org.apache.hadoop.hdfs.server.diskbalancer.connectors."" +
            ""JsonNodeConnector"");

  }",1
"@Test
  public void testResolverWithNoPreference() throws IOException {
    MultipleDestinationMountTableResolver mountTableResolver =
        mockAvailableSpaceResolver(1.0f);
    // Since we don't have any preference, it will
    // always chose the maximum-available-space subcluster.
    PathLocation loc = mountTableResolver.getDestinationForPath(""/space"");
    assertEquals(""subcluster9"",
        loc.getDestinations().get(0).getNameserviceId());

    loc = mountTableResolver.getDestinationForPath(""/space/subdir"");
    assertEquals(""subcluster9"",
        loc.getDestinations().get(0).getNameserviceId());
  }",1
"@Test
  public void testDefaultNameServiceEnable() throws IOException {
    assertTrue(mountTable.isDefaultNSEnable());
    mountTable.setDefaultNameService(""3"");
    mountTable.removeEntry(""/"");

    assertEquals(""3->/unknown"",
        mountTable.getDestinationForPath(""/unknown"").toString());

    Map<String, String> map = getMountTableEntry(""4"", ""/unknown"");
    mountTable.addEntry(MountTable.newInstance(""/unknown"", map));
    mountTable.setDefaultNSEnable(false);
    assertFalse(mountTable.isDefaultNSEnable());

    assertEquals(""4->/unknown"",
        mountTable.getDestinationForPath(""/unknown"").toString());
    try {
      mountTable.getDestinationForPath(""/"");
      fail(""The getDestinationForPath call should fail."");
    }",1
"@Test
  public void testGetConnectionWithConcurrency() throws Exception {
    Map<ConnectionPoolId, ConnectionPool> poolMap = connManager.getPools();
    Configuration copyConf = new Configuration(conf);
    copyConf.setInt(RBFConfigKeys.DFS_ROUTER_MAX_CONCURRENCY_PER_CONNECTION_KEY, 20);

    ConnectionPool pool = new ConnectionPool(
        copyConf, TEST_NN_ADDRESS, TEST_USER1, 1, 10, 0.5f,
        ClientProtocol.class, null);
    poolMap.put(
        new ConnectionPoolId(TEST_USER1, TEST_NN_ADDRESS, ClientProtocol.class),
        pool);
    assertEquals(1, pool.getNumConnections());
    // one connection can process the maximum number of requests concurrently.
    for (int i = 0; i < 20; i++) {
      ConnectionContext cc = pool.getConnection();
      assertTrue(cc.isUsable());
      cc.getClient();
    }",1
"@Test
  public void testAddOrderMountTable() throws IOException {
    testAddOrderMountTable(DestinationOrder.HASH);
    testAddOrderMountTable(DestinationOrder.LOCAL);
    testAddOrderMountTable(DestinationOrder.RANDOM);
    testAddOrderMountTable(DestinationOrder.HASH_ALL);
  }",1
"@Test
  public void testSharedEditsMissingLogs() throws Exception {
    removeStandbyNameDirs();

    CheckpointSignature sig = nn0.getRpcServer().rollEditLog();
    assertEquals(3, sig.getCurSegmentTxId());

    // Should have created edits_1-2 in shared edits dir
    URI editsUri = cluster.getSharedEditsDir(0, maxNNCount - 1);
    File editsDir = new File(editsUri);
    File currentDir = new File(editsDir, ""current"");
    File editsSegment = new File(currentDir,
        NNStorage.getFinalizedEditsFileName(1, 2));
    GenericTestUtils.assertExists(editsSegment);
    GenericTestUtils.assertExists(currentDir);

    // Delete the segment.
    assertTrue(editsSegment.delete());

    // Trying to bootstrap standby should now fail since the edit
    // logs aren't available in the shared dir.
    LogCapturer logs = GenericTestUtils.LogCapturer.captureLogs(
        LoggerFactory.getLogger(BootstrapStandby.class));
    try {
      assertEquals(BootstrapStandby.ERR_CODE_LOGS_UNAVAILABLE, forceBootstrap(1));
    }",1
"@Test
  public void testSingleProxyFailover() throws Exception {
    String singleNS = ""mycluster-"" + Time.monotonicNow();
    URI singleNNUri = new URI(""hdfs://"" + singleNS);
    Configuration singleConf = new Configuration();
    singleConf.set(HdfsClientConfigKeys.DFS_NAMESERVICES, singleNS);
    singleConf.set(HdfsClientConfigKeys.
        DFS_HA_NAMENODES_KEY_PREFIX + ""."" + singleNS, ""nn1"");

    singleConf.set(HdfsClientConfigKeys.
            DFS_NAMENODE_RPC_ADDRESS_KEY + ""."" + singleNS + "".nn1"",
        RandomStringUtils.randomAlphabetic(8) + "".foo.bar:9820"");
    ClientProtocol active = Mockito.mock(ClientProtocol.class);
    Mockito
        .when(active.getBlockLocations(anyString(), anyLong(), anyLong()))
        .thenThrow(new RemoteException(""java.io.FileNotFoundException"",
            ""File does not exist!""));

    RequestHedgingProxyProvider<ClientProtocol> provider =
        new RequestHedgingProxyProvider<>(singleConf, singleNNUri,
            ClientProtocol.class, createFactory(active));
    try {
      provider.getProxy().proxy.getBlockLocations(""/tmp/test.file"", 0L, 20L);
      Assert.fail(""Should fail since the active namenode throws""
          + "" FileNotFoundException!"");
    }",1
"@Test
  public void testCorruptBlock() throws Exception {
    // Create a file with single block with two replicas
    final Path file = getTestPath(""testCorruptBlock"");
    final short replicaCount = 2;
    createFile(file, 100, replicaCount);
    DFSTestUtil.waitForReplication(fs, file, replicaCount, 15000);

    // Disable the heartbeats, so that no corrupted replica
    // can be fixed
    for (DataNode dn : cluster.getDataNodes()) {
      DataNodeTestUtils.setHeartbeatsDisabledForTests(dn, true);
    }",1
"@Test
  public void testEditLogTailing() throws Exception {
    HdfsConfiguration conf = new HdfsConfiguration();
    conf.setInt(DFSConfigKeys.DFS_HA_TAILEDITS_PERIOD_KEY, 1);
    conf.setInt(DFSConfigKeys.DFS_METRICS_PERCENTILES_INTERVALS_KEY, 60);
    MiniDFSCluster dfsCluster = null;
    try {
      dfsCluster = new MiniDFSCluster.Builder(conf)
          .numDataNodes(0)
          .nnTopology(MiniDFSNNTopology.simpleHATopology())
          .build();
      DistributedFileSystem dfs = dfsCluster.getFileSystem(0);
      dfsCluster.transitionToActive(0);
      dfsCluster.waitActive();

      Path testDir = new Path(""/testdir"");
      dfs.mkdir(testDir, FsPermission.getDefault());

      dfsCluster.getNameNodeRpc(0).rollEditLog();
      Thread.sleep(2 * 1000);

      // We need to get the metrics for the SBN (excluding the NN from dfs
      // cluster created in setUp() and the ANN).
      MetricsRecordBuilder rb = getMetrics(NN_METRICS+""-2"");
      assertQuantileGauges(""EditLogTailTime60s"", rb);
      assertQuantileGauges(""EditLogFetchTime60s"", rb);
      assertQuantileGauges(""NumEditLogLoaded60s"", rb, ""Count"");
      assertQuantileGauges(""EditLogTailInterval60s"", rb);
      assertCounterGt(""EditLogTailTimeNumOps"", 0L, rb);
      assertCounterGt(""EditLogFetchTimeNumOps"", 0L, rb);
      assertCounterGt(""NumEditLogLoadedNumOps"", 0L, rb);
      assertCounterGt(""EditLogTailIntervalNumOps"", 0L, rb);
    }",1
"@Test
  public void testMissingBlock() throws Exception {
    // Create a file with single block with two replicas
    Path file = getTestPath(""testMissingBlocks"");
    createFile(file, 100, (short)1);
    
    // Corrupt the only replica of the block to result in a missing block
    LocatedBlock block = NameNodeAdapter.getBlockLocations(
        cluster.getNameNode(), file.toString(), 0, 1).get(0);
    cluster.getNamesystem().writeLock();
    try {
      bm.findAndMarkBlockAsCorrupt(block.getBlock(), block.getLocations()[0],
          ""STORAGE_ID"", ""TEST"");
    }",1
"@Test
  public void testReadWriteOps() throws Exception {
    MetricsRecordBuilder rb = getMetrics(NN_METRICS);
    long startWriteCounter = MetricsAsserts.getLongCounter(""TransactionsNumOps"",
        rb);
    Path file1_Path = new Path(TEST_ROOT_DIR_PATH, ""ReadData.dat"");

    //Perform create file operation
    createFile(file1_Path, 1024, (short) 2);

    // Perform read file operation on earlier created file
    readFile(fs, file1_Path);
    MetricsRecordBuilder rbNew = getMetrics(NN_METRICS);
    assertTrue(MetricsAsserts.getLongCounter(""TransactionsNumOps"", rbNew) >
        startWriteCounter);
  }",1
"@Test
  public void testRemoveRandom() throws Exception {
    final int n = NUM_SNAPSHOTS;
    testRemove(""Random"", n, i -> ThreadLocalRandom.current().nextInt(n - i));
  }",1
"@Test
  public void testDiffReportWithRenameAndAppend() throws Exception {
    final Path root = new Path(""/"");
    final Path foo = new Path(root, ""foo"");
    DFSTestUtil.createFile(hdfs, foo, BLOCKSIZE, REPLICATION, SEED);

    SnapshotTestHelper.createSnapshot(hdfs, root, ""s0"");
    final Path bar = new Path(root, ""bar"");
    hdfs.rename(foo, bar);
    DFSTestUtil.appendFile(hdfs, bar, 10); // append 10 bytes
    SnapshotTestHelper.createSnapshot(hdfs, root, ""s1"");

    // we always put modification on the file before rename
    verifyDiffReport(root, ""s0"", ""s1"",
        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes("""")),
        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes(""foo"")),
        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(""foo""),
            DFSUtil.string2Bytes(""bar"")));
  }",1
"@Test
  public void testBackupNodeTailsEdits() throws Exception {
    Configuration conf = new HdfsConfiguration();
    HAUtil.setAllowStandbyReads(conf, true);
    MiniDFSCluster cluster = null;
    FileSystem fileSys = null;
    BackupNode backup = null;

    try {
      cluster = new MiniDFSCluster.Builder(conf)
                                  .numDataNodes(0).build();
      fileSys = cluster.getFileSystem();
      backup = startBackupNode(conf, StartupOption.BACKUP, 1);
      
      BackupImage bnImage = (BackupImage) backup.getFSImage();
      testBNInSync(cluster, backup, 1);
      
      // Force a roll -- BN should roll with NN.
      NameNode nn = cluster.getNameNode();
      NamenodeProtocols nnRpc = nn.getRpcServer();
      nnRpc.rollEditLog();
      assertEquals(bnImage.getEditLog().getCurSegmentTxId(),
          nn.getFSImage().getEditLog().getCurSegmentTxId());
      
      // BN should stay in sync after roll
      testBNInSync(cluster, backup, 2);
      
      long nnImageBefore =
        nn.getFSImage().getStorage().getMostRecentCheckpointTxId();
      // BN checkpoint
      backup.doCheckpoint();
      
      // NN should have received a new image
      long nnImageAfter =
        nn.getFSImage().getStorage().getMostRecentCheckpointTxId();
      
      assertTrue(""nn should have received new checkpoint. before: "" +
          nnImageBefore + "" after: "" + nnImageAfter,
          nnImageAfter > nnImageBefore);

      // BN should stay in sync after checkpoint
      testBNInSync(cluster, backup, 3);

      // Stop BN
      StorageDirectory sd = bnImage.getStorage().getStorageDir(0);
      backup.stop();
      backup = null;
      
      // When shutting down the BN, it shouldn't finalize logs that are
      // still open on the NN
      EditLogFile editsLog = FSImageTestUtil.findLatestEditsLog(sd);
      assertEquals(editsLog.getFirstTxId(),
          nn.getFSImage().getEditLog().getCurSegmentTxId());
      assertTrue(""Should not have finalized "" + editsLog,
          editsLog.isInProgress());
      
      // do some edits
      assertTrue(fileSys.mkdirs(new Path(""/edit-while-bn-down"")));
  
      // start a new backup node
      backup = startBackupNode(conf, StartupOption.BACKUP, 1);

      testBNInSync(cluster, backup, 4);
      assertNotNull(backup.getNamesystem()
          .getFileInfo(""/edit-while-bn-down"", false, false, false));
      
      // Trigger an unclean shutdown of the backup node. Backup node will not
      // unregister from the active when this is done simulating a node crash.
      backup.stop(false);
           
      // do some edits on the active. This should go through without failing.
      // This will verify that active is still up and can add entries to
      // master editlog.
      assertTrue(fileSys.mkdirs(new Path(""/edit-while-bn-down-2"")));
      
    }",1
"@Test
  public void testChooseTarget() throws Exception {
    doTestChooseTargetNormalCase();
    doTestChooseTargetSpecialCase();
  }",1
"@Test
  public void testAskForTransactionsMidfile() throws IOException {
    File f = new File(TestEditLog.TEST_DIR + ""/askfortransactionsmidfile"");
    NNStorage storage = setupEdits(Collections.<URI>singletonList(f.toURI()), 
                                   10);
    StorageDirectory sd = storage.dirIterator(NameNodeDirType.EDITS).next();
    
    FileJournalManager jm = new FileJournalManager(conf, sd, storage);
    
    // 10 rolls, so 11 rolled files, 110 txids total.
    final int TOTAL_TXIDS = 10 * 11;
    for (int txid = 1; txid <= TOTAL_TXIDS; txid++) {
      assertEquals((TOTAL_TXIDS - txid) + 1, getNumberOfTransactions(jm, txid,
          true, false));
    }",1
"@Test
  public void testInprogressRecoveryMixed() throws IOException {
    File f1 = new File(TestEditLog.TEST_DIR + ""/mixtest0"");
    File f2 = new File(TestEditLog.TEST_DIR + ""/mixtest1"");
    File f3 = new File(TestEditLog.TEST_DIR + ""/mixtest2"");
    
    List<URI> editUris = ImmutableList.of(f1.toURI(), f2.toURI(), f3.toURI());

    // abort after the 5th roll 
    NNStorage storage = setupEdits(editUris,
                                   5, new AbortSpec(5, 1));
    Iterator<StorageDirectory> dirs = storage.dirIterator(NameNodeDirType.EDITS);
    StorageDirectory sd = dirs.next();
    FileJournalManager jm = new FileJournalManager(conf, sd, storage);
    assertEquals(6*TXNS_PER_ROLL, getNumberOfTransactions(jm, 1, true, false));
    
    sd = dirs.next();
    jm = new FileJournalManager(conf, sd, storage);
    assertEquals(5*TXNS_PER_ROLL + TXNS_PER_FAIL, getNumberOfTransactions(jm, 1,
        true, false));

    sd = dirs.next();
    jm = new FileJournalManager(conf, sd, storage);
    assertEquals(6*TXNS_PER_ROLL, getNumberOfTransactions(jm, 1, true, false));
  }",1
"@Test
  public void testDisplayRecentEditLogOpCodes() throws IOException {
    // start a cluster
    Configuration conf = getConf();
    MiniDFSCluster cluster = null;
    FileSystem fileSys = null;
    cluster = new MiniDFSCluster.Builder(conf).numDataNodes(NUM_DATA_NODES)
        .enableManagedDfsDirsRedundancy(false).build();
    cluster.waitActive();
    fileSys = cluster.getFileSystem();
    final FSNamesystem namesystem = cluster.getNamesystem();

    FSImage fsimage = namesystem.getFSImage();
    for (int i = 0; i < 20; i++) {
      fileSys.mkdirs(new Path(""/tmp/tmp"" + i));
    }",1
"@Test
  public void testValidateEditLogWithCorruptBody() throws IOException {
    File testDir = new File(TEST_DIR, ""testValidateEditLogWithCorruptBody"");
    SortedMap<Long, Long> offsetToTxId = Maps.newTreeMap();
    final int NUM_TXNS = 20;
    File logFile = prepareUnfinalizedTestEditLog(testDir, NUM_TXNS,
        offsetToTxId);
    // Back up the uncorrupted log
    File logFileBak = new File(testDir, logFile.getName() + "".bak"");
    Files.copy(logFile, logFileBak);
    EditLogValidation validation =
        EditLogFileInputStream.scanEditLog(logFile, Long.MAX_VALUE, true);
    assertTrue(!validation.hasCorruptHeader());
    // We expect that there will be an OP_START_LOG_SEGMENT, followed by
    // NUM_TXNS opcodes, followed by an OP_END_LOG_SEGMENT.
    assertEquals(NUM_TXNS + 1, validation.getEndTxId());
    // Corrupt each edit and verify that validation continues to work
    for (Map.Entry<Long, Long> entry : offsetToTxId.entrySet()) {
      long txOffset = entry.getKey();
      long txId = entry.getValue();

      // Restore backup, corrupt the txn opcode
      Files.copy(logFileBak, logFile);
      corruptByteInFile(logFile, txOffset);
      validation = EditLogFileInputStream.scanEditLog(logFile,
          Long.MAX_VALUE, true);
      long expectedEndTxId = (txId == (NUM_TXNS + 1)) ?
          NUM_TXNS : (NUM_TXNS + 1);
      assertEquals(""Failed when corrupting txn opcode at "" + txOffset,
          expectedEndTxId, validation.getEndTxId());
      assertTrue(!validation.hasCorruptHeader());
    }",1
"@Test
  public void testValidateEmptyEditLog() throws IOException {
    File testDir = new File(TEST_DIR, ""testValidateEmptyEditLog"");
    SortedMap<Long, Long> offsetToTxId = Maps.newTreeMap();
    File logFile = prepareUnfinalizedTestEditLog(testDir, 0, offsetToTxId);
    // Truncate the file so that there is nothing except the header and
    // layout flags section.
    truncateFile(logFile, 8);
    EditLogValidation validation =
        EditLogFileInputStream.scanEditLog(logFile, Long.MAX_VALUE, true);
    assertTrue(!validation.hasCorruptHeader());
    assertEquals(HdfsServerConstants.INVALID_TXID, validation.getEndTxId());
  }",1
"@Test
  public void testDigest() throws IOException {
    Configuration conf = new Configuration();
    MiniDFSCluster cluster = null;
    try {
      cluster = new MiniDFSCluster.Builder(conf).numDataNodes(0).build();
      DistributedFileSystem fs = cluster.getFileSystem();
      fs.setSafeMode(SafeModeAction.ENTER);
      fs.saveNamespace();
      fs.setSafeMode(SafeModeAction.LEAVE);
      File currentDir = FSImageTestUtil.getNameNodeCurrentDirs(cluster, 0).get(
          0);
      File fsimage = FSImageTestUtil.findNewestImageFile(currentDir
          .getAbsolutePath());
      assertEquals(MD5FileUtils.readStoredMd5ForFile(fsimage),
          MD5FileUtils.computeMd5ForFile(fsimage));
    }",1
"@Test
  public void testHasNonEcBlockUsingStripedIDForLoadSnapshot()
      throws IOException{
    // start a cluster
    Configuration conf = new HdfsConfiguration();
    MiniDFSCluster cluster = null;
    try {
      cluster = new MiniDFSCluster.Builder(conf).numDataNodes(9)
          .build();
      cluster.waitActive();
      DistributedFileSystem fs = cluster.getFileSystem();
      FSNamesystem fns = cluster.getNamesystem();

      String testDir = ""/test_block_manager"";
      String testFile = ""testfile_loadSnapshot"";
      String testFilePath = testDir + ""/"" + testFile;
      String clientName = ""testUser_loadSnapshot"";
      String clientMachine = ""testMachine_loadSnapshot"";
      long blkId = -1;
      long blkNumBytes = 1024;
      long timestamp = 1426222918;

      Path d = new Path(testDir);
      fs.mkdir(d, new FsPermission(""755""));
      fs.allowSnapshot(d);

      Path p = new Path(testFilePath);
      DFSTestUtil.createFile(fs, p, 0, (short) 1, 1);
      BlockInfoContiguous cBlk = new BlockInfoContiguous(
          new Block(blkId, blkNumBytes, timestamp), (short)3);
      INodeFile file = (INodeFile)fns.getFSDirectory().getINode(testFilePath);
      file.toUnderConstruction(clientName, clientMachine);
      file.addBlock(cBlk);
      TestINodeFile.toCompleteFile(file);

      fs.createSnapshot(d,""testHasNonEcBlockUsingStripeID"");
      fs.truncate(p,0);
      fns.enterSafeMode(false);
      fns.saveNamespace(0, 0);
      cluster.restartNameNodes();
      cluster.waitActive();
      fns = cluster.getNamesystem();
      assertTrue(fns.getBlockManager().hasNonEcBlockUsingStripedID());

      cluster.shutdown();
      cluster = null;
    }",1
"@Test
  public void testPersist() throws IOException {
    Configuration conf = new Configuration();
    testPersistHelper(conf);
  }",1
"@Test
  public void testSaveAndLoadErasureCodingPolicies() throws IOException{
    Configuration conf = new Configuration();
    final int blockSize = 16 * 1024 * 1024;
    conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, blockSize);
    try (MiniDFSCluster cluster =
             new MiniDFSCluster.Builder(conf).numDataNodes(10).build()) {
      cluster.waitActive();
      DistributedFileSystem fs = cluster.getFileSystem();
      DFSTestUtil.enableAllECPolicies(fs);

      // Save namespace and restart NameNode
      fs.setSafeMode(SafeModeAction.ENTER);
      fs.saveNamespace();
      fs.setSafeMode(SafeModeAction.LEAVE);

      cluster.restartNameNodes();
      cluster.waitActive();

      assertEquals(""Erasure coding policy number should match"",
          SystemErasureCodingPolicies.getPolicies().size(),
          ErasureCodingPolicyManager.getInstance().getPolicies().length);

      // Add new erasure coding policy
      ECSchema newSchema = new ECSchema(""rs"", 5, 4);
      ErasureCodingPolicy newPolicy =
          new ErasureCodingPolicy(newSchema, 2 * 1024, (byte) 254);
      ErasureCodingPolicy[] policies = new ErasureCodingPolicy[]{newPolicy}",1
"@Test
  public void testSaveAndLoadFileUnderReplicationPolicyDir()
      throws IOException {
    Configuration conf = new Configuration();
    MiniDFSCluster cluster = null;
    try {
      cluster = new MiniDFSCluster.Builder(conf).build();
      cluster.waitActive();
      FSNamesystem fsn = cluster.getNamesystem();
      DistributedFileSystem fs = cluster.getFileSystem();
      DFSTestUtil.enableAllECPolicies(fs);
      ErasureCodingPolicy replicaPolicy =
          SystemErasureCodingPolicies.getReplicationPolicy();
      ErasureCodingPolicy defaultEcPolicy =
          StripedFileTestUtil.getDefaultECPolicy();

      final Path ecDir = new Path(""/ec"");
      final Path replicaDir = new Path(ecDir, ""replica"");
      final Path replicaFile1 = new Path(replicaDir, ""f1"");
      final Path replicaFile2 = new Path(replicaDir, ""f2"");

      // create root directory
      fs.mkdir(ecDir, null);
      fs.setErasureCodingPolicy(ecDir, defaultEcPolicy.getName());

      // create directory, and set replication Policy
      fs.mkdir(replicaDir, null);
      fs.setErasureCodingPolicy(replicaDir, replicaPolicy.getName());

      // create an empty file f1
      fs.create(replicaFile1).close();

      // create an under-construction file f2
      FSDataOutputStream out = fs.create(replicaFile2, (short) 2);
      out.writeBytes(""hello"");
      ((DFSOutputStream) out.getWrappedStream()).hsync(EnumSet
          .of(SyncFlag.UPDATE_LENGTH));

      // checkpoint
      fs.setSafeMode(SafeModeAction.ENTER);
      fs.saveNamespace();
      fs.setSafeMode(SafeModeAction.LEAVE);

      cluster.restartNameNode();
      cluster.waitActive();
      fs = cluster.getFileSystem();

      assertTrue(fs.getFileStatus(ecDir).isDirectory());
      assertTrue(fs.getFileStatus(replicaDir).isDirectory());
      assertTrue(fs.exists(replicaFile1));
      assertTrue(fs.exists(replicaFile2));

      // check directories
      assertEquals(""Directory should have default EC policy."",
          defaultEcPolicy, fs.getErasureCodingPolicy(ecDir));
      assertEquals(""Directory should hide replication EC policy."",
          null, fs.getErasureCodingPolicy(replicaDir));

      // check file1
      assertEquals(""File should not have EC policy."", null,
          fs.getErasureCodingPolicy(replicaFile1));
      // check internals of file2
      INodeFile file2Node =
          fsn.dir.getINode4Write(replicaFile2.toString()).asFile();
      assertEquals(""hello"".length(), file2Node.computeFileSize());
      assertTrue(file2Node.isUnderConstruction());
      BlockInfo[] blks = file2Node.getBlocks();
      assertEquals(1, blks.length);
      assertEquals(BlockUCState.UNDER_CONSTRUCTION, blks[0].getBlockUCState());
      assertEquals(""File should return expected replication factor."",
          2, blks[0].getReplication());
      assertEquals(""File should not have EC policy."", null,
          fs.getErasureCodingPolicy(replicaFile2));
      // check lease manager
      Lease lease = fsn.leaseManager.getLease(file2Node);
      Assert.assertNotNull(lease);
    }",1
"@Test
  public void testAclGroupDeny() throws IOException {
    INodeFile inodeFile = createINodeFile(inodeRoot, ""file1"", ""bruce"", ""sales"",
      (short)0604);
    addAcl(inodeFile,
      aclEntry(ACCESS, USER, READ_WRITE),
      aclEntry(ACCESS, GROUP, NONE),
      aclEntry(ACCESS, MASK, NONE),
      aclEntry(ACCESS, OTHER, READ));
    assertPermissionGranted(BRUCE, ""/file1"", READ_WRITE);
    assertPermissionGranted(CLARK, ""/file1"", READ);
    assertPermissionDenied(DIANA, ""/file1"", READ);
    assertPermissionDenied(DIANA, ""/file1"", WRITE);
    assertPermissionDenied(DIANA, ""/file1"", EXECUTE);
    assertPermissionDenied(DIANA, ""/file1"", READ_WRITE);
    assertPermissionDenied(DIANA, ""/file1"", READ_EXECUTE);
    assertPermissionDenied(DIANA, ""/file1"", WRITE_EXECUTE);
    assertPermissionDenied(DIANA, ""/file1"", ALL);
  }",1
"@Test
  public void testAclGroupMask() throws IOException {
    INodeFile inodeFile = createINodeFile(inodeRoot, ""file1"", ""bruce"", ""execs"",
      (short)0644);
    addAcl(inodeFile,
      aclEntry(ACCESS, USER, READ_WRITE),
      aclEntry(ACCESS, GROUP, READ_WRITE),
      aclEntry(ACCESS, MASK, READ),
      aclEntry(ACCESS, OTHER, READ));
    assertPermissionGranted(BRUCE, ""/file1"", READ_WRITE);
    assertPermissionGranted(CLARK, ""/file1"", READ);
    assertPermissionDenied(CLARK, ""/file1"", WRITE);
    assertPermissionDenied(CLARK, ""/file1"", EXECUTE);
    assertPermissionDenied(CLARK, ""/file1"", READ_WRITE);
    assertPermissionDenied(CLARK, ""/file1"", READ_EXECUTE);
    assertPermissionDenied(CLARK, ""/file1"", WRITE_EXECUTE);
    assertPermissionDenied(CLARK, ""/file1"", ALL);
  }",1
"@Test
  public void testAclNamedGroup() throws IOException {
    INodeFile inodeFile = createINodeFile(inodeRoot, ""file1"", ""bruce"", ""execs"",
      (short)0640);
    addAcl(inodeFile,
      aclEntry(ACCESS, USER, READ_WRITE),
      aclEntry(ACCESS, GROUP, READ),
      aclEntry(ACCESS, GROUP, ""sales"", READ),
      aclEntry(ACCESS, MASK, READ),
      aclEntry(ACCESS, OTHER, NONE));
    assertPermissionGranted(BRUCE, ""/file1"", READ_WRITE);
    assertPermissionGranted(CLARK, ""/file1"", READ);
    assertPermissionGranted(DIANA, ""/file1"", READ);
    assertPermissionDenied(DIANA, ""/file1"", WRITE);
    assertPermissionDenied(DIANA, ""/file1"", EXECUTE);
    assertPermissionDenied(DIANA, ""/file1"", READ_WRITE);
    assertPermissionDenied(DIANA, ""/file1"", READ_EXECUTE);
    assertPermissionDenied(DIANA, ""/file1"", ALL);
  }",1
"@Test
  public void testAclNamedGroupDeny() throws IOException {
    INodeFile inodeFile = createINodeFile(inodeRoot, ""file1"", ""bruce"", ""sales"",
      (short)0644);
    addAcl(inodeFile,
      aclEntry(ACCESS, USER, READ_WRITE),
      aclEntry(ACCESS, GROUP, READ),
      aclEntry(ACCESS, GROUP, ""execs"", NONE),
      aclEntry(ACCESS, MASK, READ),
      aclEntry(ACCESS, OTHER, READ));
    assertPermissionGranted(BRUCE, ""/file1"", READ_WRITE);
    assertPermissionGranted(DIANA, ""/file1"", READ);
    assertPermissionDenied(CLARK, ""/file1"", READ);
    assertPermissionDenied(CLARK, ""/file1"", WRITE);
    assertPermissionDenied(CLARK, ""/file1"", EXECUTE);
    assertPermissionDenied(CLARK, ""/file1"", READ_WRITE);
    assertPermissionDenied(CLARK, ""/file1"", READ_EXECUTE);
    assertPermissionDenied(CLARK, ""/file1"", WRITE_EXECUTE);
    assertPermissionDenied(CLARK, ""/file1"", ALL);
  }",1
"@Test
  public void testAclNamedGroupTraverseDeny() throws IOException {
    INodeDirectory inodeDir = createINodeDirectory(inodeRoot, ""dir1"", ""bruce"",
      ""execs"", (short)0755);
    INodeFile inodeFile = createINodeFile(inodeDir, ""file1"", ""bruce"", ""execs"",
      (short)0644);
    addAcl(inodeDir,
      aclEntry(ACCESS, USER, ALL),
      aclEntry(ACCESS, GROUP, READ_EXECUTE),
      aclEntry(ACCESS, GROUP, ""sales"", NONE),
      aclEntry(ACCESS, MASK, READ_EXECUTE),
      aclEntry(ACCESS, OTHER, READ_EXECUTE));
    assertPermissionGranted(BRUCE, ""/dir1/file1"", READ_WRITE);
    assertPermissionGranted(CLARK, ""/dir1/file1"", READ);
    assertPermissionDenied(DIANA, ""/dir1/file1"", READ);
    assertPermissionDenied(DIANA, ""/dir1/file1"", WRITE);
    assertPermissionDenied(DIANA, ""/dir1/file1"", EXECUTE);
    assertPermissionDenied(DIANA, ""/dir1/file1"", READ_WRITE);
    assertPermissionDenied(DIANA, ""/dir1/file1"", READ_EXECUTE);
    assertPermissionDenied(DIANA, ""/dir1/file1"", WRITE_EXECUTE);
    assertPermissionDenied(DIANA, ""/dir1/file1"", ALL);
  }",1
"@Test
  public void testAclOther() throws IOException {
    INodeFile inodeFile = createINodeFile(inodeRoot, ""file1"", ""bruce"", ""sales"",
      (short)0774);
    addAcl(inodeFile,
      aclEntry(ACCESS, USER, ALL),
      aclEntry(ACCESS, USER, ""diana"", ALL),
      aclEntry(ACCESS, GROUP, READ_WRITE),
      aclEntry(ACCESS, MASK, ALL),
      aclEntry(ACCESS, OTHER, READ));
    assertPermissionGranted(BRUCE, ""/file1"", ALL);
    assertPermissionGranted(DIANA, ""/file1"", ALL);
    assertPermissionGranted(CLARK, ""/file1"", READ);
    assertPermissionDenied(CLARK, ""/file1"", WRITE);
    assertPermissionDenied(CLARK, ""/file1"", EXECUTE);
    assertPermissionDenied(CLARK, ""/file1"", READ_WRITE);
    assertPermissionDenied(CLARK, ""/file1"", READ_EXECUTE);
    assertPermissionDenied(CLARK, ""/file1"", WRITE_EXECUTE);
    assertPermissionDenied(CLARK, ""/file1"", ALL);
  }",1
"@Test
  public void testConcatRelativeTargetPath() throws IOException {
    Path dir = new Path(""/dir"");
    Path trg = new Path(""trg"");
    Path src = new Path(dir, ""src"");
    dfs.setWorkingDirectory(dir);
    DFSTestUtil.createFile(dfs, trg, blockSize, REPL_FACTOR, 1);
    DFSTestUtil.createFile(dfs, src, blockSize, REPL_FACTOR, 1);
    dfs.concat(trg, new Path[]{src}",1
"@Test
  public void testIllegalArg() throws IOException {
    long fileLen = blockSize*3;
    
    Path parentDir  = new Path (""/parentTrg"");
    assertTrue(dfs.mkdirs(parentDir));
    Path trg = new Path(parentDir, ""trg"");
    DFSTestUtil.createFile(dfs, trg, fileLen, REPL_FACTOR, 1);

    // must be in the same dir
    {
      // create first file
      Path dir1 = new Path (""/dir1"");
      assertTrue(dfs.mkdirs(dir1));
      Path src = new Path(dir1, ""src"");
      DFSTestUtil.createFile(dfs, src, fileLen, REPL_FACTOR, 1);
      
      try {
        dfs.concat(trg, new Path [] {src}",1
"@Test
  public void testAuthzDelegationToProvider() throws Exception {
    LOG.info(""Test not bypassing provider"");
    String[] users = {""u1""}",1
"@Test
  public void testDelegationToProvider() throws Exception {
    Assert.assertTrue(CALLED.contains(""start""));
    FileSystem fs = FileSystem.get(miniDFS.getConfiguration(0));
    final Path tmpPath = new Path(""/tmp"");
    final Path fooPath = new Path(""/tmp/foo"");

    fs.mkdirs(tmpPath);
    fs.setPermission(tmpPath, new FsPermission(HDFS_PERMISSION));
    UserGroupInformation ugi = UserGroupInformation.createUserForTesting(""u1"",
        new String[]{""g1""}",1
"@Test
  public void testDotdotInodePath() throws Exception {
    final Configuration conf = new Configuration();
    MiniDFSCluster cluster = null;
    DFSClient client = null;
    try {
      cluster = new MiniDFSCluster.Builder(conf).numDataNodes(1).build();
      cluster.waitActive();
      final DistributedFileSystem hdfs = cluster.getFileSystem();
      final FSDirectory fsdir = cluster.getNamesystem().getFSDirectory();

      final Path dir = new Path(""/dir"");
      hdfs.mkdirs(dir);
      long dirId = fsdir.getINode(dir.toString()).getId();
      long parentId = fsdir.getINode(""/"").getId();
      String testPath = ""/.reserved/.inodes/"" + dirId + ""/.."";

      client = new DFSClient(DFSUtilClient.getNNAddress(conf), conf);
      HdfsFileStatus status = client.getFileInfo(testPath);
      assertTrue(parentId == status.getFileId());
      
      // Test root's parent is still root
      testPath = ""/.reserved/.inodes/"" + parentId + ""/.."";
      status = client.getFileInfo(testPath);
      assertTrue(parentId == status.getFileId());
      
    }",1
"@Test
  public void testInodePath() throws IOException {
    // For a non .inodes path the regular components are returned
    String path = ""/a/b/c"";
    INode inode = createTreeOfInodes(path);
    // For an any inode look up return inode corresponding to ""c"" from /a/b/c
    FSDirectory fsd = Mockito.mock(FSDirectory.class);
    Mockito.doReturn(inode).when(fsd).getInode(Mockito.anyLong());

    // Tests for FSDirectory#resolvePath()
    // Non inode regular path
    String resolvedPath = FSDirectory.resolvePath(path, fsd);
    assertEquals(path, resolvedPath);

    // Inode path with no trailing separator
    String testPath = ""/.reserved/.inodes/1"";
    resolvedPath = FSDirectory.resolvePath(testPath, fsd);
    assertEquals(path, resolvedPath);

    // Inode path with trailing separator
    testPath = ""/.reserved/.inodes/1/"";
    resolvedPath = FSDirectory.resolvePath(testPath, fsd);
    assertEquals(path, resolvedPath);

    // Inode relative path
    testPath = ""/.reserved/.inodes/1/d/e/f"";
    resolvedPath = FSDirectory.resolvePath(testPath, fsd);
    assertEquals(""/a/b/c/d/e/f"", resolvedPath);

    // A path with just .inodes  returns the path as is
    testPath = ""/.reserved/.inodes"";
    resolvedPath = FSDirectory.resolvePath(testPath, fsd);
    assertEquals(testPath, resolvedPath);

    // Root inode path
    testPath = ""/.reserved/.inodes/"" + INodeId.ROOT_INODE_ID;
    resolvedPath = FSDirectory.resolvePath(testPath, fsd);
    assertEquals(""/"", resolvedPath);

    // An invalid inode path should remain unresolved
    testPath = ""/.invalid/.inodes/1"";
    resolvedPath = FSDirectory.resolvePath(testPath, fsd);
    assertEquals(testPath, resolvedPath);

    // Test path with nonexistent(deleted or wrong id) inode
    Mockito.doReturn(null).when(fsd).getInode(Mockito.anyLong());
    testPath = ""/.reserved/.inodes/1234"";
    try {
      String realPath = FSDirectory.resolvePath(testPath, fsd);
      fail(""Path should not be resolved:"" + realPath);
    }",1
"@Test
  public void testLocationLimitInListingOps() throws Exception {
    final Configuration conf = new Configuration();
    conf.setInt(DFSConfigKeys.DFS_LIST_LIMIT, 9); // 3 blocks * 3 replicas
    MiniDFSCluster cluster = null;
    try {
      cluster = new MiniDFSCluster.Builder(conf).numDataNodes(3).build();
      cluster.waitActive();
      final DistributedFileSystem hdfs = cluster.getFileSystem();
      ArrayList<String> source = new ArrayList<String>();

      // tmp1 holds files with 3 blocks, 3 replicas
      // tmp2 holds files with 3 blocks, 1 replica
      hdfs.mkdirs(new Path(""/tmp1""));
      hdfs.mkdirs(new Path(""/tmp2""));

      source.add(""f1"");
      source.add(""f2"");

      int numEntries = source.size();
      for (int j=0;j<numEntries;j++) {
          DFSTestUtil.createFile(hdfs, new Path(""/tmp1/""+source.get(j)), 4096,
          3*1024-100, 1024, (short) 3, 0);
      }",1
"@Test
  public void testLastContactTime() throws Exception {
    Configuration conf = new Configuration();
    conf.setInt(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY, 1);
    conf.setInt(DFSConfigKeys.DFS_NAMENODE_HEARTBEAT_RECHECK_INTERVAL_KEY, 1);
    MiniDFSCluster cluster = null;
    HostsFileWriter hostsFileWriter = new HostsFileWriter();
    hostsFileWriter.initialize(conf, ""temp/TestNameNodeMXBean"");

    try {
      cluster = new MiniDFSCluster.Builder(conf, baseDir.getRoot()).numDataNodes(3).build();
      cluster.waitActive();

      FSNamesystem fsn = cluster.getNameNode().namesystem;

      MBeanServer mbs = ManagementFactory.getPlatformMBeanServer();
      ObjectName mxbeanName = new ObjectName(
        ""Hadoop:service=NameNode,name=NameNodeInfo"");

      List<String> hosts = new ArrayList<>();
      for(DataNode dn : cluster.getDataNodes()) {
        hosts.add(dn.getDisplayName());
      }",1
"@Test
  public void testOldInProgress() throws IOException {
    TestCaseDescription tc = new TestCaseDescription();
    tc.addRoot(""/foo1"", NameNodeDirType.IMAGE_AND_EDITS);
    tc.addImage(""/foo1/current/"" + getImageFileName(100), true);
    tc.addImage(""/foo1/current/"" + getImageFileName(200), true);
    tc.addImage(""/foo1/current/"" + getImageFileName(300), false);
    tc.addImage(""/foo1/current/"" + getImageFileName(400), false);
    tc.addLog(""/foo1/current/"" + getInProgressEditsFileName(101), true);
    runTest(tc);
  }",1
"@Test
  public void testNNThroughput() throws Exception {
    Configuration conf = new HdfsConfiguration();
    conf.setInt(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, 16);
    File nameDir = new File(MiniDFSCluster.getBaseDirectory(), ""name"");
    conf.set(DFSConfigKeys.DFS_NAMENODE_NAME_DIR_KEY,
        nameDir.getAbsolutePath());
    DFSTestUtil.formatNameNode(conf);
    NNThroughputBenchmark.runBenchmark(conf, new String[] {""-op"", ""all""}",1
"@Test
  public void testClientSideExceptionOnJustOneDir() throws IOException {
    Configuration conf = new HdfsConfiguration();
    MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf)
      .numDataNodes(0).build();
    NNStorage mockStorage = Mockito.mock(NNStorage.class);
    List<File> localPaths = ImmutableList.of(
        new File(""/xxxxx-does-not-exist/blah""),
        new File(TEST_DIR, ""testfile"")    
        );
       
    try {
      URL fsName = DFSUtil.getInfoServer(
          cluster.getNameNode().getServiceRpcAddress(), conf,
          DFSUtil.getHttpClientScheme(conf)).toURL();

      String id = ""getimage=1&txid=0"";

      TransferFsImage.getFileClient(fsName, id, localPaths, mockStorage, false);      
      Mockito.verify(mockStorage).reportErrorOnFile(localPaths.get(0));
      assertTrue(""The valid local file should get saved properly"",
          localPaths.get(1).length() > 0);
    }",1
"@Test
  public void testChooseTargetWithTopology() throws Exception {
    BlockStoragePolicy policy1 = new BlockStoragePolicy((byte) 9, ""TEST1"",
        new StorageType[]{StorageType.SSD, StorageType.DISK,
            StorageType.ARCHIVE}",1
"@Test  
  public void testDataTransferProtocol() throws IOException {
    Random random = new Random();
    int oneMil = 1024*1024;
    Path file = new Path(""dataprotocol.dat"");
    int numDataNodes = 1;
    
    Configuration conf = new HdfsConfiguration();
    conf.setInt(DFSConfigKeys.DFS_REPLICATION_KEY, numDataNodes); 
    MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).numDataNodes(numDataNodes).build();
    try {
    cluster.waitActive();
    datanode = cluster.getFileSystem().getDataNodeStats(DatanodeReportType.LIVE)[0];
    dnAddr = NetUtils.createSocketAddr(datanode.getXferAddr());
    FileSystem fileSys = cluster.getFileSystem();
    
    int fileLen = Math.min(conf.getInt(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, 4096), 4096);
    
      DFSTestUtil.createFile(fileSys, file, fileLen, fileLen,
          fileSys.getDefaultBlockSize(file),
          fileSys.getDefaultReplication(file), 0L);

    // get the first blockid for the file
    final ExtendedBlock firstBlock = DFSTestUtil.getFirstBlock(fileSys, file);
    final String poolId = firstBlock.getBlockPoolId();
    long newBlockId = firstBlock.getBlockId() + 1;

    recvBuf.reset();
    sendBuf.reset();
    
    // bad version
    recvOut.writeShort((short)(DataTransferProtocol.DATA_TRANSFER_VERSION-1));
    sendOut.writeShort((short)(DataTransferProtocol.DATA_TRANSFER_VERSION-1));
    sendRecvData(""Wrong Version"", true);

    // bad ops
    sendBuf.reset();
    sendOut.writeShort((short)DataTransferProtocol.DATA_TRANSFER_VERSION);
    sendOut.writeByte(Op.WRITE_BLOCK.code - 1);
    sendRecvData(""Wrong Op Code"", true);
    
    /* Test OP_WRITE_BLOCK */
    sendBuf.reset();
    
    DataChecksum badChecksum = Mockito.spy(DEFAULT_CHECKSUM);
    Mockito.doReturn(-1).when(badChecksum).getBytesPerChecksum();

    writeBlock(poolId, newBlockId, badChecksum);
    recvBuf.reset();
    sendResponse(Status.ERROR, null, null, recvOut);
    sendRecvData(""wrong bytesPerChecksum while writing"", true);

    sendBuf.reset();
    recvBuf.reset();
    writeBlock(poolId, ++newBlockId, DEFAULT_CHECKSUM);

    PacketHeader hdr = new PacketHeader(
      4,     // size of packet
      0,     // offset in block,
      100,   // seqno
      false, // last packet
      -1 - random.nextInt(oneMil), // bad datalen
      false);
    hdr.write(sendOut);

    sendResponse(Status.SUCCESS, """", null, recvOut);
    new PipelineAck(100, new int[] {PipelineAck.combineHeader
      (PipelineAck.ECN.DISABLED, Status.ERROR)}",1
"@Test
  public void testIdempotentClose() throws Exception {
    final int numBlocks = 2;
    DFSTestUtil.createStripedFile(cluster, filePath, null, numBlocks,
        stripesPerBlock, false, ecPolicy);

    try (DFSInputStream in = fs.getClient().open(filePath.toString())) {
      assertTrue(in instanceof DFSStripedInputStream);
      // Close twice
      in.close();
    }",1
"@Test
  public void testRefreshBlock() throws Exception {
    final int numBlocks = 4;
    DFSTestUtil.createStripedFile(cluster, filePath, null, numBlocks,
        stripesPerBlock, false, ecPolicy);
    LocatedBlocks lbs = fs.getClient().namenode.getBlockLocations(
        filePath.toString(), 0, blockGroupSize * numBlocks);
    final DFSStripedInputStream in = new DFSStripedInputStream(fs.getClient(),
        filePath.toString(), false, ecPolicy, null);

    List<LocatedBlock> lbList = lbs.getLocatedBlocks();
    for (LocatedBlock aLbList : lbList) {
      LocatedStripedBlock lsb = (LocatedStripedBlock) aLbList;
      LocatedBlock[] blks = StripedBlockUtil.parseStripedBlockGroup(lsb,
          cellSize, dataBlocks, parityBlocks);
      for (int j = 0; j < dataBlocks; j++) {
        LocatedBlock refreshed = in.refreshLocatedBlock(blks[j]);
        assertEquals(blks[j].getBlock(), refreshed.getBlock());
        assertEquals(blks[j].getStartOffset(), refreshed.getStartOffset());
        assertArrayEquals(blks[j].getLocations(), refreshed.getLocations());
      }",1
"@Test
  public void testFileMoreThanABlockGroup3() throws Exception {
    testOneFile(""/MoreThanABlockGroup3"",
        blockSize * dataBlocks * 3 + cellSize * dataBlocks
        + cellSize + 123);
  }",1
"@Test
  public void testFileMoreThanOneStripe1() throws Exception {
    testOneFile(""/MoreThanOneStripe1"", cellSize * dataBlocks + 123);
  }",1
"@Test
  public void testFileSmallerThanOneCell1() throws Exception {
    testOneFile(""/SmallerThanOneCell"", 1);
  }",1
"@Test
  public void testFileSmallerThanOneCell2() throws Exception {
    testOneFile(""/SmallerThanOneCell"", cellSize - 1);
  }",1
"@Test
  public void testFileSmallerThanOneStripe2() throws Exception {
    testOneFile(""/SmallerThanOneStripe"", cellSize + 123);
  }",1
"@Test
  public void testStreamFlush() throws Exception {
    final byte[] bytes = StripedFileTestUtil.generateBytes(blockSize *
        dataBlocks * 3 + cellSize * dataBlocks + cellSize + 123);
    try (FSDataOutputStream os = fs.create(new Path(""/ec-file-1""))) {
      assertFalse(
          ""DFSStripedOutputStream should not have hflush() capability yet!"",
          os.hasCapability(StreamCapability.HFLUSH.getValue()));
      assertFalse(
          ""DFSStripedOutputStream should not have hsync() capability yet!"",
          os.hasCapability(StreamCapability.HSYNC.getValue()));
      try (InputStream is = new ByteArrayInputStream(bytes)) {
        IOUtils.copyBytes(is, os, bytes.length);
        os.hflush();
        IOUtils.copyBytes(is, os, bytes.length);
        os.hsync();
        IOUtils.copyBytes(is, os, bytes.length);
      }",1
"@Test
  public void testDFSClient() throws Exception {
    Configuration conf = getTestConfiguration();
    final long grace = 1000L;
    MiniDFSCluster cluster = null;
    LeaseRenewer.setLeaseRenewerGraceDefault(grace);

    try {
      cluster = new MiniDFSCluster.Builder(conf).numDataNodes(2).build();
      final String filepathstring = ""/test/LeaseChecker/foo"";
      final Path[] filepaths = new Path[4];
      for(int i = 0; i < filepaths.length; i++) {
        filepaths[i] = new Path(filepathstring + i);
      }",1
"@Test
  public void testInvalidScriptMappingFileReadStatistics() throws Exception {
    // Even though network location of the client machine is unknown,
    // MiniDFSCluster's datanode is on the local host and thus the network
    // distance is 0.
    testReadFileSystemStatistics(0, true, true);
  }",1
"@Test
  public void testListStatusOfSnapshotDirs() throws IOException {
    MiniDFSCluster cluster = new MiniDFSCluster.Builder(getTestConfiguration())
        .build();
    try {
      DistributedFileSystem dfs = cluster.getFileSystem();
      dfs.create(new Path(""/parent/test1/dfsclose/file-0""));
      Path snapShotDir = new Path(""/parent/test1/"");
      dfs.allowSnapshot(snapShotDir);

      FileStatus status = dfs.getFileStatus(new Path(""/parent/test1""));
      assertTrue(status.isSnapshotEnabled());
      status = dfs.getFileStatus(new Path(""/parent/""));
      assertFalse(status.isSnapshotEnabled());
    }",1
"@Test
  public void testStatistics() throws IOException {
    FileSystem.getStatistics(HdfsConstants.HDFS_URI_SCHEME,
        DistributedFileSystem.class).reset();
    @SuppressWarnings(""unchecked"")
    ThreadLocal<StatisticsData> data = (ThreadLocal<StatisticsData>)
        Whitebox.getInternalState(
        FileSystem.getStatistics(HdfsConstants.HDFS_URI_SCHEME,
        DistributedFileSystem.class), ""threadData"");
    data.set(null);

    int lsLimit = 2;
    final Configuration conf = getTestConfiguration();
    conf.setInt(DFSConfigKeys.DFS_LIST_LIMIT, lsLimit);
    final MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).build();
    try {
      cluster.waitActive();
      final FileSystem fs = cluster.getFileSystem();
      Path dir = new Path(""/test"");
      Path file = new Path(dir, ""file"");

      int readOps = 0;
      int writeOps = 0;
      int largeReadOps = 0;

      long opCount = getOpStatistics(OpType.MKDIRS);
      fs.mkdirs(dir);
      checkStatistics(fs, readOps, ++writeOps, largeReadOps);
      checkOpStatistics(OpType.MKDIRS, opCount + 1);
      
      opCount = getOpStatistics(OpType.CREATE);
      FSDataOutputStream out = fs.create(file, (short)1);
      out.close();
      checkStatistics(fs, readOps, ++writeOps, largeReadOps);
      checkOpStatistics(OpType.CREATE, opCount + 1);

      opCount = getOpStatistics(OpType.GET_FILE_STATUS);
      FileStatus status = fs.getFileStatus(file);
      checkStatistics(fs, ++readOps, writeOps, largeReadOps);
      checkOpStatistics(OpType.GET_FILE_STATUS, opCount + 1);
      
      opCount = getOpStatistics(OpType.GET_FILE_BLOCK_LOCATIONS);
      fs.getFileBlockLocations(file, 0, 0);
      checkStatistics(fs, ++readOps, writeOps, largeReadOps);
      checkOpStatistics(OpType.GET_FILE_BLOCK_LOCATIONS, opCount + 1);
      fs.getFileBlockLocations(status, 0, 0);
      checkStatistics(fs, ++readOps, writeOps, largeReadOps);
      checkOpStatistics(OpType.GET_FILE_BLOCK_LOCATIONS, opCount + 2);
      
      opCount = getOpStatistics(OpType.OPEN);
      FSDataInputStream in = fs.open(file);
      in.close();
      checkStatistics(fs, ++readOps, writeOps, largeReadOps);
      checkOpStatistics(OpType.OPEN, opCount + 1);
      
      opCount = getOpStatistics(OpType.SET_REPLICATION);
      fs.setReplication(file, (short)2);
      checkStatistics(fs, readOps, ++writeOps, largeReadOps);
      checkOpStatistics(OpType.SET_REPLICATION, opCount + 1);
      
      opCount = getOpStatistics(OpType.RENAME);
      Path file1 = new Path(dir, ""file1"");
      fs.rename(file, file1);
      checkStatistics(fs, readOps, ++writeOps, largeReadOps);
      checkOpStatistics(OpType.RENAME, opCount + 1);
      
      opCount = getOpStatistics(OpType.GET_CONTENT_SUMMARY);
      fs.getContentSummary(file1);
      checkStatistics(fs, ++readOps, writeOps, largeReadOps);
      checkOpStatistics(OpType.GET_CONTENT_SUMMARY, opCount + 1);
      
      
      // Iterative ls test
      long mkdirOp = getOpStatistics(OpType.MKDIRS);
      long listStatusOp = getOpStatistics(OpType.LIST_STATUS);
      long locatedListStatusOP = getOpStatistics(OpType.LIST_LOCATED_STATUS);
      for (int i = 0; i < 10; i++) {
        Path p = new Path(dir, Integer.toString(i));
        fs.mkdirs(p);
        mkdirOp++;
        FileStatus[] list = fs.listStatus(dir);
        if (list.length > lsLimit) {
          // if large directory, then count readOps and largeReadOps by 
          // number times listStatus iterates
          int iterations = (int)Math.ceil((double)list.length/lsLimit);
          largeReadOps += iterations;
          readOps += iterations;
          listStatusOp += iterations;
        }",1
"@Test
  public void testGetFileStatusOnFile() throws Exception {
    checkFile(fs, file1, 1);
    // test getFileStatus on a file
    FileStatus status = fs.getFileStatus(file1);
    assertFalse(file1 + "" should be a file"", status.isDirectory());
    assertEquals(blockSize, status.getBlockSize());
    assertEquals(1, status.getReplication());
    assertEquals(fileSize, status.getLen());
    ContractTestUtils.assertNotErasureCoded(fs, file1);
    assertEquals(file1.makeQualified(fs.getUri(),
        fs.getWorkingDirectory()).toString(), 
        status.getPath().toString());
    assertTrue(file1 + "" should have erasure coding unset in "" +
            ""FileStatus#toString(): "" + status,
        status.toString().contains(""isErasureCoded=false""));
  }",1
"@Test
  public void testListStatusOnFile() throws IOException {
    FileStatus[] stats = fs.listStatus(file1);
    assertEquals(1, stats.length);
    FileStatus status = stats[0];
    assertFalse(file1 + "" should be a file"", status.isDirectory());
    assertEquals(blockSize, status.getBlockSize());
    assertEquals(1, status.getReplication());
    assertEquals(fileSize, status.getLen());
    ContractTestUtils.assertNotErasureCoded(fs, file1);
    assertEquals(file1.makeQualified(fs.getUri(),
        fs.getWorkingDirectory()).toString(), 
        status.getPath().toString());

    RemoteIterator<FileStatus> itor = fc.listStatus(file1);
    status = itor.next();
    assertEquals(stats[0], status);
    assertFalse(file1 + "" should be a file"", status.isDirectory());
  }",1
"@Test
  public void testBlockAllocationAdjustsUsageConservatively() 
      throws Exception {
    final Path parent = new Path(
        PathUtils.getTestDir(getClass()).getPath(),
        GenericTestUtils.getMethodName());
    assertTrue(dfs.mkdirs(parent));

    DFSAdmin admin = new DFSAdmin(conf);
    Path dir = new Path(parent, ""test"");
    Path file1 = new Path(parent, ""test/test1"");
    Path file2 = new Path(parent, ""test/test2"");
    boolean exceededQuota = false;
    final int QUOTA_SIZE = 3 * DEFAULT_BLOCK_SIZE; // total space usage including
                                           // repl.
    final int FILE_SIZE = DEFAULT_BLOCK_SIZE / 2;
    ContentSummary c;

    // Create the directory and set the quota
    assertTrue(dfs.mkdirs(dir));
    runCommand(admin, false, ""-setSpaceQuota"", Integer.toString(QUOTA_SIZE),
         dir.toString());

    // Creating a file should use half the quota
    DFSTestUtil.createFile(dfs, file1, FILE_SIZE, (short) 3, 1L);
    DFSTestUtil.waitReplication(dfs, file1, (short) 3);
    c = dfs.getContentSummary(dir);
    compareQuotaUsage(c, dfs, dir);
    checkContentSummary(c, webhdfs.getContentSummary(dir));
    assertEquals(""Quota is half consumed"", QUOTA_SIZE / 2,
                 c.getSpaceConsumed());

    // We can not create the 2nd file because even though the total spaced
    // used by two files (2 * 3 * 512/2) would fit within the quota (3 * 512)
    // when a block for a file is created the space used is adjusted
    // conservatively (3 * block size, ie assumes a full block is written)
    // which will violate the quota (3 * block size) since we've already
    // used half the quota for the first file.
    try {
      DFSTestUtil.createFile(dfs, file2, FILE_SIZE, (short) 3, 1L);
    }",1
"@Test
  public void testHugeFileCount() throws IOException {
    final Path parent = new Path(
        PathUtils.getTestDir(getClass()).getPath(),
        GenericTestUtils.getMethodName());
    assertTrue(dfs.mkdirs(parent));

    for (int i = 1; i <= 5; i++) {
      FSDataOutputStream out =
          dfs.create(new Path(parent, ""Folder1/"" + ""file"" + i),(short)1);
      out.close();
    }",1
"@Test
  public void testMaxSpaceQuotas() throws Exception {
    final Path parent = new Path(
        PathUtils.getTestDir(getClass()).getPath(),
        GenericTestUtils.getMethodName());
    assertTrue(dfs.mkdirs(parent));

    final FileSystem fs = cluster.getFileSystem();
    assertTrue(""Not a HDFS: ""+fs.getUri(),
                fs instanceof DistributedFileSystem);
    final DistributedFileSystem dfs = (DistributedFileSystem)fs;

    // create test directory
    final Path testFolder = new Path(parent, ""testFolder"");
    assertTrue(dfs.mkdirs(testFolder));

    // setting namespace quota to Long.MAX_VALUE - 1 should work
    dfs.setQuota(testFolder, Long.MAX_VALUE - 1, 10);
    ContentSummary c = dfs.getContentSummary(testFolder);
    compareQuotaUsage(c, dfs, testFolder);
    assertTrue(""Quota not set properly"", c.getQuota() == Long.MAX_VALUE - 1);

    // setting diskspace quota to Long.MAX_VALUE - 1 should work
    dfs.setQuota(testFolder, 10, Long.MAX_VALUE - 1);
    c = dfs.getContentSummary(testFolder);
    compareQuotaUsage(c, dfs, testFolder);
    assertTrue(""Quota not set properly"", c.getSpaceQuota() == Long.MAX_VALUE - 1);

    // setting namespace quota to Long.MAX_VALUE should not work + no error
    dfs.setQuota(testFolder, Long.MAX_VALUE, 10);
    c = dfs.getContentSummary(testFolder);
    compareQuotaUsage(c, dfs, testFolder);
    assertTrue(""Quota should not have changed"", c.getQuota() == 10);

    // setting diskspace quota to Long.MAX_VALUE should not work + no error
    dfs.setQuota(testFolder, 10, Long.MAX_VALUE);
    c = dfs.getContentSummary(testFolder);
    compareQuotaUsage(c, dfs, testFolder);
    assertTrue(""Quota should not have changed"", c.getSpaceQuota() == 10);

    // setting namespace quota to Long.MAX_VALUE + 1 should not work + error
    try {
      dfs.setQuota(testFolder, Long.MAX_VALUE + 1, 10);
      fail(""Exception not thrown"");
    }",1
"@Test
  public void testSpaceQuotaExceptionOnClose() throws Exception {
    GenericTestUtils.setLogLevel(DFSOutputStream.LOG, Level.TRACE);
    GenericTestUtils.setLogLevel(DataStreamer.LOG, Level.TRACE);
    final DFSAdmin dfsAdmin = new DFSAdmin(conf);
    final Path dir = new Path(
        PathUtils.getTestDir(getClass()).getPath(),
        GenericTestUtils.getMethodName());
    assertTrue(dfs.mkdirs(dir));
    final String[] args = new String[] {""-setSpaceQuota"", ""1"", dir.toString()}",1
"@Test
  public void testSpaceQuotaExceptionOnFlush() throws Exception {
    GenericTestUtils.setLogLevel(DFSOutputStream.LOG, Level.TRACE);
    GenericTestUtils.setLogLevel(DataStreamer.LOG, Level.TRACE);
    GenericTestUtils.setLogLevel(DFSClient.LOG, Level.TRACE);
    final DFSAdmin dfsAdmin = new DFSAdmin(conf);
    final Path dir = new Path(
        PathUtils.getTestDir(getClass()).getPath(),
        GenericTestUtils.getMethodName());
    assertTrue(dfs.mkdirs(dir));
    final String[] args = new String[] {""-setSpaceQuota"", ""1"", dir.toString()}",1
"@Test
  public void testGenerated() throws IOException {
    // edits generated by nnHelper (MiniDFSCluster), should have all op codes
    // binary, XML, reparsed binary
    String edits = nnHelper.generateEdits();
    LOG.info(""Generated edits="" + edits);
    String editsParsedXml = folder.newFile(""editsParsed.xml"").getAbsolutePath();
    String editsReparsed = folder.newFile(""editsParsed"").getAbsolutePath();
    // capital case extension
    String editsParsedXML_caseInSensitive =
        folder.newFile(""editsRecoveredParsed.XML"").getAbsolutePath();

    // parse to XML then back to binary
    assertEquals(0, runOev(edits, editsParsedXml, ""xml"", false));
    assertEquals(0, runOev(edits, editsParsedXML_caseInSensitive, ""xml"", false));
    assertEquals(0, runOev(editsParsedXml, editsReparsed, ""binary"", false));
    assertEquals(0,
        runOev(editsParsedXML_caseInSensitive, editsReparsed, ""binary"", false));


    // judgment time
    assertTrue(""Edits "" + edits + "" should have all op codes"",
        hasAllOpCodes(edits));
    LOG.info(""Comparing generated file "" + editsReparsed
        + "" with reference file "" + edits);
    assertTrue(
        ""Generated edits and reparsed (bin to XML to bin) should be same"",
        filesEqualIgnoreTrailingZeros(edits, editsReparsed));
  }",1
"@Test
  public void testStored() throws IOException {
    // reference edits stored with source code (see build.xml)
    final String cacheDir = System.getProperty(""test.cache.data"",
        ""target/test-classes"");
    // binary, XML, reparsed binary
    String editsStored = cacheDir + ""/editsStored"";
    String editsStoredParsedXml = cacheDir + ""/editsStoredParsed.xml"";
    String editsStoredReparsed = cacheDir + ""/editsStoredReparsed"";
    // reference XML version of editsStored (see build.xml)
    String editsStoredXml = cacheDir + ""/editsStored.xml"";

    // parse to XML then back to binary
    assertEquals(0, runOev(editsStored, editsStoredParsedXml, ""xml"", false));
    assertEquals(0,
        runOev(editsStoredParsedXml, editsStoredReparsed, ""binary"", false));

    // judgement time
    assertTrue(""Edits "" + editsStored + "" should have all op codes"",
        hasAllOpCodes(editsStored));
    assertTrue(""Reference XML edits and parsed to XML should be same"",
        FileUtils.contentEqualsIgnoreEOL(new File(editsStoredXml),
            new File(editsStoredParsedXml), ""UTF-8""));
    assertTrue(
        ""Reference edits and reparsed (bin to XML to bin) should be same"",
        filesEqualIgnoreTrailingZeros(editsStored, editsStoredReparsed));
  }",1
"@Test
  public void testFileDistributionCalculatorForException() throws Exception {
    File fsimageFile = null;
    Configuration conf = new Configuration();
    // Avoid using the same cluster dir to cause the global originalFsimage
    // file to be cleared.
    conf.set(HDFS_MINIDFS_BASEDIR, GenericTestUtils.getRandomizedTempPath());
    HashMap<String, FileStatus> files = Maps.newHashMap();

    // Create a initial fsimage file
    try (MiniDFSCluster cluster =
        new MiniDFSCluster.Builder(conf).numDataNodes(1).build()) {
      cluster.waitActive();
      DistributedFileSystem hdfs = cluster.getFileSystem();

      // Create a reasonable namespace
      Path dir = new Path(""/dir"");
      hdfs.mkdirs(dir);
      files.put(dir.toString(), pathToFileEntry(hdfs, dir.toString()));
      // Create files with byte size that can't be divided by step size,
      // the byte size for here are 3, 9, 15, 21.
      for (int i = 0; i < FILES_PER_DIR; i++) {
        Path file = new Path(dir, ""file"" + i);
        DFSTestUtil.createFile(hdfs, file, 6 * i + 3, (short) 1, 0);

        files.put(file.toString(),
            pathToFileEntry(hdfs, file.toString()));
      }",1
"@Test
  public void testWebImageViewer() throws Exception {
    WebImageViewer viewer = new WebImageViewer(
        NetUtils.createSocketAddr(""localhost:0""));
    try {
      viewer.initServer(originalFsimage.getAbsolutePath());
      int port = viewer.getPort();

      // create a WebHdfsFileSystem instance
      URI uri = new URI(""webhdfs://localhost:"" + String.valueOf(port));
      Configuration conf = new Configuration();
      WebHdfsFileSystem webhdfs = (WebHdfsFileSystem)FileSystem.get(uri, conf);

      // verify the number of directories
      FileStatus[] statuses = webhdfs.listStatus(new Path(""/""));
      assertEquals(dirCount, statuses.length);

      // verify the number of files in the directory
      statuses = webhdfs.listStatus(new Path(""/dir0""));
      assertEquals(FILES_PER_DIR, statuses.length);

      // compare a file
      FileStatus status = webhdfs.listStatus(new Path(""/dir0/file0""))[0];
      FileStatus expected = writtenFiles.get(""/dir0/file0"");
      compareFile(expected, status);

      // LISTSTATUS operation to an empty directory
      statuses = webhdfs.listStatus(new Path(""/emptydir""));
      assertEquals(0, statuses.length);

      // LISTSTATUS operation to a invalid path
      URL url = new URL(""http://localhost:"" + port +
                    ""/webhdfs/v1/invalid/?op=LISTSTATUS"");
      verifyHttpResponseCode(HttpURLConnection.HTTP_NOT_FOUND, url);

      // LISTSTATUS operation to a invalid prefix
      url = new URL(""http://localhost:"" + port + ""/foo"");
      verifyHttpResponseCode(HttpURLConnection.HTTP_NOT_FOUND, url);

      // Verify the Erasure Coded empty file status
      Path emptyECFilePath = new Path(""/ec/EmptyECFile.txt"");
      FileStatus actualEmptyECFileStatus =
          webhdfs.getFileStatus(new Path(emptyECFilePath.toString()));
      FileStatus expectedEmptyECFileStatus = writtenFiles.get(
          emptyECFilePath.toString());
      System.out.println(webhdfs.getFileStatus(new Path(emptyECFilePath
              .toString())));
      compareFile(expectedEmptyECFileStatus, actualEmptyECFileStatus);

      // Verify the Erasure Coded small file status
      Path smallECFilePath = new Path(""/ec/SmallECFile.txt"");
      FileStatus actualSmallECFileStatus =
          webhdfs.getFileStatus(new Path(smallECFilePath.toString()));
      FileStatus expectedSmallECFileStatus = writtenFiles.get(
          smallECFilePath.toString());
      compareFile(expectedSmallECFileStatus, actualSmallECFileStatus);

      // GETFILESTATUS operation
      status = webhdfs.getFileStatus(new Path(""/dir0/file0""));
      compareFile(expected, status);

      // GETFILESTATUS operation to a invalid path
      url = new URL(""http://localhost:"" + port +
                    ""/webhdfs/v1/invalid/?op=GETFILESTATUS"");
      verifyHttpResponseCode(HttpURLConnection.HTTP_NOT_FOUND, url);

      // invalid operation
      url = new URL(""http://localhost:"" + port + ""/webhdfs/v1/?op=INVALID"");
      verifyHttpResponseCode(HttpURLConnection.HTTP_BAD_REQUEST, url);

      // invalid method
      url = new URL(""http://localhost:"" + port + ""/webhdfs/v1/?op=LISTSTATUS"");
      HttpURLConnection connection = (HttpURLConnection) url.openConnection();
      connection.setRequestMethod(""POST"");
      connection.connect();
      assertEquals(HttpURLConnection.HTTP_BAD_METHOD,
          connection.getResponseCode());
    }",1
"@Test
  public void testReturnedTokenIsNull() throws Exception {
    WebHdfsFileSystem fs = mock(WebHdfsFileSystem.class);
    doReturn(null).when(fs).getDelegationToken(anyString());
    Path p = new Path(f.getRoot().getAbsolutePath(), tokenFile);
    DelegationTokenFetcher.saveDelegationToken(conf, fs, null, p);
    // When Token returned is null, TokenFile should not exist
    Assert.assertFalse(p.getFileSystem(conf).exists(p));

  }",1
"@Test
  public void testOverwriteFile() throws IOException {
    assertTrue(""Creating empty dst file"", DST_FILE.createNewFile());
    
    OutputStream fos = new AtomicFileOutputStream(DST_FILE);
    
    assertTrue(""Empty file still exists"", DST_FILE.exists());
    fos.write(TEST_STRING.getBytes());
    fos.flush();
    
    // Original contents still in place
    assertEquals("""", DFSTestUtil.readFile(DST_FILE));

    fos.close();

    // New contents replace original file
    String readBackData = DFSTestUtil.readFile(DST_FILE);
    assertEquals(TEST_STRING, readBackData);
  }",1
"@Test
  public void testVerifyMD5FileBadDigest() throws Exception {
    MD5FileUtils.saveMD5File(TEST_FILE, MD5Hash.digest(new byte[0]));
    try {
      MD5FileUtils.verifySavedMD5(TEST_FILE, TEST_MD5);
      fail(""Did not throw"");
    }",1
"@Test
  public void testByteRange() throws IOException {
    ByteRangeInputStream.URLOpener oMock = getMockURLOpener(
        new URL(""http://test""));
    ByteRangeInputStream.URLOpener rMock = getMockURLOpener(null);
    ByteRangeInputStream bris = new ByteRangeInputStreamImpl(oMock, rMock);

    bris.seek(0);

    assertEquals(""getPos wrong"", 0, bris.getPos());

    bris.read();

    assertEquals(""Initial call made incorrectly (offset check)"",
        0, bris.startPos);
    assertEquals(""getPos should return 1 after reading one byte"", 1,
        bris.getPos());
    verify(oMock, times(1)).connect(0, false);

    bris.read();

    assertEquals(""getPos should return 2 after reading two bytes"", 2,
        bris.getPos());
    // No additional connections should have been made (no seek)
    verify(oMock, times(1)).connect(0, false);

    rMock.setURL(new URL(""http://resolvedurl/""));

    bris.seek(100);
    bris.read();

    assertEquals(""Seek to 100 bytes made incorrectly (offset Check)"",
        100, bris.startPos);
    assertEquals(""getPos should return 101 after reading one byte"", 101,
        bris.getPos());
    verify(rMock, times(1)).connect(100, true);

    bris.seek(101);
    bris.read();

    // Seek to 101 should not result in another request
    verify(rMock, times(1)).connect(100, true);
    verify(rMock, times(0)).connect(101, true);

    bris.seek(2500);
    bris.read();

    assertEquals(""Seek to 2500 bytes made incorrectly (offset Check)"",
        2500, bris.startPos);

    doReturn(getMockConnection(null))
        .when(rMock).connect(anyLong(), anyBoolean());
    bris.seek(500);
    try {
      bris.read();
      fail(""Exception should be thrown when content-length is not given"");
    }",1
"@Test
  public void testPropagatedClose() throws IOException {
    ByteRangeInputStream bris =
        mock(ByteRangeInputStream.class, CALLS_REAL_METHODS);
    InputStreamAndFileLength mockStream = new InputStreamAndFileLength(1L,
        mock(InputStream.class));
    doReturn(mockStream).when(bris).openInputStream(Mockito.anyLong());
    Whitebox.setInternalState(bris, ""status"",
                              ByteRangeInputStream.StreamStatus.SEEK);

    int brisOpens = 0;
    int brisCloses = 0;
    int isCloses = 0;

    // first open, shouldn't close underlying stream
    bris.getInputStream();
    verify(bris, times(++brisOpens)).openInputStream(Mockito.anyLong());
    verify(bris, times(brisCloses)).close();
    verify(mockStream.in, times(isCloses)).close();

    // stream is open, shouldn't close underlying stream
    bris.getInputStream();
    verify(bris, times(brisOpens)).openInputStream(Mockito.anyLong());
    verify(bris, times(brisCloses)).close();
    verify(mockStream.in, times(isCloses)).close();

    // seek forces a reopen, should close underlying stream
    bris.seek(1);
    bris.getInputStream();
    verify(bris, times(++brisOpens)).openInputStream(Mockito.anyLong());
    verify(bris, times(brisCloses)).close();
    verify(mockStream.in, times(++isCloses)).close();

    // verify that the underlying stream isn't closed after a seek
    // ie. the state was correctly updated
    bris.getInputStream();
    verify(bris, times(brisOpens)).openInputStream(Mockito.anyLong());
    verify(bris, times(brisCloses)).close();
    verify(mockStream.in, times(isCloses)).close();

    // seeking to same location should be a no-op
    bris.seek(1);
    bris.getInputStream();
    verify(bris, times(brisOpens)).openInputStream(Mockito.anyLong());
    verify(bris, times(brisCloses)).close();
    verify(mockStream.in, times(isCloses)).close();

    // close should of course close
    bris.close();
    verify(bris, times(++brisCloses)).close();
    verify(mockStream.in, times(++isCloses)).close();

    // it's already closed, underlying stream should not close
    bris.close();
    verify(bris, times(++brisCloses)).close();
    verify(mockStream.in, times(isCloses)).close();

    // it's closed, don't reopen it
    boolean errored = false;
    try {
      bris.getInputStream();
    }",1
"@Test
  public void testContentSummary() throws Exception {
    final Configuration conf = WebHdfsTestUtil.createConf();
    final Path path = new Path(""/QuotaDir"");
    cluster = new MiniDFSCluster.Builder(conf).numDataNodes(0).build();
    final WebHdfsFileSystem webHdfs = WebHdfsTestUtil.getWebHdfsFileSystem(conf,
        WebHdfsConstants.WEBHDFS_SCHEME);
    final DistributedFileSystem dfs = cluster.getFileSystem();
    dfs.mkdirs(path);
    dfs.setQuotaByStorageType(path, StorageType.DISK, 100000);
    ContentSummary contentSummary = webHdfs.getContentSummary(path);
    Assert
        .assertTrue((contentSummary.getTypeQuota(StorageType.DISK) == 100000));
  }",1
"@Test
  public void testStoragePolicy() throws Exception {
    final Configuration conf = WebHdfsTestUtil.createConf();
    final Path path = new Path(""/file"");
    cluster = new MiniDFSCluster.Builder(conf).numDataNodes(0).build();
    final DistributedFileSystem dfs = cluster.getFileSystem();
    final WebHdfsFileSystem webHdfs = WebHdfsTestUtil.getWebHdfsFileSystem(conf,
        WebHdfsConstants.WEBHDFS_SCHEME);

    // test getAllStoragePolicies
    Assert.assertTrue(Arrays.equals(dfs.getAllStoragePolicies().toArray(),
        webHdfs.getAllStoragePolicies().toArray()));

    // test get/set/unset policies
    DFSTestUtil.createFile(dfs, path, 0, (short) 1, 0L);
    // get defaultPolicy
    BlockStoragePolicySpi defaultdfsPolicy = dfs.getStoragePolicy(path);
    // set policy through webhdfs
    webHdfs.setStoragePolicy(path, HdfsConstants.COLD_STORAGE_POLICY_NAME);
    // get policy from dfs
    BlockStoragePolicySpi dfsPolicy = dfs.getStoragePolicy(path);
    // get policy from webhdfs
    BlockStoragePolicySpi webHdfsPolicy = webHdfs.getStoragePolicy(path);
    Assert.assertEquals(HdfsConstants.COLD_STORAGE_POLICY_NAME.toString(),
        webHdfsPolicy.getName());
    Assert.assertEquals(webHdfsPolicy, dfsPolicy);
    // unset policy
    webHdfs.unsetStoragePolicy(path);
    Assert.assertEquals(defaultdfsPolicy, webHdfs.getStoragePolicy(path));
  }",1
"@Test
  public void testWebHdfsDeleteSnapshot() throws Exception {
    final Configuration conf = WebHdfsTestUtil.createConf();
    cluster = new MiniDFSCluster.Builder(conf).numDataNodes(0).build();
    cluster.waitActive();
    final DistributedFileSystem dfs = cluster.getFileSystem();
    final FileSystem webHdfs = WebHdfsTestUtil.getWebHdfsFileSystem(conf,
        WebHdfsConstants.WEBHDFS_SCHEME);

    final Path foo = new Path(""/foo"");
    dfs.mkdirs(foo);
    dfs.allowSnapshot(foo);

    webHdfs.createSnapshot(foo, ""s1"");
    final Path spath = webHdfs.createSnapshot(foo, null);
    Assert.assertTrue(webHdfs.exists(spath));
    final Path s1path = SnapshotTestHelper.getSnapshotRoot(foo, ""s1"");
    Assert.assertTrue(webHdfs.exists(s1path));

    // delete operation snapshot name as null
    try {
      webHdfs.deleteSnapshot(foo, null);
      fail(""Expected IllegalArgumentException"");
    }",1
"@Test
  public void  testClose() {
    TestCompressorStream testCompressorStream = new TestCompressorStream();
    try {
      testCompressorStream.close(); 
    }",1
"@Test
  public void testConfigureLZOCodec() throws IOException {
    // Dummy codec
    String defaultCodec = ""org.apache.hadoop.io.compress.DefaultCodec"";
    Compression.Algorithm.conf.set(
        Compression.Algorithm.CONF_LZO_CLASS, defaultCodec);
    assertEquals(defaultCodec,
        Compression.Algorithm.LZO.getCodec().getClass().getName());
  }",1
"@Test
  public void testUnsortedTFileFeatures() throws IOException {
    unsortedWithSomeCodec(""none"");
    unsortedWithSomeCodec(""gz"");
  }",1
"@Test
  public void testMultiThreadedStatOnError() throws Exception {
    final String testInvalidFilePath = ""C:\\nonexisting_path\\nonexisting_file"";

    int numOfThreads = 10;
    ExecutorService executorService =
        Executors.newFixedThreadPool(numOfThreads);
    for (int i = 0; i < numOfThreads; i++) {
      try {
        Future<Boolean> result =
            executorService.submit(() -> doStatTest(testInvalidFilePath));
        result.get();
      }",1
"@Test
  public void testIOExceptionInWriterConstructor() {
    Path dirNameSpy = spy(TEST_FILE);
    BloomMapFile.Reader reader = null;
    BloomMapFile.Writer writer = null;
    try {
      writer = new BloomMapFile.Writer(conf, TEST_FILE,
          MapFile.Writer.keyClass(IntWritable.class),
          MapFile.Writer.valueClass(Text.class));
      writer.append(new IntWritable(1), new Text(""123124142""));
      writer.close();

      when(dirNameSpy.getFileSystem(conf)).thenThrow(new IOException());
      reader = new BloomMapFile.Reader(dirNameSpy, conf,
          MapFile.Reader.comparator(new WritableComparator(IntWritable.class)));

      assertNull(""testIOExceptionInWriterConstructor error !!!"",
          reader.getBloomFilter());
    }",1
"@Test
  public void testCloseStreams() throws IOException {
    File tmpFile = null;
    FileOutputStream fos;
    BufferedOutputStream bos;
    FileOutputStream nullStream = null;

    try {
      tmpFile = new File(GenericTestUtils.getTestDir(), ""testCloseStreams.txt"");
      fos = new FileOutputStream(tmpFile) {
        @Override
        public void close() throws IOException {
          throw new IOException();
        }",1
"@Test
  public void testFix() {
    final String INDEX_LESS_MAP_FILE = ""testFix.mapfile"";
    int PAIR_SIZE = 20;
    MapFile.Writer writer = null;
    try {
      FileSystem fs = FileSystem.getLocal(conf);
      Path dir = new Path(TEST_DIR, INDEX_LESS_MAP_FILE);
      writer = createWriter(INDEX_LESS_MAP_FILE, IntWritable.class, Text.class);
      for (int i = 0; i < PAIR_SIZE; i++)
        writer.append(new IntWritable(0), new Text(""value""));
      writer.close();

      File indexFile = new File(""."", ""."" + INDEX_LESS_MAP_FILE + ""/index"");
      boolean isDeleted = false;
      if (indexFile.exists())
        isDeleted = indexFile.delete();

      if (isDeleted)
        assertTrue(""testFix error !!!"",
            MapFile.fix(fs, dir, IntWritable.class, Text.class, true, conf) == PAIR_SIZE);
    }",1
"@Test
  public void testPathExplosionWriterCreation() {
    Path path = new Path(TEST_DIR, ""testPathExplosionWriterCreation.mapfile"");
    String TEST_ERROR_MESSAGE = ""Mkdirs failed to create directory ""
        + path.getName();
    MapFile.Writer writer = null;
    try {
      FileSystem fsSpy = spy(FileSystem.get(conf));
      Path pathSpy = spy(path);
      when(fsSpy.mkdirs(path)).thenThrow(new IOException(TEST_ERROR_MESSAGE));

      when(pathSpy.getFileSystem(conf)).thenReturn(fsSpy);

      writer = new MapFile.Writer(conf, pathSpy,
          MapFile.Writer.keyClass(IntWritable.class),
          MapFile.Writer.valueClass(IntWritable.class));
      fail(""fail in testPathExplosionWriterCreation !!!"");
    }",1
"@Test
  public void testReaderGetClosest() throws Exception {
    final String TEST_METHOD_KEY = ""testReaderWithWrongKeyClass.mapfile"";
    MapFile.Writer writer = null;
    MapFile.Reader reader = null;
    try {
      writer = createWriter(TEST_METHOD_KEY, IntWritable.class, Text.class);

      for (int i = 0; i < 10; i++)
        writer.append(new IntWritable(i), new Text(""value"" + i));
      writer.close();

      reader = createReader(TEST_METHOD_KEY, Text.class);
      reader.getClosest(new Text(""2""), new Text(""""));
      fail(""no excepted exception in testReaderWithWrongKeyClass !!!"");
    }",1
"@Test
  public void testRenameWithException() {
    final String ERROR_MESSAGE = ""Can't rename file"";
    final String NEW_FILE_NAME = ""test-new.mapfile"";
    final String OLD_FILE_NAME = ""test-old.mapfile"";
    MapFile.Writer writer = null;
    try {
      FileSystem fs = FileSystem.getLocal(conf);
      FileSystem spyFs = spy(fs);

      writer = createWriter(OLD_FILE_NAME, IntWritable.class, IntWritable.class);
      writer.close();

      Path oldDir = new Path(TEST_DIR, OLD_FILE_NAME);
      Path newDir = new Path(TEST_DIR, NEW_FILE_NAME);
      when(spyFs.rename(oldDir, newDir)).thenThrow(
          new IOException(ERROR_MESSAGE));

      MapFile.rename(spyFs, oldDir.toString(), newDir.toString());
      fail(""testRenameWithException no exception error !!!"");
    }",1
"@Test
  public void testClose() throws IOException {
    Configuration conf = new Configuration();
    LocalFileSystem fs = FileSystem.getLocal(conf);
  
    // create a sequence file 1
    Path path1 = new Path(GenericTestUtils.getTempPath(""test1.seq""));
    SequenceFile.Writer writer = SequenceFile.createWriter(fs, conf, path1,
        Text.class, NullWritable.class, CompressionType.BLOCK);
    writer.append(new Text(""file1-1""), NullWritable.get());
    writer.append(new Text(""file1-2""), NullWritable.get());
    writer.close();
  
    Path path2 = new Path(GenericTestUtils.getTempPath(""test2.seq""));
    writer = SequenceFile.createWriter(fs, conf, path2, Text.class,
        NullWritable.class, CompressionType.BLOCK);
    writer.append(new Text(""file2-1""), NullWritable.get());
    writer.append(new Text(""file2-2""), NullWritable.get());
    writer.close();
  
    // Create a reader which uses 4 BuiltInZLibInflater instances
    SequenceFile.Reader reader = new SequenceFile.Reader(fs, path1, conf);
    // Returns the 4 BuiltInZLibInflater instances to the CodecPool
    reader.close();
    // The second close _could_ erroneously returns the same 
    // 4 BuiltInZLibInflater instances to the CodecPool again
    reader.close();
  
    // The first reader gets 4 BuiltInZLibInflater instances from the CodecPool
    SequenceFile.Reader reader1 = new SequenceFile.Reader(fs, path1, conf);
    // read first value from reader1
    Text text = new Text();
    reader1.next(text);
    assertEquals(""file1-1"", text.toString());
    
    // The second reader _could_ get the same 4 BuiltInZLibInflater 
    // instances from the CodePool as reader1
    SequenceFile.Reader reader2 = new SequenceFile.Reader(fs, path2, conf);
    
    // read first value from reader2
    reader2.next(text);
    assertEquals(""file2-1"", text.toString());
    // read second value from reader1
    reader1.next(text);
    assertEquals(""file1-2"", text.toString());
    // read second value from reader2 (this throws an exception)
    reader2.next(text);
    assertEquals(""file2-2"", text.toString());
  
    assertFalse(reader1.next(text));
    assertFalse(reader2.next(text));
  }",1
"@Test
  public void testCreateUsesFsArg() throws Exception {
    FileSystem fs = FileSystem.getLocal(conf);
    FileSystem spyFs = Mockito.spy(fs);
    Path p = new Path(GenericTestUtils.getTempPath(""testCreateUsesFSArg.seq""));
    SequenceFile.Writer writer = SequenceFile.createWriter(
        spyFs, conf, p, NullWritable.class, NullWritable.class);
    writer.close();
    Mockito.verify(spyFs).getDefaultReplication(p);
  }",1
"@Test
  public void testCreateWriterOnExistingFile() throws IOException {
    Configuration conf = new Configuration();
    FileSystem fs = FileSystem.getLocal(conf);
    Path name = new Path(new Path(GenericTestUtils.getTempPath(
        ""createWriterOnExistingFile"")), ""file"");

    fs.create(name);
    SequenceFile.createWriter(fs, conf, name, RandomDatum.class,
        RandomDatum.class, 512, (short) 1, 4096, false,
        CompressionType.NONE, null, new Metadata());
  }",1
"@Test
  public void testSerializationAvailability() throws IOException {
    Configuration conf = new Configuration();
    Path path = new Path(GenericTestUtils.getTempPath(
        ""serializationAvailability""));
    // Check if any serializers aren't found.
    try {
      SequenceFile.createWriter(
          conf,
          SequenceFile.Writer.file(path),
          SequenceFile.Writer.keyClass(String.class),
          SequenceFile.Writer.valueClass(NullWritable.class));
      // Note: This may also fail someday if JavaSerialization
      // is activated by default.
      fail(""Must throw IOException for missing serializer for the Key class"");
    }",1
"@Test
  public void testSetFileAccessMethods() {
    try {
      FileSystem fs = FileSystem.getLocal(conf);
      int size = 10;
      writeData(fs, size);
      SetFile.Reader reader = createReader(fs);
      assertTrue(""testSetFileWithConstruction1 error !!!"", reader.next(new IntWritable(0)));
      // don't know why reader.get(i) return i+1
      assertEquals(""testSetFileWithConstruction2 error !!!"", new IntWritable(size/2 + 1), reader.get(new IntWritable(size/2)));      
      assertNull(""testSetFileWithConstruction3 error !!!"", reader.get(new IntWritable(size*2)));
    }",1
"@Test
  public void testBufferWrapperNested() throws IOException {
    ByteArrayOutputStream baos = new ByteArrayOutputStream();
    DataOutputStream dos = new DataOutputStream(baos);
    writable.write(dos);
    message1.writeDelimitedTo(dos);
    message2.writeDelimitedTo(dos);
    ByteBuffer bb = ByteBuffer.wrap(baos.toByteArray());
    RpcWritable.Buffer buf1 = RpcWritable.Buffer.wrap(bb);
    Assert.assertEquals(baos.size(), bb.remaining());
    Assert.assertEquals(baos.size(), buf1.remaining());

    Object actual = buf1.newInstance(LongWritable.class, null);
    Assert.assertEquals(writable, actual);
    int left = bb.remaining();
    Assert.assertTrue(left > 0);
    Assert.assertEquals(left, buf1.remaining());

    // original bb now appears empty, but rpc writable has a slice of the bb.
    RpcWritable.Buffer buf2 = buf1.newInstance(RpcWritable.Buffer.class, null);
    Assert.assertEquals(0, bb.remaining());
    Assert.assertEquals(0, buf1.remaining());
    Assert.assertEquals(left, buf2.remaining());

    actual = buf2.getValue(EchoRequestProto.getDefaultInstance());
    Assert.assertEquals(message1, actual);
    Assert.assertTrue(buf2.remaining() > 0);

    actual = buf2.getValue(EchoRequestProto.getDefaultInstance());
    Assert.assertEquals(message2, actual);
    Assert.assertEquals(0, buf2.remaining());
  }",1
"@Test
  public void testWritableWrapper() throws IOException {
    // serial writable in byte buffer
    ByteArrayOutputStream baos = new ByteArrayOutputStream();
    writable.write(new DataOutputStream(baos));
    ByteBuffer bb = ByteBuffer.wrap(baos.toByteArray());

    // deserial
    LongWritable actual = RpcWritable.wrap(new LongWritable())
        .readFrom(bb);
    Assert.assertEquals(writable, actual);
    Assert.assertEquals(0, bb.remaining());
  }",1
"@Test
  public void testPool() throws Exception {
    final Random r = new Random();
    final Configuration conf = new Configuration();
    conf.setLong(FilePool.GRIDMIX_MIN_FILE, 3 * 1024);
    final FilePool pool = new FilePool(conf, base);
    pool.refresh();
    final ArrayList<FileStatus> files = new ArrayList<FileStatus>();

    // ensure 1k, 2k files excluded
    final int expectedPoolSize = (NFILES / 2 * (NFILES / 2 + 1) - 6) * 1024;
    assertEquals(expectedPoolSize, pool.getInputFiles(Long.MAX_VALUE, files));
    assertEquals(NFILES - 4, files.size());

    // exact match
    files.clear();
    assertEquals(expectedPoolSize, pool.getInputFiles(expectedPoolSize, files));

    // match random within 12k
    files.clear();
    final long rand = r.nextInt(expectedPoolSize);
    assertTrue(""Missed: "" + rand,
        (NFILES / 2) * 1024 > rand - pool.getInputFiles(rand, files));

    // all files
    conf.setLong(FilePool.GRIDMIX_MIN_FILE, 0);
    pool.refresh();
    files.clear();
    assertEquals((NFILES / 2 * (NFILES / 2 + 1)) * 1024,
        pool.getInputFiles(Long.MAX_VALUE, files));
  }",1
"@Test
  public void testStriper() throws Exception {
    final Random r = new Random();
    final Configuration conf = new Configuration();
    final FileSystem fs = FileSystem.getLocal(conf).getRaw();
    conf.setLong(FilePool.GRIDMIX_MIN_FILE, 3 * 1024);
    final FilePool pool = new FilePool(conf, base) {
      @Override
      public BlockLocation[] locationsFor(FileStatus stat, long start, long len)
          throws IOException {
        return new BlockLocation[] { new BlockLocation() }",1
"@Test
  public void testRepeat() throws Exception {
    final Configuration conf = new Configuration();
    Arrays.fill(loc, """");
    Arrays.fill(start, 0L);
    Arrays.fill(len, BLOCK);

    final ByteArrayOutputStream out = fillVerif();
    final FileQueue q =
      new FileQueue(new CombineFileSplit(paths, start, len, loc), conf);
    final byte[] verif = out.toByteArray();
    final byte[] check = new byte[2 * NFILES * BLOCK];
    q.read(check, 0, NFILES * BLOCK);
    assertArrayEquals(verif, Arrays.copyOf(check, NFILES * BLOCK));

    final byte[] verif2 = new byte[2 * NFILES * BLOCK];
    System.arraycopy(verif, 0, verif2, 0, verif.length);
    System.arraycopy(verif, 0, verif2, verif.length, verif.length);
    q.read(check, 0, 2 * NFILES * BLOCK);
    assertArrayEquals(verif2, check);

  }",1
"@Test
  public void testPseudoLocalFsFileNames() throws IOException {
    PseudoLocalFs pfs = new PseudoLocalFs();
    Configuration conf = new Configuration();
    conf.setClass(""fs.pseudo.impl"", PseudoLocalFs.class, FileSystem.class);

    Path path = new Path(""pseudo:///myPsedoFile.1234"");
    FileSystem testFs = path.getFileSystem(conf);
    assertEquals(""Failed to obtain a pseudo local file system object from path"",
                 pfs.getUri().getScheme(), testFs.getUri().getScheme());

    // Validate PseudoLocalFS operations on URI of some other file system
    path = new Path(""file:///myPsedoFile.12345"");
    validateGetFileStatus(pfs, path, false);
    validateCreate(pfs, path, false);
    validateOpen(pfs, path, false);
    validateExists(pfs, path, false);

    path = new Path(""pseudo:///myPsedoFile"");//.<fileSize> missing
    validateGetFileStatus(pfs, path, false);
    validateCreate(pfs, path, false);
    validateOpen(pfs, path, false);
    validateExists(pfs, path, false);

    // thing after final '.' is not a number
    path = new Path(""pseudo:///myPsedoFile.txt"");
    validateGetFileStatus(pfs, path, false);
    validateCreate(pfs, path, false);
    validateOpen(pfs, path, false);
    validateExists(pfs, path, false);

    // Generate valid file name(relative path) and validate operations on it
    long fileSize = 231456;
    path = PseudoLocalFs.generateFilePath(""my.Psedo.File"", fileSize);
    // Validate the above generateFilePath()
    assertEquals(""generateFilePath() failed."", fileSize,
                 pfs.validateFileNameFormat(path));

    validateGetFileStatus(pfs, path, true);
    validateCreate(pfs, path, true);
    validateOpen(pfs, path, true);
    validateExists(pfs, path, true);

    // Validate operations on valid qualified path
    path = new Path(""myPsedoFile.1237"");
    path = pfs.makeQualified(path);
    validateGetFileStatus(pfs, path, true);
    validateCreate(pfs, path, true);
    validateOpen(pfs, path, true);
    validateExists(pfs, path, true);
  }",1
"@Test
  public void testCollect() throws IOException {
    this.handler = new NativeCollectorOnlyHandler(taskContext, nativeHandler, pusher, combiner);
    handler.collect(new BytesWritable(), new BytesWritable(), 100);
    handler.close();
    handler.close();

    Mockito.verify(pusher, Mockito.times(1)).collect(any(BytesWritable.class),
        any(BytesWritable.class), anyInt());

    Mockito.verify(pusher, Mockito.times(1)).close();
    Mockito.verify(combiner, Mockito.times(1)).close();
    Mockito.verify(nativeHandler, Mockito.times(1)).close();
  }",1
"@Test
  public void testIgnoreDirs() throws Exception {
    Configuration conf = getConfiguration();
    conf.setBoolean(FileInputFormat.INPUT_DIR_NONRECURSIVE_IGNORE_SUBDIRS, true);
    conf.setInt(FileInputFormat.LIST_STATUS_NUM_THREADS, numThreads);
    conf.set(org.apache.hadoop.mapreduce.lib.input.FileInputFormat.INPUT_DIR, ""test:///a1"");
    MockFileSystem mockFs = (MockFileSystem) new Path(""test:///"").getFileSystem(conf);
    JobConf job = new JobConf(conf);
    TextInputFormat fileInputFormat = new TextInputFormat();
    fileInputFormat.configure(job);
    InputSplit[] splits = fileInputFormat.getSplits(job, 1);
    Assert.assertEquals(""Input splits are not correct"", 1, splits.length);
    FileSystem.closeAll();
  }",1
"@Test
  public void testListStatusSimple() throws IOException {
    Configuration conf = new Configuration();
    conf.setInt(FileInputFormat.LIST_STATUS_NUM_THREADS, numThreads);

    List<Path> expectedPaths = org.apache.hadoop.mapreduce.lib.input.TestFileInputFormat
        .configureTestSimple(conf, localFs);

    JobConf jobConf = new JobConf(conf);
    TextInputFormat fif = new TextInputFormat();
    fif.configure(jobConf);
    FileStatus[] statuses = fif.listStatus(jobConf);

    org.apache.hadoop.mapreduce.lib.input.TestFileInputFormat
        .verifyFileStatuses(expectedPaths, Lists.newArrayList(statuses),
            localFs);
  }",1
"@Test
  public void testFailAbortV1() throws Exception {
    testFailAbortInternal(1);
  }",1
"@Test
  public void testMapOnlyNoOutputV2() throws Exception {
    testMapOnlyNoOutputInternal(2);
  }",1
"@Test
  public void testRecoveryV2() throws Exception {
    testRecoveryInternal(2, 2);
  }",1
"@Test
  public void testBadIndex() throws Exception {
    final int parts = 30;
    fs.delete(p, true);
    conf.setInt(MRJobConfig.SHUFFLE_INDEX_CACHE, 1);
    IndexCache cache = new IndexCache(conf);

    Path f = new Path(p, ""badindex"");
    FSDataOutputStream out = fs.create(f, false);
    CheckedOutputStream iout = new CheckedOutputStream(out, new CRC32());
    DataOutputStream dout = new DataOutputStream(iout);
    for (int i = 0; i < parts; ++i) {
      for (int j = 0; j < MapTask.MAP_OUTPUT_INDEX_RECORD_LENGTH / 8; ++j) {
        if (0 == (i % 3)) {
          dout.writeLong(i);
        }",1
"@Test
  public void testRemoveMap() throws Exception {
    // This test case use two thread to call getIndexInformation and 
    // removeMap concurrently, in order to construct race condition.
    // This test case may not repeatable. But on my macbook this test 
    // fails with probability of 100% on code before MAPREDUCE-2541,
    // so it is repeatable in practice.
    fs.delete(p, true);
    conf.setInt(MRJobConfig.SHUFFLE_INDEX_CACHE, 10);
    // Make a big file so removeMapThread almost surely runs faster than 
    // getInfoThread 
    final int partsPerMap = 100000;
    final int bytesPerFile = partsPerMap * 24;
    final IndexCache cache = new IndexCache(conf);

    final Path big = new Path(p, ""bigIndex"");
    final String user = 
      UserGroupInformation.getCurrentUser().getShortUserName();
    writeFile(fs, big, bytesPerFile, partsPerMap);
    
    // run multiple times
    for (int i = 0; i < 20; ++i) {
      Thread getInfoThread = new Thread() {
        @Override
        public void run() {
          try {
            cache.getIndexInformation(""bigIndex"", partsPerMap, big, user);
          }",1
"@Test
  public void testComplexName() throws Exception {
    OutputStream os = getFileSystem().create(new Path(getInputDir(),
        ""text.txt""));
    Writer wr = new OutputStreamWriter(os);
    wr.write(""b a\n"");
    wr.close();

    JobConf conf = createJobConf();
    conf.setJobName(""[name][some other value that gets truncated internally that this test attempts to aggravate]"");

    conf.setInputFormat(TextInputFormat.class);

    conf.setOutputKeyClass(LongWritable.class);
    conf.setOutputValueClass(Text.class);

    conf.setMapperClass(IdentityMapper.class);

    FileInputFormat.setInputPaths(conf, getInputDir());

    FileOutputFormat.setOutputPath(conf, getOutputDir());

    JobClient.runJob(conf);

    Path[] outputFiles = FileUtil.stat2Paths(
                           getFileSystem().listStatus(getOutputDir(),
                           new Utils.OutputFileUtils.OutputFilesFilter()));
    assertEquals(1, outputFiles.length);
    InputStream is = getFileSystem().open(outputFiles[0]);
    BufferedReader reader = new BufferedReader(new InputStreamReader(is));
    assertEquals(""0\tb a"", reader.readLine());
    assertNull(reader.readLine());
    reader.close();
  }",1
"@Test
  public void testBzipWithMultibyteDelimiter() throws IOException {
    String testFileName = ""compressedMultibyteDelimiter.txt.bz2"";
    // firstSplitLength < (headers + blockMarker) will pass always since no
    // records will be read (in the test file that is byte 0..9)
    // firstSplitlength > (compressed file length - one compressed block
    // size + 1) will also always pass since the second split will be empty
    // (833 bytes is the last block start in the used data file)
    int firstSplitLength = 100;
    URL testFileUrl = getClass().getClassLoader().getResource(testFileName);
    assertNotNull(""Cannot find "" + testFileName, testFileUrl);
    File testFile = new File(testFileUrl.getFile());
    long testFileSize = testFile.length();
    Path testFilePath = new Path(testFile.getAbsolutePath());
    assertTrue(""Split size is smaller than header length"",
        firstSplitLength > 9);
    assertTrue(""Split size is larger than compressed file size "" +
        testFilePath, testFileSize > firstSplitLength);

    Configuration conf = new Configuration();
    conf.setInt(org.apache.hadoop.mapreduce.lib.input.
        LineRecordReader.MAX_LINE_LENGTH, Integer.MAX_VALUE);

    String delimiter = ""<E-LINE>\r\r\n"";
    conf.set(""textinputformat.record.delimiter"", delimiter);
    testSplitRecordsForFile(conf, firstSplitLength, testFileSize,
        testFilePath);
  }",1
"@Test
  public void testRecordSpanningMultipleSplits()
      throws IOException {
    checkRecordSpanningMultipleSplits(""recordSpanningMultipleSplits.txt"",
        10, false);
  }",1
"@Test
  public void testRecordSpanningMultipleSplitsCompressed()
      throws IOException {
    // The file is generated with bz2 block size of 100k. The split size
    // needs to be larger than that for the CompressedSplitLineReader to
    // work.
    checkRecordSpanningMultipleSplits(""recordSpanningMultipleSplits.txt.bz2"",
        200 * 1000, true);
  }",1
"@Test
  public void testRenameMapOutputForReduce() throws Exception {
    final JobConf conf = new JobConf();

    final MROutputFiles mrOutputFiles = new MROutputFiles();
    mrOutputFiles.setConf(conf);

    // make sure both dirs are distinct
    //
    conf.set(MRConfig.LOCAL_DIR, localDirs[0].toString());
    final Path mapOut = mrOutputFiles.getOutputFileForWrite(1);
    conf.set(MRConfig.LOCAL_DIR, localDirs[1].toString());
    final Path mapOutIdx = mrOutputFiles.getOutputIndexFileForWrite(1);
    Assert.assertNotEquals(""Paths must be different!"",
        mapOut.getParent(), mapOutIdx.getParent());

    // make both dirs part of LOCAL_DIR
    conf.setStrings(MRConfig.LOCAL_DIR, localDirs);

    final FileContext lfc = FileContext.getLocalFSFileContext(conf);
    lfc.create(mapOut, EnumSet.of(CREATE)).close();
    lfc.create(mapOutIdx, EnumSet.of(CREATE)).close();

    final JobId jobId = MRBuilderUtils.newJobId(12345L, 1, 2);
    final TaskId tid = MRBuilderUtils.newTaskId(jobId, 0, TaskType.MAP);
    final TaskAttemptId taid = MRBuilderUtils.newTaskAttemptId(tid, 0);

    LocalContainerLauncher.renameMapOutputForReduce(conf, taid, mrOutputFiles);
  }",1
"@Test
  public void testDuplicateDownload() throws Exception {
    JobID jobId = new JobID();
    JobConf conf = new JobConf();
    conf.setClass(""fs.mock.impl"", MockFileSystem.class, FileSystem.class);

    URI mockBase = new URI(""mock://test-nn1/"");
    when(mockfs.getUri()).thenReturn(mockBase);
    Path working = new Path(""mock://test-nn1/user/me/"");
    when(mockfs.getWorkingDirectory()).thenReturn(working);
    when(mockfs.resolvePath(any(Path.class))).thenAnswer(
        (Answer<Path>) args -> (Path) args.getArguments()[0]);

    final URI file = new URI(""mock://test-nn1/user/me/file.txt#link"");
    final Path filePath = new Path(file);
    File link = new File(""link"");

    when(mockfs.getFileStatus(any(Path.class))).thenAnswer(new Answer<FileStatus>() {
      @Override
      public FileStatus answer(InvocationOnMock args) throws Throwable {
        Path p = (Path) args.getArguments()[0];
        if (""file.txt"".equals(p.getName())) {
          return createMockTestFileStatus(filePath);
        }",1
"@Test
    public void testgetLocations() throws IOException{
        JobConf job= new JobConf();
      
      File tmpFile = File.createTempFile(""test"",""txt"");
      tmpFile.createNewFile();
      OutputStream out=new FileOutputStream(tmpFile);
      out.write(""tempfile"".getBytes());
      out.flush();
      out.close();
      Path[] path= {new Path(tmpFile.getAbsolutePath())}",1
"@Test
  public void testValueIterator() throws Exception {
    Path tmpDir = new Path(""build/test/test.reduce.task"");
    Configuration conf = new Configuration();
    for (Pair[] testCase: testCases) {
      runValueIterator(tmpDir, testCase, conf, null);
    }",1
"@Test
  public void testValueIteratorWithCompression() throws Exception {
    Path tmpDir = new Path(""build/test/test.reduce.task.compression"");
    Configuration conf = new Configuration();
    DefaultCodec codec = new DefaultCodec();
    codec.setConf(conf);
    for (Pair[] testCase: testCases) {
      runValueIterator(tmpDir, testCase, conf, codec);
    }",1
"@Test
  public void testFormat() throws Exception {
    JobConf job = new JobConf(conf);
    FileSystem fs = FileSystem.getLocal(conf);
    Path dir = new Path(System.getProperty(""test.build.data"",""."") + ""/mapred"");
    Path file = new Path(dir, ""test.seq"");
    
    Reporter reporter = Reporter.NULL;
    
    int seed = new Random().nextInt();
    //LOG.info(""seed = ""+seed);
    Random random = new Random(seed);

    fs.delete(dir, true);

    FileInputFormat.setInputPaths(job, dir);

    // for a variety of lengths
    for (int length = 0; length < MAX_LENGTH;
         length+= random.nextInt(MAX_LENGTH/10)+1) {

      //LOG.info(""creating; entries = "" + length);

      // create a file with length entries
      SequenceFile.Writer writer =
        SequenceFile.createWriter(fs, conf, file,
                                  IntWritable.class, BytesWritable.class);
      try {
        for (int i = 0; i < length; i++) {
          IntWritable key = new IntWritable(i);
          byte[] data = new byte[random.nextInt(10)];
          random.nextBytes(data);
          BytesWritable value = new BytesWritable(data);
          writer.append(key, value);
        }",1
"@Test
  public void testFormat() throws Exception {
    JobConf job = new JobConf();
    job.set(JobContext.TASK_ATTEMPT_ID, attempt);
    FileOutputFormat.setOutputPath(job, workDir.getParent().getParent());
    FileOutputFormat.setWorkOutputPath(job, workDir);
    FileSystem fs = workDir.getFileSystem(job);
    if (!fs.mkdirs(workDir)) {
      fail(""Failed to create output directory"");
    }",1
"@Test
  public void testCheckpointCreateDirect() throws Exception {
    checkpointCreate(ByteBuffer.allocateDirect(BUFSIZE));
  }",1
"@Test
  public void testDetermineCacheVisibilities() throws IOException {
    fs.setPermission(TEST_VISIBILITY_PARENT_DIR,
        new FsPermission((short)00777));
    fs.setPermission(TEST_VISIBILITY_CHILD_DIR,
        new FsPermission((short)00777));
    fs.setWorkingDirectory(TEST_VISIBILITY_CHILD_DIR);
    Job job = Job.getInstance(conf);
    Path relativePath = new Path(SECOND_CACHE_FILE);
    Path wildcardPath = new Path(""*"");
    Map<URI, FileStatus> statCache = new HashMap<>();
    Configuration jobConf;

    job.addCacheFile(firstCacheFile.toUri());
    job.addCacheFile(relativePath.toUri());
    jobConf = job.getConfiguration();

    // skip test if scratch dir is not PUBLIC
    assumeTrue(TEST_VISIBILITY_PARENT_DIR + "" is not public"",
        ClientDistributedCacheManager.isPublic(
            jobConf, TEST_VISIBILITY_PARENT_DIR.toUri(), statCache));

    ClientDistributedCacheManager.determineCacheVisibilities(jobConf,
        statCache);
    // We use get() instead of getBoolean() so we can tell the difference
    // between wrong and missing
    assertEquals(""The file paths were not found to be publicly visible ""
        + ""even though the full path is publicly accessible"",
        ""true,true"", jobConf.get(MRJobConfig.CACHE_FILE_VISIBILITIES));
    checkCacheEntries(statCache, null, firstCacheFile, relativePath);

    job = Job.getInstance(conf);
    job.addCacheFile(wildcardPath.toUri());
    jobConf = job.getConfiguration();
    statCache.clear();

    ClientDistributedCacheManager.determineCacheVisibilities(jobConf,
        statCache);
    // We use get() instead of getBoolean() so we can tell the difference
    // between wrong and missing
    assertEquals(""The file path was not found to be publicly visible ""
        + ""even though the full path is publicly accessible"",
        ""true"", jobConf.get(MRJobConfig.CACHE_FILE_VISIBILITIES));
    checkCacheEntries(statCache, null, wildcardPath.getParent());

    Path qualifiedParent = fs.makeQualified(TEST_VISIBILITY_PARENT_DIR);
    fs.setPermission(TEST_VISIBILITY_PARENT_DIR,
        new FsPermission((short)00700));
    job = Job.getInstance(conf);
    job.addCacheFile(firstCacheFile.toUri());
    job.addCacheFile(relativePath.toUri());
    jobConf = job.getConfiguration();
    statCache.clear();

    ClientDistributedCacheManager.determineCacheVisibilities(jobConf,
        statCache);
    // We use get() instead of getBoolean() so we can tell the difference
    // between wrong and missing
    assertEquals(""The file paths were found to be publicly visible ""
        + ""even though the parent directory is not publicly accessible"",
        ""false,false"", jobConf.get(MRJobConfig.CACHE_FILE_VISIBILITIES));
    checkCacheEntries(statCache, qualifiedParent,
        firstCacheFile, relativePath);

    job = Job.getInstance(conf);
    job.addCacheFile(wildcardPath.toUri());
    jobConf = job.getConfiguration();
    statCache.clear();

    ClientDistributedCacheManager.determineCacheVisibilities(jobConf,
        statCache);
    // We use get() instead of getBoolean() so we can tell the difference
    // between wrong and missing
    assertEquals(""The file path was found to be publicly visible ""
        + ""even though the parent directory is not publicly accessible"",
        ""false"", jobConf.get(MRJobConfig.CACHE_FILE_VISIBILITIES));
    checkCacheEntries(statCache, qualifiedParent, wildcardPath.getParent());
  }",1
"@Test
  public void testListLocatedStatus() throws Exception {
    Configuration conf = getConfiguration();
    conf.setBoolean(""fs.test.impl.disable.cache"", false);
    conf.setInt(FileInputFormat.LIST_STATUS_NUM_THREADS, numThreads);
    conf.set(org.apache.hadoop.mapreduce.lib.input.FileInputFormat.INPUT_DIR,
        ""test:///a1/a2"");
    MockFileSystem mockFs =
        (MockFileSystem) new Path(""test:///"").getFileSystem(conf);
    Assert.assertEquals(""listLocatedStatus already called"",
        0, mockFs.numListLocatedStatusCalls);
    JobConf job = new JobConf(conf);
    TextInputFormat fileInputFormat = new TextInputFormat();
    fileInputFormat.configure(job);
    InputSplit[] splits = fileInputFormat.getSplits(job, 1);
    Assert.assertEquals(""Input splits are not correct"", 2, splits.length);
    Assert.assertEquals(""listLocatedStatuss calls"",
        1, mockFs.numListLocatedStatusCalls);
    FileSystem.closeAll();
  }",1
"@Test
  public void testListStatusNestedRecursive() throws IOException {
    Configuration conf = new Configuration();
    conf.setInt(FileInputFormat.LIST_STATUS_NUM_THREADS, numThreads);

    List<Path> expectedPaths = org.apache.hadoop.mapreduce.lib.input.TestFileInputFormat
        .configureTestNestedRecursive(conf, localFs);
    JobConf jobConf = new JobConf(conf);
    TextInputFormat fif = new TextInputFormat();
    fif.configure(jobConf);
    FileStatus[] statuses = fif.listStatus(jobConf);

    org.apache.hadoop.mapreduce.lib.input.TestFileInputFormat
        .verifyFileStatuses(expectedPaths, Lists.newArrayList(statuses),
            localFs);
  }",1
"@Test
  public void testMultipleClose() throws IOException {
    URL testFileUrl = getClass().getClassLoader().
        getResource(""recordSpanningMultipleSplits.txt.bz2"");
    assertNotNull(""Cannot find recordSpanningMultipleSplits.txt.bz2"",
        testFileUrl);
    File testFile = new File(testFileUrl.getFile());
    Path testFilePath = new Path(testFile.getAbsolutePath());
    long testFileSize = testFile.length();
    Configuration conf = new Configuration();
    conf.setInt(org.apache.hadoop.mapreduce.lib.input.
        LineRecordReader.MAX_LINE_LENGTH, Integer.MAX_VALUE);
    FileSplit split = new FileSplit(testFilePath, 0, testFileSize,
        (String[])null);

    LineRecordReader reader = new LineRecordReader(conf, split);
    LongWritable key = new LongWritable();
    Text value = new Text();
    //noinspection StatementWithEmptyBody
    while (reader.next(key, value)) ;
    reader.close();
    reader.close();

    BZip2Codec codec = new BZip2Codec();
    codec.setConf(conf);
    Set<Decompressor> decompressors = new HashSet<Decompressor>();
    for (int i = 0; i < 10; ++i) {
      decompressors.add(CodecPool.getDecompressor(codec));
    }",1
"@Test
  public void testUncompressedInputWithLargeSplitSize() throws Exception {
    Configuration conf = new Configuration();
    // single char delimiter
    String inputData = ""abcde +fghij+ klmno+pqrst+uvwxyz"";
    Path inputFile = createInputFile(conf, inputData);
    conf.set(""textinputformat.record.delimiter"", ""+"");
    // split size over max value of integer
    long longSplitSize = (long)Integer.MAX_VALUE + 1;
    for (int bufferSize = 1; bufferSize <= inputData.length(); bufferSize++) {
      conf.setInt(""io.file.buffer.size"", bufferSize);
      testLargeSplitRecordForFile(conf, longSplitSize, inputData.length(),
          inputFile);
    }",1
"@Test
  public void testUncompressedInputCustomDelimiterPosValue()
      throws Exception {
    Configuration conf = new Configuration();
    conf.setInt(""io.file.buffer.size"", 10);
    conf.setInt(org.apache.hadoop.mapreduce.lib.input.
        LineRecordReader.MAX_LINE_LENGTH, Integer.MAX_VALUE);
    String inputData = ""abcdefghij++kl++mno"";
    Path inputFile = createInputFile(conf, inputData);
    String delimiter = ""++"";
    byte[] recordDelimiterBytes = delimiter.getBytes(StandardCharsets.UTF_8);
    // the first split must contain two records to make sure that it also pulls
    // in the record from the 2nd split
    int splitLength = 15;
    FileSplit split = new FileSplit(inputFile, 0, splitLength, (String[]) null);
    LineRecordReader reader = new LineRecordReader(conf, split,
        recordDelimiterBytes);
    LongWritable key = new LongWritable();
    Text value = new Text();
    // Get first record: ""abcdefghij""
    assertTrue(""Expected record got nothing"", reader.next(key, value));
    assertEquals(""Wrong length for record value"", 10, value.getLength());
    // Position should be 12 right after ""abcdefghij++""
    assertEquals(""Wrong position after record read"", 12, reader.getPos());
    // Get second record: ""kl""
    assertTrue(""Expected record got nothing"", reader.next(key, value));
    assertEquals(""Wrong length for record value"", 2, value.getLength());
    // Position should be 16 right after ""abcdefghij++kl++""
    assertEquals(""Wrong position after record read"", 16, reader.getPos());
    // Get third record: ""mno""
    assertTrue(""Expected record got nothing"", reader.next(key, value));
    assertEquals(""Wrong length for record value"", 3, value.getLength());
    // Position should be 19 right after ""abcdefghij++kl++mno""
    assertEquals(""Wrong position after record read"", 19, reader.getPos());
    assertFalse(reader.next(key, value));
    assertEquals(""Wrong position after record read"", 19, reader.getPos());
    reader.close();
    // No record is in the second split because the second split will drop
    // the first record, which was already reported by the first split.
    split = new FileSplit(inputFile, splitLength,
        inputData.length() - splitLength, (String[]) null);
    reader = new LineRecordReader(conf, split, recordDelimiterBytes);
    // The position should be 19 right after ""abcdefghij++kl++mno"" and should
    // not change
    assertEquals(""Wrong position after record read"", 19, reader.getPos());
    assertFalse(""Unexpected record returned"", reader.next(key, value));
    assertEquals(""Wrong position after record read"", 19, reader.getPos());
    reader.close();

    // multi char delimiter with starting part of the delimiter in the data
    inputData = ""abcd+efgh++ijk++mno"";
    inputFile = createInputFile(conf, inputData);
    splitLength = 5;
    split = new FileSplit(inputFile, 0, splitLength, (String[]) null);
    reader = new LineRecordReader(conf, split, recordDelimiterBytes);
    // Get first record: ""abcd+efgh""
    assertTrue(""Expected record got nothing"", reader.next(key, value));
    assertEquals(""Wrong position after record read"", 11, reader.getPos());
    assertEquals(""Wrong length for record value"", 9, value.getLength());
    // should have jumped over the delimiter, no record
    assertFalse(""Unexpected record returned"", reader.next(key, value));
    assertEquals(""Wrong position after record read"", 11, reader.getPos());
    reader.close();
    // next split: check for duplicate or dropped records
    split = new FileSplit(inputFile, splitLength,
        inputData.length() - splitLength, (String[]) null);
    reader = new LineRecordReader(conf, split, recordDelimiterBytes);
    // Get second record: ""ijk"" first in this split
    assertTrue(""Expected record got nothing"", reader.next(key, value));
    assertEquals(""Wrong position after record read"", 16, reader.getPos());
    assertEquals(""Wrong length for record value"", 3, value.getLength());
    // Get third record: ""mno"" second in this split
    assertTrue(""Expected record got nothing"", reader.next(key, value));
    assertEquals(""Wrong position after record read"", 19, reader.getPos());
    assertEquals(""Wrong length for record value"", 3, value.getLength());
    // should be at the end of the input
    assertFalse(reader.next(key, value));
    assertEquals(""Wrong position after record read"", 19, reader.getPos());
    reader.close();

    inputData = ""abcd|efgh|+|ij|kl|+|mno|pqr"";
    inputFile = createInputFile(conf, inputData);
    delimiter = ""|+|"";
    recordDelimiterBytes = delimiter.getBytes(StandardCharsets.UTF_8);
    // walking over the buffer and split sizes checks for proper processing
    // of the ambiguous bytes of the delimiter
    for (int bufferSize = 1; bufferSize <= inputData.length(); bufferSize++) {
      for (int splitSize = 1; splitSize < inputData.length(); splitSize++) {
        conf.setInt(""io.file.buffer.size"", bufferSize);
        split = new FileSplit(inputFile, 0, bufferSize, (String[]) null);
        reader = new LineRecordReader(conf, split, recordDelimiterBytes);
        // Get first record: ""abcd|efgh"" always possible
        assertTrue(""Expected record got nothing"", reader.next(key, value));
        assertThat(value.toString()).isEqualTo(""abcd|efgh"");
        assertEquals(""Wrong position after record read"", 9, value.getLength());
        // Position should be 12 right after ""|+|""
        int recordPos = 12;
        assertEquals(""Wrong position after record read"", recordPos,
            reader.getPos());
        // get the next record: ""ij|kl"" if the split/buffer allows it
        if (reader.next(key, value)) {
          // check the record info: ""ij|kl""
          assertThat(value.toString()).isEqualTo(""ij|kl"");
          // Position should be 20 right after ""|+|""
          recordPos = 20;
          assertEquals(""Wrong position after record read"", recordPos,
              reader.getPos());
        }",1
"@Test
  public void testAddInputPathWithMapper() {
    final JobConf conf = new JobConf();
    MultipleInputs.addInputPath(conf, new Path(""/foo""), TextInputFormat.class,
       MapClass.class);
    MultipleInputs.addInputPath(conf, new Path(""/bar""),
       KeyValueTextInputFormat.class, MapClass2.class);
    final Map<Path, InputFormat> inputs = MultipleInputs
       .getInputFormatMap(conf);
    final Map<Path, Class<? extends Mapper>> maps = MultipleInputs
       .getMapperTypeMap(conf);

    assertEquals(TextInputFormat.class, inputs.get(new Path(""/foo"")).getClass());
    assertEquals(KeyValueTextInputFormat.class, inputs.get(new Path(""/bar""))
       .getClass());
    assertEquals(MapClass.class, maps.get(new Path(""/foo"")));
    assertEquals(MapClass2.class, maps.get(new Path(""/bar"")));
  }",1
"@Test
  public void testRuntimeExRun() throws Exception {
    run(false, true);
  }",1
"@Test
  public void testMapFileOutputCommitterV1() throws Exception {
    testMapFileOutputCommitterInternal(1);
  }",1
"@Test
  public void testRecoveryV1() throws Exception {
    testRecoveryInternal(1, 1);
  }",1
"@Test
  public void testPathsWithFragNoSchemeAbsolute() throws IOException {
    ResourceConf.Builder b = new ResourceConf.Builder();
    b.setNumOfTmpFiles(5);
    b.setNumOfTmpLibJars(2);
    b.setNumOfTmpArchives(2);
    b.setJobJar(true);
    b.setPathsWithFrags(true);
    b.setPathsWithScheme(false);
    b.setAbsolutePaths(true);
    ResourceConf rConf = b.build();
    JobConf jConf = new JobConf();
    JobResourceUploader uploader = new StubedUploader(jConf);

    runTmpResourcePathTest(uploader, rConf, jConf, expectedFilesWithFrags,
        expectedArchivesWithFrags, basicExpectedJobJar);
  }",1
"@Test
  public void testPathsWithNoFragWithSchemeAbsolute() throws IOException {
    ResourceConf.Builder b = new ResourceConf.Builder();
    b.setNumOfTmpFiles(5);
    b.setNumOfTmpLibJars(2);
    b.setNumOfTmpArchives(2);
    b.setJobJar(true);
    b.setPathsWithFrags(false);
    b.setPathsWithScheme(true);
    b.setAbsolutePaths(true);
    ResourceConf rConf = b.build();
    JobConf jConf = new JobConf();
    JobResourceUploader uploader = new StubedUploader(jConf);

    runTmpResourcePathTest(uploader, rConf, jConf, expectedFilesNoFrags,
        expectedArchivesNoFrags, basicExpectedJobJar);
  }",1
"@Test
  public void testMRAsyncDiskService() throws Throwable {
  
    FileSystem localFileSystem = FileSystem.getLocal(new Configuration());
    String[] vols = new String[]{TEST_ROOT_DIR + ""/0"",
        TEST_ROOT_DIR + ""/1""}",1
"@Test
  public void testMRWebAppRedirection() throws Exception {

    String[] schemePrefix =
        { WebAppUtils.HTTP_PREFIX, WebAppUtils.HTTPS_PREFIX }",1
"@Test
  public void testBlacklistedNodesXML() throws Exception {
    WebResource r = resource();
    ClientResponse response = r.path(""ws"").path(""v1"").path(""mapreduce"")
        .path(""blacklistednodes"").accept(MediaType.APPLICATION_XML)
        .get(ClientResponse.class);
    assertEquals(MediaType.APPLICATION_XML + ""; "" + JettyUtils.UTF_8,
        response.getType().toString());
    String xml = response.getEntity(String.class);
    verifyBlacklistedNodesInfoXML(xml, appContext);
  }",1
"@Test
  public void testCreateDirsWithoutFileSystem() throws Exception {
    Configuration conf = new YarnConfiguration();
    conf.set(FileSystem.FS_DEFAULT_NAME_KEY, ""hdfs://localhost:1"");
    testTryCreateHistoryDirs(conf, false);
  }",1
"@Test
  public void testGetLocalInetAddress() throws Exception {
    assertNotNull(NetUtils.getLocalInetAddress(""127.0.0.1""));
    assertNull(NetUtils.getLocalInetAddress(""invalid-address-for-test""));
    assertNull(NetUtils.getLocalInetAddress(null));
  }",1
"@Test
  public void testClearingCachedMappings() throws IOException {
    File mapFile = File.createTempFile(getClass().getSimpleName() +
        "".testClearingCachedMappings"", "".txt"");
    Files.asCharSink(mapFile, StandardCharsets.UTF_8).write(
        hostName1 + "" /rack1\n"" + hostName2 + ""\t/rack2\n"");
    mapFile.deleteOnExit();

    TableMapping mapping = new TableMapping();

    Configuration conf = new Configuration();
    conf.set(NET_TOPOLOGY_TABLE_MAPPING_FILE_KEY, mapFile.getCanonicalPath());
    mapping.setConf(conf);

    List<String> names = new ArrayList<String>();
    names.add(hostName1);
    names.add(hostName2);

    List<String> result = mapping.resolve(names);
    assertEquals(names.size(), result.size());
    assertEquals(""/rack1"", result.get(0));
    assertEquals(""/rack2"", result.get(1));

    Files.asCharSink(mapFile, StandardCharsets.UTF_8).write("""");

    mapping.reloadCachedMappings();

    names = new ArrayList<String>();
    names.add(hostName1);
    names.add(hostName2);

    result = mapping.resolve(names);
    assertEquals(names.size(), result.size());
    assertEquals(NetworkTopology.DEFAULT_RACK, result.get(0));
    assertEquals(NetworkTopology.DEFAULT_RACK, result.get(1));
  }",1
"@Test
  public void testRMRf() throws Throwable {
    mkPath(""/rm"", CreateMode.PERSISTENT);
    mkPath(""/rm/child"", CreateMode.PERSISTENT);
    curatorService.zkDelete(""/rm"", true, null);
    verifyNotExists(""/rm"");
    curatorService.zkDelete(""/rm"", true, null);
  }",1
"@Test
  public void testVerifyExists() throws Throwable {
    pathMustExist(""/"");
  }",1
"@Test
  public void testExists() throws IOException {
    System.out.println(""pre-create test path"");
    fs.mkdirs(new Path(""test/registryTestNode""));

    System.out.println(""Check for existing node"");
    boolean exists = registry.exists(""test/registryTestNode"");
    Assert.assertTrue(exists);

    System.out.println(""Check for  non-existing node"");
    exists = registry.exists(""test/nonExistentNode"");
    Assert.assertFalse(exists);
  }",1
"@Test
  public void testDeleteServiceEntry() throws Throwable {
    putExampleServiceEntry(ENTRY_PATH, 0);
    operations.delete(ENTRY_PATH, false);
  }",1
"@Test
  public void testMkdirNoParent() throws Throwable {
    String path = ENTRY_PATH + ""/missing"";
    try {
      operations.mknode(path, false);
      RegistryPathStatus stat = operations.stat(path);
      fail(""Got a status "" + stat);
    }",1
"@Test
  public void testPutGetServiceEntry() throws Throwable {
    ServiceRecord written = putExampleServiceEntry(ENTRY_PATH, 0,
        PersistencePolicies.APPLICATION);
    ServiceRecord resolved = operations.resolve(ENTRY_PATH);
    validateEntry(resolved);
    assertMatches(written, resolved);
  }",1
"@Test
  public void testStatDirectory() throws Throwable {
    String empty = ""/empty"";
    operations.mknode(empty, false);
    operations.stat(empty);
  }",1
"@Test
  public void testStatRootPath() throws Throwable {
    operations.mknode(""/"", false);
    operations.stat(""/"");
    operations.list(""/"");
    operations.list(""/"");
  }",1
"@Test
  public void testAAAALookup() throws Exception {
    ServiceRecord record = getMarshal().fromBytes(""somepath"",
        CONTAINER_RECORD.getBytes());
    getRegistryDNS().register(
        ""/registry/users/root/services/org-apache-slider/test1/components/""
            + ""ctr-e50-1451931954322-0016-01-000002"",
        record);

    // start assessing whether correct records are available
    List<Record> recs = assertDNSQuery(
        ""ctr-e50-1451931954322-0016-01-000002.dev.test."", Type.AAAA, 1);
    assertEquals(""wrong result"", ""172.17.0.19"",
        ((AAAARecord) recs.get(0)).getAddress().getHostAddress());

    recs = assertDNSQuery(""httpd-1.test1.root.dev.test."", Type.AAAA, 1);
    assertTrue(""not an ARecord"", recs.get(0) instanceof AAAARecord);
  }",1
"@Test
  public void testContainerRegistrationPersistanceAbsent() throws Exception {
    ServiceRecord record = marshal.fromBytes(""somepath"",
        CONTAINER_RECORD_YARN_PERSISTANCE_ABSENT.getBytes());
    registryDNS.register(
        ""/registry/users/root/services/org-apache-slider/test1/components/""
            + ""ctr-e50-1451931954322-0016-01-000003"",
         record);

    Name name =
        Name.fromString(""ctr-e50-1451931954322-0016-01-000002.dev.test."");
    Record question = Record.newRecord(name, Type.A, DClass.IN);
    Message query = Message.newQuery(question);
    byte[] responseBytes = registryDNS.generateReply(query, null);
    Message response = new Message(responseBytes);
    assertEquals(""Excepting NXDOMAIN as Record must not have regsisterd wrong"",
        Rcode.NXDOMAIN, response.getRcode());
  }",1
"@Test
  public void testNegativeLookup() throws Exception {
    ServiceRecord record = getMarshal().fromBytes(""somepath"",
        CONTAINER_RECORD.getBytes());
    getRegistryDNS().register(
        ""/registry/users/root/services/org-apache-slider/test1/components/""
            + ""ctr-e50-1451931954322-0016-01-000002"",
        record);

    // start assessing whether correct records are available
    Name name = Name.fromString(""missing.dev.test."");
    Record question = Record.newRecord(name, Type.A, DClass.IN);
    Message query = Message.newQuery(question);

    byte[] responseBytes = getRegistryDNS().generateReply(query, null);
    Message response = new Message(responseBytes);
    assertEquals(""not successful"", Rcode.NXDOMAIN, response.getRcode());
    assertNotNull(""Null response"", response);
    assertEquals(""Questions do not match"", query.getQuestion(),
        response.getQuestion());
    List<Record> sectionArray = response.getSection(Section.AUTHORITY);
    assertEquals(""Wrong number of recs in AUTHORITY"", isSecure() ? 2 : 1,
        sectionArray.size());
    boolean soaFound = false;
    for (Record rec : sectionArray) {
      soaFound = rec.getType() == Type.SOA;
      if (soaFound) {
        break;
      }",1
"@Test
  public void testReverseLookup() throws Exception {
    ServiceRecord record = getMarshal().fromBytes(""somepath"",
        CONTAINER_RECORD.getBytes());
    getRegistryDNS().register(
        ""/registry/users/root/services/org-apache-slider/test1/components/""
            + ""ctr-e50-1451931954322-0016-01-000002"",
        record);

    // start assessing whether correct records are available
    List<Record> recs = assertDNSQuery(
        ""19.0.17.172.in-addr.arpa."", Type.PTR, 1);
    assertEquals(""wrong result"",
        ""httpd-1.test1.root.dev.test."",
        ((PTRRecord) recs.get(0)).getTarget().toString());
  }",1
"@Test
  public void testJksProvider() throws Exception {
    Configuration conf = new Configuration();
    final Path jksPath = new Path(tmpDir.toString(), ""test.jks"");
    final String ourUrl =
        JavaKeyStoreProvider.SCHEME_NAME + ""://file"" + jksPath.toUri();

    File file = new File(tmpDir, ""test.jks"");
    file.delete();
    conf.set(CredentialProviderFactory.CREDENTIAL_PROVIDER_PATH, ourUrl);
    checkSpecificProvider(conf, ourUrl);
    Path path = ProviderUtils.unnestUri(new URI(ourUrl));
    FileSystem fs = path.getFileSystem(conf);
    FileStatus s = fs.getFileStatus(path);
    assertEquals(""rw-------"", s.getPermission().toString());
    assertTrue(file + "" should exist"", file.isFile());

    // check permission retention after explicit change
    fs.setPermission(path, new FsPermission(""777""));
    checkPermissionRetention(conf, ourUrl, path);
  }",1
"@Test
  public void testGetSecrets() throws Exception {
    File testDir = new File(System.getProperty(""test.build.data"",
        ""target/test-dir""));
    testDir.mkdirs();
    String secretValue = ""hadoop"";
    File secretFile = new File(testDir, ""http-secret.txt"");
    Writer writer = new FileWriter(secretFile);
    writer.write(secretValue);
    writer.close();

    FileSignerSecretProvider secretProvider
            = new FileSignerSecretProvider();
    Properties secretProviderProps = new Properties();
    secretProviderProps.setProperty(
            AuthenticationFilter.SIGNATURE_SECRET_FILE,
        secretFile.getAbsolutePath());
    secretProvider.init(secretProviderProps, null, -1);
    Assert.assertArrayEquals(secretValue.getBytes(),
        secretProvider.getCurrentSecret());
    byte[][] allSecrets = secretProvider.getAllSecrets();
    Assert.assertEquals(1, allSecrets.length);
    Assert.assertArrayEquals(secretValue.getBytes(), allSecrets[0]);
  }",1
"@Test
  public void testFileOutput() throws Throwable {
    File f = new File(""target/kdiag.txt"");
    kdiag(ARG_KEYLEN, KEYLEN,
        ARG_KEYTAB, keytab.getAbsolutePath(),
        ARG_PRINCIPAL, ""foo@EXAMPLE.COM"",
        ARG_OUTPUT, f.getAbsolutePath());
    LOG.info(""Output of {}",1
"@Test
  public void testKeytabAndPrincipal() throws Throwable {
    kdiag(ARG_KEYLEN, KEYLEN,
        ARG_KEYTAB, keytab.getAbsolutePath(),
        ARG_PRINCIPAL, ""foo@EXAMPLE.COM"");
  }",1
"@Test
  public void testNoKeytab() throws Throwable {
    kdiagFailure(CAT_KERBEROS, ARG_KEYLEN, KEYLEN,
        ARG_KEYTAB, ""target/nofile"");
  }",1
"@Test
  public void testSecure() throws Throwable {
    kdiagFailure(CAT_CONFIG, ARG_KEYLEN, KEYLEN, ARG_SECURE);
  }",1
"@Test
  public void testCreate() throws Exception {
    Configuration conf = new HdfsConfiguration();
    conf.setBoolean(DFSConfigKeys.DFS_PERMISSIONS_ENABLED_KEY, true);
    conf.set(FsPermission.UMASK_LABEL, ""000"");
    MiniDFSCluster cluster = null;
    FileSystem fs = null;

    try {
      cluster = new MiniDFSCluster.Builder(conf).numDataNodes(3).build();
      cluster.waitActive();
      fs = FileSystem.get(conf);
      FsPermission rootPerm = checkPermission(fs, ""/"", null);
      FsPermission inheritPerm = FsPermission.createImmutable(
          (short)(rootPerm.toShort() | 0300));

      FsPermission dirPerm = new FsPermission((short)0777);
      fs.mkdirs(new Path(""/a1/a2/a3""), dirPerm);
      checkPermission(fs, ""/a1"", dirPerm);
      checkPermission(fs, ""/a1/a2"", dirPerm);
      checkPermission(fs, ""/a1/a2/a3"", dirPerm);

      dirPerm = new FsPermission((short)0123);
      FsPermission permission = FsPermission.createImmutable(
        (short)(dirPerm.toShort() | 0300));
      fs.mkdirs(new Path(""/aa/1/aa/2/aa/3""), dirPerm);
      checkPermission(fs, ""/aa/1"", permission);
      checkPermission(fs, ""/aa/1/aa/2"", permission);
      checkPermission(fs, ""/aa/1/aa/2/aa/3"", dirPerm);

      FsPermission filePerm = new FsPermission((short)0444);
      Path p = new Path(""/b1/b2/b3.txt"");
      FSDataOutputStream out = fs.create(p, filePerm,
          true, conf.getInt(CommonConfigurationKeys.IO_FILE_BUFFER_SIZE_KEY, 4096),
          fs.getDefaultReplication(p), fs.getDefaultBlockSize(p), null);
      out.write(123);
      out.close();
      checkPermission(fs, ""/b1"", inheritPerm);
      checkPermission(fs, ""/b1/b2"", inheritPerm);
      checkPermission(fs, ""/b1/b2/b3.txt"", filePerm);
      
      conf.set(FsPermission.UMASK_LABEL, ""022"");
      permission = 
        FsPermission.createImmutable((short)0666);
      FileSystem.mkdirs(fs, new Path(""/c1""), new FsPermission(permission));
      FileSystem.create(fs, new Path(""/c1/c2.txt""),
          new FsPermission(permission));
      checkPermission(fs, ""/c1"", permission);
      checkPermission(fs, ""/c1/c2.txt"", permission);
    }",1
"@Test
  public void testStaticMapUpdate() throws IOException {
    assumeNotWindows();
    File tempStaticMapFile = File.createTempFile(""nfs-"", "".map"");
    tempStaticMapFile.delete();
    Configuration conf = new Configuration();
    conf.setLong(IdMappingConstant.USERGROUPID_UPDATE_MILLIS_KEY, 1000);    
    conf.set(IdMappingConstant.STATIC_ID_MAPPING_FILE_KEY,
        tempStaticMapFile.getPath());

    ShellBasedIdMapping refIdMapping =
        new ShellBasedIdMapping(conf, true);
    ShellBasedIdMapping incrIdMapping = new ShellBasedIdMapping(conf);

    BiMap<Integer, String> uidNameMap = refIdMapping.getUidNameMap();
    BiMap<Integer, String> gidNameMap = refIdMapping.getGidNameMap();

    // Force empty map, to see effect of incremental map update of calling
    // getUid()
    incrIdMapping.clearNameMaps();
    uidNameMap = refIdMapping.getUidNameMap();
    for (BiMap.Entry<Integer, String> me : uidNameMap.entrySet()) {
      tempStaticMapFile.delete();
      incrIdMapping.clearNameMaps();
      Integer id = me.getKey();
      String name = me.getValue();

      // The static map is empty, so the id found for ""name"" would be
      // the same as ""id""
      Integer nid = incrIdMapping.getUid(name);
      assertEquals(id, nid);
      
      // Clear map and update staticMap file
      incrIdMapping.clearNameMaps();
      Integer rid = id + 10000;
      String smapStr = ""uid "" + rid + "" "" + id;
      createStaticMapFile(tempStaticMapFile, smapStr);

      // Now the id found for ""name"" should be the id specified by
      // the staticMap
      nid = incrIdMapping.getUid(name);
      assertEquals(rid, nid);
    }",1
"@Test
  public void testExternalTokenFiles() throws Exception {
    StringBuilder tokenFullPathnames = new StringBuilder();
    String tokenFilenames = ""token1,token2"";
    String tokenFiles[] = StringUtils.getTrimmedStrings(tokenFilenames);
    final File testDir = new File(""target"",
        TestUserGroupInformation.class.getName() + ""-tmpDir"").getAbsoluteFile();
    String testDirPath = testDir.getAbsolutePath();

    // create path for token files
    for (String tokenFile: tokenFiles) {
      if (tokenFullPathnames.length() > 0) {
        tokenFullPathnames.append("","");
      }",1
"@Test
  public void testFixedAndLocalWhiteList() throws IOException {

    String[] fixedIps = {""10.119.103.112"", ""10.221.102.0/23""}",1
"@Test
  public void testCommandLine() throws Exception {
    try {
      try {
        FileUtil.fullyDelete(OUTPUT_DIR.getAbsoluteFile());
      }",1
"@Test
  public void testAtomicCommitExistingFinal() throws IOException {
    TaskAttemptContext taskAttemptContext = getTaskAttemptContext(config);
    JobContext jobContext = new JobContextImpl(taskAttemptContext.getConfiguration(),
        taskAttemptContext.getTaskAttemptID().getJobID());
    Configuration conf = jobContext.getConfiguration();


    String workPath = ""/tmp1/"" + String.valueOf(rand.nextLong());
    String finalPath = ""/tmp1/"" + String.valueOf(rand.nextLong());
    FileSystem fs = null;
    try {
      OutputCommitter committer = new CopyCommitter(null, taskAttemptContext);
      fs = FileSystem.get(conf);
      fs.mkdirs(new Path(workPath));
      fs.mkdirs(new Path(finalPath));

      conf.set(CONF_LABEL_TARGET_WORK_PATH, workPath);
      conf.set(CONF_LABEL_TARGET_FINAL_PATH, finalPath);
      conf.setBoolean(DistCpConstants.CONF_LABEL_ATOMIC_COPY, true);

      assertPathExists(fs, ""Work path"", new Path(workPath));
      assertPathExists(fs, ""Final path"", new Path(finalPath));
      try {
        committer.commitJob(jobContext);
        Assert.fail(""Should not be able to atomic-commit to pre-existing path."");
      }",1
"@Test
  public void testFileInRootDir() throws Throwable {
    expectShouldDelete(FILE0, false);
    expectShouldDelete(FILE0, false);
  }",1
"@Test
  public void testAppendOption() {
    final DistCpOptions.Builder builder = new DistCpOptions.Builder(
        Collections.singletonList(new Path(""hdfs://localhost:8020/source"")),
        new Path(""hdfs://localhost:8020/target/""))
        .withSyncFolder(true)
        .withAppend(true);
    Assert.assertTrue(builder.build().shouldAppend());

    try {
      // make sure -append is only valid when -update is specified
      new DistCpOptions.Builder(
          Collections.singletonList(new Path(""hdfs://localhost:8020/source"")),
          new Path(""hdfs://localhost:8020/target/""))
          .withAppend(true)
          .build();
      fail(""Append should fail if update option is not specified"");
    }",1
"@Test
  public void testPreserve() {
    DistCpOptions options = new DistCpOptions.Builder(
        new Path(""hdfs://localhost:8020/source/first""),
        new Path(""hdfs://localhost:8020/target/""))
        .build();
    Assert.assertFalse(options.shouldPreserve(FileAttribute.BLOCKSIZE));
    Assert.assertFalse(options.shouldPreserve(FileAttribute.REPLICATION));
    Assert.assertFalse(options.shouldPreserve(FileAttribute.PERMISSION));
    Assert.assertFalse(options.shouldPreserve(FileAttribute.USER));
    Assert.assertFalse(options.shouldPreserve(FileAttribute.GROUP));
    Assert.assertFalse(options.shouldPreserve(FileAttribute.CHECKSUMTYPE));

    options = new DistCpOptions.Builder(
        new Path(""hdfs://localhost:8020/source/first""),
        new Path(""hdfs://localhost:8020/target/""))
        .preserve(FileAttribute.ACL)
        .build();
    Assert.assertFalse(options.shouldPreserve(FileAttribute.BLOCKSIZE));
    Assert.assertFalse(options.shouldPreserve(FileAttribute.REPLICATION));
    Assert.assertFalse(options.shouldPreserve(FileAttribute.PERMISSION));
    Assert.assertFalse(options.shouldPreserve(FileAttribute.USER));
    Assert.assertFalse(options.shouldPreserve(FileAttribute.GROUP));
    Assert.assertFalse(options.shouldPreserve(FileAttribute.CHECKSUMTYPE));
    Assert.assertTrue(options.shouldPreserve(FileAttribute.ACL));

    options = new DistCpOptions.Builder(
        new Path(""hdfs://localhost:8020/source/first""),
        new Path(""hdfs://localhost:8020/target/""))
        .preserve(FileAttribute.BLOCKSIZE)
        .preserve(FileAttribute.REPLICATION)
        .preserve(FileAttribute.PERMISSION)
        .preserve(FileAttribute.USER)
        .preserve(FileAttribute.GROUP)
        .preserve(FileAttribute.CHECKSUMTYPE)
        .build();

    Assert.assertTrue(options.shouldPreserve(FileAttribute.BLOCKSIZE));
    Assert.assertTrue(options.shouldPreserve(FileAttribute.REPLICATION));
    Assert.assertTrue(options.shouldPreserve(FileAttribute.PERMISSION));
    Assert.assertTrue(options.shouldPreserve(FileAttribute.USER));
    Assert.assertTrue(options.shouldPreserve(FileAttribute.GROUP));
    Assert.assertTrue(options.shouldPreserve(FileAttribute.CHECKSUMTYPE));
    Assert.assertFalse(options.shouldPreserve(FileAttribute.XATTR));
  }",1
"@Test
  public void testSetNumListtatusThreads() {
    final DistCpOptions.Builder builder = new DistCpOptions.Builder(
        new Path(""hdfs://localhost:8020/source/first""),
        new Path(""hdfs://localhost:8020/target/""));
    // If command line argument isn't set, we expect .getNumListstatusThreads
    // option to be zero (so that we know when to override conf properties).
    Assert.assertEquals(0, builder.build().getNumListstatusThreads());

    builder.withNumListstatusThreads(12);
    Assert.assertEquals(12, builder.build().getNumListstatusThreads());

    builder.withNumListstatusThreads(0);
    Assert.assertEquals(0, builder.build().getNumListstatusThreads());

    // Ignore large number of threads.
    builder.withNumListstatusThreads(MAX_NUM_LISTSTATUS_THREADS * 2);
    Assert.assertEquals(MAX_NUM_LISTSTATUS_THREADS,
        builder.build().getNumListstatusThreads());
  }",1
"@Test
  public void testSetOverwrite() {
    final DistCpOptions.Builder builder = new DistCpOptions.Builder(
        Collections.singletonList(new Path(""hdfs://localhost:8020/source"")),
        new Path(""hdfs://localhost:8020/target/""));
    Assert.assertFalse(builder.build().shouldOverwrite());

    builder.withOverwrite(true);
    Assert.assertTrue(builder.build().shouldOverwrite());

    try {
      builder.withSyncFolder(true).build();
      Assert.fail(""Update and overwrite aren't allowed together"");
    }",1
"@Test
  public void testSetWorkPath() {
    final DistCpOptions.Builder builder = new DistCpOptions.Builder(
        Collections.singletonList(new Path(""hdfs://localhost:8020/source"")),
        new Path(""hdfs://localhost:8020/target/""));
    Assert.assertNull(builder.build().getAtomicWorkPath());

    builder.withAtomicCommit(true);
    Assert.assertNull(builder.build().getAtomicWorkPath());

    final Path workPath = new Path(""hdfs://localhost:8020/work"");
    builder.withAtomicWorkPath(workPath);
    Assert.assertEquals(workPath, builder.build().getAtomicWorkPath());
  }",1
"@Test
  public void testVerboseLog() {
    final DistCpOptions.Builder builder = new DistCpOptions.Builder(
        Collections.singletonList(new Path(""hdfs://localhost:8020/source"")),
        new Path(""hdfs://localhost:8020/target/""));
    Assert.assertFalse(builder.build().shouldVerboseLog());

    try {
      builder.withVerboseLog(true).build();
      fail(""-v should fail if -log option is not specified"");
    }",1
"@Test
  public void testFallback() throws Exception {
    // the source/target dir are not snapshottable dir
    Assert.assertFalse(sync());
    // make sure the source path has been updated to the snapshot path
    final Path spath = new Path(source,
        HdfsConstants.DOT_SNAPSHOT_DIR + Path.SEPARATOR + ""s2"");
    Assert.assertEquals(spath, context.getSourcePaths().get(0));

    // reset source path in options
    context.setSourcePaths(Collections.singletonList(source));
    // the source/target does not have the given snapshots
    dfs.allowSnapshot(source);
    dfs.allowSnapshot(target);
    Assert.assertFalse(sync());
    Assert.assertEquals(spath, context.getSourcePaths().get(0));

    // reset source path in options
    context.setSourcePaths(Collections.singletonList(source));
    dfs.createSnapshot(source, ""s1"");
    dfs.createSnapshot(source, ""s2"");
    dfs.createSnapshot(target, ""s1"");
    Assert.assertTrue(sync());

    // reset source paths in options
    context.setSourcePaths(Collections.singletonList(source));
    // changes have been made in target
    final Path subTarget = new Path(target, ""sub"");
    dfs.mkdirs(subTarget);
    Assert.assertFalse(sync());
    // make sure the source path has been updated to the snapshot path
    Assert.assertEquals(spath, context.getSourcePaths().get(0));

    // reset source paths in options
    context.setSourcePaths(Collections.singletonList(source));
    dfs.delete(subTarget, true);
    Assert.assertTrue(sync());
  }",1
"@Test
  public void testSync7() throws Exception {
    initData7(source);
    initData7(target);
    enableAndCreateFirstSnapshot();
    int numCreatedModified = changeData7(source);
    dfs.createSnapshot(source, ""s2"");

    testAndVerify(numCreatedModified);
  }",1
"@Test
  public void testMultiFileTargetMissing() {
    caseMultiFileTargetMissing(false);
    caseMultiFileTargetMissing(true);
  }",1
"@Test
  public void testMultiFileTargetPresent() {
    caseMultiFileTargetPresent(false);
    caseMultiFileTargetPresent(true);
  }",1
"@Test
  public void testSingleFileTargetFile() {
    caseSingleFileTargetFile(false);
    caseSingleFileTargetFile(true);
  }",1
"@Test
  public void testRun() throws Exception {
    final URI uri = cluster.getFileSystem().getUri();
    final String pathString = uri.toString();
    Path fileSystemPath = new Path(pathString);
    Path source = new Path(fileSystemPath.toString() + ""/tmp/source"");
    Path target = new Path(fileSystemPath.toString() + ""/tmp/target"");
    Path listingPath = new Path(fileSystemPath.toString() + ""/tmp/META/fileList.seq"");
    DistCpOptions options = new DistCpOptions.Builder(
        Collections.singletonList(source), target).build();
    DistCpContext context = new DistCpContext(options);
    context.setTargetPathExists(false);
    new GlobbedCopyListing(new Configuration(), CREDENTIALS)
        .buildListing(listingPath, context);

    verifyContents(listingPath);
  }",1
"@Test
  public void testGlobFiles() throws Exception {
    final Path sub1 = new Path(inputPath, ""dir1"");
    final Path sub2 = new Path(inputPath, ""dir2"");
    fs.mkdirs(sub1);
    String fileName = ""a"";
    createFile(inputPath, fs, sub1.getName(), fileName);
    createFile(inputPath, fs, sub2.getName(), fileName);
    createFile(inputPath, fs, sub1.getName(), ""b""); // not part of result

    final String glob =  ""dir{1,2}",1
"@Test
  public void testRelativePath() throws Exception {
    final Path sub1 = new Path(inputPath, ""dir1"");
    fs.mkdirs(sub1);
    createFile(inputPath, fs, sub1.getName(), ""a"");
    final FsShell shell = new FsShell(conf);

    final List<String> originalPaths = lsr(shell, ""input"");
    System.out.println(""originalPaths: "" + originalPaths);

    // make the archive:
    final String fullHarPathStr = makeArchive();

    // compare results:
    final List<String> harPaths = lsr(shell, fullHarPathStr);
    Assert.assertEquals(originalPaths, harPaths);
  }",1
"@Test
  public void testExclusionsOption() {
    DistCpOptions options = OptionsParser.parse(new String[] {
        ""hdfs://localhost:8020/source/first"",
        ""hdfs://localhost:8020/target/""}",1
"@Test
  public void testPreserveOnFileUpwardRecursion() throws IOException {
    FileSystem fs = FileSystem.get(config);
    EnumSet<FileAttribute> attributes = EnumSet.allOf(FileAttribute.class);
    // Remove ACL because tests run with dfs.namenode.acls.enabled false
    attributes.remove(FileAttribute.ACL);
    
    Path src = new Path(""/tmp/src2"");
    Path f0 = new Path(""/f0"");
    Path f1 = new Path(""/d1/f1"");
    Path f2 = new Path(""/d1/d2/f2"");
    Path d1 = new Path(""/d1/"");
    Path d2 = new Path(""/d1/d2/"");

    createFile(fs, src);
    createFile(fs, f0);
    createFile(fs, f1);
    createFile(fs, f2);

    fs.setPermission(src, almostFullPerm);
    fs.setOwner(src, ""somebody"", ""somebody-group"");
    fs.setTimes(src, 0, 0);
    fs.setReplication(src, (short) 1);

    fs.setPermission(d1, fullPerm);
    fs.setOwner(d1, ""anybody"", ""anybody-group"");
    fs.setTimes(d1, 400, 400);
    fs.setReplication(d1, (short) 3);

    fs.setPermission(d2, fullPerm);
    fs.setOwner(d2, ""anybody"", ""anybody-group"");
    fs.setTimes(d2, 300, 300);
    fs.setReplication(d2, (short) 3);

    fs.setPermission(f0, fullPerm);
    fs.setOwner(f0, ""anybody"", ""anybody-group"");
    fs.setTimes(f0, 200, 200);
    fs.setReplication(f0, (short) 3);

    fs.setPermission(f1, fullPerm);
    fs.setOwner(f1, ""anybody"", ""anybody-group"");
    fs.setTimes(f1, 200, 200);
    fs.setReplication(f1, (short) 3);

    fs.setPermission(f2, fullPerm);
    fs.setOwner(f2, ""anybody"", ""anybody-group"");
    fs.setTimes(f2, 200, 200);
    fs.setReplication(f2, (short) 3);

    CopyListingFileStatus srcStatus = new CopyListingFileStatus(fs.getFileStatus(src));

    DistCpUtils.preserve(fs, f2, srcStatus, attributes, false);

    cluster.triggerHeartbeats();

    // FileStatus.equals only compares path field, must explicitly compare all fields
    // attributes of src -> f2 ? should be yes
    assertStatusEqual(fs, f2, srcStatus);

    // attributes of src -> f1 ? should be no
    CopyListingFileStatus f1Status = new CopyListingFileStatus(fs.getFileStatus(f1));
    Assert.assertFalse(srcStatus.getPermission().equals(f1Status.getPermission()));
    Assert.assertFalse(srcStatus.getOwner().equals(f1Status.getOwner()));
    Assert.assertFalse(srcStatus.getGroup().equals(f1Status.getGroup()));
    Assert.assertFalse(srcStatus.getAccessTime() == f1Status.getAccessTime());
    Assert.assertFalse(srcStatus.getModificationTime() == f1Status.getModificationTime());
    Assert.assertFalse(srcStatus.getReplication() == f1Status.getReplication());

    // attributes of src -> f0 ? should be no
    CopyListingFileStatus f0Status = new CopyListingFileStatus(fs.getFileStatus(f0));
    Assert.assertFalse(srcStatus.getPermission().equals(f0Status.getPermission()));
    Assert.assertFalse(srcStatus.getOwner().equals(f0Status.getOwner()));
    Assert.assertFalse(srcStatus.getGroup().equals(f0Status.getGroup()));
    Assert.assertFalse(srcStatus.getAccessTime() == f0Status.getAccessTime());
    Assert.assertFalse(srcStatus.getModificationTime() == f0Status.getModificationTime());
    Assert.assertFalse(srcStatus.getReplication() == f0Status.getReplication());

    // attributes of src -> d2 ? should be no
    CopyListingFileStatus d2Status = new CopyListingFileStatus(fs.getFileStatus(d2));
    Assert.assertFalse(srcStatus.getPermission().equals(d2Status.getPermission()));
    Assert.assertFalse(srcStatus.getOwner().equals(d2Status.getOwner()));
    Assert.assertFalse(srcStatus.getGroup().equals(d2Status.getGroup()));
    Assert.assertTrue(d2Status.getAccessTime() == 300);
    Assert.assertTrue(d2Status.getModificationTime() == 300);
    Assert.assertFalse(srcStatus.getReplication() == d2Status.getReplication());

    // attributes of src -> d1 ? should be no
    CopyListingFileStatus d1Status = new CopyListingFileStatus(fs.getFileStatus(d1));
    Assert.assertFalse(srcStatus.getPermission().equals(d1Status.getPermission()));
    Assert.assertFalse(srcStatus.getOwner().equals(d1Status.getOwner()));
    Assert.assertFalse(srcStatus.getGroup().equals(d1Status.getGroup()));
    Assert.assertTrue(d1Status.getAccessTime() == 400);
    Assert.assertTrue(d1Status.getModificationTime() == 400);
    Assert.assertFalse(srcStatus.getReplication() == d1Status.getReplication());
  }",1
"@Test
  public void testPreserveUserOnFile() throws IOException {
    FileSystem fs = FileSystem.get(config);
    EnumSet<FileAttribute> attributes = EnumSet.of(FileAttribute.USER);

    Path dst = new Path(""/tmp/dest2"");
    Path src = new Path(""/tmp/src2"");

    createFile(fs, src);
    createFile(fs, dst);

    fs.setPermission(src, fullPerm);
    fs.setOwner(src, ""somebody"", ""somebody-group"");
    fs.setTimes(src, 0, 0);
    fs.setReplication(src, (short) 1);

    fs.setPermission(dst, noPerm);
    fs.setOwner(dst, ""nobody"", ""nobody-group"");
    fs.setTimes(dst, 100, 100);
    fs.setReplication(dst, (short) 2);

    CopyListingFileStatus srcStatus = new CopyListingFileStatus(fs.getFileStatus(src));

    DistCpUtils.preserve(fs, dst, srcStatus, attributes, false);

    CopyListingFileStatus dstStatus = new CopyListingFileStatus(fs.getFileStatus(dst));

    // FileStatus.equals only compares path field, must explicitly compare all fields
    Assert.assertFalse(srcStatus.getPermission().equals(dstStatus.getPermission()));
    Assert.assertTrue(srcStatus.getOwner().equals(dstStatus.getOwner()));
    Assert.assertFalse(srcStatus.getGroup().equals(dstStatus.getGroup()));
    Assert.assertFalse(srcStatus.getAccessTime() == dstStatus.getAccessTime());
    Assert.assertFalse(srcStatus.getModificationTime() == dstStatus.getModificationTime());
    Assert.assertFalse(srcStatus.getReplication() == dstStatus.getReplication());
  }",1
"@Test
  public void testConstructUrlsFromClasspath() throws Exception {
    File file = new File(testDir, ""file"");
    assertTrue(""Create file"", file.createNewFile());

    File dir = new File(testDir, ""dir"");
    assertTrue(""Make dir"", dir.mkdir());

    File jarsDir = new File(testDir, ""jarsdir"");
    assertTrue(""Make jarsDir"", jarsDir.mkdir());
    File nonJarFile = new File(jarsDir, ""nonjar"");
    assertTrue(""Create non-jar file"", nonJarFile.createNewFile());
    File jarFile = new File(jarsDir, ""a.jar"");
    assertTrue(""Create jar file"", jarFile.createNewFile());

    File nofile = new File(testDir, ""nofile"");
    // don't create nofile

    StringBuilder cp = new StringBuilder();
    cp.append(file.getAbsolutePath()).append(File.pathSeparator)
      .append(dir.getAbsolutePath()).append(File.pathSeparator)
      .append(jarsDir.getAbsolutePath() + ""/*"").append(File.pathSeparator)
      .append(nofile.getAbsolutePath()).append(File.pathSeparator)
      .append(nofile.getAbsolutePath() + ""/*"").append(File.pathSeparator);
    
    URL[] urls = constructUrlsFromClasspath(cp.toString());
    
    assertEquals(3, urls.length);
    assertEquals(file.toURI().toURL(), urls[0]);
    assertEquals(dir.toURI().toURL(), urls[1]);
    assertEquals(jarFile.toURI().toURL(), urls[2]);
    // nofile should be ignored
  }",1
"@Test
  public void testGetResource() throws IOException {
    URL testJar = makeTestJar().toURI().toURL();
    
    ClassLoader currentClassLoader = getClass().getClassLoader();
    ClassLoader appClassloader = new ApplicationClassLoader(
        new URL[] { testJar }",1
"@Test
  public void testJarReplace() throws IOException {
    // Run the command twice with the same output jar file, and expect success.
    testJar();
    testJar();
  }",1
"@Test
  public void testForBadFIle() throws IOException {
    String[] ips = { ""10.221.102/23""}",1
"@Test
  public void testCreateFailsInConstructor() throws Throwable {
    run(FindClass.E_CREATE_FAILED,
        FindClass.A_CREATE,
        ""org.apache.hadoop.util.TestFindClass$FailInConstructor"");
  }",1
"@Test
  public void testCreateFailsInPrivateConstructor() throws Throwable {
    run(FindClass.E_CREATE_FAILED,
        FindClass.A_CREATE,
        ""org.apache.hadoop.util.TestFindClass$PrivateConstructor"");
  }",1
"@Test
  public void testFailsNoSuchResource() throws Throwable {
    run(FindClass.E_NOT_FOUND,
        FindClass.A_RESOURCE,
        ""org/apache/hadoop/util/ThereIsNoSuchClass.class"");
  }",1
"@Test
  public void testLoadPrivateClass() throws Throwable {
    run(FindClass.SUCCESS,
        FindClass.A_LOAD, ""org.apache.hadoop.util.TestFindClass$PrivateClass"");
  }",1
"@Test
  public void testConfWithMultipleOpts() throws Exception {
    String[] args = new String[2];
    args[0] = ""--conf=foo"";
    args[1] = ""--conf=bar"";
    GenericOptionsParser g = new GenericOptionsParser(args);
    assertEquals(""1st conf param is incorrect"",
      ""foo"", g.getCommandLine().getOptionValues(""conf"")[0]);
    assertEquals(""2st conf param is incorrect"",
      ""bar"", g.getCommandLine().getOptionValues(""conf"")[1]);
  }",1
"@Test
  public void testHostFileReaderWithSpaces() throws Exception {
    FileWriter efw = new FileWriter(excludesFile);
    FileWriter ifw = new FileWriter(includesFile);

    efw.write(""#DFS-Hosts-excluded\n"");
    efw.write(""   somehost somehost2"");
    efw.write(""   somehost3 # somehost4"");
    efw.close();

    ifw.write(""#Hosts-in-DFS\n"");
    ifw.write(""   somehost somehost2"");
    ifw.write(""   somehost3 # somehost4"");
    ifw.close();

    HostsFileReader hfp = new HostsFileReader(includesFile, excludesFile);

    int includesLen = hfp.getHosts().size();
    int excludesLen = hfp.getExcludedHosts().size();

    assertEquals(3, includesLen);
    assertEquals(3, excludesLen);

    assertTrue(hfp.getHosts().contains(""somehost3""));
    assertFalse(hfp.getHosts().contains(""somehost5""));
    assertFalse(hfp.getHosts().contains(""somehost4""));

    assertTrue(hfp.getExcludedHosts().contains(""somehost3""));
    assertFalse(hfp.getExcludedHosts().contains(""somehost5""));
    assertFalse(hfp.getExcludedHosts().contains(""somehost4""));

  }",1
"@Test
  public void testExistingManifest() throws Exception {
    File dir = GenericTestUtils
        .getTestDir(TestJarFinder.class.getName() + ""-testExistingManifest"");
    delete(dir);
    dir.mkdirs();

    File metaInfDir = new File(dir, ""META-INF"");
    metaInfDir.mkdirs();
    File manifestFile = new File(metaInfDir, ""MANIFEST.MF"");
    Manifest manifest = new Manifest();
    OutputStream os = new FileOutputStream(manifestFile);
    manifest.write(os);
    os.close();

    File propsFile = new File(dir, ""props.properties"");
    Writer writer = new FileWriter(propsFile);
    new Properties().store(writer, """");
    writer.close();
    ByteArrayOutputStream baos = new ByteArrayOutputStream();
    JarOutputStream zos = new JarOutputStream(baos);
    JarFinder.jarDir(dir, """", zos);
    JarInputStream jis =
      new JarInputStream(new ByteArrayInputStream(baos.toByteArray()));
    Assert.assertNotNull(jis.getManifest());
    jis.close();
  }",1
"@Test
  public void testExpandedClasspath() throws Exception {
    //picking a class that is for sure in a directory in the classpath
    //in this case the JAR is created on the fly
    String jar = JarFinder.getJar(TestJarFinder.class);
    Assert.assertTrue(new File(jar).exists());
  }",1
"@Test
  public void testJar() throws Exception {

    //picking a class that is for sure in a JAR in the classpath
    String jar = JarFinder.getJar(LoggerFactory.class);
    Assert.assertTrue(new File(jar).exists());
  }",1
"@Test
  public void testFileSystemEmptyPath() throws Throwable {
    File tempFile = File.createTempFile(""Keyval"", "".json"");
    Path tempPath = new Path(tempFile.toURI());
    LocalFileSystem fs = FileSystem.getLocal(new Configuration());
    try {
      LambdaTestUtils.intercept(PathIOException.class,
          () -> serDeser.load(fs, tempPath));
      fs.delete(tempPath, false);
      LambdaTestUtils.intercept(FileNotFoundException.class,
          () -> serDeser.load(fs, tempPath));
    }",1
"@Test
  public void testFileSystemRoundTrip() throws Throwable {
    File tempFile = File.createTempFile(""Keyval"", "".json"");
    tempFile.delete();
    Path tempPath = new Path(tempFile.toURI());
    LocalFileSystem fs = FileSystem.getLocal(new Configuration());
    try {
      serDeser.save(fs, tempPath, source, false);
      assertEquals(""JSON loaded with load(fs, path)"",
          source,
          serDeser.load(fs, tempPath));
      assertEquals(""JSON loaded with load(fs, path, status)"",
          source,
          serDeser.load(fs, tempPath, fs.getFileStatus(tempPath)));
    }",1
"@Test
  public void testGetDeclaredFieldsIncludingInherited() {
    Parent child = new Parent() {
      private int childField;
      @SuppressWarnings(""unused"")
      public int getChildField() { return childField; }",1
"@Test
  public void testBigJar() throws Exception {
    Random r = new Random(System.currentTimeMillis());
    File dir = new File(TEST_ROOT_DIR, Long.toHexString(r.nextLong()));
    Assert.assertTrue(dir.mkdirs());
    File input = generateBigJar(dir);
    File output = new File(dir, ""job2.jar"");
    try {
      try (InputStream is = new FileInputStream(input)) {
        RunJar.unJarAndSave(is, dir, ""job2.jar"", Pattern.compile("".*""));
      }",1
"@Test
  public void testUnJarDoesNotLooseLastModify() throws Exception {
    File unjarDir = getUnjarDir(""unjar-lastmod"");

    // Unjar everything
    RunJar.unJar(new File(TEST_ROOT_DIR, TEST_JAR_NAME),
            unjarDir, MATCH_ANY);

    String failureMessage = ""Last modify time was lost during unJar"";
    assertEquals(failureMessage, MOCKED_NOW, new File(unjarDir, TestRunJar.FOOBAR_TXT).lastModified());
    assertEquals(failureMessage, MOCKED_NOW_PLUS_TWO_SEC, new File(unjarDir, FOOBAZ_TXT).lastModified());
  }",1
"@Test
  public void testUnJarWithPattern() throws Exception {
    File unjarDir = getUnjarDir(""unjar-pattern"");

    // Unjar only a regex
    RunJar.unJar(new File(TEST_ROOT_DIR, TEST_JAR_NAME),
                 unjarDir,
                 Pattern.compile("".*baz.*""));
    assertFalse(""foobar not unpacked"",
                new File(unjarDir, TestRunJar.FOOBAR_TXT).exists());
    assertTrue(""foobaz unpacked"",
               new File(unjarDir, FOOBAZ_TXT).exists());
  }",1
"@Test
  public void testBinWinUtilsNotAFile() throws Throwable {
    try {
      File bin = new File(methodDir, ""bin"");
      File winutils = new File(bin, WINUTILS_EXE);
      winutils.mkdirs();
      assertWinutilsResolveFailed(methodDir, E_NOT_EXECUTABLE_FILE);
    }",1
"@Test
  public void testHadoopBinNotADir() throws Throwable {
    File bin = new File(methodDir, ""bin"");
    touch(bin);
    try {
      assertWinutilsResolveFailed(methodDir, E_NOT_DIRECTORY);
    }",1
"@Test
  public void testHadoopHomeEmptyDoubleQuotes() throws Throwable {
    assertHomeResolveFailed(""\""\"""", E_HADOOP_PROPS_EMPTY);
  }",1
"@Test
  public void testHadoopHomeValid() throws Throwable {
    File f = checkHadoopHomeInner(rootTestDir.getCanonicalPath());
    assertEquals(rootTestDir, f);
  }",1
"@Test
  public void testHadoopHomeValidQuoted() throws Throwable {
    File f = checkHadoopHomeInner('""'+ rootTestDir.getCanonicalPath() + '""');
    assertEquals(rootTestDir, f);
  }",1
"@Test
  public void testShellCommandTimeout() throws Throwable {
    Assume.assumeFalse(WINDOWS);
    String rootDir = rootTestDir.getAbsolutePath();
    File shellFile = new File(rootDir, ""timeout.sh"");
    String timeoutCommand = ""sleep 4; echo \""hello\"""";
    Shell.ShellCommandExecutor shexc;
    try (PrintWriter writer = new PrintWriter(new FileOutputStream(shellFile))) {
      writer.println(timeoutCommand);
      writer.close();
    }",1
"@Test
  public void testAsyncAPIPollTimeout() {
    testAsyncAPIPollTimeoutHelper(null, false);
    testAsyncAPIPollTimeoutHelper(0L, true);
    testAsyncAPIPollTimeoutHelper(1L, true);
  }",1
"@Test
  public void testApplicationInit2() {
    WrappedApplication wa = null;
    try {
      wa = new WrappedApplication(2, 314159265358979L, ""yak"", 3);
      wa.initApplication();
      wa.initContainer(0);
      assertEquals(ApplicationState.INITING, wa.app.getApplicationState());
      assertEquals(1, wa.app.getContainers().size());

      wa.applicationInited();
      assertEquals(ApplicationState.RUNNING, wa.app.getApplicationState());
      verify(wa.containerBus).handle(
          argThat(new ContainerInitMatcher(wa.containers.get(0)
              .getContainerId())));

      wa.initContainer(1);
      wa.initContainer(2);
      assertEquals(ApplicationState.RUNNING, wa.app.getApplicationState());
      assertEquals(3, wa.app.getContainers().size());

      for (int i = 1; i < wa.containers.size(); i++) {
        verify(wa.containerBus).handle(
            argThat(new ContainerInitMatcher(wa.containers.get(i)
                .getContainerId())));
      }",1
"@Test
  public void testSpecialCharSymlinks() throws IOException  {

    File shellFile = null;
    File tempFile = null;
    String badSymlink = Shell.WINDOWS ? ""foo@zz_#!-+bar.cmd"" :
      ""-foo@zz%_#*&!-+= bar()"";
    File symLinkFile = null;

    try {
      shellFile = Shell.appendScriptExtension(tmpDir, ""hello"");
      tempFile = Shell.appendScriptExtension(tmpDir, ""temp"");
      String timeoutCommand = Shell.WINDOWS ? ""@echo \""hello\"""" :
        ""echo \""hello\"""";
      PrintWriter writer = new PrintWriter(new FileOutputStream(shellFile));
      FileUtil.setExecutable(shellFile, true);
      writer.println(timeoutCommand);
      writer.close();

      Map<Path, List<String>> resources =
          new HashMap<Path, List<String>>();
      Path path = new Path(shellFile.getAbsolutePath());
      resources.put(path, Arrays.asList(badSymlink));

      FileOutputStream fos = new FileOutputStream(tempFile);

      Map<String, String> env = new HashMap<String, String>();
      List<String> commands = new ArrayList<String>();
      if (Shell.WINDOWS) {
        commands.add(""cmd"");
        commands.add(""/c"");
        commands.add(""\"""" + badSymlink + ""\"""");
      }",1
"@Test
  public void testExecutorPath() {
    String containerExePath = PrivilegedOperationExecutor
        .getContainerExecutorExecutablePath(nullConf);

    //In case HADOOP_YARN_HOME isn't set, CWD is used. If conf is null or
    //NM_LINUX_CONTAINER_EXECUTOR_PATH is not set, then a defaultPath is
    //constructed.
    String yarnHomeEnvVar = System.getenv(""HADOOP_YARN_HOME"");
    String yarnHome = yarnHomeEnvVar != null ? yarnHomeEnvVar
        : new File("""").getAbsolutePath();
    String expectedPath = yarnHome + ""/bin/container-executor"";

    Assert.assertEquals(expectedPath, containerExePath);

    containerExePath = PrivilegedOperationExecutor
        .getContainerExecutorExecutablePath(emptyConf);
    Assert.assertEquals(expectedPath, containerExePath);

    //if NM_LINUX_CONTAINER_EXECUTOR_PATH is set, this must be returned
    expectedPath = customExecutorPath;
    containerExePath = PrivilegedOperationExecutor
        .getContainerExecutorExecutablePath(confWithExecutorPath);
    Assert.assertEquals(expectedPath, containerExePath);
  }",1
"@Test
  public void testPreStart() throws Exception {
    String id = ""container_01_01"";
    String path = ""test-path/"" + id;
    ContainerId mockContainerId = mock(ContainerId.class);
    when(mockContainerId.toString()).thenReturn(id);
    Container mockContainer = mock(Container.class);
    when(mockContainer.getContainerId()).thenReturn(mockContainerId);
    when(mockCGroupsHandler
        .getPathForCGroupTasks(CGroupsHandler.CGroupController.CPU, id))
        .thenReturn(path);
    when(mockContainer.getResource()).thenReturn(Resource.newInstance(1024, 2));

    List<PrivilegedOperation> ret =
        cGroupsCpuResourceHandler.preStart(mockContainer);
    verify(mockCGroupsHandler, times(1))
        .createCGroup(CGroupsHandler.CGroupController.CPU, id);
    verify(mockCGroupsHandler, times(1))
        .updateCGroupParam(CGroupsHandler.CGroupController.CPU, id,
            CGroupsHandler.CGROUP_CPU_SHARES, String
                .valueOf(CGroupsCpuResourceHandlerImpl.CPU_DEFAULT_WEIGHT * 2));

    // don't set quota or period
    verify(mockCGroupsHandler, never())
        .updateCGroupParam(eq(CGroupsHandler.CGroupController.CPU), eq(id),
            eq(CGroupsHandler.CGROUP_CPU_PERIOD_US), anyString());
    verify(mockCGroupsHandler, never())
        .updateCGroupParam(eq(CGroupsHandler.CGroupController.CPU), eq(id),
            eq(CGroupsHandler.CGROUP_CPU_QUOTA_US), anyString());
    Assert.assertNotNull(ret);
    Assert.assertEquals(1, ret.size());
    PrivilegedOperation op = ret.get(0);
    Assert.assertEquals(PrivilegedOperation.OperationType.ADD_PID_TO_CGROUP,
        op.getOperationType());
    List<String> args = op.getArguments();
    Assert.assertEquals(1, args.size());
    Assert.assertEquals(PrivilegedOperation.CGROUP_ARG_PREFIX + path,
        args.get(0));
  }",1
"@Test
  public void testPreStartRestrictedContainers() throws Exception {
    String id = ""container_01_01"";
    String path = ""test-path/"" + id;
    int defaultVCores = 8;
    Configuration conf = new YarnConfiguration();
    conf.setBoolean(
        YarnConfiguration.NM_LINUX_CONTAINER_CGROUPS_STRICT_RESOURCE_USAGE,
        true);
    int cpuPerc = 75;
    conf.setInt(YarnConfiguration.NM_RESOURCE_PERCENTAGE_PHYSICAL_CPU_LIMIT,
        cpuPerc);
    cGroupsCpuResourceHandler.bootstrap(plugin, conf);
    InOrder cpuLimitOrder = inOrder(mockCGroupsHandler);
    cpuLimitOrder.verify(mockCGroupsHandler, times(1))
        .updateCGroupParam(CGroupsHandler.CGroupController.CPU, """",
            CGroupsHandler.CGROUP_CPU_PERIOD_US, String.valueOf(""333333""));
    cpuLimitOrder.verify(mockCGroupsHandler, times(1))
        .updateCGroupParam(CGroupsHandler.CGroupController.CPU, """",
            CGroupsHandler.CGROUP_CPU_QUOTA_US,
            String.valueOf(CGroupsCpuResourceHandlerImpl.MAX_QUOTA_US));
    float yarnCores = (cpuPerc * numProcessors) / 100;
    int[] containerVCores = { 2, 4 }",1
"@Test
  public void testCGgroupNotFound() throws Exception {
    writeToFile(""proc/41/cgroup"",
        ""7:devices:/yarn/container_1"",
        ""6:cpuacct,cpu:/yarn/container_1"",
        ""5:pids:/yarn/container_1"",
        ""4:memory:/yarn/container_1""
    );

    CGroupsResourceCalculator calculator = createCalculator();
    calculator.updateProcessTree();
    assertEquals(-1, calculator.getCumulativeCpuTime());
  }",1
"@Test
  public void testGetIpAndHost() throws Exception {
    dockerInspectCommand.getIpAndHost();
    assertEquals(""inspect"", StringUtils.join("","",
        dockerInspectCommand.getDockerCommandWithArguments()
            .get(""docker-command"")));
    assertEquals(""{{range(.NetworkSettings.Networks)}",1
"@Test
  public void testEnabledSandboxWithWhitelist()
      throws ContainerExecutionException{
    String[] inputCommand = {
        ""$JAVA_HOME/bin/java jar -Djava.security.manager MyJob.jar""
    }",1
"@Test
  public void testEnforcingMode() throws ContainerExecutionException {
    String[] nonJavaCommands = {
        ""bash malicious_script.sh"",
        ""python malicious_script.py""
    }",1
"@Test
  public void testNotifySCMFail() throws Exception {
    Configuration conf = new Configuration();
    conf.setBoolean(YarnConfiguration.SHARED_CACHE_ENABLED, true);
    LocalResource resource = mock(LocalResource.class);
    Path localPath = mock(Path.class);
    when(localPath.getName()).thenReturn(""foo.jar"");
    String user = ""joe"";
    FileSystem fs = mock(FileSystem.class);
    // return false when rename is called
    when(fs.rename(isA(Path.class), isA(Path.class))).thenReturn(true);
    FileSystem localFs = FileSystem.getLocal(conf);
    SharedCacheUploader spied =
        createSpiedUploader(resource, localPath, user, conf, null, fs,
            localFs);
    // stub verifyAccess() to return true
    doReturn(true).when(spied).verifyAccess();
    // stub getActualPath()
    doReturn(localPath).when(spied).getActualPath();
    // stub computeChecksum()
    doReturn(""abcdef0123456789"").when(spied).computeChecksum(isA(Path.class));
    // stub uploadFile() to return true
    doReturn(true).when(spied).uploadFile(isA(Path.class), isA(Path.class));
    // stub notifySharedCacheManager to return true
    doReturn(false).when(spied).notifySharedCacheManager(isA(String.class),
        isA(String.class));

    assertFalse(spied.call());
    verify(fs).delete(isA(Path.class), anyBoolean());
  }",1
"@Test
  public void testSuccess() throws Exception {
    Configuration conf = new Configuration();
    conf.setBoolean(YarnConfiguration.SHARED_CACHE_ENABLED, true);
    LocalResource resource = mock(LocalResource.class);
    Path localPath = mock(Path.class);
    when(localPath.getName()).thenReturn(""foo.jar"");
    String user = ""joe"";
    SCMUploaderProtocol scmClient = mock(SCMUploaderProtocol.class);
    SCMUploaderNotifyResponse response = mock(SCMUploaderNotifyResponse.class);
    when(response.getAccepted()).thenReturn(true);
    when(scmClient.notify(isA(SCMUploaderNotifyRequest.class))).
        thenReturn(response);
    FileSystem fs = mock(FileSystem.class);
    // return false when rename is called
    when(fs.rename(isA(Path.class), isA(Path.class))).thenReturn(true);
    FileSystem localFs = FileSystem.getLocal(conf);
    SharedCacheUploader spied =
        createSpiedUploader(resource, localPath, user, conf, scmClient, fs,
            localFs);
    // stub verifyAccess() to return true
    doReturn(true).when(spied).verifyAccess();
    // stub getActualPath()
    doReturn(localPath).when(spied).getActualPath();
    // stub computeChecksum()
    doReturn(""abcdef0123456789"").when(spied).computeChecksum(isA(Path.class));
    // stub uploadFile() to return true
    doReturn(true).when(spied).uploadFile(isA(Path.class), isA(Path.class));
    // stub notifySharedCacheManager to return true
    doReturn(true).when(spied).notifySharedCacheManager(isA(String.class),
        isA(String.class));

    assertTrue(spied.call());
  }",1
"@Test
  public void testAuxServicesMeta() throws IOException {
    Configuration conf = getABConf();
    final AuxServices aux = new AuxServices(MOCK_AUX_PATH_HANDLER,
        MOCK_CONTEXT, MOCK_DEL_SERVICE);
    aux.init(conf);

    int latch = 1;
    for (Service s : aux.getServices()) {
      assertEquals(INITED, s.getServiceState());
      if (s instanceof ServiceA) { latch *= 2; }",1
"@Test
  public void testChangeContainerResource() throws Exception {
    containerManager.start();
    File scriptFile = Shell.appendScriptExtension(tmpDir, ""scriptFile"");
    PrintWriter fileWriter = new PrintWriter(scriptFile);
    // Construct the Container-id
    ContainerId cId = createContainerId(0);
    if (Shell.WINDOWS) {
      fileWriter.println(""@ping -n 100 127.0.0.1 >nul"");
    }",1
"@Test
  public void testFinishResourceLocalizationForApplicationResource()
      throws IOException {
    String user = ""somebody"";
    ApplicationId appId = ApplicationId.newInstance(1, 1);

    // start and finish a local resource for an application
    Path appRsrcPath = new Path(""hdfs://some/app/resource"");
    LocalResourcePBImpl rsrcPb = (LocalResourcePBImpl)
        LocalResource.newInstance(
            URL.fromPath(appRsrcPath),
            LocalResourceType.ARCHIVE, LocalResourceVisibility.APPLICATION,
            123L, 456L);
    LocalResourceProto appRsrcProto = rsrcPb.getProto();
    Path appRsrcLocalPath = new Path(""/some/local/dir/for/apprsrc"");
    stateStore.startResourceLocalization(user, appId, appRsrcProto,
        appRsrcLocalPath);
    LocalizedResourceProto appLocalizedProto =
        LocalizedResourceProto.newBuilder()
          .setResource(appRsrcProto)
          .setLocalPath(appRsrcLocalPath.toString())
          .setSize(1234567L)
          .build();
    stateStore.finishResourceLocalization(user, appId, appLocalizedProto);

    List<LocalizedResourceProto> completedResources =
        new ArrayList<LocalizedResourceProto>();
    Map<LocalResourceProto, Path> startedResources =
        new HashMap<LocalResourceProto, Path>();

    // restart and verify only app resource is completed
    restartStateStore();
    RecoveredLocalizationState state = stateStore.loadLocalizationState();
    LocalResourceTrackerState pubts = state.getPublicTrackerState();
    completedResources = loadCompletedResources(
        pubts.getCompletedResourcesIterator());
    startedResources = loadStartedResources(
        pubts.getStartedResourcesIterator());
    assertTrue(completedResources.isEmpty());
    assertTrue(startedResources.isEmpty());
    Map<String, RecoveredUserResources> userResources =
        loadUserResources(state.getIterator());
    assertEquals(1, userResources.size());
    RecoveredUserResources rur = userResources.get(user);
    LocalResourceTrackerState privts = rur.getPrivateTrackerState();
    assertNotNull(privts);
    completedResources = loadCompletedResources(
        privts.getCompletedResourcesIterator());
    startedResources = loadStartedResources(
        privts.getStartedResourcesIterator());
    assertTrue(completedResources.isEmpty());
    assertTrue(startedResources.isEmpty());
    assertEquals(1, rur.getAppTrackerStates().size());
    LocalResourceTrackerState appts = rur.getAppTrackerStates().get(appId);
    assertNotNull(appts);
    completedResources = loadCompletedResources(
        appts.getCompletedResourcesIterator());
    startedResources = loadStartedResources(
        appts.getStartedResourcesIterator());
    assertTrue(startedResources.isEmpty());
    assertEquals(1, completedResources.size());
    assertEquals(appLocalizedProto,
        completedResources.iterator().next());
  }",1
"@Test
  public void testUnexpectedKeyDoesntThrowException() throws IOException {
    // test empty when no state
    List<RecoveredContainerState> recoveredContainers =
        loadContainersState(stateStore.getContainerStateIterator());
    assertTrue(recoveredContainers.isEmpty());

    ApplicationId appId = ApplicationId.newInstance(1234, 3);
    ApplicationAttemptId appAttemptId = ApplicationAttemptId.newInstance(appId,
        4);
    ContainerId containerId = ContainerId.newContainerId(appAttemptId, 5);
    StartContainerRequest startContainerRequest = storeMockContainer(
        containerId);

    // add a invalid key
    byte[] invalidKey = (""ContainerManager/containers/""
    + containerId.toString() + ""/invalidKey1234"").getBytes();
    stateStore.getDB().put(invalidKey, new byte[1]);
    restartStateStore();
    recoveredContainers =
        loadContainersState(stateStore.getContainerStateIterator());
    assertEquals(1, recoveredContainers.size());
    RecoveredContainerState rcs = recoveredContainers.get(0);
    assertEquals(RecoveredContainerStatus.REQUESTED, rcs.getStatus());
    assertEquals(ContainerExitStatus.INVALID, rcs.getExitCode());
    assertEquals(false, rcs.getKilled());
    assertEquals(startContainerRequest, rcs.getStartRequest());
    assertTrue(rcs.getDiagnostics().isEmpty());
    assertEquals(RecoveredContainerType.KILL, rcs.getRecoveryType());
    // assert unknown keys are cleaned up finally
    assertNotNull(stateStore.getDB().get(invalidKey));
    stateStore.removeContainer(containerId);
    assertNull(stateStore.getDB().get(invalidKey));
  }",1
"@Test
  public void testRecovery() throws Exception {
    Random r = new Random();
    long seed = r.nextLong();
    r.setSeed(seed);
    System.out.println(""SEED: "" + seed);
    List<Path> baseDirs = buildDirs(r, base, 4);
    createDirs(new Path("".""), baseDirs);
    List<Path> content = buildDirs(r, new Path("".""), 10);
    for (Path b : baseDirs) {
      createDirs(b, content);
    }",1
"@Test
  public void testConcurrentAccess() throws IOException {
    // Initialize DirectoryCollection with a file instead of a directory
    
    String[] dirs = {testFile.getPath()}",1
"@Test
  public void testCreationOfNodeLabelsProviderService()
      throws InterruptedException {
    try {
      NodeManager nodeManager = new NodeManager();
      Configuration conf = new Configuration();
      NodeLabelsProvider labelsProviderService =
          nodeManager.createNodeLabelsProvider(conf);
      Assert
          .assertNull(
              ""LabelsProviderService should not be initialized in default configuration"",
              labelsProviderService);

      // With valid className
      conf.set(
          YarnConfiguration.NM_NODE_LABELS_PROVIDER_CONFIG,
          ""org.apache.hadoop.yarn.server.nodemanager.nodelabels.ConfigurationNodeLabelsProvider"");
      labelsProviderService = nodeManager.createNodeLabelsProvider(conf);
      Assert.assertNotNull(""LabelsProviderService should be initialized When ""
          + ""node labels provider class is configured"", labelsProviderService);

      // With invalid className
      conf.set(YarnConfiguration.NM_NODE_LABELS_PROVIDER_CONFIG,
          ""org.apache.hadoop.yarn.server.nodemanager.NodeManager"");
      try {
        labelsProviderService = nodeManager.createNodeLabelsProvider(conf);
        Assert.fail(""Expected to throw IOException on Invalid configuration"");
      }",1
"@Test
  public void testSelectCgroup() {
    File cpu = new File(cgroupDir, ""cpu"");
    File cpuNoExist = new File(cgroupDir, ""cpuNoExist"");
    File memory = new File(cgroupDir, ""memory"");
    try {
      CgroupsLCEResourcesHandler handler = new CgroupsLCEResourcesHandler();
      Map<String, Set<String>> cgroups = new LinkedHashMap<>();

      Assert.assertTrue(""temp dir should be created"", cpu.mkdirs());
      Assert.assertTrue(""temp dir should be created"", memory.mkdirs());
      Assert.assertFalse(""temp dir should not be created"", cpuNoExist.exists());

      cgroups.put(
          memory.getAbsolutePath(), Collections.singleton(""memory""));
      cgroups.put(
          cpuNoExist.getAbsolutePath(), Collections.singleton(""cpu""));
      cgroups.put(cpu.getAbsolutePath(), Collections.singleton(""cpu""));
      String selectedCPU = handler.findControllerInMtab(""cpu"", cgroups);
      Assert.assertEquals(""Wrong CPU mount point selected"",
          cpu.getAbsolutePath(), selectedCPU);
    }",1
"@Test
  public void testHDFSBackedProvider() throws Exception {
    File testSchedulerConfigurationDir = new File(
        TestMutableCSConfigurationProvider.class.getResource("""").getPath()
            + TestMutableCSConfigurationProvider.class.getSimpleName());
    FileUtils.deleteDirectory(testSchedulerConfigurationDir);
    testSchedulerConfigurationDir.mkdirs();

    Configuration conf = new Configuration(false);
    conf.set(YarnConfiguration.SCHEDULER_CONFIGURATION_STORE_CLASS,
        YarnConfiguration.FS_CONFIGURATION_STORE);
    conf.set(YarnConfiguration.SCHEDULER_CONFIGURATION_FS_PATH,
        testSchedulerConfigurationDir.getAbsolutePath());
    writeConf(conf, testSchedulerConfigurationDir.getAbsolutePath());

    confProvider.init(conf);
    assertNull(confProvider.loadConfiguration(conf)
        .get(""yarn.scheduler.capacity.root.a.goodKey""));

    LogMutation log = confProvider.logAndApplyMutation(TEST_USER, goodUpdate);
    confProvider.confirmPendingMutation(log, true);
    assertEquals(""goodVal"", confProvider.loadConfiguration(conf)
        .get(""yarn.scheduler.capacity.root.a.goodKey""));

    assertNull(confProvider.loadConfiguration(conf).get(
        ""yarn.scheduler.capacity.root.a.badKey""));
    log = confProvider.logAndApplyMutation(TEST_USER, badUpdate);
    confProvider.confirmPendingMutation(log, false);
    assertNull(confProvider.loadConfiguration(conf).get(
        ""yarn.scheduler.capacity.root.a.badKey""));

    confProvider.formatConfigurationInStore(conf);
    assertNull(confProvider.loadConfiguration(conf)
        .get(""yarn.scheduler.capacity.root.a.goodKey""));

  }",1
"@Test
  public void testGetNodeLabels() throws Exception {
    MockRM rm = new MockRM() {
      protected ClientRMService createClientRMService() {
        return new ClientRMService(this.rmContext, scheduler,
            this.rmAppManager, this.applicationACLsManager,
            this.queueACLsManager, this.getRMContext()
                .getRMDelegationTokenSecretManager());
      }",1
"@Test
  public void testGetQueueInfo() throws Exception {
    ResourceScheduler scheduler = mock(ResourceScheduler.class);
    RMContext rmContext = mock(RMContext.class);
    mockRMContext(scheduler, rmContext);

    ApplicationACLsManager mockAclsManager = mock(ApplicationACLsManager.class);
    QueueACLsManager mockQueueACLsManager = mock(QueueACLsManager.class);
    when(mockQueueACLsManager.checkAccess(any(UserGroupInformation.class),
        any(QueueACL.class), any(RMApp.class), any(String.class),
        any()))
        .thenReturn(true);
    when(mockAclsManager.checkAccess(any(UserGroupInformation.class),
        any(ApplicationAccessType.class), any(),
        any(ApplicationId.class))).thenReturn(true);

    ClientRMService rmService = new ClientRMService(rmContext, scheduler,
        null, mockAclsManager, mockQueueACLsManager, null);
    GetQueueInfoRequest request = recordFactory
        .newRecordInstance(GetQueueInfoRequest.class);
    request.setQueueName(""testqueue"");
    request.setIncludeApplications(true);
    GetQueueInfoResponse queueInfo = rmService.getQueueInfo(request);
    List<ApplicationReport> applications = queueInfo.getQueueInfo()
        .getApplications();
    Assert.assertEquals(2, applications.size());
    Map<String, QueueConfigurations> queueConfigsByPartition =
        queueInfo.getQueueInfo().getQueueConfigurations();
    Assert.assertEquals(1, queueConfigsByPartition.size());
    Assert.assertTrue(queueConfigsByPartition.containsKey(""*""));
    QueueConfigurations queueConfigs = queueConfigsByPartition.get(""*"");
    Assert.assertEquals(0.5f, queueConfigs.getCapacity(), 0.0001f);
    Assert.assertEquals(0.1f, queueConfigs.getAbsoluteCapacity(), 0.0001f);
    Assert.assertEquals(1.0f, queueConfigs.getMaxCapacity(), 0.0001f);
    Assert.assertEquals(1.0f, queueConfigs.getAbsoluteMaxCapacity(), 0.0001f);
    Assert.assertEquals(0.2f, queueConfigs.getMaxAMPercentage(), 0.0001f);

    request.setQueueName(""nonexistentqueue"");
    request.setIncludeApplications(true);
    // should not throw exception on nonexistent queue
    queueInfo = rmService.getQueueInfo(request);

    // Case where user does not have application access
    ApplicationACLsManager mockAclsManager1 =
        mock(ApplicationACLsManager.class);
    QueueACLsManager mockQueueACLsManager1 =
        mock(QueueACLsManager.class);
    when(mockQueueACLsManager1.checkAccess(any(UserGroupInformation.class),
        any(QueueACL.class), any(RMApp.class), any(String.class),
        any()))
        .thenReturn(false);
    when(mockAclsManager1.checkAccess(any(UserGroupInformation.class),
        any(ApplicationAccessType.class), anyString(),
        any(ApplicationId.class))).thenReturn(false);

    ClientRMService rmService1 = new ClientRMService(rmContext, scheduler,
        null, mockAclsManager1, mockQueueACLsManager1, null);
    request.setQueueName(""testqueue"");
    request.setIncludeApplications(true);
    GetQueueInfoResponse queueInfo1 = rmService1.getQueueInfo(request);
    List<ApplicationReport> applications1 = queueInfo1.getQueueInfo()
        .getApplications();
    Assert.assertEquals(0, applications1.size());
  }",1
"@Test
  public void testDumpingSchedulerLogs() throws Exception {

    ResourceManager mockRM = mock(ResourceManager.class);
    Configuration conf = new YarnConfiguration();
    HttpServletRequest mockHsr = mockHttpServletRequestByUserName(""non-admin"");
    ApplicationACLsManager aclsManager = new ApplicationACLsManager(conf);
    when(mockRM.getApplicationACLsManager()).thenReturn(aclsManager);
    RMWebServices webSvc =
        new RMWebServices(mockRM, conf, mock(HttpServletResponse.class));

    // nothing should happen
    webSvc.dumpSchedulerLogs(""1"", mockHsr);
    waitforLogDump(50);
    checkSchedulerLogFileAndCleanup();

    conf.setBoolean(YarnConfiguration.YARN_ACL_ENABLE, true);
    conf.setStrings(YarnConfiguration.YARN_ADMIN_ACL, ""admin"");
    aclsManager = new ApplicationACLsManager(conf);
    when(mockRM.getApplicationACLsManager()).thenReturn(aclsManager);
    webSvc = new RMWebServices(mockRM, conf, mock(HttpServletResponse.class));
    boolean exceptionThrown = false;
    try {
      webSvc.dumpSchedulerLogs(""1"", mockHsr);
      fail(""Dumping logs should fail"");
    }",1
"@Test
  public void testComponentDependency() throws Exception{
    ApplicationId applicationId = ApplicationId.newInstance(123456, 1);
    Service exampleApp = new Service();
    exampleApp.setVersion(""v1"");
    exampleApp.setId(applicationId.toString());
    exampleApp.setName(""testComponentDependency"");
    exampleApp.addComponent(createComponent(""compa"", 1, ""sleep 1000""));
    // Let compb depends on compa;
    Component compb = createComponent(""compb"", 1, ""sleep 1000"", Component
        .RestartPolicyEnum.ON_FAILURE, Collections.singletonList(""compa""));
    // Let compb depends on compb;
    Component compc = createComponent(""compc"", 1, ""sleep 1000"", Component
        .RestartPolicyEnum.NEVER, Collections.singletonList(""compb""));

    exampleApp.addComponent(compb);
    exampleApp.addComponent(compc);

    MockServiceAM am = new MockServiceAM(exampleApp);
    am.init(conf);
    am.start();

    // compa ready
    Assert.assertTrue(am.getComponent(""compa"").areDependenciesReady());
    //compb not ready
    Assert.assertFalse(am.getComponent(""compb"").areDependenciesReady());

    // feed 1 container to compa,
    am.feedContainerToComp(exampleApp, 1, ""compa"");
    // waiting for compb's dependencies are satisfied
    am.waitForDependenciesSatisfied(""compb"");

    // feed 1 container to compb,
    am.feedContainerToComp(exampleApp, 2, ""compb"");
    // waiting for compc's dependencies are satisfied
    am.waitForDependenciesSatisfied(""compc"");

    // feed 1 container to compb
    am.feedContainerToComp(exampleApp, 2, ""compb"");
    am.flexComponent(""compa"", 2);
    am.waitForNumDesiredContainers(""compa"", 2);

    // compb dependencies not satisfied again.
    Assert.assertFalse(am.getComponent(""compb"").areDependenciesReady());
    am.stop();
  }",1
"@Test
  public void testTrackPageHtmlTemplate() throws Exception {
    String trackTemplate = FileUtils.readFileToString(
            new File(""src/main/html/track.html.template""), StandardCharsets.UTF_8);
    String trackedQueueInfo = """";
    Set<String> trackedQueues = new HashSet<String>();
    trackedQueues.add(""sls_queue_1"");
    trackedQueues.add(""sls_queue_2"");
    trackedQueues.add(""sls_queue_3"");
    for(String queue : trackedQueues) {
      trackedQueueInfo += ""<option value='Queue "" + queue + ""'>""
              + queue + ""</option>"";
    }",1
