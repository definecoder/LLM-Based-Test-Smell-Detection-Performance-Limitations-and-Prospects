App,TestFilePath,ProductionFilePath,TestFileCode,ProductionFileCode,present_smells,found_smells,false_positive,false_negative,true_positive,true_negative
49552_5.0_jfreechart_testcloning_legenditemlabelgenerator,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/49552_test.java,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/49552_actual.java,"/* ===========================================================
 * JFreeChart : a free chart library for the Java(tm) platform
 * ===========================================================
 *
 * (C) Copyright 2000-2022, by David Gilbert and Contributors.
 *
 * Project Info:  http://www.jfree.org/jfreechart/index.html
 *
 * This library is free software; you can redistribute it and/or modify it
 * under the terms of the GNU Lesser General Public License as published by
 * the Free Software Foundation; either version 2.1 of the License, or
 * (at your option) any later version.
 *
 * This library is distributed in the hope that it will be useful, but
 * WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
 * or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public
 * License for more details.
 *
 * You should have received a copy of the GNU Lesser General Public
 * License along with this library; if not, write to the Free Software
 * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301,
 * USA.
 *
 * [Oracle and Java are registered trademarks of Oracle and/or its affiliates. 
 * Other names may be trademarks of their respective owners.]
 *
 * -------------------------------------
 * AbstractCategoryItemRendererTest.java
 * -------------------------------------
 * (C) Copyright 2004-2022, by David Gilbert and Contributors.
 *
 * Original Author:  David Gilbert;
 * Contributor(s):   -;
 *
 */

package org.jfree.chart.renderer.category;

import org.junit.jupiter.api.Test;

import java.text.DecimalFormat;
import java.text.NumberFormat;

import org.jfree.chart.legend.LegendItemCollection;
import org.jfree.chart.labels.IntervalCategoryItemLabelGenerator;
import org.jfree.chart.labels.StandardCategoryItemLabelGenerator;
import org.jfree.chart.labels.StandardCategorySeriesLabelGenerator;
import org.jfree.chart.labels.StandardCategoryToolTipGenerator;
import org.jfree.chart.plot.CategoryPlot;
import org.jfree.chart.urls.StandardCategoryURLGenerator;
import org.jfree.data.Range;
import org.jfree.data.category.DefaultCategoryDataset;

import static org.junit.jupiter.api.Assertions.*;

/**
 * Tests for the {@link AbstractCategoryItemRenderer} class.
 */
public class AbstractCategoryItemRendererTest {

    /**
     * Checks that all fields are distinguished.
     */
    @Test
    public void testEquals() {
        BarRenderer r1 = new BarRenderer();
        BarRenderer r2 = new BarRenderer();
        assertEquals(r1, r2);

        // the plot field is NOT tested

        // toolTipGeneratorList
        r1.setSeriesToolTipGenerator(1, new StandardCategoryToolTipGenerator());
        assertNotEquals(r1, r2);
        r2.setSeriesToolTipGenerator(1, new StandardCategoryToolTipGenerator());
        assertEquals(r1, r2);

        // defaultToolTipGenerator
        r1.setDefaultToolTipGenerator(new StandardCategoryToolTipGenerator(""{2}"",
                NumberFormat.getInstance()));
        assertNotEquals(r1, r2);
        r2.setDefaultToolTipGenerator(new StandardCategoryToolTipGenerator(""{2}"",
                NumberFormat.getInstance()));
        assertEquals(r1, r2);

        // itemLabelGeneratorList
        r1.setSeriesItemLabelGenerator(1,
                new StandardCategoryItemLabelGenerator());
        assertNotEquals(r1, r2);
        r2.setSeriesItemLabelGenerator(1,
                new StandardCategoryItemLabelGenerator());
        assertEquals(r1, r2);

        // defaultItemLabelGenerator
        r1.setDefaultItemLabelGenerator(new StandardCategoryItemLabelGenerator(
                ""{2}"", NumberFormat.getInstance()));
        assertNotEquals(r1, r2);
        r2.setDefaultItemLabelGenerator(new StandardCategoryItemLabelGenerator(
                ""{2}"", NumberFormat.getInstance()));
        assertEquals(r1, r2);

        // urlGeneratorList
        r1.setSeriesItemURLGenerator(1, new StandardCategoryURLGenerator());
        assertNotEquals(r1, r2);
        r2.setSeriesItemURLGenerator(1, new StandardCategoryURLGenerator());
        assertEquals(r1, r2);

        // defaultItemURLGenerator
        r1.setDefaultItemURLGenerator(new StandardCategoryURLGenerator(
                ""abc.html""));
        assertNotEquals(r1, r2);
        r2.setDefaultItemURLGenerator(new StandardCategoryURLGenerator(
                ""abc.html""));
        assertEquals(r1, r2);

        // legendItemLabelGenerator
        r1.setLegendItemLabelGenerator(new StandardCategorySeriesLabelGenerator(
                ""XYZ""));
        assertNotEquals(r1, r2);
        r2.setLegendItemLabelGenerator(new StandardCategorySeriesLabelGenerator(
                ""XYZ""));
        assertEquals(r1, r2);

        // legendItemToolTipGenerator
        r1.setLegendItemToolTipGenerator(
                new StandardCategorySeriesLabelGenerator(""ToolTip""));
        assertNotEquals(r1, r2);
        r2.setLegendItemToolTipGenerator(
                new StandardCategorySeriesLabelGenerator(""ToolTip""));
        assertEquals(r1, r2);

        // legendItemURLGenerator
        r1.setLegendItemURLGenerator(
                new StandardCategorySeriesLabelGenerator(""URL""));
        assertNotEquals(r1, r2);
        r2.setLegendItemURLGenerator(
                new StandardCategorySeriesLabelGenerator(""URL""));
        assertEquals(r1, r2);
    }

    @Test
    public void testEquals_ObjectList() {
        BarRenderer r1 = new BarRenderer();
        r1.setSeriesItemLabelGenerator(0, new StandardCategoryItemLabelGenerator());
        BarRenderer r2 = new BarRenderer();
        r2.setSeriesItemLabelGenerator(0, new StandardCategoryItemLabelGenerator());
        assertEquals(r1, r2);
        r2.setSeriesItemLabelGenerator(1, new StandardCategoryItemLabelGenerator(""X"", new DecimalFormat(""0.0"")));
        assertNotEquals(r1, r2);
    }

    @Test
    public void testEquals_ObjectList2() {
        BarRenderer r1 = new BarRenderer();
        r1.setSeriesToolTipGenerator(0, new StandardCategoryToolTipGenerator());
        BarRenderer r2 = new BarRenderer();
        r2.setSeriesToolTipGenerator(0, new StandardCategoryToolTipGenerator());
        assertEquals(r1, r2);
        r2.setSeriesToolTipGenerator(1, new StandardCategoryToolTipGenerator(""X"", new DecimalFormat(""0.0"")));
        assertNotEquals(r1, r2);
    }

    @Test
    public void testEquals_ObjectList3() {
        BarRenderer r1 = new BarRenderer();
        r1.setSeriesItemURLGenerator(0, new StandardCategoryURLGenerator());
        BarRenderer r2 = new BarRenderer();
        r2.setSeriesItemURLGenerator(0, new StandardCategoryURLGenerator());
        assertEquals(r1, r2);
        r2.setSeriesItemURLGenerator(1, new StandardCategoryURLGenerator());
        assertNotEquals(r1, r2);
    }

    /**
     * Confirm that cloning works.
     * 
     * @throws java.lang.CloneNotSupportedException
     */
    @Test
    public void testCloning1() throws CloneNotSupportedException {
        AbstractCategoryItemRenderer r1 = new BarRenderer();
        AbstractCategoryItemRenderer r2 = (BarRenderer) r1.clone();
        assertNotSame(r1, r2);
        assertSame(r1.getClass(), r2.getClass());
        assertEquals(r1, r2);

        r1 = new BarRenderer();
        r1.setSeriesItemLabelGenerator(0,
                new StandardCategoryItemLabelGenerator());
        r2 = (BarRenderer) r1.clone();

        assertNotSame(r1, r2);
        assertSame(r1.getClass(), r2.getClass());
        assertEquals(r1, r2);

        r1 = new BarRenderer();
        r1.setDefaultItemLabelGenerator(new StandardCategoryItemLabelGenerator());
        r2 = (BarRenderer) r1.clone();
        assertNotSame(r1, r2);
        assertSame(r1.getClass(), r2.getClass());
        assertEquals(r1, r2);
    }

    /**
     * Confirm that cloning works.
     * 
     * @throws java.lang.CloneNotSupportedException
     */
    @Test
    public void testCloning2() throws CloneNotSupportedException {
        BarRenderer r1 = new BarRenderer();
        r1.setDefaultItemLabelGenerator(new IntervalCategoryItemLabelGenerator());
        BarRenderer r2 = (BarRenderer) r1.clone();

        assertNotSame(r1, r2);
        assertSame(r1.getClass(), r2.getClass());
        assertEquals(r1, r2);

        r1 = new BarRenderer();
        r1.setSeriesItemLabelGenerator(0,
                new IntervalCategoryItemLabelGenerator());
        r2 = (BarRenderer) r1.clone();
        assertNotSame(r1, r2);
        assertSame(r1.getClass(), r2.getClass());
        assertEquals(r1, r2);

        r1 = new BarRenderer();
        r1.setDefaultItemLabelGenerator(new IntervalCategoryItemLabelGenerator());
        r2 = (BarRenderer) r1.clone();

        assertNotSame(r1, r2);
        assertSame(r1.getClass(), r2.getClass());
        assertEquals(r1, r2);
    }

    /**
     * Check that the legendItemLabelGenerator is cloned.
     * 
     * @throws java.lang.CloneNotSupportedException
     */
    @Test
    public void testCloning_LegendItemLabelGenerator() throws CloneNotSupportedException {
        StandardCategorySeriesLabelGenerator generator
                = new StandardCategorySeriesLabelGenerator(""Series {0}"");
        BarRenderer r1 = new BarRenderer();
        r1.setLegendItemLabelGenerator(generator);
        BarRenderer r2 = (BarRenderer) r1.clone();

        assertNotSame(r1, r2);
        assertSame(r1.getClass(), r2.getClass());
        assertEquals(r1, r2);

        // check that the generator has been cloned
        assertNotSame(r1.getLegendItemLabelGenerator(), r2.getLegendItemLabelGenerator());
    }

    /**
     * Check that the legendItemToolTipGenerator is cloned.
     * 
     * @throws java.lang.CloneNotSupportedException
     */
    @Test
    public void testCloning_LegendItemToolTipGenerator() throws CloneNotSupportedException {
        StandardCategorySeriesLabelGenerator generator
                = new StandardCategorySeriesLabelGenerator(""Series {0}"");
        BarRenderer r1 = new BarRenderer();
        r1.setLegendItemToolTipGenerator(generator);
        BarRenderer r2 = (BarRenderer) r1.clone();

        assertNotSame(r1, r2);
        assertSame(r1.getClass(), r2.getClass());
        assertEquals(r1, r2);

        // check that the generator has been cloned
        assertNotSame(r1.getLegendItemToolTipGenerator(), r2.getLegendItemToolTipGenerator());
    }

    /**
     * Check that the legendItemURLGenerator is cloned.
     * 
     * @throws java.lang.CloneNotSupportedException
     */
    @Test
    public void testCloning_LegendItemURLGenerator() throws CloneNotSupportedException {
        StandardCategorySeriesLabelGenerator generator
                = new StandardCategorySeriesLabelGenerator(""Series {0}"");
        BarRenderer r1 = new BarRenderer();
        r1.setLegendItemURLGenerator(generator);
        BarRenderer r2 = (BarRenderer) r1.clone();
        assertNotSame(r1, r2);
        assertSame(r1.getClass(), r2.getClass());
        assertEquals(r1, r2);

        // check that the generator has been cloned
        assertNotSame(r1.getLegendItemURLGenerator(), r2.getLegendItemURLGenerator());
    }

    /**
     * Some checks for the findRangeBounds() method.
     */
    @Test
    public void testFindRangeBounds() {
        AbstractCategoryItemRenderer r = new LineAndShapeRenderer();
        assertNull(r.findRangeBounds(null));

        // an empty dataset should return a null range
        DefaultCategoryDataset<String, String> dataset = new DefaultCategoryDataset<>();
        assertNull(r.findRangeBounds(dataset));

        dataset.addValue(1.0, ""R1"", ""C1"");
        assertEquals(new Range(1.0, 1.0), r.findRangeBounds(dataset));

        dataset.addValue(-2.0, ""R1"", ""C2"");
        assertEquals(new Range(-2.0, 1.0), r.findRangeBounds(dataset));

        dataset.addValue(null, ""R1"", ""C3"");
        assertEquals(new Range(-2.0, 1.0), r.findRangeBounds(dataset));
    }

    /**
     * A test that reproduces the problem reported in bug 2947660.
     */
    @Test
    public void test2947660() {
        AbstractCategoryItemRenderer r = new LineAndShapeRenderer();
        assertNotNull(r.getLegendItems());
        assertEquals(0, r.getLegendItems().getItemCount());

        DefaultCategoryDataset<String, String> dataset = new DefaultCategoryDataset<>();
        CategoryPlot<String, String> plot = new CategoryPlot<>();
        plot.setDataset(dataset);
        plot.setRenderer(r);
        assertEquals(0, r.getLegendItems().getItemCount());

        dataset.addValue(1.0, ""S1"", ""C1"");
        LegendItemCollection lic = r.getLegendItems();
        assertEquals(1, lic.getItemCount());
        assertEquals(""S1"", lic.get(0).getLabel());
    }

}

","/* ===========================================================
 * JFreeChart : a free chart library for the Java(tm) platform
 * ===========================================================
 *
 * (C) Copyright 2000-2022, by David Gilbert and Contributors.
 *
 * Project Info:  http://www.jfree.org/jfreechart/index.html
 *
 * This library is free software; you can redistribute it and/or modify it
 * under the terms of the GNU Lesser General Public License as published by
 * the Free Software Foundation; either version 2.1 of the License, or
 * (at your option) any later version.
 *
 * This library is distributed in the hope that it will be useful, but
 * WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
 * or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public
 * License for more details.
 *
 * You should have received a copy of the GNU Lesser General Public
 * License along with this library; if not, write to the Free Software
 * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301,
 * USA.
 *
 * [Oracle and Java are registered trademarks of Oracle and/or its affiliates. 
 * Other names may be trademarks of their respective owners.]
 *
 * ---------------------------------
 * AbstractCategoryItemRenderer.java
 * ---------------------------------
 * (C) Copyright 2002-2022, by David Gilbert.
 *
 * Original Author:  David Gilbert;
 * Contributor(s):   Richard Atkinson;
 *                   Peter Kolb (patch 2497611);
 * 
 */

package org.jfree.chart.renderer.category;

import java.awt.AlphaComposite;
import java.awt.Composite;
import java.awt.Font;
import java.awt.GradientPaint;
import java.awt.Graphics2D;
import java.awt.Paint;
import java.awt.RenderingHints;
import java.awt.Shape;
import java.awt.Stroke;
import java.awt.geom.Ellipse2D;
import java.awt.geom.Line2D;
import java.awt.geom.Point2D;
import java.awt.geom.Rectangle2D;
import java.io.Serializable;

import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.Objects;

import org.jfree.chart.legend.LegendItem;
import org.jfree.chart.legend.LegendItemCollection;
import org.jfree.chart.axis.CategoryAxis;
import org.jfree.chart.axis.ValueAxis;
import org.jfree.chart.entity.CategoryItemEntity;
import org.jfree.chart.entity.EntityCollection;
import org.jfree.chart.event.RendererChangeEvent;
import org.jfree.chart.labels.CategoryItemLabelGenerator;
import org.jfree.chart.labels.CategorySeriesLabelGenerator;
import org.jfree.chart.labels.CategoryToolTipGenerator;
import org.jfree.chart.labels.ItemLabelPosition;
import org.jfree.chart.labels.StandardCategorySeriesLabelGenerator;
import org.jfree.chart.plot.CategoryCrosshairState;
import org.jfree.chart.plot.CategoryMarker;
import org.jfree.chart.plot.CategoryPlot;
import org.jfree.chart.plot.DrawingSupplier;
import org.jfree.chart.plot.IntervalMarker;
import org.jfree.chart.plot.Marker;
import org.jfree.chart.plot.PlotOrientation;
import org.jfree.chart.plot.PlotRenderingInfo;
import org.jfree.chart.plot.ValueMarker;
import org.jfree.chart.renderer.AbstractRenderer;
import org.jfree.chart.text.TextUtils;
import org.jfree.chart.util.GradientPaintTransformer;
import org.jfree.chart.api.LengthAdjustmentType;
import org.jfree.chart.api.RectangleAnchor;
import org.jfree.chart.api.RectangleEdge;
import org.jfree.chart.api.RectangleInsets;
import org.jfree.chart.urls.CategoryURLGenerator;
import org.jfree.chart.internal.CloneUtils;
import org.jfree.chart.internal.Args;
import org.jfree.chart.api.PublicCloneable;
import org.jfree.chart.api.SortOrder;
import org.jfree.data.KeyedValues2DItemKey;
import org.jfree.data.Range;
import org.jfree.data.category.CategoryDataset;
import org.jfree.data.general.DatasetUtils;

/**
 * An abstract base class that you can use to implement a new
 * {@link CategoryItemRenderer}.  When you create a new
 * {@link CategoryItemRenderer} you are not required to extend this class,
 * but it makes the job easier.
 */
public abstract class AbstractCategoryItemRenderer extends AbstractRenderer
        implements CategoryItemRenderer, Cloneable, PublicCloneable,
        Serializable {

    /** For serialization. */
    private static final long serialVersionUID = 1247553218442497391L;

    /** The plot that the renderer is assigned to. */
    private CategoryPlot plot;

    /** A list of item label generators (one per series). */
    private Map<Integer, CategoryItemLabelGenerator> itemLabelGeneratorMap;

    /** The default item label generator. */
    private CategoryItemLabelGenerator defaultItemLabelGenerator;

    /** A list of tool tip generators (one per series). */
    private Map<Integer, CategoryToolTipGenerator> toolTipGeneratorMap;

    /** The default tool tip generator. */
    private CategoryToolTipGenerator defaultToolTipGenerator;

    /** A list of item label generators (one per series). */
    private Map<Integer, CategoryURLGenerator> itemURLGeneratorMap;

    /** The default item label generator. */
    private CategoryURLGenerator defaultItemURLGenerator;

    /** The legend item label generator. */
    private CategorySeriesLabelGenerator legendItemLabelGenerator;

    /** The legend item tool tip generator. */
    private CategorySeriesLabelGenerator legendItemToolTipGenerator;

    /** The legend item URL generator. */
    private CategorySeriesLabelGenerator legendItemURLGenerator;

    /** The number of rows in the dataset (temporary record). */
    private transient int rowCount;

    /** The number of columns in the dataset (temporary record). */
    private transient int columnCount;

    /**
     * Creates a new renderer with no tool tip generator and no URL generator.
     * The defaults (no tool tip or URL generators) have been chosen to
     * minimise the processing required to generate a default chart.  If you
     * require tool tips or URLs, then you can easily add the required
     * generators.
     */
    protected AbstractCategoryItemRenderer() {
        this.itemLabelGeneratorMap = new HashMap<>();
        this.toolTipGeneratorMap = new HashMap<>();
        this.itemURLGeneratorMap = new HashMap<>();
        this.legendItemLabelGenerator
                = new StandardCategorySeriesLabelGenerator();
    }

    /**
     * Returns the number of passes through the dataset required by the
     * renderer.  This method returns {@code 1}, subclasses should
     * override if they need more passes.
     *
     * @return The pass count.
     */
    @Override
    public int getPassCount() {
        return 1;
    }

    /**
     * Returns the plot that the renderer has been assigned to (where
     * {@code null} indicates that the renderer is not currently assigned
     * to a plot).
     *
     * @return The plot (possibly {@code null}).
     *
     * @see #setPlot(CategoryPlot)
     */
    @Override
    public CategoryPlot getPlot() {
        return this.plot;
    }

    /**
     * Sets the plot that the renderer has been assigned to.  This method is
     * usually called by the {@link CategoryPlot}, in normal usage you
     * shouldn't need to call this method directly.
     *
     * @param plot  the plot ({@code null} not permitted).
     *
     * @see #getPlot()
     */
    @Override
    public void setPlot(CategoryPlot plot) {
        Args.nullNotPermitted(plot, ""plot"");
        this.plot = plot;
    }

    // ITEM LABEL GENERATOR

    /**
     * Returns the item label generator for a data item.  This implementation
     * simply passes control to the {@link #getSeriesItemLabelGenerator(int)}
     * method.  If, for some reason, you want a different generator for
     * individual items, you can override this method.
     *
     * @param row  the row index (zero based).
     * @param column  the column index (zero based).
     *
     * @return The generator (possibly {@code null}).
     */
    @Override
    public CategoryItemLabelGenerator getItemLabelGenerator(int row,
            int column) {
        return getSeriesItemLabelGenerator(row);
    }

    /**
     * Returns the item label generator for a series.
     *
     * @param series  the series index (zero based).
     *
     * @return The generator (possibly {@code null}).
     *
     * @see #setSeriesItemLabelGenerator(int, CategoryItemLabelGenerator)
     */
    @Override
    public CategoryItemLabelGenerator getSeriesItemLabelGenerator(int series) {

        // otherwise look up the generator table
        CategoryItemLabelGenerator generator = this.itemLabelGeneratorMap.get(
                series);
        if (generator == null) {
            generator = this.defaultItemLabelGenerator;
        }
        return generator;
    }

    /**
     * Sets the item label generator for a series and sends a
     * {@link RendererChangeEvent} to all registered listeners.
     *
     * @param series  the series index (zero based).
     * @param generator  the generator ({@code null} permitted).
     *
     * @see #getSeriesItemLabelGenerator(int)
     */
    @Override
    public void setSeriesItemLabelGenerator(int series,
            CategoryItemLabelGenerator generator) {
        setSeriesItemLabelGenerator(series, generator, true);
    }
    
    /**
     * Sets the item label generator for a series and sends a
     * {@link RendererChangeEvent} to all registered listeners.
     *
     * @param series  the series index (zero based).
     * @param generator  the generator ({@code null} permitted).
     * @param notify  notify listeners?
     *
     * @see #getSeriesItemLabelGenerator(int)
     */
    @Override
    public void setSeriesItemLabelGenerator(int series,
            CategoryItemLabelGenerator generator, boolean notify) {
        this.itemLabelGeneratorMap.put(series, generator);
        if (notify) {
            fireChangeEvent();
        }
    }

    /**
     * Returns the default item label generator.
     *
     * @return The generator (possibly {@code null}).
     *
     * @see #setDefaultItemLabelGenerator(CategoryItemLabelGenerator)
     */
    @Override
    public CategoryItemLabelGenerator getDefaultItemLabelGenerator() {
        return this.defaultItemLabelGenerator;
    }

    /**
     * Sets the default item label generator and sends a
     * {@link RendererChangeEvent} to all registered listeners.
     *
     * @param generator  the generator ({@code null} permitted).
     *
     * @see #getDefaultItemLabelGenerator()
     */
    @Override
    public void setDefaultItemLabelGenerator(
            CategoryItemLabelGenerator generator) {
        setDefaultItemLabelGenerator(generator, true);
    }
    
    /**
     * Sets the default item label generator and sends a
     * {@link RendererChangeEvent} to all registered listeners.
     *
     * @param generator  the generator ({@code null} permitted).
     * @param notify  notify listeners?
     *
     * @see #getDefaultItemLabelGenerator()
     */
    @Override
    public void setDefaultItemLabelGenerator(
            CategoryItemLabelGenerator generator, boolean notify) {
        this.defaultItemLabelGenerator = generator;
        if (notify) {
            fireChangeEvent();
        }
    }

    // TOOL TIP GENERATOR

    /**
     * Returns the tool tip generator that should be used for the specified
     * item.  This method looks up the generator using the ""three-layer""
     * approach outlined in the general description of this interface.  You
     * can override this method if you want to return a different generator per
     * item.
     *
     * @param row  the row index (zero-based).
     * @param column  the column index (zero-based).
     *
     * @return The generator (possibly {@code null}).
     */
    @Override
    public CategoryToolTipGenerator getToolTipGenerator(int row, int column) {

        CategoryToolTipGenerator result = getSeriesToolTipGenerator(row);
        if (result == null) {
            result = this.defaultToolTipGenerator;
        }
        return result;
    }

    /**
     * Returns the tool tip generator for the specified series (a ""layer 1""
     * generator).
     *
     * @param series  the series index (zero-based).
     *
     * @return The tool tip generator (possibly {@code null}).
     *
     * @see #setSeriesToolTipGenerator(int, CategoryToolTipGenerator)
     */
    @Override
    public CategoryToolTipGenerator getSeriesToolTipGenerator(int series) {
        return this.toolTipGeneratorMap.get(series);
    }

    /**
     * Sets the tool tip generator for a series and sends a
     * {@link RendererChangeEvent} to all registered listeners.
     *
     * @param series  the series index (zero-based).
     * @param generator  the generator ({@code null} permitted).
     *
     * @see #getSeriesToolTipGenerator(int)
     */
    @Override
    public void setSeriesToolTipGenerator(int series,
            CategoryToolTipGenerator generator) {
        setSeriesToolTipGenerator(series, generator, true);
    }
    
    /**
     * Sets the tool tip generator for a series and sends a
     * {@link RendererChangeEvent} to all registered listeners.
     *
     * @param series  the series index (zero-based).
     * @param generator  the generator ({@code null} permitted).
     * @param notify  notify listeners?
     *
     * @see #getSeriesToolTipGenerator(int)
     */
    @Override
    public void setSeriesToolTipGenerator(int series,
            CategoryToolTipGenerator generator, boolean notify) {
        this.toolTipGeneratorMap.put(series, generator);
        if (notify) {
            fireChangeEvent();
        }
    }

    /**
     * Returns the default tool tip generator (the ""layer 2"" generator).
     *
     * @return The tool tip generator (possibly {@code null}).
     *
     * @see #setDefaultToolTipGenerator(CategoryToolTipGenerator)
     */
    @Override
    public CategoryToolTipGenerator getDefaultToolTipGenerator() {
        return this.defaultToolTipGenerator;
    }

    /**
     * Sets the default tool tip generator and sends a {@link RendererChangeEvent}
     * to all registered listeners.
     *
     * @param generator  the generator ({@code null} permitted).
     *
     * @see #getDefaultToolTipGenerator()
     */
    @Override
    public void setDefaultToolTipGenerator(CategoryToolTipGenerator generator) {
        setDefaultToolTipGenerator(generator, true);
    }
    
    /**
     * Sets the default tool tip generator and sends a {@link RendererChangeEvent}
     * to all registered listeners.
     *
     * @param generator  the generator ({@code null} permitted).
     * @param notify  notify listeners?
     *
     * @see #getDefaultToolTipGenerator()
     */
    @Override
    public void setDefaultToolTipGenerator(CategoryToolTipGenerator generator, boolean notify) {
        this.defaultToolTipGenerator = generator;
        if (notify) {
            fireChangeEvent();
        }
    }

    // URL GENERATOR

    /**
     * Returns the URL generator for a data item.  This method just calls the
     * getSeriesItemURLGenerator method, but you can override this behaviour if
     * you want to.
     *
     * @param row  the row index (zero based).
     * @param column  the column index (zero based).
     *
     * @return The URL generator.
     */
    @Override
    public CategoryURLGenerator getItemURLGenerator(int row, int column) {
        return getSeriesItemURLGenerator(row);
    }

    /**
     * Returns the URL generator for a series.
     *
     * @param series  the series index (zero based).
     *
     * @return The URL generator for the series.
     *
     * @see #setSeriesItemURLGenerator(int, CategoryURLGenerator)
     */
    @Override
    public CategoryURLGenerator getSeriesItemURLGenerator(int series) {
        // otherwise look up the generator table
        CategoryURLGenerator generator = this.itemURLGeneratorMap.get(series);
        if (generator == null) {
            generator = this.defaultItemURLGenerator;
        }
        return generator;
    }

    /**
     * Sets the URL generator for a series and sends a
     * {@link RendererChangeEvent} to all registered listeners.
     *
     * @param series  the series index (zero based).
     * @param generator  the generator.
     *
     * @see #getSeriesItemURLGenerator(int)
     */
    @Override
    public void setSeriesItemURLGenerator(int series,
            CategoryURLGenerator generator) {
        setSeriesItemURLGenerator(series, generator, true);
    }
    
    /**
     * Sets the URL generator for a series and sends a
     * {@link RendererChangeEvent} to all registered listeners.
     *
     * @param series  the series index (zero based).
     * @param generator  the generator.
     * @param notify  notify listeners?
     *
     * @see #getSeriesItemURLGenerator(int)
     */
    @Override
    public void setSeriesItemURLGenerator(int series,
            CategoryURLGenerator generator, boolean notify) {
        this.itemURLGeneratorMap.put(series, generator);
        if (notify) {
            fireChangeEvent();
        }
    }

    /**
     * Returns the default item URL generator.
     *
     * @return The item URL generator.
     *
     * @see #setDefaultItemURLGenerator(CategoryURLGenerator)
     */
    @Override
    public CategoryURLGenerator getDefaultItemURLGenerator() {
        return this.defaultItemURLGenerator;
    }

    /**
     * Sets the default item URL generator and sends a
     * {@link RendererChangeEvent} to all registered listeners.
     *
     * @param generator  the item URL generator ({@code null} permitted).
     *
     * @see #getDefaultItemURLGenerator()
     */
    @Override
    public void setDefaultItemURLGenerator(CategoryURLGenerator generator) {
        setDefaultItemURLGenerator(generator, true);
    }
    
    /**
     * Sets the default item URL generator and sends a
     * {@link RendererChangeEvent} to all registered listeners.
     *
     * @param generator  the item URL generator ({@code null} permitted).
     * @param notify  notify listeners?
     *
     * @see #getDefaultItemURLGenerator()
     */
    @Override
    public void setDefaultItemURLGenerator(CategoryURLGenerator generator, boolean notify) {
        this.defaultItemURLGenerator = generator;
        if (notify) {
            fireChangeEvent();
        }
    }

    /**
     * Returns the number of rows in the dataset.  This value is updated in the
     * {@link AbstractCategoryItemRenderer#initialise} method.
     *
     * @return The row count.
     */
    public int getRowCount() {
        return this.rowCount;
    }

    /**
     * Returns the number of columns in the dataset.  This value is updated in
     * the {@link AbstractCategoryItemRenderer#initialise} method.
     *
     * @return The column count.
     */
    public int getColumnCount() {
        return this.columnCount;
    }

    /**
     * Creates a new state instance---this method is called from the
     * {@link #initialise(Graphics2D, Rectangle2D, CategoryPlot, int,
     * PlotRenderingInfo)} method.  Subclasses can override this method if
     * they need to use a subclass of {@link CategoryItemRendererState}.
     *
     * @param info  collects plot rendering info ({@code null} permitted).
     *
     * @return The new state instance (never {@code null}).
     */
    protected CategoryItemRendererState createState(PlotRenderingInfo info) {
        return new CategoryItemRendererState(info);
    }

    /**
     * Initialises the renderer and returns a state object that will be used
     * for the remainder of the drawing process for a single chart.  The state
     * object allows for the fact that the renderer may be used simultaneously
     * by multiple threads (each thread will work with a separate state object).
     *
     * @param g2  the graphics device.
     * @param dataArea  the data area.
     * @param plot  the plot.
     * @param rendererIndex  the renderer index.
     * @param info  an object for returning information about the structure of
     *              the plot ({@code null} permitted).
     *
     * @return The renderer state.
     */
    @Override
    public CategoryItemRendererState initialise(Graphics2D g2,
            Rectangle2D dataArea, CategoryPlot plot, int rendererIndex,
            PlotRenderingInfo info) {

        setPlot(plot);
        CategoryDataset data = plot.getDataset(rendererIndex);
        if (data != null) {
            this.rowCount = data.getRowCount();
            this.columnCount = data.getColumnCount();
        } else {
            this.rowCount = 0;
            this.columnCount = 0;
        }
        CategoryItemRendererState state = createState(info);
        state.setElementHinting(plot.fetchElementHintingFlag());
        int[] visibleSeriesTemp = new int[this.rowCount];
        int visibleSeriesCount = 0;
        for (int row = 0; row < this.rowCount; row++) {
            if (isSeriesVisible(row)) {
                visibleSeriesTemp[visibleSeriesCount] = row;
                visibleSeriesCount++;
            }
        }
        int[] visibleSeries = new int[visibleSeriesCount];
        System.arraycopy(visibleSeriesTemp, 0, visibleSeries, 0,
                visibleSeriesCount);
        state.setVisibleSeriesArray(visibleSeries);
        return state;
    }

    /**
     * Adds a {@code KEY_BEGIN_ELEMENT} hint to the graphics target.  This
     * hint is recognised by <b>JFreeSVG</b> (in theory it could be used by 
     * other {@code Graphics2D} implementations also).
     * 
     * @param g2  the graphics target ({@code null} not permitted).
     * @param rowKey  the row key that identifies the element ({@code null} not
     *     permitted).
     * @param columnKey  the column key that identifies the element 
     *     ({@code null} not permitted). 
     */
    protected void beginElementGroup(Graphics2D g2, Comparable rowKey,
            Comparable columnKey) {
        beginElementGroup(g2, new KeyedValues2DItemKey(rowKey, columnKey));    
    }
    
    /**
     * Returns the range of values the renderer requires to display all the
     * items from the specified dataset.
     *
     * @param dataset  the dataset ({@code null} permitted).
     *
     * @return The range (or {@code null} if the dataset is
     *         {@code null} or empty).
     */
    @Override
    public Range findRangeBounds(CategoryDataset dataset) {
        return findRangeBounds(dataset, false);
    }

    /**
     * Returns the range of values the renderer requires to display all the
     * items from the specified dataset.
     *
     * @param dataset  the dataset ({@code null} permitted).
     * @param includeInterval  include the y-interval if the dataset has one.
     *
     * @return The range ({@code null} if the dataset is {@code null} or empty).
     */
    protected Range findRangeBounds(CategoryDataset dataset,
            boolean includeInterval) {
        if (dataset == null) {
            return null;
        }
        if (getDataBoundsIncludesVisibleSeriesOnly()) {
            List visibleSeriesKeys = new ArrayList();
            int seriesCount = dataset.getRowCount();
            for (int s = 0; s < seriesCount; s++) {
                if (isSeriesVisible(s)) {
                    visibleSeriesKeys.add(dataset.getRowKey(s));
                }
            }
            return DatasetUtils.findRangeBounds(dataset,
                    visibleSeriesKeys, includeInterval);
        }
        else {
            return DatasetUtils.findRangeBounds(dataset, includeInterval);
        }
    }

    /**
     * Returns the Java2D coordinate for the middle of the specified data item.
     *
     * @param rowKey  the row key.
     * @param columnKey  the column key.
     * @param dataset  the dataset.
     * @param axis  the axis.
     * @param area  the data area.
     * @param edge  the edge along which the axis lies.
     *
     * @return The Java2D coordinate for the middle of the item.
     */
    @Override
    public double getItemMiddle(Comparable<?> rowKey, Comparable<?> columnKey,
                                CategoryDataset<?, ?> dataset, CategoryAxis axis, Rectangle2D area,
                                RectangleEdge edge) {
        return axis.getCategoryMiddle(columnKey, dataset.getColumnKeys(), area,
                edge);
    }

    /**
     * Draws a background for the data area.  The default implementation just
     * gets the plot to draw the background, but some renderers will override
     * this behaviour.
     *
     * @param g2  the graphics device.
     * @param plot  the plot.
     * @param dataArea  the data area.
     */
    @Override
    public void drawBackground(Graphics2D g2, CategoryPlot plot,
            Rectangle2D dataArea) {
        plot.drawBackground(g2, dataArea);
    }

    /**
     * Draws an outline for the data area.  The default implementation just
     * gets the plot to draw the outline, but some renderers will override this
     * behaviour.
     *
     * @param g2  the graphics device.
     * @param plot  the plot.
     * @param dataArea  the data area.
     */
    @Override
    public void drawOutline(Graphics2D g2, CategoryPlot plot,
            Rectangle2D dataArea) {
        plot.drawOutline(g2, dataArea);
    }

    /**
     * Draws a grid line against the domain axis.
     * <P>
     * Note that this default implementation assumes that the horizontal axis
     * is the domain axis. If this is not the case, you will need to override
     * this method.
     *
     * @param g2  the graphics device.
     * @param plot  the plot.
     * @param dataArea  the area for plotting data.
     * @param value  the Java2D value at which the grid line should be drawn.
     *
     */
    @Override
    public void drawDomainGridline(Graphics2D g2, CategoryPlot plot,
           Rectangle2D dataArea, double value) {

        Line2D line = null;
        PlotOrientation orientation = plot.getOrientation();

        if (orientation == PlotOrientation.HORIZONTAL) {
            line = new Line2D.Double(dataArea.getMinX(), value,
                    dataArea.getMaxX(), value);
        }
        else if (orientation == PlotOrientation.VERTICAL) {
            line = new Line2D.Double(value, dataArea.getMinY(), value,
                    dataArea.getMaxY());
        }

        Paint paint = plot.getDomainGridlinePaint();
        if (paint == null) {
            paint = CategoryPlot.DEFAULT_GRIDLINE_PAINT;
        }
        g2.setPaint(paint);

        Stroke stroke = plot.getDomainGridlineStroke();
        if (stroke == null) {
            stroke = CategoryPlot.DEFAULT_GRIDLINE_STROKE;
        }
        g2.setStroke(stroke);
        Object saved = g2.getRenderingHint(RenderingHints.KEY_STROKE_CONTROL);
        g2.setRenderingHint(RenderingHints.KEY_STROKE_CONTROL, 
                RenderingHints.VALUE_STROKE_NORMALIZE);
        g2.draw(line);
        g2.setRenderingHint(RenderingHints.KEY_STROKE_CONTROL, saved);
    }

    /**
     * Draws a line perpendicular to the range axis.
     *
     * @param g2  the graphics device.
     * @param plot  the plot.
     * @param axis  the value axis.
     * @param dataArea  the area for plotting data.
     * @param value  the value at which the grid line should be drawn.
     * @param paint  the paint ({@code null} not permitted).
     * @param stroke  the stroke ({@code null} not permitted).
     */
    @Override
    public void drawRangeLine(Graphics2D g2, CategoryPlot plot, ValueAxis axis,
            Rectangle2D dataArea, double value, Paint paint, Stroke stroke) {

        Range range = axis.getRange();
        if (!range.contains(value)) {
            return;
        }

        PlotOrientation orientation = plot.getOrientation();
        Line2D line = null;
        double v = axis.valueToJava2D(value, dataArea, plot.getRangeAxisEdge());
        if (orientation == PlotOrientation.HORIZONTAL) {
            line = new Line2D.Double(v, dataArea.getMinY(), v,
                    dataArea.getMaxY());
        } else if (orientation == PlotOrientation.VERTICAL) {
            line = new Line2D.Double(dataArea.getMinX(), v,
                    dataArea.getMaxX(), v);
        }

        g2.setPaint(paint);
        g2.setStroke(stroke);
        Object saved = g2.getRenderingHint(RenderingHints.KEY_STROKE_CONTROL);
        g2.setRenderingHint(RenderingHints.KEY_STROKE_CONTROL, 
                RenderingHints.VALUE_STROKE_NORMALIZE);
        g2.draw(line);
        g2.setRenderingHint(RenderingHints.KEY_STROKE_CONTROL, saved);
    }

    /**
     * Draws a marker for the domain axis.
     *
     * @param g2  the graphics device (not {@code null}).
     * @param plot  the plot (not {@code null}).
     * @param axis  the range axis (not {@code null}).
     * @param marker  the marker to be drawn (not {@code null}).
     * @param dataArea  the area inside the axes (not {@code null}).
     *
     * @see #drawRangeMarker(Graphics2D, CategoryPlot, ValueAxis, Marker,
     *     Rectangle2D)
     */
    @Override
    public void drawDomainMarker(Graphics2D g2, CategoryPlot plot,
            CategoryAxis axis, CategoryMarker marker, Rectangle2D dataArea) {

        Comparable category = marker.getKey();
        CategoryDataset dataset = plot.getDataset(plot.getIndexOf(this));
        int columnIndex = dataset.getColumnIndex(category);
        if (columnIndex < 0) {
            return;
        }

        final Composite savedComposite = g2.getComposite();
        g2.setComposite(AlphaComposite.getInstance(
                AlphaComposite.SRC_OVER, marker.getAlpha()));

        PlotOrientation orientation = plot.getOrientation();
        Rectangle2D bounds;
        if (marker.getDrawAsLine()) {
            double v = axis.getCategoryMiddle(columnIndex,
                    dataset.getColumnCount(), dataArea,
                    plot.getDomainAxisEdge());
            Line2D line = null;
            if (orientation == PlotOrientation.HORIZONTAL) {
                line = new Line2D.Double(dataArea.getMinX(), v,
                        dataArea.getMaxX(), v);
            }
            else if (orientation == PlotOrientation.VERTICAL) {
                line = new Line2D.Double(v, dataArea.getMinY(), v,
                        dataArea.getMaxY());
            } else {
                throw new IllegalStateException();
            }
            g2.setPaint(marker.getPaint());
            g2.setStroke(marker.getStroke());
            g2.draw(line);
            bounds = line.getBounds2D();
        }
        else {
            double v0 = axis.getCategoryStart(columnIndex,
                    dataset.getColumnCount(), dataArea,
                    plot.getDomainAxisEdge());
            double v1 = axis.getCategoryEnd(columnIndex,
                    dataset.getColumnCount(), dataArea,
                    plot.getDomainAxisEdge());
            Rectangle2D area = null;
            if (orientation == PlotOrientation.HORIZONTAL) {
                area = new Rectangle2D.Double(dataArea.getMinX(), v0,
                        dataArea.getWidth(), (v1 - v0));
            }
            else if (orientation == PlotOrientation.VERTICAL) {
                area = new Rectangle2D.Double(v0, dataArea.getMinY(),
                        (v1 - v0), dataArea.getHeight());
            }
            g2.setPaint(marker.getPaint());
            g2.fill(area);
            bounds = area;
        }

        String label = marker.getLabel();
        RectangleAnchor anchor = marker.getLabelAnchor();
        if (label != null) {
            Font labelFont = marker.getLabelFont();
            g2.setFont(labelFont);
            g2.setPaint(marker.getLabelPaint());
            Point2D coordinates = calculateDomainMarkerTextAnchorPoint(
                    g2, orientation, dataArea, bounds, marker.getLabelOffset(),
                    marker.getLabelOffsetType(), anchor);
            TextUtils.drawAlignedString(label, g2,
                    (float) coordinates.getX(), (float) coordinates.getY(),
                    marker.getLabelTextAnchor());
        }
        g2.setComposite(savedComposite);
    }

    /**
     * Draws a marker for the range axis.
     *
     * @param g2  the graphics device (not {@code null}).
     * @param plot  the plot (not {@code null}).
     * @param axis  the range axis (not {@code null}).
     * @param marker  the marker to be drawn (not {@code null}).
     * @param dataArea  the area inside the axes (not {@code null}).
     *
     * @see #drawDomainMarker(Graphics2D, CategoryPlot, CategoryAxis,
     *     CategoryMarker, Rectangle2D)
     */
    @Override
    public void drawRangeMarker(Graphics2D g2, CategoryPlot plot,
            ValueAxis axis, Marker marker, Rectangle2D dataArea) {

        if (marker instanceof ValueMarker) {
            ValueMarker vm = (ValueMarker) marker;
            double value = vm.getValue();
            Range range = axis.getRange();

            if (!range.contains(value)) {
                return;
            }

            final Composite savedComposite = g2.getComposite();
            g2.setComposite(AlphaComposite.getInstance(
                    AlphaComposite.SRC_OVER, marker.getAlpha()));

            PlotOrientation orientation = plot.getOrientation();
            double v = axis.valueToJava2D(value, dataArea,
                    plot.getRangeAxisEdge());
            Line2D line = null;
            if (orientation == PlotOrientation.HORIZONTAL) {
                line = new Line2D.Double(v, dataArea.getMinY(), v,
                        dataArea.getMaxY());
            }
            else if (orientation == PlotOrientation.VERTICAL) {
                line = new Line2D.Double(dataArea.getMinX(), v,
                        dataArea.getMaxX(), v);
            } else {
                throw new IllegalStateException();
            }

            g2.setPaint(marker.getPaint());
            g2.setStroke(marker.getStroke());
            g2.draw(line);

            String label = marker.getLabel();
            RectangleAnchor anchor = marker.getLabelAnchor();
            if (label != null) {
                Font labelFont = marker.getLabelFont();
                g2.setFont(labelFont);
                Point2D coordinates = calculateRangeMarkerTextAnchorPoint(
                        g2, orientation, dataArea, line.getBounds2D(),
                        marker.getLabelOffset(), LengthAdjustmentType.EXPAND,
                        anchor);
                Rectangle2D rect = TextUtils.calcAlignedStringBounds(label, g2, 
                        (float) coordinates.getX(), (float) coordinates.getY(), 
                        marker.getLabelTextAnchor());
                g2.setPaint(marker.getLabelBackgroundColor());
                g2.fill(rect);
                g2.setPaint(marker.getLabelPaint());
                TextUtils.drawAlignedString(label, g2, 
                        (float) coordinates.getX(), (float) coordinates.getY(),
                        marker.getLabelTextAnchor());
            }
            g2.setComposite(savedComposite);
        }
        else if (marker instanceof IntervalMarker) {
            IntervalMarker im = (IntervalMarker) marker;
            double start = im.getStartValue();
            double end = im.getEndValue();
            Range range = axis.getRange();
            if (!(range.intersects(start, end))) {
                return;
            }

            final Composite savedComposite = g2.getComposite();
            g2.setComposite(AlphaComposite.getInstance(
                    AlphaComposite.SRC_OVER, marker.getAlpha()));

            double start2d = axis.valueToJava2D(start, dataArea,
                    plot.getRangeAxisEdge());
            double end2d = axis.valueToJava2D(end, dataArea,
                    plot.getRangeAxisEdge());
            double low = Math.min(start2d, end2d);
            double high = Math.max(start2d, end2d);

            PlotOrientation orientation = plot.getOrientation();
            Rectangle2D rect = null;
            if (orientation == PlotOrientation.HORIZONTAL) {
                // clip left and right bounds to data area
                low = Math.max(low, dataArea.getMinX());
                high = Math.min(high, dataArea.getMaxX());
                rect = new Rectangle2D.Double(low,
                        dataArea.getMinY(), high - low,
                        dataArea.getHeight());
            }
            else if (orientation == PlotOrientation.VERTICAL) {
                // clip top and bottom bounds to data area
                low = Math.max(low, dataArea.getMinY());
                high = Math.min(high, dataArea.getMaxY());
                rect = new Rectangle2D.Double(dataArea.getMinX(),
                        low, dataArea.getWidth(),
                        high - low);
            }
            Paint p = marker.getPaint();
            if (p instanceof GradientPaint) {
                GradientPaint gp = (GradientPaint) p;
                GradientPaintTransformer t = im.getGradientPaintTransformer();
                if (t != null) {
                    gp = t.transform(gp, rect);
                }
                g2.setPaint(gp);
            }
            else {
                g2.setPaint(p);
            }
            g2.fill(rect);

            // now draw the outlines, if visible...
            if (im.getOutlinePaint() != null && im.getOutlineStroke() != null) {
                if (orientation == PlotOrientation.VERTICAL) {
                    Line2D line = new Line2D.Double();
                    double x0 = dataArea.getMinX();
                    double x1 = dataArea.getMaxX();
                    g2.setPaint(im.getOutlinePaint());
                    g2.setStroke(im.getOutlineStroke());
                    if (range.contains(start)) {
                        line.setLine(x0, start2d, x1, start2d);
                        g2.draw(line);
                    }
                    if (range.contains(end)) {
                        line.setLine(x0, end2d, x1, end2d);
                        g2.draw(line);
                    }
                } else { // PlotOrientation.HORIZONTAL
                    Line2D line = new Line2D.Double();
                    double y0 = dataArea.getMinY();
                    double y1 = dataArea.getMaxY();
                    g2.setPaint(im.getOutlinePaint());
                    g2.setStroke(im.getOutlineStroke());
                    if (range.contains(start)) {
                        line.setLine(start2d, y0, start2d, y1);
                        g2.draw(line);
                    }
                    if (range.contains(end)) {
                        line.setLine(end2d, y0, end2d, y1);
                        g2.draw(line);
                    }
                }
            }

            String label = marker.getLabel();
            RectangleAnchor anchor = marker.getLabelAnchor();
            if (label != null) {
                Font labelFont = marker.getLabelFont();
                g2.setFont(labelFont);
                Point2D coords = calculateRangeMarkerTextAnchorPoint(
                        g2, orientation, dataArea, rect,
                        marker.getLabelOffset(), marker.getLabelOffsetType(),
                        anchor);
                Rectangle2D r = TextUtils.calcAlignedStringBounds(label, 
                        g2, (float) coords.getX(), (float) coords.getY(), 
                        marker.getLabelTextAnchor());
                g2.setPaint(marker.getLabelBackgroundColor());
                g2.fill(r);
                g2.setPaint(marker.getLabelPaint());
                TextUtils.drawAlignedString(label, g2,
                        (float) coords.getX(), (float) coords.getY(),
                        marker.getLabelTextAnchor());
            }
            g2.setComposite(savedComposite);
        }
    }

    /**
     * Calculates the {@code (x, y)} coordinates for drawing the label for a 
     * marker on the range axis.
     *
     * @param g2  the graphics device.
     * @param orientation  the plot orientation.
     * @param dataArea  the data area.
     * @param markerArea  the rectangle surrounding the marker.
     * @param markerOffset  the marker offset.
     * @param labelOffsetType  the label offset type.
     * @param anchor  the label anchor.
     *
     * @return The coordinates for drawing the marker label.
     */
    protected Point2D calculateDomainMarkerTextAnchorPoint(Graphics2D g2,
            PlotOrientation orientation, Rectangle2D dataArea,
            Rectangle2D markerArea, RectangleInsets markerOffset,
            LengthAdjustmentType labelOffsetType, RectangleAnchor anchor) {

        Rectangle2D anchorRect = null;
        if (orientation == PlotOrientation.HORIZONTAL) {
            anchorRect = markerOffset.createAdjustedRectangle(markerArea,
                    LengthAdjustmentType.CONTRACT, labelOffsetType);
        } else if (orientation == PlotOrientation.VERTICAL) {
            anchorRect = markerOffset.createAdjustedRectangle(markerArea,
                    labelOffsetType, LengthAdjustmentType.CONTRACT);
        }
        return anchor.getAnchorPoint(anchorRect);
    }

    /**
     * Calculates the (x, y) coordinates for drawing a marker label.
     *
     * @param g2  the graphics device.
     * @param orientation  the plot orientation.
     * @param dataArea  the data area.
     * @param markerArea  the rectangle surrounding the marker.
     * @param markerOffset  the marker offset.
     * @param labelOffsetType  the label offset type.
     * @param anchor  the label anchor.
     *
     * @return The coordinates for drawing the marker label.
     */
    protected Point2D calculateRangeMarkerTextAnchorPoint(Graphics2D g2,
            PlotOrientation orientation, Rectangle2D dataArea,
            Rectangle2D markerArea, RectangleInsets markerOffset,
            LengthAdjustmentType labelOffsetType, RectangleAnchor anchor) {

        Rectangle2D anchorRect = null;
        if (orientation == PlotOrientation.HORIZONTAL) {
            anchorRect = markerOffset.createAdjustedRectangle(markerArea,
                    labelOffsetType, LengthAdjustmentType.CONTRACT);
        } else if (orientation == PlotOrientation.VERTICAL) {
            anchorRect = markerOffset.createAdjustedRectangle(markerArea,
                    LengthAdjustmentType.CONTRACT, labelOffsetType);
        }
        return anchor.getAnchorPoint(anchorRect);

    }

    /**
     * Returns a legend item for a series.  This default implementation will
     * return {@code null} if {@link #isSeriesVisible(int)} or
     * {@link #isSeriesVisibleInLegend(int)} returns {@code false}.
     *
     * @param datasetIndex  the dataset index (zero-based).
     * @param series  the series index (zero-based).
     *
     * @return The legend item (possibly {@code null}).
     *
     * @see #getLegendItems()
     */
    @Override
    public LegendItem getLegendItem(int datasetIndex, int series) {

        CategoryPlot p = getPlot();
        if (p == null) {
            return null;
        }

        // check that a legend item needs to be displayed...
        if (!isSeriesVisible(series) || !isSeriesVisibleInLegend(series)) {
            return null;
        }

        CategoryDataset dataset = p.getDataset(datasetIndex);
        String label = this.legendItemLabelGenerator.generateLabel(dataset,
                series);
        String description = label;
        String toolTipText = null;
        if (this.legendItemToolTipGenerator != null) {
            toolTipText = this.legendItemToolTipGenerator.generateLabel(
                    dataset, series);
        }
        String urlText = null;
        if (this.legendItemURLGenerator != null) {
            urlText = this.legendItemURLGenerator.generateLabel(dataset,
                    series);
        }
        Shape shape = lookupLegendShape(series);
        Paint paint = lookupSeriesPaint(series);
        Paint outlinePaint = lookupSeriesOutlinePaint(series);
        Stroke outlineStroke = lookupSeriesOutlineStroke(series);

        LegendItem item = new LegendItem(label, description, toolTipText,
                urlText, shape, paint, outlineStroke, outlinePaint);
        item.setLabelFont(lookupLegendTextFont(series));
        Paint labelPaint = lookupLegendTextPaint(series);
        if (labelPaint != null) {
            item.setLabelPaint(labelPaint);
        }
        item.setSeriesKey(dataset.getRowKey(series));
        item.setSeriesIndex(series);
        item.setDataset(dataset);
        item.setDatasetIndex(datasetIndex);
        return item;
    }

    /**
     * Tests this renderer for equality with another object.
     *
     * @param obj  the object.
     *
     * @return {@code true} or {@code false}.
     */
    @Override
    public boolean equals(Object obj) {
        if (obj == this) {
            return true;
        }
        if (!(obj instanceof AbstractCategoryItemRenderer)) {
            return false;
        }
        AbstractCategoryItemRenderer that = (AbstractCategoryItemRenderer) obj;

        if (!Objects.equals(this.itemLabelGeneratorMap, that.itemLabelGeneratorMap)) {
            return false;
        }
        if (!Objects.equals(this.defaultItemLabelGenerator, that.defaultItemLabelGenerator)) {
            return false;
        }
        if (!Objects.equals(this.toolTipGeneratorMap, that.toolTipGeneratorMap)) {
            return false;
        }
        if (!Objects.equals(this.defaultToolTipGenerator, that.defaultToolTipGenerator)) {
            return false;
        }
        if (!Objects.equals(this.itemURLGeneratorMap, that.itemURLGeneratorMap)) {
            return false;
        }
        if (!Objects.equals(this.defaultItemURLGenerator, that.defaultItemURLGenerator)) {
            return false;
        }
        if (!Objects.equals(this.legendItemLabelGenerator, that.legendItemLabelGenerator)) {
            return false;
        }
        if (!Objects.equals(this.legendItemToolTipGenerator, that.legendItemToolTipGenerator)) {
            return false;
        }
        if (!Objects.equals(this.legendItemURLGenerator, that.legendItemURLGenerator)) {
            return false;
        }
        return super.equals(obj);
    }

    /**
     * Returns a hash code for the renderer.
     *
     * @return The hash code.
     */
    @Override
    public int hashCode() {
        int result = super.hashCode();
        return result;
    }

    /**
     * Returns the drawing supplier from the plot.
     *
     * @return The drawing supplier (possibly {@code null}).
     */
    @Override
    public DrawingSupplier getDrawingSupplier() {
        DrawingSupplier result = null;
        CategoryPlot cp = getPlot();
        if (cp != null) {
            result = cp.getDrawingSupplier();
        }
        return result;
    }

    /**
     * Considers the current (x, y) coordinate and updates the crosshair point
     * if it meets the criteria (usually means the (x, y) coordinate is the
     * closest to the anchor point so far).
     *
     * @param crosshairState  the crosshair state ({@code null} permitted,
     *                        but the method does nothing in that case).
     * @param rowKey  the row key.
     * @param columnKey  the column key.
     * @param value  the data value.
     * @param datasetIndex  the dataset index.
     * @param transX  the x-value translated to Java2D space.
     * @param transY  the y-value translated to Java2D space.
     * @param orientation  the plot orientation ({@code null} not permitted).
     */
    protected void updateCrosshairValues(CategoryCrosshairState crosshairState,
            Comparable rowKey, Comparable columnKey, double value,
            int datasetIndex,
            double transX, double transY, PlotOrientation orientation) {

        Args.nullNotPermitted(orientation, ""orientation"");

        if (crosshairState != null) {
            if (this.plot.isRangeCrosshairLockedOnData()) {
                // both axes
                crosshairState.updateCrosshairPoint(rowKey, columnKey, value,
                        datasetIndex, transX, transY, orientation);
            }
            else {
                crosshairState.updateCrosshairX(rowKey, columnKey,
                        datasetIndex, transX, orientation);
            }
        }
    }

    /**
     * Draws an item label.
     *
     * @param g2  the graphics device.
     * @param orientation  the orientation.
     * @param dataset  the dataset.
     * @param row  the row.
     * @param column  the column.
     * @param x  the x coordinate (in Java2D space).
     * @param y  the y coordinate (in Java2D space).
     * @param negative  indicates a negative value (which affects the item
     *                  label position).
     */
    protected void drawItemLabel(Graphics2D g2, PlotOrientation orientation,
            CategoryDataset dataset, int row, int column,
            double x, double y, boolean negative) {

        CategoryItemLabelGenerator generator = getItemLabelGenerator(row,
                column);
        if (generator != null) {
            Font labelFont = getItemLabelFont(row, column);
            Paint paint = getItemLabelPaint(row, column);
            g2.setFont(labelFont);
            g2.setPaint(paint);
            String label = generator.generateLabel(dataset, row, column);
            ItemLabelPosition position;
            if (!negative) {
                position = getPositiveItemLabelPosition(row, column);
            }
            else {
                position = getNegativeItemLabelPosition(row, column);
            }
            Point2D anchorPoint = calculateLabelAnchorPoint(
                    position.getItemLabelAnchor(), x, y, orientation);
            TextUtils.drawRotatedString(label, g2,
                    (float) anchorPoint.getX(), (float) anchorPoint.getY(),
                    position.getTextAnchor(),
                    position.getAngle(), position.getRotationAnchor());
        }

    }

    /**
     * Returns an independent copy of the renderer.  The {@code plot}
     * reference is shallow copied.
     *
     * @return A clone.
     *
     * @throws CloneNotSupportedException  can be thrown if one of the objects
     *         belonging to the renderer does not support cloning (for example,
     *         an item label generator).
     */
    @Override
    public Object clone() throws CloneNotSupportedException {
        AbstractCategoryItemRenderer clone
            = (AbstractCategoryItemRenderer) super.clone();

        if (this.itemLabelGeneratorMap != null) {
            clone.itemLabelGeneratorMap = CloneUtils.cloneMapValues(
                    this.itemLabelGeneratorMap);
        }

        if (this.defaultItemLabelGenerator != null) {
            if (this.defaultItemLabelGenerator instanceof PublicCloneable) {
                PublicCloneable pc
                        = (PublicCloneable) this.defaultItemLabelGenerator;
                clone.defaultItemLabelGenerator
                        = (CategoryItemLabelGenerator) pc.clone();
            }
            else {
                throw new CloneNotSupportedException(
                        ""ItemLabelGenerator not cloneable."");
            }
        }

        if (this.toolTipGeneratorMap != null) {
            clone.toolTipGeneratorMap = CloneUtils.cloneMapValues(
                    this.toolTipGeneratorMap);
        }

        if (this.defaultToolTipGenerator != null) {
            if (this.defaultToolTipGenerator instanceof PublicCloneable) {
                PublicCloneable pc
                        = (PublicCloneable) this.defaultToolTipGenerator;
                clone.defaultToolTipGenerator
                        = (CategoryToolTipGenerator) pc.clone();
            }
            else {
                throw new CloneNotSupportedException(
                        ""Default tool tip generator not cloneable."");
            }
        }

        if (this.itemURLGeneratorMap != null) {
            clone.itemURLGeneratorMap = CloneUtils.cloneMapValues(
                    this.itemURLGeneratorMap);
        }

        if (this.defaultItemURLGenerator != null) {
            if (this.defaultItemURLGenerator instanceof PublicCloneable) {
                PublicCloneable pc
                        = (PublicCloneable) this.defaultItemURLGenerator;
                clone.defaultItemURLGenerator = (CategoryURLGenerator) pc.clone();
            }
            else {
                throw new CloneNotSupportedException(
                        ""Default item URL generator not cloneable."");
            }
        }

        if (this.legendItemLabelGenerator instanceof PublicCloneable) {
            clone.legendItemLabelGenerator = (CategorySeriesLabelGenerator)
                    CloneUtils.clone((Object) this.legendItemLabelGenerator);
        }
        if (this.legendItemToolTipGenerator instanceof PublicCloneable) {
            clone.legendItemToolTipGenerator = (CategorySeriesLabelGenerator)
                    CloneUtils.clone((Object) this.legendItemToolTipGenerator);
        }
        if (this.legendItemURLGenerator instanceof PublicCloneable) {
            clone.legendItemURLGenerator = (CategorySeriesLabelGenerator)
                    CloneUtils.clone((Object) this.legendItemURLGenerator);
        }
        return clone;
    }

    /**
     * Returns a domain axis for a plot.
     *
     * @param plot  the plot.
     * @param index  the axis index.
     *
     * @return A domain axis.
     */
    protected CategoryAxis getDomainAxis(CategoryPlot plot, int index) {
        CategoryAxis result = plot.getDomainAxis(index);
        if (result == null) {
            result = plot.getDomainAxis();
        }
        return result;
    }

    /**
     * Returns a range axis for a plot.
     *
     * @param plot  the plot.
     * @param index  the axis index.
     *
     * @return A range axis.
     */
    protected ValueAxis getRangeAxis(CategoryPlot plot, int index) {
        ValueAxis result = plot.getRangeAxis(index);
        if (result == null) {
            result = plot.getRangeAxis();
        }
        return result;
    }

    /**
     * Returns a (possibly empty) collection of legend items for the series
     * that this renderer is responsible for drawing.
     *
     * @return The legend item collection (never {@code null}).
     *
     * @see #getLegendItem(int, int)
     */
    @Override
    public LegendItemCollection getLegendItems() {
        LegendItemCollection result = new LegendItemCollection();
        if (this.plot == null) {
            return result;
        }
        int index = this.plot.getIndexOf(this);
        CategoryDataset dataset = this.plot.getDataset(index);
        if (dataset == null) {
            return result;
        }
        int seriesCount = dataset.getRowCount();
        if (plot.getRowRenderingOrder().equals(SortOrder.ASCENDING)) {
            for (int i = 0; i < seriesCount; i++) {
                if (isSeriesVisibleInLegend(i)) {
                    LegendItem item = getLegendItem(index, i);
                    if (item != null) {
                        result.add(item);
                    }
                }
            }
        }
        else {
            for (int i = seriesCount - 1; i >= 0; i--) {
                if (isSeriesVisibleInLegend(i)) {
                    LegendItem item = getLegendItem(index, i);
                    if (item != null) {
                        result.add(item);
                    }
                }
            }
        }
        return result;
    }

    /**
     * Returns the legend item label generator.
     *
     * @return The label generator (never {@code null}).
     *
     * @see #setLegendItemLabelGenerator(CategorySeriesLabelGenerator)
     */
    public CategorySeriesLabelGenerator getLegendItemLabelGenerator() {
        return this.legendItemLabelGenerator;
    }

    /**
     * Sets the legend item label generator and sends a
     * {@link RendererChangeEvent} to all registered listeners.
     *
     * @param generator  the generator ({@code null} not permitted).
     *
     * @see #getLegendItemLabelGenerator()
     */
    public void setLegendItemLabelGenerator(
            CategorySeriesLabelGenerator generator) {
        Args.nullNotPermitted(generator, ""generator"");
        this.legendItemLabelGenerator = generator;
        fireChangeEvent();
    }

    /**
     * Returns the legend item tool tip generator.
     *
     * @return The tool tip generator (possibly {@code null}).
     *
     * @see #setLegendItemToolTipGenerator(CategorySeriesLabelGenerator)
     */
    public CategorySeriesLabelGenerator getLegendItemToolTipGenerator() {
        return this.legendItemToolTipGenerator;
    }

    /**
     * Sets the legend item tool tip generator and sends a
     * {@link RendererChangeEvent} to all registered listeners.
     *
     * @param generator  the generator ({@code null} permitted).
     *
     * @see #setLegendItemToolTipGenerator(CategorySeriesLabelGenerator)
     */
    public void setLegendItemToolTipGenerator(
            CategorySeriesLabelGenerator generator) {
        this.legendItemToolTipGenerator = generator;
        fireChangeEvent();
    }

    /**
     * Returns the legend item URL generator.
     *
     * @return The URL generator (possibly {@code null}).
     *
     * @see #setLegendItemURLGenerator(CategorySeriesLabelGenerator)
     */
    public CategorySeriesLabelGenerator getLegendItemURLGenerator() {
        return this.legendItemURLGenerator;
    }

    /**
     * Sets the legend item URL generator and sends a
     * {@link RendererChangeEvent} to all registered listeners.
     *
     * @param generator  the generator ({@code null} permitted).
     *
     * @see #getLegendItemURLGenerator()
     */
    public void setLegendItemURLGenerator(
            CategorySeriesLabelGenerator generator) {
        this.legendItemURLGenerator = generator;
        fireChangeEvent();
    }
    
    /**
     * Adds an entity with the specified hotspot.
     *
     * @param entities  the entity collection.
     * @param dataset  the dataset.
     * @param row  the row index.
     * @param column  the column index.
     * @param hotspot  the hotspot ({@code null} not permitted).
     */
    protected void addItemEntity(EntityCollection entities,
            CategoryDataset dataset, int row, int column, Shape hotspot) {
        Args.nullNotPermitted(hotspot, ""hotspot"");
        if (!getItemCreateEntity(row, column)) {
            return;
        }
        String tip = null;
        CategoryToolTipGenerator tipster = getToolTipGenerator(row, column);
        if (tipster != null) {
            tip = tipster.generateToolTip(dataset, row, column);
        }
        String url = null;
        CategoryURLGenerator urlster = getItemURLGenerator(row, column);
        if (urlster != null) {
            url = urlster.generateURL(dataset, row, column);
        }
        CategoryItemEntity entity = new CategoryItemEntity(hotspot, tip, url,
                dataset, dataset.getRowKey(row), dataset.getColumnKey(column));
        entities.add(entity);
    }

    /**
     * Adds an entity to the collection.
     *
     * @param entities  the entity collection being populated.
     * @param hotspot  the entity area (if {@code null} a default will be
     *              used).
     * @param dataset  the dataset.
     * @param row  the series.
     * @param column  the item.
     * @param entityX  the entity's center x-coordinate in user space (only
     *                 used if {@code area} is {@code null}).
     * @param entityY  the entity's center y-coordinate in user space (only
     *                 used if {@code area} is {@code null}).
     */
    protected void addEntity(EntityCollection entities, Shape hotspot,
                             CategoryDataset dataset, int row, int column,
                             double entityX, double entityY) {
        if (!getItemCreateEntity(row, column)) {
            return;
        }
        Shape s = hotspot;
        if (hotspot == null) {
            double r = getDefaultEntityRadius();
            double w = r * 2;
            if (getPlot().getOrientation() == PlotOrientation.VERTICAL) {
                s = new Ellipse2D.Double(entityX - r, entityY - r, w, w);
            }
            else {
                s = new Ellipse2D.Double(entityY - r, entityX - r, w, w);
            }
        }
        String tip = null;
        CategoryToolTipGenerator generator = getToolTipGenerator(row, column);
        if (generator != null) {
            tip = generator.generateToolTip(dataset, row, column);
        }
        String url = null;
        CategoryURLGenerator urlster = getItemURLGenerator(row, column);
        if (urlster != null) {
            url = urlster.generateURL(dataset, row, column);
        }
        CategoryItemEntity entity = new CategoryItemEntity(s, tip, url,
                dataset, dataset.getRowKey(row), dataset.getColumnKey(column));
        entities.add(entity);
    }

}
","['Assertion Roulette', 'Eager Test']","['Assertion Roulette', 'Eager Test', 'Lazy Test', 'Redundant Assertion', 'Magic Number Test', 'Mystery Guest', 'Sensitive Equality', 'Sleepy Test', 'Unknown Test']",7,0,2,10
36382_33.0_alien4cloud_parsecomplexproperty,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/36382_test.java,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/36382_actual.java,"package org.alien4cloud.tosca.utils;

import com.google.common.collect.Lists;
import com.google.common.collect.Maps;
import lombok.extern.slf4j.Slf4j;
import org.alien4cloud.tosca.model.definitions.AbstractPropertyValue;
import org.alien4cloud.tosca.model.definitions.ComplexPropertyValue;
import org.alien4cloud.tosca.model.definitions.ConcatPropertyValue;
import org.alien4cloud.tosca.model.definitions.FunctionPropertyValue;
import org.alien4cloud.tosca.model.definitions.PropertyValue;
import org.alien4cloud.tosca.model.definitions.ScalarPropertyValue;
import org.alien4cloud.tosca.model.templates.AbstractInstantiableTemplate;
import org.alien4cloud.tosca.model.templates.Capability;
import org.alien4cloud.tosca.model.templates.NodeTemplate;
import org.alien4cloud.tosca.model.templates.Topology;
import org.alien4cloud.tosca.normative.constants.ToscaFunctionConstants;
import org.junit.Assert;
import org.junit.Test;

import java.util.Map;

/**
 * Test for function evaluation over a topology.
 */
public class FunctionEvaluatorTest {

    private static PropertyValue resolveValue(FunctionEvaluatorContext evaluatorContext, AbstractInstantiableTemplate template,
            Map<String, AbstractPropertyValue> properties, AbstractPropertyValue evaluatedProperty) {
        AbstractPropertyValue propertyValue = FunctionEvaluator.tryResolveValue(evaluatorContext, template, properties, evaluatedProperty);
        if (propertyValue == null || propertyValue instanceof PropertyValue) {
            return (PropertyValue) propertyValue;
        } else {
            throw new IllegalArgumentException(""The resolved value is not a property value but a "" + propertyValue.getClass());
        }
    }

    private void setPropertiesValues(Map<String, AbstractPropertyValue> properties, String prefix) {
        ScalarPropertyValue scalarPropValue = new ScalarPropertyValue(prefix + ""scalar value"");
        Map<String, Object> complex = Maps.newHashMap();
        complex.put(""scalar"", prefix + ""complex scalar value"");
        complex.put(""map"", Maps.newHashMap());
        ((Map) complex.get(""map"")).put(""element_1"", prefix + ""element 1 value"");
        ((Map) complex.get(""map"")).put(""element_2"", prefix + ""element 2 value"");
        complex.put(""list"", Lists.newArrayList(prefix + ""list value 1"", prefix + ""list value 2""));
        ComplexPropertyValue complexPropValue = new ComplexPropertyValue(complex);

        FunctionPropertyValue getInputPropValue = new FunctionPropertyValue(ToscaFunctionConstants.GET_INPUT, Lists.newArrayList(""scalar_input""));
        FunctionPropertyValue getSecretPropValue = new FunctionPropertyValue(ToscaFunctionConstants.GET_SECRET, Lists.newArrayList(""my/path""));
        FunctionPropertyValue getScalarPropValue = new FunctionPropertyValue(ToscaFunctionConstants.GET_PROPERTY, Lists.newArrayList(""SELF"", ""scalar_prop""));
        FunctionPropertyValue getComplexPropValue = new FunctionPropertyValue(ToscaFunctionConstants.GET_PROPERTY, Lists.newArrayList(""SELF"",
                ""complex_prop.scalar""));
        FunctionPropertyValue getComplexPropListValue = new FunctionPropertyValue(ToscaFunctionConstants.GET_PROPERTY, Lists.newArrayList(""SELF"",
                ""complex_prop.list[1]""));
        FunctionPropertyValue getComplexPropMapValue = new FunctionPropertyValue(ToscaFunctionConstants.GET_PROPERTY, Lists.newArrayList(""SELF"",
                ""complex_prop.map.element_1""));
        ConcatPropertyValue concatPropValue = new ConcatPropertyValue();
        concatPropValue.setFunction_concat(""concat"");
        concatPropValue.setParameters(Lists.newArrayList(new ScalarPropertyValue(""input is: ""), getInputPropValue, new ScalarPropertyValue("" property is: ""),
                getScalarPropValue));
        ConcatPropertyValue concatGetSecretPropValue = new ConcatPropertyValue();
        concatGetSecretPropValue.setFunction_concat(""concat"");
        concatGetSecretPropValue.setParameters(Lists.newArrayList(new ScalarPropertyValue(""input is: ""), getSecretPropValue));
        FunctionPropertyValue getConcatPropValue = new FunctionPropertyValue(ToscaFunctionConstants.GET_PROPERTY, Lists.newArrayList(""SELF"", ""concat_prop""));

        ConcatPropertyValue concatGetConcatPropValue = new ConcatPropertyValue();
        concatGetConcatPropValue.setFunction_concat(""concat"");
        concatGetConcatPropValue.setParameters(Lists.newArrayList(new ScalarPropertyValue(""get concat is: ""), getConcatPropValue));

        properties.put(""scalar_prop"", scalarPropValue);
        properties.put(""complex_prop"", complexPropValue);
        properties.put(""get_input_prop"", getInputPropValue);
        properties.put(""get_secret_prop"", getSecretPropValue);
        properties.put(""get_scalar_prop"", getScalarPropValue);
        properties.put(""get_complex_prop"", getComplexPropValue);
        properties.put(""get_complex_prop_list"", getComplexPropListValue);
        properties.put(""get_complex_prop_map"", getComplexPropMapValue);
        properties.put(""concat_prop"", concatPropValue);
        properties.put(""concat_prop_and_get_secret"", concatGetSecretPropValue);
        properties.put(""get_concat_prop"", getConcatPropValue);
        properties.put(""concat_get_concat_prop"", concatGetConcatPropValue);
    }

    /**
     * Generate the topology below (no need type validations) and inputs to be used for next tests.
     * 
     * @return A function evaluator context for the test topology.
     */
    private FunctionEvaluatorContext getEvaluationContext() {
        NodeTemplate myNode = new NodeTemplate();
        myNode.setProperties(Maps.newHashMap());
        setPropertiesValues(myNode.getProperties(), """");

        Capability capability = new Capability();
        myNode.setCapabilities(Maps.newHashMap());
        myNode.getCapabilities().put(""my_capability"", capability);
        capability.setProperties(Maps.newHashMap());
        setPropertiesValues(capability.getProperties(), ""capa "");

        Topology topology = new Topology();
        topology.setNodeTemplates(Maps.newHashMap());
        topology.getNodeTemplates().put(""my_node"", myNode);

        Map<String, AbstractPropertyValue> inputs = Maps.newHashMap();
        inputs.put(""scalar_input"", new ScalarPropertyValue(""scalar input value""));

        return new FunctionEvaluatorContext(topology, inputs);
    }

    @Test
    public void evaluateNullProperty() {
        Assert.assertNull(resolveValue(null, null, null, null));
    }

    @Test
    public void nodeGetInputProp() {
        FunctionEvaluatorContext context = getEvaluationContext();
        NodeTemplate template = context.getTopology().getNodeTemplates().get(""my_node"");

        PropertyValue resolved = resolveValue(context, template, template.getProperties(), template.getProperties().get(""get_input_prop""));
        Assert.assertNotNull(resolved);
        Assert.assertEquals(ScalarPropertyValue.class, resolved.getClass());
        Assert.assertEquals(""scalar input value"", resolved.getValue());
    }

    @Test
    public void nodeGetSecretProp() {
        FunctionEvaluatorContext context = getEvaluationContext();
        NodeTemplate template = context.getTopology().getNodeTemplates().get(""my_node"");

        AbstractPropertyValue resolved = FunctionEvaluator.tryResolveValue(context, template, template.getProperties(),
                template.getProperties().get(""get_secret_prop""));
        Assert.assertNotNull(resolved);
        Assert.assertEquals(FunctionPropertyValue.class, resolved.getClass());
        Assert.assertEquals(""get_secret"", ((FunctionPropertyValue) resolved).getFunction());
        Assert.assertEquals(""my/path"", ((FunctionPropertyValue) resolved).getParameters().get(0));

    }

    @Test
    public void nodeGetScalarProp() {
        FunctionEvaluatorContext context = getEvaluationContext();
        NodeTemplate template = context.getTopology().getNodeTemplates().get(""my_node"");

        PropertyValue resolved = resolveValue(context, template, template.getProperties(), template.getProperties().get(""get_scalar_prop""));
        Assert.assertNotNull(resolved);
        Assert.assertEquals(ScalarPropertyValue.class, resolved.getClass());
        Assert.assertEquals(""scalar value"", resolved.getValue());
    }

    @Test
    public void nodeGetComplexProp() {
        FunctionEvaluatorContext context = getEvaluationContext();
        NodeTemplate template = context.getTopology().getNodeTemplates().get(""my_node"");

        PropertyValue resolved = resolveValue(context, template, template.getProperties(), template.getProperties().get(""get_complex_prop""));
        Assert.assertNotNull(resolved);
        Assert.assertEquals(ScalarPropertyValue.class, resolved.getClass());
        Assert.assertEquals(""complex scalar value"", resolved.getValue());
    }

    @Test
    public void nodeGetComplexPropList() {
        FunctionEvaluatorContext context = getEvaluationContext();
        NodeTemplate template = context.getTopology().getNodeTemplates().get(""my_node"");

        PropertyValue resolved = resolveValue(context, template, template.getProperties(), template.getProperties().get(""get_complex_prop_list""));
        Assert.assertNotNull(resolved);
        Assert.assertEquals(ScalarPropertyValue.class, resolved.getClass());
        Assert.assertEquals(""list value 2"", resolved.getValue());
    }

    @Test
    public void nodeGetComplexPropMap() {
        FunctionEvaluatorContext context = getEvaluationContext();
        NodeTemplate template = context.getTopology().getNodeTemplates().get(""my_node"");

        PropertyValue resolved = resolveValue(context, template, template.getProperties(), template.getProperties().get(""get_complex_prop_map""));
        Assert.assertNotNull(resolved);
        Assert.assertEquals(ScalarPropertyValue.class, resolved.getClass());
        Assert.assertEquals(""element 1 value"", resolved.getValue());
    }

    @Test
    public void nodeConcatProp() {
        FunctionEvaluatorContext context = getEvaluationContext();
        NodeTemplate template = context.getTopology().getNodeTemplates().get(""my_node"");

        PropertyValue resolved = resolveValue(context, template, template.getProperties(), template.getProperties().get(""concat_prop""));
        Assert.assertNotNull(resolved);
        Assert.assertEquals(ScalarPropertyValue.class, resolved.getClass());
        Assert.assertEquals(""input is: scalar input value property is: scalar value"", resolved.getValue());
    }

    @Test
    public void nodeGetConcatProp() {
        FunctionEvaluatorContext context = getEvaluationContext();
        NodeTemplate template = context.getTopology().getNodeTemplates().get(""my_node"");

        PropertyValue resolved = resolveValue(context, template, template.getProperties(), template.getProperties().get(""get_concat_prop""));
        Assert.assertNotNull(resolved);
        Assert.assertEquals(ScalarPropertyValue.class, resolved.getClass());
        Assert.assertEquals(""input is: scalar input value property is: scalar value"", resolved.getValue());
    }

    @Test
    public void nodeConcatGetConcatProp() {
        FunctionEvaluatorContext context = getEvaluationContext();
        NodeTemplate template = context.getTopology().getNodeTemplates().get(""my_node"");

        PropertyValue resolved = resolveValue(context, template, template.getProperties(), template.getProperties().get(""concat_get_concat_prop""));
        Assert.assertNotNull(resolved);
        Assert.assertEquals(ScalarPropertyValue.class, resolved.getClass());
        Assert.assertEquals(""get concat is: input is: scalar input value property is: scalar value"", resolved.getValue());
    }

    @Test
    public void concatOfAnGetSecretShouldFailed() {
        FunctionEvaluatorContext context = getEvaluationContext();
        NodeTemplate template = context.getTopology().getNodeTemplates().get(""my_node"");

        AbstractPropertyValue resolved = FunctionEvaluator.tryResolveValue(context, template, template.getProperties(), template.getProperties().get(""concat_prop_and_get_secret""));
        Assert.assertNotNull(resolved);
        Assert.assertEquals(ConcatPropertyValue.class, resolved.getClass());
        ConcatPropertyValue resolvedConcat = (ConcatPropertyValue) resolved;
        Assert.assertEquals(ScalarPropertyValue.class, resolvedConcat.getParameters().get(0).getClass());
        Assert.assertEquals(FunctionPropertyValue.class, resolvedConcat.getParameters().get(1).getClass());
    }
}","package alien4cloud.paas.function;

import alien4cloud.paas.IPaaSTemplate;
import alien4cloud.paas.exception.NotSupportedException;
import alien4cloud.paas.model.InstanceInformation;
import alien4cloud.paas.model.PaaSNodeTemplate;
import alien4cloud.paas.model.PaaSRelationshipTemplate;
import alien4cloud.utils.AlienConstants;
import alien4cloud.utils.AlienUtils;
import alien4cloud.utils.MapUtil;
import alien4cloud.utils.PropertyUtil;
import com.google.common.collect.Lists;
import com.google.common.collect.Maps;
import lombok.extern.slf4j.Slf4j;
import org.alien4cloud.tosca.model.definitions.AbstractPropertyValue;
import org.alien4cloud.tosca.model.definitions.AttributeDefinition;
import org.alien4cloud.tosca.model.definitions.ConcatPropertyValue;
import org.alien4cloud.tosca.model.definitions.FunctionPropertyValue;
import org.alien4cloud.tosca.model.definitions.IValue;
import org.alien4cloud.tosca.model.definitions.PropertyDefinition;
import org.alien4cloud.tosca.model.definitions.PropertyValue;
import org.alien4cloud.tosca.model.definitions.ScalarPropertyValue;
import org.alien4cloud.tosca.model.templates.Capability;
import org.alien4cloud.tosca.model.templates.NodeTemplate;
import org.alien4cloud.tosca.model.templates.Requirement;
import org.alien4cloud.tosca.model.templates.Topology;
import org.alien4cloud.tosca.model.types.AbstractInheritableToscaType;
import org.alien4cloud.tosca.model.types.AbstractToscaType;
import org.alien4cloud.tosca.normative.ToscaNormativeUtil;
import org.alien4cloud.tosca.normative.constants.ToscaFunctionConstants;
import org.alien4cloud.tosca.normative.types.ToscaTypes;
import org.apache.commons.lang3.StringUtils;

import java.util.List;
import java.util.Map;

/**
 * Utility class to process functions defined in attributes or operations input level:
 * ex:<br>
 *
 * <pre>
 * attributes:
 *   url: ""http://get_property: [the_node_template_1, the_property_name_1]:get_property: [the_node_template_2, the_property_name_2 ]/super""
 * </pre>
 */
@Slf4j
@SuppressWarnings({ ""unchecked"", ""rawtypes"" })
public final class FunctionEvaluator {

    /**
     * Parse an attribute value that can be : {@link ConcatPropertyValue} / {@link AttributeDefinition}
     *
     * @param attributeId
     * @param attributeValue
     * @param topology
     * @param runtimeInformations
     * @param currentInstance
     * @param basePaaSTemplate
     * @param builtPaaSTemplates
     * @return
     */
    public static String parseAttribute(String attributeId, IValue attributeValue, Topology topology,
            Map<String, Map<String, InstanceInformation>> runtimeInformations, String currentInstance,
            IPaaSTemplate<? extends AbstractToscaType> basePaaSTemplate, Map<String, PaaSNodeTemplate> builtPaaSTemplates) {

        if (attributeValue == null) {
            return null;
        }

        // handle AttributeDefinition type
        if (attributeValue instanceof AttributeDefinition) {
            String runtimeAttributeValue = extractRuntimeInformationAttribute(runtimeInformations, currentInstance, Lists.newArrayList(basePaaSTemplate),
                    attributeId);
            if (runtimeAttributeValue != null) {
                if (!runtimeAttributeValue.contains(""=Error!]"") && !runtimeAttributeValue.equals("""")) {
                    return runtimeAttributeValue;
                }
            }

            return ((AttributeDefinition) attributeValue).getDefault();
        }

        // handle concat function
        if (attributeValue instanceof ConcatPropertyValue) {
            StringBuilder evaluatedAttribute = new StringBuilder();
            ConcatPropertyValue concatPropertyValue = (ConcatPropertyValue) attributeValue;
            for (IValue concatParam : concatPropertyValue.getParameters()) {
                // scalar type
                if (concatParam instanceof ScalarPropertyValue) {
                    // scalar case
                    evaluatedAttribute.append(((ScalarPropertyValue) concatParam).getValue());
                } else if (concatParam instanceof PropertyDefinition) {
                    // Definition case
                    // TODO : ?? what should i do here ?? currently returns default value in the definition
                    evaluatedAttribute.append(((PropertyDefinition) concatParam).getDefault());
                } else if (concatParam instanceof FunctionPropertyValue) {
                    // Function case
                    FunctionPropertyValue functionPropertyValue = (FunctionPropertyValue) concatParam;
                    List<? extends IPaaSTemplate> paasTemplates = getPaaSTemplatesFromKeyword(basePaaSTemplate, functionPropertyValue.getTemplateName(),
                            builtPaaSTemplates);
                    switch (functionPropertyValue.getFunction()) {
                    case ToscaFunctionConstants.GET_ATTRIBUTE:
                        evaluatedAttribute.append(extractRuntimeInformationAttribute(runtimeInformations, currentInstance, paasTemplates,
                                functionPropertyValue.getElementNameToFetch()));
                        break;
                    case ToscaFunctionConstants.GET_PROPERTY:
                        evaluatedAttribute.append(extractRuntimeInformationProperty(topology, functionPropertyValue.getElementNameToFetch(), paasTemplates));
                        break;
                    case ToscaFunctionConstants.GET_OPERATION_OUTPUT:
                        String defaultValue = ""<"" + functionPropertyValue.getElementNameToFetch() + "">"";
                        evaluatedAttribute.append(extractRuntimeInformationOperationOutput(runtimeInformations, currentInstance, paasTemplates,
                                functionPropertyValue, defaultValue));
                        break;
                    default:
                        log.warn(""Function [{}] is not yet handled in concat operation."", functionPropertyValue.getFunction());
                        break;
                    }

                }
            }
            return evaluatedAttribute.toString();
        }

        // handle functions. For now, only support Get_OPERATION_OUTPUT on attributes scope
        if (attributeValue instanceof FunctionPropertyValue) {
            FunctionPropertyValue function = (FunctionPropertyValue) attributeValue;
            switch (function.getFunction()) {
            case ToscaFunctionConstants.GET_OPERATION_OUTPUT:
                List<? extends IPaaSTemplate> paasTemplates = getPaaSTemplatesFromKeyword(basePaaSTemplate, function.getTemplateName(), builtPaaSTemplates);
                return extractRuntimeInformationOperationOutput(runtimeInformations, currentInstance, paasTemplates, function, null);
            default:
                return null;
            }
        }

        return null;
    }

    private static String extractRuntimeInformationOperationOutput(Map<String, Map<String, InstanceInformation>> runtimeInformations, String instanceId,
            List<? extends IPaaSTemplate> nodes, FunctionPropertyValue function, String defaultValue) {
        String outputRQN = AlienUtils.prefixWith(AlienConstants.OPERATION_NAME_SEPARATOR, function.getElementNameToFetch(), function.getInterfaceName(),
                function.getOperationName());
        // return the first found
        for (IPaaSTemplate node : nodes) {
            String nodeName = node.getId();
            if (runtimeInformations.get(nodeName) != null) {
                Map<String, String> outputs;
                // get value for an instance if instance number found
                if (runtimeInformations.get(nodeName).containsKey(instanceId)) {
                    outputs = runtimeInformations.get(nodeName).get(instanceId).getOperationsOutputs();
                } else {
                    outputs = runtimeInformations.get(nodeName).entrySet().iterator().next().getValue().getAttributes();
                }
                String formatedOutputName = ToscaNormativeUtil.formatedOperationOutputName(nodeName, function.getInterfaceName(), function.getOperationName(),
                        function.getElementNameToFetch());
                if (outputs.containsKey(formatedOutputName)) {
                    return outputs.get(formatedOutputName);
                }
            }
        }
        log.warn(""Couldn't find output [ {} ] in nodes [ {} ]"", outputRQN, nodes.toString());
        return defaultValue;
    }

    /**
     * Extract property value from runtime informations
     *
     * @param topology
     * @param propertyOrAttributeName
     * @param nodes
     * @return
     */
    private static String extractRuntimeInformationProperty(Topology topology, String propertyOrAttributeName, List<? extends IPaaSTemplate> nodes) {
        AbstractPropertyValue propertyOrAttributeValue;
        NodeTemplate template = null;
        for (IPaaSTemplate node : nodes) {
            String nodeName = node.getId();
            template = topology.getNodeTemplates().get(nodeName);
            if (template != null && template.getProperties() != null) {
                propertyOrAttributeValue = template.getProperties().get(propertyOrAttributeName);
                if (propertyOrAttributeValue != null) {
                    return PropertyUtil.getScalarValue(propertyOrAttributeValue);
                }
            }
        }
        log.warn(""Couldn't find property [ {} ] of node [ {} ]"", propertyOrAttributeName, nodes);
        return ""["" + nodes + ""."" + propertyOrAttributeName + ""=Error!]"";
    }

    /**
     * Return the first matching value in parent nodes hierarchy
     *
     * @param runtimeInformations
     * @param currentInstance
     * @param nodes
     * @param propertyOrAttributeName
     * @return runtime value
     */
    private static String extractRuntimeInformationAttribute(Map<String, Map<String, InstanceInformation>> runtimeInformations, String currentInstance,
            List<? extends IPaaSTemplate> nodes, String propertyOrAttributeName) {
        Map<String, String> attributes = null;
        // return the first found
        for (IPaaSTemplate node : nodes) {
            String nodeName = node.getId();
            // get the current attribute value
            if (runtimeInformations.get(nodeName) != null) {
                // get value for an instance if instance number found
                if (runtimeInformations.get(nodeName).containsKey(currentInstance)) {
                    attributes = runtimeInformations.get(nodeName).get(currentInstance).getAttributes();
                } else {
                    attributes = runtimeInformations.get(nodeName).entrySet().iterator().next().getValue().getAttributes();
                }
                if (attributes.containsKey(propertyOrAttributeName)) {
                    return attributes.get(propertyOrAttributeName);
                }
            }
        }
        log.warn(""Couldn't find attribute [ {} ] in nodes [ {} ]"", propertyOrAttributeName, nodes.toString());
        return ""<"" + propertyOrAttributeName + "">""; // value not yet computed (or won't be computes)
    }

    /**
     * Return the paaS entities based on a keyword. This latest can be a special keyword (SELF, SOURCE, TARGET, HOST), or a node template name
     *
     * @param basePaaSTemplate The base PaaSTemplate for which to get the entity name
     * @param keyword The
     * @param builtPaaSTemplates
     * @return a list of PaaSTemplate(relationship or node) resulting from the evaluation
     */
    public static List<? extends IPaaSTemplate> getPaaSTemplatesFromKeyword(IPaaSTemplate<? extends AbstractToscaType> basePaaSTemplate, String keyword,
            Map<String, PaaSNodeTemplate> builtPaaSTemplates) {
        switch (keyword) {
        case ToscaFunctionConstants.SELF:
            return Lists.<IPaaSTemplate> newArrayList(basePaaSTemplate);
        case ToscaFunctionConstants.HOST:
            return getWithParentsNodes(getHostNode(basePaaSTemplate));
        case ToscaFunctionConstants.SOURCE:
            return getWithParentsNodes(getSourceNode(basePaaSTemplate, builtPaaSTemplates));
        case ToscaFunctionConstants.TARGET:
            return getWithParentsNodes(getTargetNode(basePaaSTemplate, builtPaaSTemplates));
        default:
            // FIXME if the keyword is in fact the name of a relationship??
            return Lists.<IPaaSTemplate> newArrayList(getPaaSNodeOrFail(keyword, builtPaaSTemplates));
        }
    }

    /**
     * Evaluate a get_property function type
     *
     * @param functionParam The property function of type {@link FunctionPropertyValue} to evaluate
     * @param basePaaSTemplate The base PaaSTemplate in which the parameter is defined. Can be a {@link PaaSRelationshipTemplate} or a {@link PaaSNodeTemplate}.
     * @param builtPaaSTemplates A map < {@link String}, {@link PaaSNodeTemplate}> of built nodetemplates of the processed topology. Note that these
     *            {@link PaaSNodeTemplate}s should have been built, thus referencing their related parents and relationships.
     * @return the String result of the function evalutation
     */
    public static String evaluateGetPropertyFunction(FunctionPropertyValue functionParam,
            IPaaSTemplate<? extends AbstractInheritableToscaType> basePaaSTemplate, Map<String, PaaSNodeTemplate> builtPaaSTemplates) {
        AbstractPropertyValue propertyValue = processGetPropertyFunction(functionParam, basePaaSTemplate, builtPaaSTemplates);
        if (propertyValue == null) {
            return null;
        } else if (!(propertyValue instanceof PropertyValue)) {
            throw new NotSupportedException(""Not a property value "" + propertyValue);
        } else {
            return PropertyUtil.serializePropertyValue(propertyValue);
        }
    }

    /**
     * Instead of evaluate get property function to a scalar value, this method will just evaluate to the value that the get_property points to
     * 
     * @param functionParam the function
     * @param basePaaSTemplate the template
     * @param builtPaaSTemplates all templates
     * @return the value (which can be a function and not only scalar value)
     */
    public static AbstractPropertyValue processGetPropertyFunction(FunctionPropertyValue functionParam,
            IPaaSTemplate<? extends AbstractInheritableToscaType> basePaaSTemplate, Map<String, PaaSNodeTemplate> builtPaaSTemplates) {
        List<? extends IPaaSTemplate> paaSTemplates = getPaaSTemplatesFromKeyword(basePaaSTemplate, functionParam.getTemplateName(), builtPaaSTemplates);
        for (IPaaSTemplate paaSTemplate : paaSTemplates) {
            AbstractPropertyValue propertyValue = getPropertyFromTemplateOrCapability(paaSTemplate, functionParam.getCapabilityOrRequirementName(),
                    functionParam.getElementNameToFetch());
            // return the first value found
            if (propertyValue != null) {
                return propertyValue;
            }
        }
        return null;
    }

    private static AbstractPropertyValue getPropertyValue(Map<String, AbstractPropertyValue> properties, Map<String, PropertyDefinition> propertyDefinitions,
            String propertyAccessPath) {
        if (properties == null || !properties.containsKey(propertyAccessPath)) {
            String propertyName = PropertyUtil.getPropertyNameFromComplexPath(propertyAccessPath);
            if (propertyName == null) {
                // Non complex
                return PropertyUtil.getDefaultFromPropertyDefinitions(propertyAccessPath, propertyDefinitions);

            } else {
                // Complex
                PropertyDefinition propertyDefinition = propertyDefinitions.get(propertyName);
                AbstractPropertyValue rawValue;
                if (propertyDefinition == null) {
                    return null;
                } else if (ToscaTypes.isSimple(propertyDefinition.getType())) {
                    // It's a complex path (with '.') but the type in definition is finally simple
                    return null;
                } else if (properties != null && (rawValue = properties.get(propertyName)) != null) {
                    if (!(rawValue instanceof PropertyValue)) {
                        throw new NotSupportedException(""Only support static value in a get_property"");
                    }
                    Object value = MapUtil.get(((PropertyValue) rawValue).getValue(), propertyAccessPath.substring(propertyName.length() + 1));
                    return new ScalarPropertyValue(PropertyUtil.serializePropertyValue(value));
                } else {
                    return null;
                }
            }
        } else {
            return properties.get(propertyAccessPath);
        }
    }

    /**
     * Find a property from a template or capability / requirement if a name is provided
     * first find in capability, and then in requirement if no found.
     *
     * @param paaSTemplate
     * @param capabilityOrRequirementName
     * @param elementName
     * @return
     */
    private static AbstractPropertyValue getPropertyFromTemplateOrCapability(IPaaSTemplate<? extends AbstractInheritableToscaType> paaSTemplate,
            String capabilityOrRequirementName, String elementName) {

        // if no capability or requirement provided, return the value from the template property
        if (StringUtils.isBlank(capabilityOrRequirementName)) {
            return getPropertyValue(paaSTemplate.getTemplate().getProperties(), paaSTemplate.getIndexedToscaElement().getProperties(), elementName);
        } else if (paaSTemplate instanceof PaaSNodeTemplate) {
            // if capability or requirement name provided:
            // FIXME how should I know that the provided name is capability or a requirement name?
            NodeTemplate nodeTemplate = (NodeTemplate) paaSTemplate.getTemplate();
            AbstractPropertyValue propertyValue = null;

            Map<String, Capability> capabilities = nodeTemplate.getCapabilities();
            Map<String, Requirement> requirements = nodeTemplate.getRequirements();

            // Find in capability first
            if (capabilities != null && capabilities.get(capabilityOrRequirementName) != null
                    && capabilities.get(capabilityOrRequirementName).getProperties() != null) {
                propertyValue = capabilities.get(capabilityOrRequirementName).getProperties().get(elementName);
            }

            // if not found in capability, find in requirement
            if (propertyValue == null) {
                if (requirements != null && requirements.containsKey(capabilityOrRequirementName)
                        && requirements.get(capabilityOrRequirementName).getProperties() != null) {
                    propertyValue = requirements.get(capabilityOrRequirementName).getProperties().get(elementName);
                }
            }

            return propertyValue;
        }

        log.warn(""The keyword <"" + ToscaFunctionConstants.SELF
                + ""> can not be used on a Relationship Template level's parameter when trying to retrieve capability / requiement properties. Node<""
                + paaSTemplate.getId() + "">"");
        return null;
    }

    private static PaaSNodeTemplate getPaaSNodeOrFail(String nodeId, Map<String, PaaSNodeTemplate> builtPaaSTemplates) {
        PaaSNodeTemplate toReturn = builtPaaSTemplates.get(nodeId);
        if (toReturn == null) {
            throw new FunctionEvaluationException("" Failled to retrieve the nodeTemplate with name <"" + nodeId + "">"");
        }
        return toReturn;
    }

    private static PaaSNodeTemplate getHostNode(IPaaSTemplate<? extends AbstractToscaType> basePaaSTemplate) {
        if (basePaaSTemplate instanceof PaaSNodeTemplate) {
            // TODO Must review this management of host
            PaaSNodeTemplate template = (PaaSNodeTemplate) basePaaSTemplate;
            return template.getParent() != null ? template.getParent() : template;
        }
        throw new BadUsageKeywordException(""The keyword <"" + ToscaFunctionConstants.HOST + ""> can only be used on a NodeTemplate level's parameter. Node<""
                + basePaaSTemplate.getId() + "">"");
    }

    private static PaaSNodeTemplate getSourceNode(IPaaSTemplate<? extends AbstractToscaType> basePaaSTemplate,
            Map<String, PaaSNodeTemplate> builtPaaSTemplates) {
        if (basePaaSTemplate instanceof PaaSRelationshipTemplate) {
            return getPaaSNodeOrFail(((PaaSRelationshipTemplate) basePaaSTemplate).getSource(), builtPaaSTemplates);
        }
        throw new BadUsageKeywordException(""The keyword <"" + ToscaFunctionConstants.SOURCE + ""> can only be used on a Relationship level's parameter. Node<""
                + basePaaSTemplate.getId() + "">"");
    }

    private static PaaSNodeTemplate getTargetNode(IPaaSTemplate<? extends AbstractToscaType> basePaaSTemplate,
            Map<String, PaaSNodeTemplate> builtPaaSTemplates) {
        if (basePaaSTemplate instanceof PaaSRelationshipTemplate) {
            return getPaaSNodeOrFail(((PaaSRelationshipTemplate) basePaaSTemplate).getTemplate().getTarget(), builtPaaSTemplates);
        }
        throw new BadUsageKeywordException(""The keyword <"" + ToscaFunctionConstants.TARGET + ""> can only be used on a Relationship level's parameter. Node<""
                + basePaaSTemplate.getId() + "">."");
    }

    private static List<PaaSNodeTemplate> getWithParentsNodes(final PaaSNodeTemplate paaSNodeTemplate) {
        List<PaaSNodeTemplate> toReturn = Lists.newArrayList();
        PaaSNodeTemplate parent = paaSNodeTemplate;
        while (parent != null) {
            toReturn.add(parent);
            parent = parent.getParent();
        }
        return toReturn;
    }

    public static Map<String, String> getScalarValues(Map<String, AbstractPropertyValue> propertyValues) {
        if (propertyValues == null) {
            return null;
        }
        Map<String, String> properties = Maps.newHashMap();
        for (Map.Entry<String, AbstractPropertyValue> propertyValueEntry : propertyValues.entrySet()) {
            properties.put(propertyValueEntry.getKey(), PropertyUtil.getScalarValue(propertyValueEntry.getValue()));
        }
        return properties;
    }

    public static boolean isGetAttribute(FunctionPropertyValue function) {
        return ToscaFunctionConstants.GET_ATTRIBUTE.equals(function.getFunction());
    }

    public static boolean isGetOperationOutput(FunctionPropertyValue function) {
        return ToscaFunctionConstants.GET_OPERATION_OUTPUT.equals(function.getFunction());
    }

    /**
     * Check if the given property value is a TOSCA get_input function.
     *
     * @param propertyValue The property value to evaluate.
     * @return True if the property is a TOSCA get_input function, false if not.
     */
    public static boolean isGetInput(AbstractPropertyValue propertyValue) {
        return propertyValue instanceof FunctionPropertyValue && isGetInput((FunctionPropertyValue) propertyValue);
    }

    public static boolean isGetInput(FunctionPropertyValue function) {
        return ToscaFunctionConstants.GET_INPUT.equals(function.getFunction());
    }

    /**
     * Check if the given property value is a get_secret function.
     *
     * @param propertyValue The property value to evaluate.
     * @return True if the property is a TOSCA get_secret function, false if not.
     */
    public static boolean isGetSecret(AbstractPropertyValue propertyValue) {
        return propertyValue instanceof FunctionPropertyValue && isGetSecret((FunctionPropertyValue) propertyValue);
    }

    public static boolean isGetSecret(FunctionPropertyValue function) {
        return ToscaFunctionConstants.GET_SECRET.equals(function.getFunction());
    }
}","['Assertion Roulette', 'Lazy Test']","['Assertion Roulette', 'Lazy Test']",0,0,2,15
20624_4.0_hadoop_testnnthroughput,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/20624_test.java,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/20624_actual.java,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * ""License""); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hadoop.hdfs.server.namenode;

import java.io.File;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.FileUtil;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.hdfs.DFSConfigKeys;
import org.apache.hadoop.hdfs.DFSTestUtil;
import org.apache.hadoop.hdfs.DistributedFileSystem;
import org.apache.hadoop.hdfs.HdfsConfiguration;
import org.apache.hadoop.hdfs.MiniDFSCluster;
import org.apache.hadoop.hdfs.protocol.DirectoryListing;
import org.apache.hadoop.hdfs.protocol.HdfsFileStatus;
import org.apache.hadoop.util.ExitUtil;
import org.junit.After;
import org.junit.Assert;
import org.junit.BeforeClass;
import org.junit.Test;

public class TestNNThroughputBenchmark {

  @BeforeClass
  public static void setUp() {
    ExitUtil.disableSystemExit();
  }

  @After
  public void cleanUp() {
    FileUtil.fullyDeleteContents(new File(MiniDFSCluster.getBaseDirectory()));
  }

  /**
   * This test runs all benchmarks defined in {@link NNThroughputBenchmark}.
   */
  @Test
  public void testNNThroughput() throws Exception {
    Configuration conf = new HdfsConfiguration();
    conf.setInt(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, 16);
    File nameDir = new File(MiniDFSCluster.getBaseDirectory(), ""name"");
    conf.set(DFSConfigKeys.DFS_NAMENODE_NAME_DIR_KEY,
        nameDir.getAbsolutePath());
    DFSTestUtil.formatNameNode(conf);
    NNThroughputBenchmark.runBenchmark(conf, new String[] {""-op"", ""all""});
  }

  /**
   * This test runs all benchmarks defined in {@link NNThroughputBenchmark},
   * with explicit local -fs option.
   */
  @Test(timeout = 120000)
  public void testNNThroughputWithFsOption() throws Exception {
    Configuration conf = new HdfsConfiguration();
    conf.setInt(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, 16);
    File nameDir = new File(MiniDFSCluster.getBaseDirectory(), ""name"");
    conf.set(DFSConfigKeys.DFS_NAMENODE_NAME_DIR_KEY,
        nameDir.getAbsolutePath());
    DFSTestUtil.formatNameNode(conf);
    NNThroughputBenchmark.runBenchmark(conf,
        new String[] {""-fs"", ""file:///"", ""-op"", ""all""});
  }

  /**
   * This test runs {@link NNThroughputBenchmark} against a mini DFS cluster.
   */
  @Test(timeout = 120000)
  public void testNNThroughputAgainstRemoteNN() throws Exception {
    final Configuration conf = new HdfsConfiguration();
    conf.setInt(DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY, 16);
    MiniDFSCluster cluster = null;
    try {
      cluster = new MiniDFSCluster.Builder(conf).numDataNodes(0).build();
      cluster.waitActive();

      final Configuration benchConf = new HdfsConfiguration();
      benchConf.setInt(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, 16);
      FileSystem.setDefaultUri(benchConf, cluster.getURI());
      NNThroughputBenchmark.runBenchmark(benchConf, new String[]{""-op"", ""all""});
    } finally {
      if (cluster != null) {
        cluster.shutdown();
      }
    }
  }
  /**
   * This test runs {@link NNThroughputBenchmark} against a mini DFS cluster with
   * nonSuperUser option (useful when testing any authorization framework e.g.
   * Ranger since only super user e.g. hdfs can enter/exit safemode
   * but any request from super user is not sent for authorization).
   */
  @Test(timeout = 120000)
  public void testNNThroughputAgainstRemoteNNNonSuperUser() throws Exception {
    final Configuration conf = new HdfsConfiguration();
    conf.setInt(DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY, 16);
    MiniDFSCluster cluster = null;
    try {
      cluster = new MiniDFSCluster.Builder(conf).numDataNodes(0).build();
      cluster.waitActive();
      final Configuration benchConf = new HdfsConfiguration();
      benchConf.setInt(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, 16);
      FileSystem.setDefaultUri(benchConf, cluster.getURI());
      NNThroughputBenchmark.runBenchmark(benchConf, new String[]{""-op"", ""all"", ""-nonSuperUser""});
    } finally {
      if (cluster != null) {
        cluster.shutdown();
      }
    }
  }
  /**
   * This test runs {@link NNThroughputBenchmark} against a mini DFS cluster
   * with explicit -fs option.
   */
  @Test(timeout = 120000)
  public void testNNThroughputRemoteAgainstNNWithFsOption() throws Exception {
    final Configuration conf = new HdfsConfiguration();
    conf.setInt(DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY, 16);
    conf.setInt(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, 16);
    MiniDFSCluster cluster = null;
    try {
      cluster = new MiniDFSCluster.Builder(conf).numDataNodes(0).build();
      cluster.waitActive();

      final Configuration benchConf = new HdfsConfiguration();
      benchConf.setInt(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, 16);
      NNThroughputBenchmark.runBenchmark(benchConf,
          new String[]{""-fs"", cluster.getURI().toString(), ""-op"", ""all""});
    } finally {
      if (cluster != null) {
        cluster.shutdown();
      }
    }
  }

  /**
   * This test runs {@link NNThroughputBenchmark} against a mini DFS cluster
   * for append operation.
   */
  @Test(timeout = 120000)
  public void testNNThroughputForAppendOp() throws Exception {
    final Configuration conf = new HdfsConfiguration();
    conf.setInt(DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY, 16);
    MiniDFSCluster cluster = null;
    try {
      cluster = new MiniDFSCluster.Builder(conf).numDataNodes(0).build();
      cluster.waitActive();

      final Configuration benchConf = new HdfsConfiguration();
      benchConf.setInt(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, 16);
      FileSystem.setDefaultUri(benchConf, cluster.getURI());
      NNThroughputBenchmark.runBenchmark(benchConf,
          new String[] {""-op"", ""create"", ""-keepResults"", ""-files"", ""3"",
              ""-close"" });
      FSNamesystem fsNamesystem = cluster.getNamesystem();
      DirectoryListing listing =
          fsNamesystem.getListing(""/"", HdfsFileStatus.EMPTY_NAME, false);
      HdfsFileStatus[] partialListing = listing.getPartialListing();

      NNThroughputBenchmark.runBenchmark(benchConf,
          new String[] {""-op"", ""append"", ""-files"", ""3"", ""-useExisting"" });
      listing = fsNamesystem.getListing(""/"", HdfsFileStatus.EMPTY_NAME, false);
      HdfsFileStatus[] partialListingAfter = listing.getPartialListing();

      Assert.assertEquals(partialListing.length, partialListingAfter.length);
      for (int i = 0; i < partialListing.length; i++) {
        //Check the modification time after append operation
        Assert.assertNotEquals(partialListing[i].getModificationTime(),
            partialListingAfter[i].getModificationTime());
      }

    } finally {
      if (cluster != null) {
        cluster.shutdown();
      }
    }
  }

  /**
   * This test runs {@link NNThroughputBenchmark} against a mini DFS cluster
   * for block report operation.
   */
  @Test(timeout = 120000)
  public void testNNThroughputForBlockReportOp() throws Exception {
    final Configuration conf = new HdfsConfiguration();
    conf.setInt(DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY, 16);
    conf.setInt(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, 16);
    try (MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).
        numDataNodes(3).build()) {
      cluster.waitActive();
      final Configuration benchConf = new HdfsConfiguration();
      benchConf.setInt(DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY, 16);
      benchConf.setInt(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, 16);
      NNThroughputBenchmark.runBenchmark(benchConf,
          new String[]{""-fs"", cluster.getURI().toString(), ""-op"",
              ""blockReport"", ""-datanodes"", ""3"", ""-reports"", ""2""});
    }
  }

  /**
   * This test runs {@link NNThroughputBenchmark} against a mini DFS cluster
   * with explicit -baseDirName option.
   */
  @Test(timeout = 120000)
  public void testNNThroughputWithBaseDir() throws Exception {
    final Configuration conf = new HdfsConfiguration();
    conf.setInt(DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY, 16);
    MiniDFSCluster cluster = null;
    try {
      cluster = new MiniDFSCluster.Builder(conf).numDataNodes(0).build();
      cluster.waitActive();
      final Configuration benchConf = new HdfsConfiguration();
      benchConf.setInt(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, 16);
      FileSystem.setDefaultUri(benchConf, cluster.getURI());
      DistributedFileSystem fs = cluster.getFileSystem();

      NNThroughputBenchmark.runBenchmark(benchConf,
          new String[] {""-op"", ""create"", ""-keepResults"", ""-files"", ""3"", ""-baseDirName"",
              ""/nnThroughputBenchmark1"", ""-close""});
      Assert.assertTrue(fs.exists(new Path(""/nnThroughputBenchmark1"")));
      Assert.assertFalse(fs.exists(new Path(""/nnThroughputBenchmark"")));

      NNThroughputBenchmark.runBenchmark(benchConf,
          new String[] {""-op"", ""all"", ""-baseDirName"", ""/nnThroughputBenchmark1""});
      Assert.assertTrue(fs.exists(new Path(""/nnThroughputBenchmark1"")));
      Assert.assertFalse(fs.exists(new Path(""/nnThroughputBenchmark"")));
    } finally {
      if (cluster != null) {
        cluster.shutdown();
      }
    }
  }

  /**
   * This test runs {@link NNThroughputBenchmark} against a mini DFS cluster
   * for blockSize  with letter suffix.
   */
  @Test(timeout = 120000)
  public void testNNThroughputForBlockSizeWithLetterSuffix() throws Exception {
    final Configuration conf = new HdfsConfiguration();
    conf.setInt(DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY, 16);
    conf.set(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, ""1m"");
    try (MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).build()) {
      cluster.waitActive();
      final Configuration benchConf = new HdfsConfiguration();
      benchConf.setLong(DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY, 16);
      benchConf.set(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, ""1m"");
      FileSystem.setDefaultUri(benchConf, cluster.getURI());
      NNThroughputBenchmark.runBenchmark(benchConf,
          new String[]{""-op"", ""create"", ""-keepResults"", ""-files"", ""3"", ""-close""});
    }
  }

  /**
   * This test runs {@link NNThroughputBenchmark} against a mini DFS cluster
   * with explicit -blockSize option.
   */
  @Test(timeout = 120000)
  public void testNNThroughputWithBlockSize() throws Exception {
    final Configuration conf = new HdfsConfiguration();
    conf.setInt(DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY, 16);
    try (MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).build()) {
      cluster.waitActive();
      final Configuration benchConf = new HdfsConfiguration();
      benchConf.setLong(DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY, 16);
      FileSystem.setDefaultUri(benchConf, cluster.getURI());
      NNThroughputBenchmark.runBenchmark(benchConf,
          new String[]{""-op"", ""create"", ""-keepResults"", ""-files"", ""3"",
              ""-blockSize"", ""32"", ""-close""});
    }
  }

  /**
   * This test runs {@link NNThroughputBenchmark} against a mini DFS cluster
   * with explicit -blockSize option like 1m.
   */
  @Test(timeout = 120000)
  public void testNNThroughputBlockSizeArgWithLetterSuffix() throws Exception {
    final Configuration conf = new HdfsConfiguration();
    conf.setInt(DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY, 16);
    try (MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).build()) {
      cluster.waitActive();
      final Configuration benchConf = new HdfsConfiguration();
      benchConf.setLong(DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY, 16);
      FileSystem.setDefaultUri(benchConf, cluster.getURI());
      NNThroughputBenchmark.runBenchmark(benchConf,
          new String[]{""-op"", ""create"", ""-keepResults"", ""-files"", ""3"",
              ""-blockSize"", ""1m"", ""-close""});
    }
  }
}
","/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * ""License""); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hadoop.hdfs.server.namenode;

import java.io.File;
import java.io.FileOutputStream;
import java.io.IOException;
import java.net.InetSocketAddress;
import java.net.URI;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.EnumSet;
import java.util.List;

import org.apache.hadoop.util.Preconditions;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.crypto.CryptoProtocolVersion;
import org.apache.hadoop.fs.CreateFlag;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.permission.FsPermission;
import org.apache.hadoop.hdfs.DFSConfigKeys;
import org.apache.hadoop.hdfs.DFSTestUtil;
import org.apache.hadoop.hdfs.DFSUtilClient;
import org.apache.hadoop.hdfs.DistributedFileSystem;
import org.apache.hadoop.hdfs.HdfsConfiguration;
import org.apache.hadoop.hdfs.protocol.Block;
import org.apache.hadoop.hdfs.protocol.BlockListAsLongs;
import org.apache.hadoop.hdfs.protocol.BlockListAsLongs.BlockReportReplica;
import org.apache.hadoop.hdfs.protocol.ClientProtocol;
import org.apache.hadoop.hdfs.protocol.DatanodeID;
import org.apache.hadoop.hdfs.protocol.DatanodeInfo;
import org.apache.hadoop.hdfs.protocol.ExtendedBlock;
import org.apache.hadoop.hdfs.protocol.HdfsConstants;
import org.apache.hadoop.hdfs.protocol.LocatedBlock;
import org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB;
import org.apache.hadoop.hdfs.security.token.block.ExportedBlockKeys;
import org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerTestUtil;
import org.apache.hadoop.hdfs.server.datanode.DataNode;
import org.apache.hadoop.hdfs.server.datanode.DataStorage;
import org.apache.hadoop.hdfs.server.protocol.BlockCommand;
import org.apache.hadoop.hdfs.server.protocol.BlockReportContext;
import org.apache.hadoop.hdfs.server.protocol.DatanodeCommand;
import org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol;
import org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration;
import org.apache.hadoop.hdfs.server.protocol.DatanodeStorage;
import org.apache.hadoop.hdfs.server.protocol.NamenodeProtocol;
import org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols;
import org.apache.hadoop.hdfs.server.protocol.NamespaceInfo;
import org.apache.hadoop.hdfs.server.protocol.ReceivedDeletedBlockInfo;
import org.apache.hadoop.hdfs.server.protocol.SlowDiskReports;
import org.apache.hadoop.hdfs.server.protocol.SlowPeerReports;
import org.apache.hadoop.hdfs.server.protocol.StorageBlockReport;
import org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks;
import org.apache.hadoop.hdfs.server.protocol.StorageReport;
import org.apache.hadoop.io.EnumSetWritable;
import org.apache.hadoop.ipc.RemoteException;
import org.apache.hadoop.net.DNS;
import org.apache.hadoop.net.NetworkTopology;
import org.apache.hadoop.security.Groups;
import org.apache.hadoop.security.RefreshUserMappingsProtocol;
import org.apache.hadoop.security.UserGroupInformation;
import org.apache.hadoop.test.GenericTestUtils;
import org.apache.hadoop.util.ExitUtil;
import org.apache.hadoop.util.GenericOptionsParser;
import org.apache.hadoop.util.StringUtils;
import org.apache.hadoop.util.Time;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;
import org.apache.hadoop.util.VersionInfo;
import org.slf4j.event.Level;

/**
 * Main class for a series of name-node benchmarks.
 *
 * Each benchmark measures throughput and average execution time 
 * of a specific name-node operation, e.g. file creation or block reports.
 *
 * The benchmark does not involve any other hadoop components
 * except for the name-node. Each operation is executed
 * by calling directly the respective name-node method.
 * The name-node here is real all other components are simulated.
 *
 * For usage, please see <a href=""http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/Benchmarking.html#NNThroughputBenchmark"">the documentation</a>.
 * Meanwhile, if you change the usage of this program, please also update the
 * documentation accordingly.
 */
public class NNThroughputBenchmark implements Tool {
  private static final Logger LOG =
      LoggerFactory.getLogger(NNThroughputBenchmark.class);
  private static final int BLOCK_SIZE = 16;
  private static final String GENERAL_OPTIONS_USAGE =
      ""[-keepResults] | [-logLevel L] | [-UGCacheRefreshCount G] [-nonSuperUser]"";

  static Configuration config;
  static NameNode nameNode;
  static NamenodeProtocol nameNodeProto;
  static ClientProtocol clientProto;
  static DatanodeProtocol dataNodeProto;
  static RefreshUserMappingsProtocol refreshUserMappingsProto;
  static String bpid = null;

  NNThroughputBenchmark(Configuration conf) throws IOException {
    config = conf;
    // We do not need many handlers, since each thread simulates a handler
    // by calling name-node methods directly
    config.setInt(DFSConfigKeys.DFS_DATANODE_HANDLER_COUNT_KEY, 1);
    // Turn off minimum block size verification
    config.setInt(DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY, 0);
    // set exclude file
    config.set(DFSConfigKeys.DFS_HOSTS_EXCLUDE,
      ""${hadoop.tmp.dir}/dfs/hosts/exclude"");
    File excludeFile = new File(config.get(DFSConfigKeys.DFS_HOSTS_EXCLUDE,
      ""exclude""));
    if(!excludeFile.exists()) {
      if(!excludeFile.getParentFile().exists() && !excludeFile.getParentFile().mkdirs())
        throw new IOException(""NNThroughputBenchmark: cannot mkdir "" + excludeFile);
    }
    new FileOutputStream(excludeFile).close();
    // set include file
    config.set(DFSConfigKeys.DFS_HOSTS, ""${hadoop.tmp.dir}/dfs/hosts/include"");
    File includeFile = new File(config.get(DFSConfigKeys.DFS_HOSTS, ""include""));
    new FileOutputStream(includeFile).close();
  }

  void close() {
    if(nameNode != null)
      nameNode.stop();
  }

  static void setNameNodeLoggingLevel(Level logLevel) {
    LOG.info(""Log level = "" + logLevel.toString());
    // change log level to NameNode logs
    DFSTestUtil.setNameNodeLogLevel(logLevel);
    GenericTestUtils.setLogLevel(LoggerFactory.getLogger(
            NetworkTopology.class.getName()), logLevel);
    GenericTestUtils.setLogLevel(LoggerFactory.getLogger(
            Groups.class.getName()), logLevel);
  }

  /**
   * Base class for collecting operation statistics.
   *
   * Overload this class in order to run statistics for a 
   * specific name-node operation.
   */
  abstract class OperationStatsBase {
    private String baseDirName = ""/nnThroughputBenchmark"";
    protected static final String OP_ALL_NAME = ""all"";
    protected static final String OP_ALL_USAGE = ""-op all <other ops options>"";

    private String baseDir;
    protected short replication;
    protected long blockSize;
    protected int  numThreads = 0;        // number of threads
    protected int  numOpsRequired = 0;    // number of operations requested
    protected int  numOpsExecuted = 0;    // number of operations executed
    protected long cumulativeTime = 0;    // sum of times for each op
    protected long elapsedTime = 0;       // time from start to finish
    protected boolean keepResults = false;// don't clean base directory on exit
    protected boolean nonSuperUser = false; // enter/exit safe mode
    protected Level logLevel;             // logging level, ERROR by default
    protected int ugcRefreshCount = 0;    // user group cache refresh count

    protected List<StatsDaemon> daemons;

    /**
     * Operation name.
     */
    abstract String getOpName();

    /**
     * Parse command line arguments.
     *
     * @param args arguments
     * @throws IOException
     */
    abstract void parseArguments(List<String> args) throws IOException;

    /**
     * Generate inputs for each daemon thread.
     *
     * @param opsPerThread number of inputs for each thread.
     * @throws IOException
     */
    abstract void generateInputs(int[] opsPerThread) throws IOException;

    /**
     * This corresponds to the arg1 argument of 
     * {@link #executeOp(int, int, String)}, which can have different meanings
     * depending on the operation performed.
     *
     * @param daemonId id of the daemon calling this method
     * @return the argument
     */
    abstract String getExecutionArgument(int daemonId);

    /**
     * Execute name-node operation.
     *
     * @param daemonId id of the daemon calling this method.
     * @param inputIdx serial index of the operation called by the deamon.
     * @param arg1 operation specific argument.
     * @return time of the individual name-node call.
     * @throws IOException
     */
    abstract long executeOp(int daemonId, int inputIdx, String arg1) throws IOException;

    /**
     * Print the results of the benchmarking.
     */
    abstract void printResults();

    OperationStatsBase() {
      baseDir = baseDirName + ""/"" + getOpName();
      replication = (short) config.getInt(DFSConfigKeys.DFS_REPLICATION_KEY, 3);
      blockSize = config.getLongBytes(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, BLOCK_SIZE);
      numOpsRequired = 10;
      numThreads = 3;
      logLevel = Level.ERROR;
      ugcRefreshCount = Integer.MAX_VALUE;
    }

    void benchmark() throws IOException {
      daemons = new ArrayList<StatsDaemon>();
      long start = 0;
      try {
        numOpsExecuted = 0;
        cumulativeTime = 0;
        if(numThreads < 1)
          return;
        int tIdx = 0; // thread index < nrThreads
        int opsPerThread[] = new int[numThreads];
        for(int opsScheduled = 0; opsScheduled < numOpsRequired;
                                  opsScheduled += opsPerThread[tIdx++]) {
          // execute  in a separate thread
          opsPerThread[tIdx] = (numOpsRequired-opsScheduled)/(numThreads-tIdx);
          if(opsPerThread[tIdx] == 0)
            opsPerThread[tIdx] = 1;
        }
        // if numThreads > numOpsRequired then the remaining threads will do nothing
        for(; tIdx < numThreads; tIdx++)
          opsPerThread[tIdx] = 0;
        generateInputs(opsPerThread);
        setNameNodeLoggingLevel(logLevel);
        for(tIdx=0; tIdx < numThreads; tIdx++)
          daemons.add(new StatsDaemon(tIdx, opsPerThread[tIdx], this));
        start = Time.now();
        LOG.info(""Starting "" + numOpsRequired + "" "" + getOpName() + ""(s)."");
        for(StatsDaemon d : daemons)
          d.start();
      } finally {
        while(isInProgress()) {
          // try {Thread.sleep(500);} catch (InterruptedException e) {}
        }
        elapsedTime = Time.now() - start;
        for(StatsDaemon d : daemons) {
          incrementStats(d.localNumOpsExecuted, d.localCumulativeTime);
          // System.out.println(d.toString() + "": ops Exec = "" + d.localNumOpsExecuted);
        }
      }
    }

    private boolean isInProgress() {
      for(StatsDaemon d : daemons)
        if(d.isInProgress())
          return true;
      return false;
    }

    void cleanUp() throws IOException {
      if (!nonSuperUser) {
        try {
          clientProto.setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_LEAVE,
                  false);
        } catch (Exception e){
          LOG.error(""Potentially insufficient permission: try running the tool"" +
                    ""with -nonSuperUser argument or login as super user"");
          throw e;
        }
      }
      if(!keepResults)
        clientProto.delete(getBaseDir(), true);
      else {
        if (!nonSuperUser) {
          try {
            clientProto.setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_ENTER,
                    true);
            clientProto.saveNamespace(0, 0);
          } catch (Exception e){
            LOG.error(""Potentially insufficient permission: try running the tool"" +
                      "" with -nonSuperUser argument or login as super user"");
            throw e;
          }
        }
      }
    }
    public String getBaseDirName() {
      return baseDirName;
    }
    public void setBaseDirName(String baseDirName) {
      this.baseDirName = baseDirName;
    }
    int getNumOpsExecuted() {
      return numOpsExecuted;
    }

    long getCumulativeTime() {
      return cumulativeTime;
    }

    long getElapsedTime() {
      return elapsedTime;
    }

    long getAverageTime() {
      return numOpsExecuted == 0 ? 0 : cumulativeTime / numOpsExecuted;
    }

    double getOpsPerSecond() {
      return elapsedTime == 0 ? 0 : 1000*(double)numOpsExecuted / elapsedTime;
    }

    public String getBaseDir() {
      setBaseDir(baseDirName + ""/"" + getOpName());
      return baseDir;
    }

    public void setBaseDir(String baseDir) {
      this.baseDir = baseDir;
    }

    String getClientName(int idx) {
      return getOpName() + ""-client-"" + idx;
    }

    void incrementStats(int ops, long time) {
      numOpsExecuted += ops;
      cumulativeTime += time;
    }

    /**
     * Parse first 2 arguments, corresponding to the ""-op"" option.
     *
     * @param args argument list
     * @return true if operation is all, which means that options not related
     * to this operation should be ignored, or false otherwise, meaning
     * that usage should be printed when an unrelated option is encountered.
     */
    protected boolean verifyOpArgument(List<String> args) {
      if(args.size() < 2 || ! args.get(0).startsWith(""-op""))
        printUsage();

      // process common options
      int krIndex = args.indexOf(""-keepResults"");
      keepResults = (krIndex >= 0);
      if(keepResults) {
        args.remove(krIndex);
      }

      int nonSuperUserIndex = args.indexOf(""-nonSuperUser"");
      nonSuperUser = (nonSuperUserIndex >= 0);
      if(nonSuperUser) {
        args.remove(nonSuperUserIndex);
      }

      int llIndex = args.indexOf(""-logLevel"");
      if(llIndex >= 0) {
        if(args.size() <= llIndex + 1)
          printUsage();
        logLevel = Level.valueOf(args.get(llIndex+1));
        args.remove(llIndex+1);
        args.remove(llIndex);
      }

      int ugrcIndex = args.indexOf(""-UGCacheRefreshCount"");
      if(ugrcIndex >= 0) {
        if(args.size() <= ugrcIndex + 1)
          printUsage();
        int g = Integer.parseInt(args.get(ugrcIndex+1));
        if(g > 0) ugcRefreshCount = g;
        args.remove(ugrcIndex+1);
        args.remove(ugrcIndex);
      }

      String type = args.get(1);
      if(OP_ALL_NAME.equals(type)) {
        type = getOpName();
        return true;
      }
      if(!getOpName().equals(type))
        printUsage();
      return false;
    }

    void printStats() {
      LOG.info(""--- "" + getOpName() + "" stats  ---"");
      LOG.info(""# operations: "" + getNumOpsExecuted());
      LOG.info(""Elapsed Time: "" + getElapsedTime());
      LOG.info("" Ops per sec: "" + getOpsPerSecond());
      LOG.info(""Average Time: "" + getAverageTime());
    }
  }

  /**
   * One of the threads that perform stats operations.
   */
  private class StatsDaemon extends Thread {
    private final int daemonId;
    private int opsPerThread;
    private String arg1;      // argument passed to executeOp()
    private volatile int  localNumOpsExecuted = 0;
    private volatile long localCumulativeTime = 0;
    private final OperationStatsBase statsOp;

    StatsDaemon(int daemonId, int nrOps, OperationStatsBase op) {
      this.daemonId = daemonId;
      this.opsPerThread = nrOps;
      this.statsOp = op;
      setName(toString());
    }

    @Override
    public void run() {
      localNumOpsExecuted = 0;
      localCumulativeTime = 0;
      arg1 = statsOp.getExecutionArgument(daemonId);
      try {
        benchmarkOne();
      } catch(IOException ex) {
        LOG.error(""StatsDaemon "" + daemonId + "" failed: \n""
            + StringUtils.stringifyException(ex));
      }
    }

    @Override
    public String toString() {
      return ""StatsDaemon-"" + daemonId;
    }

    void benchmarkOne() throws IOException {
      for(int idx = 0; idx < opsPerThread; idx++) {
        if((localNumOpsExecuted+1) % statsOp.ugcRefreshCount == 0)
          refreshUserMappingsProto.refreshUserToGroupsMappings();
        long stat = statsOp.executeOp(daemonId, idx, arg1);
        localNumOpsExecuted++;
        localCumulativeTime += stat;
      }
    }

    boolean isInProgress() {
      return localNumOpsExecuted < opsPerThread;
    }

    /**
     * Schedule to stop this daemon.
     */
    void terminate() {
      opsPerThread = localNumOpsExecuted;
    }
  }

  /**
   * Clean all benchmark result directories.
   */
  class CleanAllStats extends OperationStatsBase {
    // Operation types
    static final String OP_CLEAN_NAME = ""clean"";
    static final String OP_CLEAN_USAGE = ""-op clean"";

    CleanAllStats(List<String> args) {
      super();
      parseArguments(args);
      numOpsRequired = 1;
      numThreads = 1;
      keepResults = true;
    }

    @Override
    String getOpName() {
      return OP_CLEAN_NAME;
    }

    @Override
    void parseArguments(List<String> args) {
      boolean ignoreUnrelatedOptions = verifyOpArgument(args);
      if(args.size() > 2 && !ignoreUnrelatedOptions)
        printUsage();
    }

    @Override
    void generateInputs(int[] opsPerThread) throws IOException {
      // do nothing
    }

    /**
     * Does not require the argument
     */
    @Override
    String getExecutionArgument(int daemonId) {
      return null;
    }

    /**
     * Remove entire benchmark directory.
     */
    @Override
    long executeOp(int daemonId, int inputIdx, String ignore)
    throws IOException {
      if (!nonSuperUser) {
        try{
          clientProto.setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_LEAVE,
                  false);
        } catch (Exception e){
          LOG.error(""Potentially insufficient permission: try running the tool"" +
                    "" with -nonSuperUser argument or login as super user"");
          throw e;
        }
      }
      long start = Time.now();
      clientProto.delete(getBaseDirName(), true);
      long end = Time.now();
      return end-start;
    }

    @Override
    void printResults() {
      LOG.info(""--- "" + getOpName() + "" inputs ---"");
      LOG.info(""Remove directory "" + getBaseDirName());
      printStats();
    }
  }

  /**
   * File creation statistics.
   *
   * Each thread creates the same (+ or -1) number of files.
   * File names are pre-generated during initialization.
   * The created files do not have blocks.
   */
  class CreateFileStats extends OperationStatsBase {
    // Operation types
    static final String OP_CREATE_NAME = ""create"";
    static final String OP_CREATE_USAGE =
        ""-op create [-threads T] [-files N] [-blockSize S] [-filesPerDir P]""
        + "" [-baseDirName D] [-close]"";

    protected FileNameGenerator nameGenerator;
    protected String[][] fileNames;
    private boolean closeUponCreate;

    CreateFileStats(List<String> args) {
      super();
      parseArguments(args);
    }

    @Override
    String getOpName() {
      return OP_CREATE_NAME;
    }

    @Override
    void parseArguments(List<String> args) {
      boolean ignoreUnrelatedOptions = verifyOpArgument(args);
      int nrFilesPerDir = 4;
      closeUponCreate = false;
      for (int i = 2; i < args.size(); i++) {       // parse command line
        if(args.get(i).equals(""-files"")) {
          if(i+1 == args.size())  printUsage();
          numOpsRequired = Integer.parseInt(args.get(++i));
        } else if (args.get(i).equals(""-blockSize"")) {
          if(i+1 == args.size())  printUsage();
          blockSize = StringUtils.TraditionalBinaryPrefix.string2long(args.get(++i));
        } else if(args.get(i).equals(""-threads"")) {
          if(i+1 == args.size())  printUsage();
          numThreads = Integer.parseInt(args.get(++i));
        } else if(args.get(i).equals(""-filesPerDir"")) {
          if(i+1 == args.size())  printUsage();
          nrFilesPerDir = Integer.parseInt(args.get(++i));
        } else if(args.get(i).equals(""-baseDirName"")) {
          if (i + 1 == args.size()) {
            printUsage();
          }
          setBaseDirName(args.get(++i));
        } else if(args.get(i).equals(""-close"")) {
          closeUponCreate = true;
        } else if(!ignoreUnrelatedOptions)
          printUsage();
      }
      nameGenerator = new FileNameGenerator(getBaseDir(), nrFilesPerDir);
    }

    @Override
    void generateInputs(int[] opsPerThread) throws IOException {
      assert opsPerThread.length == numThreads : ""Error opsPerThread.length"";
      if (!nonSuperUser) {
        try{
          clientProto.setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_LEAVE,
              false);
        } catch (Exception e){
          LOG.error(""Potentially insufficient permission: try running the tool"" +
                    ""with -nonSuperUser argument or login as super user"");
          throw e;
        }
      }
      // int generatedFileIdx = 0;
      LOG.info(""Generate "" + numOpsRequired + "" intputs for "" + getOpName());
      LOG.info(""basedir: "" + getBaseDir());
      fileNames = new String[numThreads][];
      try {
        for(int idx=0; idx < numThreads; idx++) {
          int threadOps = opsPerThread[idx];
          fileNames[idx] = new String[threadOps];
          for(int jdx=0; jdx < threadOps; jdx++) {
            fileNames[idx][jdx] = nameGenerator.
                    getNextFileName(""ThroughputBench"");
          }
        }
      } catch (ArrayIndexOutOfBoundsException e) {
        LOG.error(""The current environment allows {} files to be created. "" +
            ""If you want to test more files, please update the -filesPerDir parameter."",
                nameGenerator.getFileCount());
        throw e;
      }
    }

    /**
     * returns client name
     */
    @Override
    String getExecutionArgument(int daemonId) {
      return getClientName(daemonId);
    }

    /**
     * Do file create.
     */
    @Override
    long executeOp(int daemonId, int inputIdx, String clientName)
    throws IOException {
      long start = Time.now();
      clientProto.create(fileNames[daemonId][inputIdx],
          FsPermission.getDefault(), clientName,
          new EnumSetWritable<CreateFlag>(EnumSet
              .of(CreateFlag.CREATE, CreateFlag.OVERWRITE)), true,
          replication, blockSize, CryptoProtocolVersion.supported(), null,
          null);
      long end = Time.now();
      for (boolean written = !closeUponCreate; !written;
        written = clientProto.complete(fileNames[daemonId][inputIdx],
            clientName, null, HdfsConstants.GRANDFATHER_INODE_ID)) {
      };
      return end-start;
    }

    @Override
    void printResults() {
      LOG.info(""--- "" + getOpName() + "" inputs ---"");
      LOG.info(""baseDir = "" + getBaseDir());
      LOG.info(""nrFiles = "" + numOpsRequired);
      LOG.info(""nrThreads = "" + numThreads);
      LOG.info(""nrFilesPerDir = "" + nameGenerator.getFilesPerDirectory());
      printStats();
    }
  }

  /**
   * Directory creation statistics.
   *
   * Each thread creates the same (+ or -1) number of directories.
   * Directory names are pre-generated during initialization.
   */
  class MkdirsStats extends OperationStatsBase {
    // Operation types
    static final String OP_MKDIRS_NAME = ""mkdirs"";
    static final String OP_MKDIRS_USAGE = ""-op mkdirs [-threads T] [-dirs N] "" +
        ""[-dirsPerDir P] [-baseDirName D]"";

    protected FileNameGenerator nameGenerator;
    protected String[][] dirPaths;

    MkdirsStats(List<String> args) {
      super();
      parseArguments(args);
    }

    @Override
    String getOpName() {
      return OP_MKDIRS_NAME;
    }

    @Override
    void parseArguments(List<String> args) {
      boolean ignoreUnrelatedOptions = verifyOpArgument(args);
      int nrDirsPerDir = 2;
      for (int i = 2; i < args.size(); i++) {       // parse command line
        if(args.get(i).equals(""-dirs"")) {
          if(i+1 == args.size())  printUsage();
          numOpsRequired = Integer.parseInt(args.get(++i));
        } else if(args.get(i).equals(""-threads"")) {
          if(i+1 == args.size())  printUsage();
          numThreads = Integer.parseInt(args.get(++i));
        } else if(args.get(i).equals(""-dirsPerDir"")) {
          if(i+1 == args.size())  printUsage();
          nrDirsPerDir = Integer.parseInt(args.get(++i));
        } else if(args.get(i).equals(""-baseDirName"")) {
          if (i + 1 == args.size()) {
            printUsage();
          }
          setBaseDirName(args.get(++i));
        } else if(!ignoreUnrelatedOptions)
          printUsage();
      }
      nameGenerator = new FileNameGenerator(getBaseDir(), nrDirsPerDir);
    }

    @Override
    void generateInputs(int[] opsPerThread) throws IOException {
      assert opsPerThread.length == numThreads : ""Error opsPerThread.length"";
      if (!nonSuperUser) {
        try {
          clientProto.setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_LEAVE,
                  false);
        } catch (Exception e){
          LOG.error(""Potentially insufficient permission: try running the tool"" +
                    "" with -nonSuperUser argument or login as super user"");
          throw e;
        }
      }
      LOG.info(""Generate "" + numOpsRequired + "" inputs for "" + getOpName());
      dirPaths = new String[numThreads][];
      try {
        for(int idx=0; idx < numThreads; idx++) {
          int threadOps = opsPerThread[idx];
          dirPaths[idx] = new String[threadOps];
          for(int jdx=0; jdx < threadOps; jdx++) {
            dirPaths[idx][jdx] = nameGenerator.
                    getNextFileName(""ThroughputBench"");
          }
        }
      } catch (ArrayIndexOutOfBoundsException e) {
        LOG.error(""The current environment allows {} directories to be created. "" +
            ""If you want to test more directories, please update the -dirsPerDir parameter."",
                nameGenerator.getFileCount());
        throw e;
      }
    }

    /**
     * returns client name
     */
    @Override
    String getExecutionArgument(int daemonId) {
      return getClientName(daemonId);
    }

    /**
     * Do mkdirs operation.
     */
    @Override
    long executeOp(int daemonId, int inputIdx, String clientName)
        throws IOException {
      long start = Time.now();
      clientProto.mkdirs(dirPaths[daemonId][inputIdx],
          FsPermission.getDefault(), true);
      long end = Time.now();
      return end-start;
    }

    @Override
    void printResults() {
      LOG.info(""--- "" + getOpName() + "" inputs ---"");
      LOG.info(""baseDir = "" + getBaseDir());
      LOG.info(""nrDirs = "" + numOpsRequired);
      LOG.info(""nrThreads = "" + numThreads);
      LOG.info(""nrDirsPerDir = "" + nameGenerator.getFilesPerDirectory());
      printStats();
    }
  }

  /**
   * Open file statistics.
   *
   * Measure how many open calls (getBlockLocations())
   * the name-node can handle per second.
   */
  class OpenFileStats extends CreateFileStats {
    // Operation types
    static final String OP_OPEN_NAME = ""open"";
    static final String OP_USAGE_ARGS =
        "" [-threads T] [-files N] [-blockSize S] [-filesPerDir P]""
        + "" [-useExisting] [-baseDirName D]"";
    static final String OP_OPEN_USAGE = 
      ""-op "" + OP_OPEN_NAME + OP_USAGE_ARGS;

    private boolean useExisting;  // do not generate files, use existing ones

    OpenFileStats(List<String> args) {
      super(args);
    }

    @Override
    String getOpName() {
      return OP_OPEN_NAME;
    }

    @Override
    void parseArguments(List<String> args) {
      int ueIndex = args.indexOf(""-useExisting"");
      useExisting = (ueIndex >= 0);
      if(useExisting) {
        args.remove(ueIndex);
      }
      super.parseArguments(args);
    }

    @Override
    void generateInputs(int[] opsPerThread) throws IOException {
      // create files using opsPerThread
      String[] createArgs = new String[] {
          ""-op"", ""create"",
          ""-threads"", String.valueOf(this.numThreads),
          ""-files"", String.valueOf(numOpsRequired),
          ""-blockSize"", String.valueOf(blockSize),
          ""-filesPerDir"",
          String.valueOf(nameGenerator.getFilesPerDirectory()),
          ""-baseDirName"", getBaseDirName(),
          ""-close""};
      List<String> createArgsList = new ArrayList<String>(Arrays.asList(createArgs));
      if (this.nonSuperUser){
        createArgsList.add(""-nonSuperUser"");
      }
      CreateFileStats opCreate =  new CreateFileStats(createArgsList);
      if(!useExisting) {  // create files if they were not created before
        opCreate.benchmark();
        LOG.info(""Created "" + numOpsRequired + "" files."");
      } else {
        LOG.info(""useExisting = true. Assuming ""
            + numOpsRequired + "" files have been created before."");
      }
      // use the same files for open
      super.generateInputs(opsPerThread);
      if(clientProto.getFileInfo(opCreate.getBaseDir()) != null
          && clientProto.getFileInfo(getBaseDir()) == null) {
        clientProto.rename(opCreate.getBaseDir(), getBaseDir());
      }
      if(clientProto.getFileInfo(getBaseDir()) == null) {
        throw new IOException(getBaseDir() + "" does not exist."");
      }
    }

    /**
     * Do file open.
     */
    @Override
    long executeOp(int daemonId, int inputIdx, String ignore)
    throws IOException {
      long start = Time.now();
      clientProto.getBlockLocations(fileNames[daemonId][inputIdx], 0L,
          blockSize);
      long end = Time.now();
      return end-start;
    }
  }

  /**
   * Delete file statistics.
   *
   * Measure how many delete calls the name-node can handle per second.
   */
  class DeleteFileStats extends OpenFileStats {
    // Operation types
    static final String OP_DELETE_NAME = ""delete"";
    static final String OP_DELETE_USAGE =
      ""-op "" + OP_DELETE_NAME + OP_USAGE_ARGS;

    DeleteFileStats(List<String> args) {
      super(args);
    }

    @Override
    String getOpName() {
      return OP_DELETE_NAME;
    }

    @Override
    long executeOp(int daemonId, int inputIdx, String ignore)
    throws IOException {
      long start = Time.now();
      clientProto.delete(fileNames[daemonId][inputIdx], false);
      long end = Time.now();
      return end-start;
    }
  }

  /**
   * Append file statistics.
   * Measure how many append calls the name-node can handle per second.
   */
  class AppendFileStats extends OpenFileStats {
    // Operation types
    static final String OP_APPEND_NAME = ""append"";
    public static final String APPEND_NEW_BLK = ""-appendNewBlk"";
    static final String OP_APPEND_USAGE =
        ""-op "" + OP_APPEND_NAME + OP_USAGE_ARGS + "" ["" + APPEND_NEW_BLK + ']';
    private boolean appendNewBlk = false;

    AppendFileStats(List<String> args) {
      super(args);
    }

    @Override
    String getOpName() {
      return OP_APPEND_NAME;
    }

    @Override
    void parseArguments(List<String> args) {
      appendNewBlk = args.contains(APPEND_NEW_BLK);
      if (this.appendNewBlk) {
        args.remove(APPEND_NEW_BLK);
      }
      super.parseArguments(args);
    }

    @Override
    long executeOp(int daemonId, int inputIdx, String ignore)
        throws IOException {
      long start = Time.now();
      String src = fileNames[daemonId][inputIdx];
      EnumSetWritable<CreateFlag> enumSet = null;
      if (appendNewBlk) {
        enumSet = new EnumSetWritable<>(EnumSet.of(CreateFlag.NEW_BLOCK));
      } else {
        enumSet = new EnumSetWritable<>(EnumSet.of(CreateFlag.APPEND));
      }
      clientProto.append(src, ""TestClient"", enumSet);
      long end = Time.now();
      return end - start;
    }
  }

  /**
   * List file status statistics.
   *
   * Measure how many get-file-status calls the name-node can handle per second.
   */
  class FileStatusStats extends OpenFileStats {
    // Operation types
    static final String OP_FILE_STATUS_NAME = ""fileStatus"";
    static final String OP_FILE_STATUS_USAGE =
      ""-op "" + OP_FILE_STATUS_NAME + OP_USAGE_ARGS;

    FileStatusStats(List<String> args) {
      super(args);
    }

    @Override
    String getOpName() {
      return OP_FILE_STATUS_NAME;
    }

    @Override
    long executeOp(int daemonId, int inputIdx, String ignore)
    throws IOException {
      long start = Time.now();
      clientProto.getFileInfo(fileNames[daemonId][inputIdx]);
      long end = Time.now();
      return end-start;
    }
  }

  /**
   * Rename file statistics.
   *
   * Measure how many rename calls the name-node can handle per second.
   */
  class RenameFileStats extends OpenFileStats {
    // Operation types
    static final String OP_RENAME_NAME = ""rename"";
    static final String OP_RENAME_USAGE =
      ""-op "" + OP_RENAME_NAME + OP_USAGE_ARGS;

    protected String[][] destNames;

    RenameFileStats(List<String> args) {
      super(args);
    }

    @Override
    String getOpName() {
      return OP_RENAME_NAME;
    }

    @Override
    void generateInputs(int[] opsPerThread) throws IOException {
      super.generateInputs(opsPerThread);
      destNames = new String[fileNames.length][];
      for(int idx=0; idx < numThreads; idx++) {
        int nrNames = fileNames[idx].length;
        destNames[idx] = new String[nrNames];
        for(int jdx=0; jdx < nrNames; jdx++)
          destNames[idx][jdx] = fileNames[idx][jdx] + "".r"";
      }
    }

    @Override
    long executeOp(int daemonId, int inputIdx, String ignore)
    throws IOException {
      long start = Time.now();
      clientProto.rename(fileNames[daemonId][inputIdx],
                      destNames[daemonId][inputIdx]);
      long end = Time.now();
      return end-start;
    }
  }

  /**
   * Minimal data-node simulator.
   */
  private static class TinyDatanode implements Comparable<String> {
    private static final long DF_CAPACITY = 100*1024*1024;
    private static final long DF_USED = 0;

    NamespaceInfo nsInfo;
    DatanodeRegistration dnRegistration;
    DatanodeStorage storage; //only one storage
    final List<BlockReportReplica> blocks;
    int nrBlocks; // actual number of blocks
    BlockListAsLongs blockReportList;
    final int dnIdx;

    private static int getNodePort(int num) throws IOException {
      int port = 1 + num;
      Preconditions.checkState(port < Short.MAX_VALUE);
      return port;
    }

    TinyDatanode(int dnIdx, int blockCapacity) throws IOException {
      this.dnIdx = dnIdx;
      this.blocks = Arrays.asList(new BlockReportReplica[blockCapacity]);
      this.nrBlocks = 0;
    }

    @Override
    public String toString() {
      return dnRegistration.toString();
    }

    String getXferAddr() {
      return dnRegistration.getXferAddr();
    }

    void register() throws IOException {
      // get versions from the namenode
      nsInfo = nameNodeProto.versionRequest();
      dnRegistration = new DatanodeRegistration(
          new DatanodeID(DNS.getDefaultIP(""default""),
              DNS.getDefaultHost(""default"", ""default""),
              DataNode.generateUuid(), getNodePort(dnIdx),
              DFSConfigKeys.DFS_DATANODE_HTTP_DEFAULT_PORT,
              DFSConfigKeys.DFS_DATANODE_HTTPS_DEFAULT_PORT,
              DFSConfigKeys.DFS_DATANODE_IPC_DEFAULT_PORT),
          new DataStorage(nsInfo),
          new ExportedBlockKeys(), VersionInfo.getVersion());
      // register datanode
      dnRegistration = dataNodeProto.registerDatanode(dnRegistration);
      dnRegistration.setNamespaceInfo(nsInfo);
      //first block reports
      storage = new DatanodeStorage(DatanodeStorage.generateUuid());
      final StorageBlockReport[] reports = {
          new StorageBlockReport(storage, BlockListAsLongs.EMPTY)
      };
      dataNodeProto.blockReport(dnRegistration, bpid, reports,
              new BlockReportContext(1, 0, System.nanoTime(), 0L));
    }

    /**
     * Send a heartbeat to the name-node.
     * Ignore reply commands.
     */
    void sendHeartbeat() throws IOException {
      // register datanode
      // TODO:FEDERATION currently a single block pool is supported
      StorageReport[] rep = { new StorageReport(storage, false,
          DF_CAPACITY, DF_USED, DF_CAPACITY - DF_USED, DF_USED, 0L) };
      DatanodeCommand[] cmds = dataNodeProto.sendHeartbeat(dnRegistration, rep,
          0L, 0L, 0, 0, 0, null, true,
          SlowPeerReports.EMPTY_REPORT, SlowDiskReports.EMPTY_REPORT)
          .getCommands();
      if(cmds != null) {
        for (DatanodeCommand cmd : cmds ) {
          if(LOG.isDebugEnabled()) {
            LOG.debug(""sendHeartbeat Name-node reply: "" + cmd.getAction());
          }
        }
      }
    }

    boolean addBlock(Block blk) {
      if(nrBlocks == blocks.size()) {
        if(LOG.isDebugEnabled()) {
          LOG.debug(""Cannot add block: datanode capacity = "" + blocks.size());
        }
        return false;
      }
      blocks.set(nrBlocks, new BlockReportReplica(blk));
      nrBlocks++;
      return true;
    }

    void formBlockReport() {
      // fill remaining slots with blocks that do not exist
      for (int idx = blocks.size()-1; idx >= nrBlocks; idx--) {
        Block block = new Block(blocks.size() - idx, 0, 0);
        blocks.set(idx, new BlockReportReplica(block));
      }
      blockReportList = BlockListAsLongs.encode(blocks);
    }

    BlockListAsLongs getBlockReportList() {
      return blockReportList;
    }

    @Override
    public int compareTo(String xferAddr) {
      return getXferAddr().compareTo(xferAddr);
    }

    /**
     * Send a heartbeat to the name-node and replicate blocks if requested.
     */
    @SuppressWarnings(""unused"") // keep it for future blockReceived benchmark
    int replicateBlocks() throws IOException {
      // register datanode
      StorageReport[] rep = { new StorageReport(storage,
          false, DF_CAPACITY, DF_USED, DF_CAPACITY - DF_USED, DF_USED, 0) };
      DatanodeCommand[] cmds = dataNodeProto.sendHeartbeat(dnRegistration,
          rep, 0L, 0L, 0, 0, 0, null, true,
          SlowPeerReports.EMPTY_REPORT, SlowDiskReports.EMPTY_REPORT)
          .getCommands();
      if (cmds != null) {
        for (DatanodeCommand cmd : cmds) {
          if (cmd.getAction() == DatanodeProtocol.DNA_TRANSFER) {
            // Send a copy of a block to another datanode
            BlockCommand bcmd = (BlockCommand)cmd;
            return transferBlocks(bcmd.getBlocks(), bcmd.getTargets(),
                                  bcmd.getTargetStorageIDs());
          }
        }
      }
      return 0;
    }

    /**
     * Transfer blocks to another data-node.
     * Just report on behalf of the other data-node
     * that the blocks have been received.
     */
    private int transferBlocks( Block blocks[],
                                DatanodeInfo xferTargets[][],
                                String targetStorageIDs[][]
                              ) throws IOException {
      for(int i = 0; i < blocks.length; i++) {
        DatanodeInfo blockTargets[] = xferTargets[i];
        for(int t = 0; t < blockTargets.length; t++) {
          DatanodeInfo dnInfo = blockTargets[t];
          String targetStorageID = targetStorageIDs[i][t];
          DatanodeRegistration receivedDNReg;
          receivedDNReg = new DatanodeRegistration(dnInfo,
            new DataStorage(nsInfo),
            new ExportedBlockKeys(), VersionInfo.getVersion());
          ReceivedDeletedBlockInfo[] rdBlocks = {
            new ReceivedDeletedBlockInfo(
                  blocks[i], ReceivedDeletedBlockInfo.BlockStatus.RECEIVED_BLOCK,
                  null) };
          StorageReceivedDeletedBlocks[] report = { new StorageReceivedDeletedBlocks(
              new DatanodeStorage(targetStorageID), rdBlocks) };
          dataNodeProto.blockReceivedAndDeleted(receivedDNReg, bpid, report);
        }
      }
      return blocks.length;
    }
  }

  /**
   * Block report statistics.
   *
   * Each thread here represents its own data-node.
   * Data-nodes send the same block report each time.
   * The block report may contain missing or non-existing blocks.
   */
  class BlockReportStats extends OperationStatsBase {
    static final String OP_BLOCK_REPORT_NAME = ""blockReport"";
    static final String OP_BLOCK_REPORT_USAGE =
        ""-op blockReport [-datanodes T] [-reports N] "" +
        ""[-blocksPerReport B] [-blocksPerFile F] [-blockSize S] [-baseDirName D]"";

    private int blocksPerReport;
    private int blocksPerFile;
    private TinyDatanode[] datanodes; // array of data-nodes sorted by name

    BlockReportStats(List<String> args) {
      super();
      numThreads = 10;
      numOpsRequired = 30;
      this.blocksPerReport = 100;
      this.blocksPerFile = 10;
      // set heartbeat interval to 3 min, so that expiration were 40 min
      config.setLong(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY, 3 * 60);
      parseArguments(args);
      // adjust replication to the number of data-nodes
      this.replication = (short)Math.min(replication, getNumDatanodes());
    }

    /**
     * Each thread pretends its a data-node here.
     */
    private int getNumDatanodes() {
      return numThreads;
    }

    @Override
    String getOpName() {
      return OP_BLOCK_REPORT_NAME;
    }

    @Override
    void parseArguments(List<String> args) {
      boolean ignoreUnrelatedOptions = verifyOpArgument(args);
      for (int i = 2; i < args.size(); i++) {       // parse command line
        if(args.get(i).equals(""-reports"")) {
          if(i+1 == args.size())  printUsage();
          numOpsRequired = Integer.parseInt(args.get(++i));
        } else if(args.get(i).equals(""-datanodes"")) {
          if(i+1 == args.size())  printUsage();
          numThreads = Integer.parseInt(args.get(++i));
        } else if(args.get(i).equals(""-blocksPerReport"")) {
          if(i+1 == args.size())  printUsage();
          blocksPerReport = Integer.parseInt(args.get(++i));
        } else if(args.get(i).equals(""-blocksPerFile"")) {
          if(i+1 == args.size())  printUsage();
          blocksPerFile = Integer.parseInt(args.get(++i));
        } else if (args.get(i).equals(""-blockSize"")) {
          if(i+1 == args.size())  printUsage();
          blockSize = StringUtils.TraditionalBinaryPrefix.string2long(args.get(++i));
        } else if(args.get(i).equals(""-baseDirName"")) {
          if (i + 1 == args.size()) {
            printUsage();
          }
          setBaseDirName(args.get(++i));
        } else if(!ignoreUnrelatedOptions)
          printUsage();
      }
    }

    @Override
    void generateInputs(int[] ignore) throws IOException {
      int nrDatanodes = getNumDatanodes();
      int nrBlocks = (int)Math.ceil((double)blocksPerReport * nrDatanodes
                                    / replication);
      int nrFiles = (int)Math.ceil((double)nrBlocks / blocksPerFile);
      datanodes = new TinyDatanode[nrDatanodes];
      // create data-nodes
      for(int idx=0; idx < nrDatanodes; idx++) {
        datanodes[idx] = new TinyDatanode(idx, blocksPerReport);
        datanodes[idx].register();
        datanodes[idx].sendHeartbeat();
      }

      // create files
      LOG.info(""Creating "" + nrFiles + "" files with "" + blocksPerFile + "" blocks each."");
      FileNameGenerator nameGenerator;
      nameGenerator = new FileNameGenerator(getBaseDir(), 100);
      String clientName = getClientName(007);
      if (!nonSuperUser) {
        try {
          clientProto.setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_LEAVE,
                  false);
        } catch (Exception e){
          LOG.error(""Potentially insufficient permission: try running the tool"" +
                    "" with -nonSuperUser argument or login as super user"");
          throw e;
        }
      }
      for(int idx=0; idx < nrFiles; idx++) {
        String fileName = nameGenerator.getNextFileName(""ThroughputBench"");
        clientProto.create(fileName, FsPermission.getDefault(), clientName,
            new EnumSetWritable<CreateFlag>(EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE)), true, replication,
            blockSize, CryptoProtocolVersion.supported(), null, null);
        ExtendedBlock lastBlock = addBlocks(fileName, clientName);
        clientProto.complete(fileName, clientName, lastBlock, HdfsConstants.GRANDFATHER_INODE_ID);
      }
      // prepare block reports
      for(int idx=0; idx < nrDatanodes; idx++) {
        datanodes[idx].formBlockReport();
      }
    }

    private ExtendedBlock addBlocks(String fileName, String clientName)
    throws IOException {
      DatanodeInfo[] excludeNodes = null;
      DatanodeInfo[] dnInfos = clientProto.getDatanodeReport(
          HdfsConstants.DatanodeReportType.LIVE);
      if (dnInfos != null && dnInfos.length > 0) {
        List<DatanodeInfo> tmpNodes = new ArrayList<>();
        String localHost = DNS.getDefaultHost(""default"", ""default"");
        for (DatanodeInfo dnInfo : dnInfos) {
          if (!localHost.equals(dnInfo.getHostName()) ||
              (dnInfo.getXferPort() > datanodes.length)) {
            tmpNodes.add(dnInfo);
          }
        }

        if (tmpNodes.size() > 0) {
          excludeNodes = tmpNodes.toArray(new DatanodeInfo[tmpNodes.size()]);
        }
      }

      ExtendedBlock prevBlock = null;
      for(int jdx = 0; jdx < blocksPerFile; jdx++) {
        LocatedBlock loc = addBlock(fileName, clientName,
            prevBlock, excludeNodes, HdfsConstants.GRANDFATHER_INODE_ID, null);
        prevBlock = loc.getBlock();
        for(DatanodeInfo dnInfo : loc.getLocations()) {
          int dnIdx = dnInfo.getXferPort() - 1;
          datanodes[dnIdx].addBlock(loc.getBlock().getLocalBlock());
          ReceivedDeletedBlockInfo[] rdBlocks = { new ReceivedDeletedBlockInfo(
              loc.getBlock().getLocalBlock(),
              ReceivedDeletedBlockInfo.BlockStatus.RECEIVED_BLOCK, null) };
          StorageReceivedDeletedBlocks[] report = { new StorageReceivedDeletedBlocks(
              new DatanodeStorage(datanodes[dnIdx].storage.getStorageID()),
              rdBlocks) };
          dataNodeProto.blockReceivedAndDeleted(datanodes[dnIdx].dnRegistration,
              bpid, report);
        }
        // IBRs are asynchronously processed by NameNode. The next
        // ClientProtocol#addBlock() may throw NotReplicatedYetException.
      }
      return prevBlock;
    }

    /**
     * Retry ClientProtocol.addBlock() if it throws NotReplicatedYetException.
     * Because addBlock() also commits the previous block,
     * it fails if enough IBRs are not processed by NameNode.
     */
    private LocatedBlock addBlock(String src, String clientName,
        ExtendedBlock previous, DatanodeInfo[] excludeNodes, long fileId,
        String[] favoredNodes) throws IOException {
      for (int i = 0; i < 30; i++) {
        try {
          return clientProto.addBlock(src, clientName,
              previous, excludeNodes, fileId, favoredNodes, null);
        } catch (NotReplicatedYetException|RemoteException e) {
          if (e instanceof RemoteException) {
            String className = ((RemoteException) e).getClassName();
            if (!className.equals(NotReplicatedYetException.class.getName())) {
              throw e;
            }
          }
          try {
            Thread.sleep(100);
          } catch (InterruptedException ie) {
            LOG.warn(""interrupted while retrying addBlock."", ie);
          }
        }
      }
      throw new IOException(""failed to add block."");
    }

    /**
     * Does not require the argument
     */
    @Override
    String getExecutionArgument(int daemonId) {
      return null;
    }

    @Override
    long executeOp(int daemonId, int inputIdx, String ignore) throws IOException {
      assert daemonId < numThreads : ""Wrong daemonId."";
      TinyDatanode dn = datanodes[daemonId];
      long start = Time.now();
      StorageBlockReport[] report = { new StorageBlockReport(
          dn.storage, dn.getBlockReportList()) };
      dataNodeProto.blockReport(dn.dnRegistration, bpid, report,
          new BlockReportContext(1, 0, System.nanoTime(), 0L));
      long end = Time.now();
      return end-start;
    }

    @Override
    void printResults() {
      String blockDistribution = """";
      String delim = ""("";
      for(int idx=0; idx < getNumDatanodes(); idx++) {
        blockDistribution += delim + datanodes[idx].nrBlocks;
        delim = "", "";
      }
      blockDistribution += "")"";
      LOG.info(""--- "" + getOpName() + "" inputs ---"");
      LOG.info(""baseDir = "" + getBaseDir());
      LOG.info(""reports = "" + numOpsRequired);
      LOG.info(""datanodes = "" + numThreads + "" "" + blockDistribution);
      LOG.info(""blocksPerReport = "" + blocksPerReport);
      LOG.info(""blocksPerFile = "" + blocksPerFile);
      printStats();
    }
  }   // end BlockReportStats

  /**
   * Measures how fast redundancy monitor can compute data-node work.
   *
   * It runs only one thread until no more work can be scheduled.
   */
  class ReplicationStats extends OperationStatsBase {
    static final String OP_REPLICATION_NAME = ""replication"";
    static final String OP_REPLICATION_USAGE =
        ""-op replication [-datanodes T] [-nodesToDecommission D] "" +
        ""[-nodeReplicationLimit C] [-totalBlocks B] [-blockSize S] ""
        + ""[-replication R] [-baseDirName D]"";

    private final BlockReportStats blockReportObject;
    private int numDatanodes;
    private int nodesToDecommission;
    private int nodeReplicationLimit;
    private int totalBlocks;
    private int numDecommissionedBlocks;
    private int numPendingBlocks;

    ReplicationStats(List<String> args) {
      super();
      numThreads = 1;
      numDatanodes = 10;
      nodesToDecommission = 1;
      nodeReplicationLimit = 100;
      totalBlocks = 100;
      parseArguments(args);
      // number of operations is 4 times the number of decommissioned
      // blocks divided by the number of needed replications scanned 
      // by the redundancy monitor in one iteration
      numOpsRequired = (totalBlocks*replication*nodesToDecommission*2)
            / (numDatanodes*numDatanodes);

      String[] blkReportArgs = {
          ""-op"", ""blockReport"",
          ""-datanodes"", String.valueOf(numDatanodes),
          ""-blocksPerReport"", String.valueOf(totalBlocks*replication/numDatanodes),
          ""-blocksPerFile"", String.valueOf(numDatanodes),
          ""-blockSize"", String.valueOf(blockSize),
          ""-baseDirName"", getBaseDirName()};
      blockReportObject = new BlockReportStats(Arrays.asList(blkReportArgs));
      numDecommissionedBlocks = 0;
      numPendingBlocks = 0;
    }

    @Override
    String getOpName() {
      return OP_REPLICATION_NAME;
    }

    @Override
    void parseArguments(List<String> args) {
      boolean ignoreUnrelatedOptions = verifyOpArgument(args);
      for (int i = 2; i < args.size(); i++) {       // parse command line
        if(args.get(i).equals(""-datanodes"")) {
          if(i+1 == args.size())  printUsage();
          numDatanodes = Integer.parseInt(args.get(++i));
        } else if(args.get(i).equals(""-nodesToDecommission"")) {
          if(i+1 == args.size())  printUsage();
          nodesToDecommission = Integer.parseInt(args.get(++i));
        } else if(args.get(i).equals(""-nodeReplicationLimit"")) {
          if(i+1 == args.size())  printUsage();
          nodeReplicationLimit = Integer.parseInt(args.get(++i));
        } else if(args.get(i).equals(""-totalBlocks"")) {
          if(i+1 == args.size())  printUsage();
          totalBlocks = Integer.parseInt(args.get(++i));
        } else if(args.get(i).equals(""-replication"")) {
          if(i+1 == args.size())  printUsage();
          replication = Short.parseShort(args.get(++i));
        } else if (args.get(i).equals(""-blockSize"")) {
          if(i+1 == args.size())  printUsage();
          blockSize = StringUtils.TraditionalBinaryPrefix.string2long(args.get(++i));
        } else if(args.get(i).equals(""-baseDirName"")) {
          if (i + 1 == args.size()) {
            printUsage();
          }
          setBaseDirName(args.get(++i));
        } else if(!ignoreUnrelatedOptions)
          printUsage();
      }
    }

    @Override
    void generateInputs(int[] ignore) throws IOException {
      final FSNamesystem namesystem = nameNode.getNamesystem();

      // start data-nodes; create a bunch of files; generate block reports.
      blockReportObject.generateInputs(ignore);
      // stop redundancy monitor thread.
      BlockManagerTestUtil.stopRedundancyThread(namesystem.getBlockManager());

      // report blocks once
      int nrDatanodes = blockReportObject.getNumDatanodes();
      for(int idx=0; idx < nrDatanodes; idx++) {
        blockReportObject.executeOp(idx, 0, null);
      }
      // decommission data-nodes
      decommissionNodes();
      // set node replication limit
      BlockManagerTestUtil.setNodeReplicationLimit(namesystem.getBlockManager(),
          nodeReplicationLimit);
    }

    private void decommissionNodes() throws IOException {
      String excludeFN = config.get(DFSConfigKeys.DFS_HOSTS_EXCLUDE, ""exclude"");
      FileOutputStream excludeFile = new FileOutputStream(excludeFN);
      excludeFile.getChannel().truncate(0L);
      int nrDatanodes = blockReportObject.getNumDatanodes();
      numDecommissionedBlocks = 0;
      for(int i=0; i < nodesToDecommission; i++) {
        TinyDatanode dn = blockReportObject.datanodes[nrDatanodes-1-i];
        numDecommissionedBlocks += dn.nrBlocks;
        excludeFile.write(dn.getXferAddr().getBytes());
        excludeFile.write('\n');
        LOG.info(""Datanode "" + dn + "" is decommissioned."");
      }
      excludeFile.close();
      clientProto.refreshNodes();
    }

    /**
     * Does not require the argument
     */
    @Override
    String getExecutionArgument(int daemonId) {
      return null;
    }

    @Override
    long executeOp(int daemonId, int inputIdx, String ignore) throws IOException {
      assert daemonId < numThreads : ""Wrong daemonId."";
      long start = Time.now();
      // compute data-node work
      int work = BlockManagerTestUtil.getComputedDatanodeWork(
          nameNode.getNamesystem().getBlockManager());
      long end = Time.now();
      numPendingBlocks += work;
      if(work == 0)
        daemons.get(daemonId).terminate();
      return end-start;
    }

    @Override
    void printResults() {
      String blockDistribution = """";
      String delim = ""("";
      for(int idx=0; idx < blockReportObject.getNumDatanodes(); idx++) {
        blockDistribution += delim + blockReportObject.datanodes[idx].nrBlocks;
        delim = "", "";
      }
      blockDistribution += "")"";
      LOG.info(""--- "" + getOpName() + "" inputs ---"");
      LOG.info(""baseDir = "" + getBaseDir());
      LOG.info(""numOpsRequired = "" + numOpsRequired);
      LOG.info(""datanodes = "" + numDatanodes + "" "" + blockDistribution);
      LOG.info(""decommissioned datanodes = "" + nodesToDecommission);
      LOG.info(""datanode replication limit = "" + nodeReplicationLimit);
      LOG.info(""total blocks = "" + totalBlocks);
      printStats();
      LOG.info(""decommissioned blocks = "" + numDecommissionedBlocks);
      LOG.info(""pending replications = "" + numPendingBlocks);
      LOG.info(""replications per sec: "" + getBlocksPerSecond());
    }

    private double getBlocksPerSecond() {
      return elapsedTime == 0 ? 0 : 1000*(double)numPendingBlocks / elapsedTime;
    }

  }   // end ReplicationStats

  static void printUsage() {
    System.err.println(""Usage: NNThroughputBenchmark""
        + ""\n\t""    + OperationStatsBase.OP_ALL_USAGE
        + "" | \n\t"" + CreateFileStats.OP_CREATE_USAGE
        + "" | \n\t"" + MkdirsStats.OP_MKDIRS_USAGE
        + "" | \n\t"" + OpenFileStats.OP_OPEN_USAGE
        + "" | \n\t"" + DeleteFileStats.OP_DELETE_USAGE
        + "" | \n\t"" + AppendFileStats.OP_APPEND_USAGE
        + "" | \n\t"" + FileStatusStats.OP_FILE_STATUS_USAGE
        + "" | \n\t"" + RenameFileStats.OP_RENAME_USAGE
        + "" | \n\t"" + BlockReportStats.OP_BLOCK_REPORT_USAGE
        + "" | \n\t"" + ReplicationStats.OP_REPLICATION_USAGE
        + "" | \n\t"" + CleanAllStats.OP_CLEAN_USAGE
        + "" | \n\t"" + GENERAL_OPTIONS_USAGE
    );
    System.err.println();
    GenericOptionsParser.printGenericCommandUsage(System.err);
    System.err.println(""If connecting to a remote NameNode with -fs option, "" +
        ""dfs.namenode.fs-limits.min-block-size should be set to 16."");
    ExitUtil.terminate(-1);
  }

  public static void runBenchmark(Configuration conf, String[] args)
      throws Exception {
    NNThroughputBenchmark bench = null;
    try {
      bench = new NNThroughputBenchmark(conf);
      ToolRunner.run(bench, args);
    } finally {
      if(bench != null)
        bench.close();
    }
  }

  /**
   * Main method of the benchmark.
   * @param aArgs command line parameters
   */
  @Override // Tool
  public int run(String[] aArgs) throws Exception {
    List<String> args = new ArrayList<String>(Arrays.asList(aArgs));
    if(args.size() < 2 || ! args.get(0).startsWith(""-op""))
      printUsage();

    String type = args.get(1);
    boolean runAll = OperationStatsBase.OP_ALL_NAME.equals(type);

    final URI nnUri = FileSystem.getDefaultUri(config);
    // Start the NameNode
    String[] argv = new String[] {};

    List<OperationStatsBase> ops = new ArrayList<OperationStatsBase>();
    OperationStatsBase opStat = null;
    try {
      if(runAll || CreateFileStats.OP_CREATE_NAME.equals(type)) {
        opStat = new CreateFileStats(args);
        ops.add(opStat);
      }
      if(runAll || MkdirsStats.OP_MKDIRS_NAME.equals(type)) {
        opStat = new MkdirsStats(args);
        ops.add(opStat);
      }
      if(runAll || OpenFileStats.OP_OPEN_NAME.equals(type)) {
        opStat = new OpenFileStats(args);
        ops.add(opStat);
      }
      if(runAll || DeleteFileStats.OP_DELETE_NAME.equals(type)) {
        opStat = new DeleteFileStats(args);
        ops.add(opStat);
      }
      if (runAll || AppendFileStats.OP_APPEND_NAME.equals(type)) {
        opStat = new AppendFileStats(args);
        ops.add(opStat);
      }
      if(runAll || FileStatusStats.OP_FILE_STATUS_NAME.equals(type)) {
        opStat = new FileStatusStats(args);
        ops.add(opStat);
      }
      if(runAll || RenameFileStats.OP_RENAME_NAME.equals(type)) {
        opStat = new RenameFileStats(args);
        ops.add(opStat);
      }
      if(runAll || BlockReportStats.OP_BLOCK_REPORT_NAME.equals(type)) {
        opStat = new BlockReportStats(args);
        ops.add(opStat);
      }
      if(runAll || ReplicationStats.OP_REPLICATION_NAME.equals(type)) {
        if (nnUri.getScheme() != null && nnUri.getScheme().equals(""hdfs"")) {
          LOG.warn(""The replication test is ignored as it does not support "" +
              ""standalone namenode in another process or on another host. "");
        } else {
          opStat = new ReplicationStats(args);
          ops.add(opStat);
        }
      }
      if(runAll || CleanAllStats.OP_CLEAN_NAME.equals(type)) {
        opStat = new CleanAllStats(args);
        ops.add(opStat);
      }
      if (ops.isEmpty()) {
        printUsage();
      }

      if (nnUri.getScheme() == null || nnUri.getScheme().equals(""file"")) {
        LOG.info(""Remote NameNode is not specified. Creating one."");
        FileSystem.setDefaultUri(config, ""hdfs://localhost:0"");
        config.set(DFSConfigKeys.DFS_NAMENODE_HTTP_ADDRESS_KEY, ""0.0.0.0:0"");
        nameNode = NameNode.createNameNode(argv, config);
        NamenodeProtocols nnProtos = nameNode.getRpcServer();
        nameNodeProto = nnProtos;
        clientProto = nnProtos;
        dataNodeProto = nnProtos;
        refreshUserMappingsProto = nnProtos;
        bpid = nameNode.getNamesystem().getBlockPoolId();
      } else {
        DistributedFileSystem dfs = (DistributedFileSystem)
            FileSystem.get(getConf());
        nameNodeProto = DFSTestUtil.getNamenodeProtocolProxy(config, nnUri,
            UserGroupInformation.getCurrentUser());
        clientProto = dfs.getClient().getNamenode();
        InetSocketAddress nnAddr = DFSUtilClient.getNNAddress(nnUri);
        dataNodeProto = new DatanodeProtocolClientSideTranslatorPB(
            nnAddr, config);
        refreshUserMappingsProto =
            DFSTestUtil.getRefreshUserMappingsProtocolProxy(config, nnAddr);
        getBlockPoolId(dfs);
      }
      // run each benchmark
      for(OperationStatsBase op : ops) {
        LOG.info(""Starting benchmark: "" + op.getOpName() + "", baseDir: "" + op.getBaseDir());
        op.benchmark();
        op.cleanUp();
      }
      // print statistics
      for(OperationStatsBase op : ops) {
        LOG.info("""");
        op.printResults();
      }
    } catch(Exception e) {
      LOG.error(StringUtils.stringifyException(e));
      throw e;
    }
    return 0;
  }

  private void getBlockPoolId(DistributedFileSystem unused)
    throws IOException {
    final NamespaceInfo nsInfo = nameNodeProto.versionRequest();
    bpid = nsInfo.getBlockPoolID();
  }

  public static void main(String[] args) throws Exception {
    runBenchmark(new HdfsConfiguration(), args);
  }

  @Override // Configurable
  public void setConf(Configuration conf) {
    config = conf;
  }

  @Override // Configurable
  public Configuration getConf() {
    return config;
  }
}
","['Assertion Roulette', 'Conditional Test Logic', 'Mystery Guest', 'Unknown Test']","['Assertion Roulette', 'Lazy Test', 'Magic Number Test', 'Sensitive Equality', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette']",3,3,1,11
26980_4.0_hadoop_testuserschainmapwithlrucache,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/26980_test.java,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/26980_actual.java,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * ""License""); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.hadoop.yarn.server.router.webapp;

import java.io.IOException;
import java.security.PrivilegedExceptionAction;
import java.util.Map;

import javax.ws.rs.core.Response;

import org.apache.hadoop.security.UserGroupInformation;
import org.apache.hadoop.yarn.exceptions.YarnException;
import org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ActivitiesInfo;
import org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppActivitiesInfo;
import org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppAttemptsInfo;
import org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppInfo;
import org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppPriority;
import org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppQueue;
import org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppState;
import org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppTimeoutInfo;
import org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppTimeoutsInfo;
import org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ApplicationStatisticsInfo;
import org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppsInfo;
import org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ClusterInfo;
import org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ClusterMetricsInfo;
import org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.LabelsToNodesInfo;
import org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.NodeInfo;
import org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.NodeLabelsInfo;
import org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.NodeToLabelsInfo;
import org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.NodesInfo;
import org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.SchedulerTypeInfo;
import org.apache.hadoop.yarn.server.router.webapp.RouterWebServices.RequestInterceptorChainWrapper;
import org.apache.hadoop.yarn.server.webapp.dao.AppAttemptInfo;
import org.apache.hadoop.yarn.server.webapp.dao.ContainerInfo;
import org.apache.hadoop.yarn.server.webapp.dao.ContainersInfo;
import org.junit.Assert;
import org.junit.Test;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Test class to validate the WebService interceptor model inside the Router.
 */
public class TestRouterWebServices extends BaseRouterWebServicesTest {

  private static final Logger LOG =
      LoggerFactory.getLogger(TestRouterWebServices.class);

  private String user = ""test1"";

  /**
   * Test that all requests get forwarded to the last interceptor in the chain
   * get back the responses.
   */
  @Test
  public void testRouterWebServicesE2E() throws Exception {

    ClusterInfo clusterInfo = get(user);
    Assert.assertNotNull(clusterInfo);

    ClusterInfo clusterInfo2 = getClusterInfo(user);
    Assert.assertNotNull(clusterInfo2);

    ClusterMetricsInfo clusterMetricsInfo = getClusterMetricsInfo(user);
    Assert.assertNotNull(clusterMetricsInfo);

    SchedulerTypeInfo schedulerTypeInfo = getSchedulerInfo(user);
    Assert.assertNotNull(schedulerTypeInfo);

    String dumpResult = dumpSchedulerLogs(user);
    Assert.assertNotNull(dumpResult);

    NodesInfo nodesInfo = getNodes(user);
    Assert.assertNotNull(nodesInfo);

    NodeInfo nodeInfo = getNode(user);
    Assert.assertNotNull(nodeInfo);

    AppsInfo appsInfo = getApps(user);
    Assert.assertNotNull(appsInfo);

    ActivitiesInfo activitiesInfo = getActivities(user);
    Assert.assertNotNull(activitiesInfo);

    AppActivitiesInfo appActiviesInfo = getAppActivities(user);
    Assert.assertNotNull(appActiviesInfo);

    ApplicationStatisticsInfo applicationStatisticsInfo =
        getAppStatistics(user);
    Assert.assertNotNull(applicationStatisticsInfo);

    AppInfo appInfo = getApp(user);
    Assert.assertNotNull(appInfo);

    AppState appState = getAppState(user);
    Assert.assertNotNull(appState);

    Response response = updateAppState(user);
    Assert.assertNotNull(response);

    NodeToLabelsInfo nodeToLabelsInfo = getNodeToLabels(user);
    Assert.assertNotNull(nodeToLabelsInfo);

    LabelsToNodesInfo labelsToNodesInfo = getLabelsToNodes(user);
    Assert.assertNotNull(labelsToNodesInfo);

    Response response2 = replaceLabelsOnNodes(user);
    Assert.assertNotNull(response2);

    Response response3 = replaceLabelsOnNode(user);
    Assert.assertNotNull(response3);

    NodeLabelsInfo nodeLabelsInfo = getClusterNodeLabels(user);
    Assert.assertNotNull(nodeLabelsInfo);

    Response response4 = addToClusterNodeLabels(user);
    Assert.assertNotNull(response4);

    Response response5 = removeFromClusterNodeLabels(user);
    Assert.assertNotNull(response5);

    NodeLabelsInfo nodeLabelsInfo2 = getLabelsOnNode(user);
    Assert.assertNotNull(nodeLabelsInfo2);

    AppPriority appPriority = getAppPriority(user);
    Assert.assertNotNull(appPriority);

    Response response6 = updateApplicationPriority(user);
    Assert.assertNotNull(response6);

    AppQueue appQueue = getAppQueue(user);
    Assert.assertNotNull(appQueue);

    Response response7 = updateAppQueue(user);
    Assert.assertNotNull(response7);

    Response response8 = createNewApplication(user);
    Assert.assertNotNull(response8);

    Response response9 = submitApplication(user);
    Assert.assertNotNull(response9);

    Response response10 = postDelegationToken(user);
    Assert.assertNotNull(response10);

    Response response11 = postDelegationTokenExpiration(user);
    Assert.assertNotNull(response11);

    Response response12 = cancelDelegationToken(user);
    Assert.assertNotNull(response12);

    Response response13 = createNewReservation(user);
    Assert.assertNotNull(response13);

    Response response14 = submitReservation(user);
    Assert.assertNotNull(response14);

    Response response15 = updateReservation(user);
    Assert.assertNotNull(response15);

    Response response16 = deleteReservation(user);
    Assert.assertNotNull(response16);

    Response response17 = listReservation(user);
    Assert.assertNotNull(response17);

    AppTimeoutInfo appTimeoutInfo = getAppTimeout(user);
    Assert.assertNotNull(appTimeoutInfo);

    AppTimeoutsInfo appTimeoutsInfo = getAppTimeouts(user);
    Assert.assertNotNull(appTimeoutsInfo);

    Response response18 = updateApplicationTimeout(user);
    Assert.assertNotNull(response18);

    AppAttemptsInfo appAttemptsInfo = getAppAttempts(user);
    Assert.assertNotNull(appAttemptsInfo);

    AppAttemptInfo appAttemptInfo = getAppAttempt(user);
    Assert.assertNotNull(appAttemptInfo);

    ContainersInfo containersInfo = getContainers(user);
    Assert.assertNotNull(containersInfo);

    ContainerInfo containerInfo = getContainer(user);
    Assert.assertNotNull(containerInfo);

    Response response19 = updateSchedulerConfiguration(user);
    Assert.assertNotNull(response19);

    Response response20 = getSchedulerConfiguration(user);
    Assert.assertNotNull(response20);
  }

  /**
   * Tests if the pipeline is created properly.
   */
  @Test
  public void testRequestInterceptorChainCreation() throws Exception {
    RESTRequestInterceptor root =
        super.getRouterWebServices().createRequestInterceptorChain();
    int index = 0;
    while (root != null) {
      // The current pipeline is:
      // PassThroughRESTRequestInterceptor - index = 0
      // PassThroughRESTRequestInterceptor - index = 1
      // PassThroughRESTRequestInterceptor - index = 2
      // MockRESTRequestInterceptor - index = 3
      switch (index) {
      case 0: // Fall to the next case
      case 1: // Fall to the next case
      case 2:
        // If index is equal to 0,1 or 2 we fall in this check
        Assert.assertEquals(PassThroughRESTRequestInterceptor.class.getName(),
            root.getClass().getName());
        break;
      case 3:
        Assert.assertEquals(MockRESTRequestInterceptor.class.getName(),
            root.getClass().getName());
        break;
      default:
        Assert.fail();
      }
      root = root.getNextInterceptor();
      index++;
    }
    Assert.assertEquals(""The number of interceptors in chain does not match"", 4,
        index);
  }

  /**
   * Test if the different chains for users are generated, and LRU cache is
   * working as expected.
   */
  @Test
  public void testUsersChainMapWithLRUCache()
      throws YarnException, IOException, InterruptedException {
    getInterceptorChain(""test1"");
    getInterceptorChain(""test2"");
    getInterceptorChain(""test3"");
    getInterceptorChain(""test4"");
    getInterceptorChain(""test5"");
    getInterceptorChain(""test6"");
    getInterceptorChain(""test7"");
    getInterceptorChain(""test8"");

    Map<String, RequestInterceptorChainWrapper> pipelines =
        getRouterWebServices().getPipelines();
    Assert.assertEquals(8, pipelines.size());

    getInterceptorChain(""test9"");
    getInterceptorChain(""test10"");
    getInterceptorChain(""test1"");
    getInterceptorChain(""test11"");

    // The cache max size is defined in TEST_MAX_CACHE_SIZE
    Assert.assertEquals(10, pipelines.size());

    RequestInterceptorChainWrapper chain = pipelines.get(""test1"");
    Assert.assertNotNull(""test1 should not be evicted"", chain);

    chain = pipelines.get(""test2"");
    Assert.assertNull(""test2 should have been evicted"", chain);
  }

  /**
   * This test validates if the RESTRequestInterceptor chain for the user
   * can build and init correctly when a multi-client process begins to
   * request RouterWebServices for the same user simultaneously.
   */
  @Test
  public void testWebPipelineConcurrent() throws InterruptedException {
    final String user = ""test1"";

    /*
     * ClientTestThread is a thread to simulate a client request to get a
     * RESTRequestInterceptor for the user.
     */
    class ClientTestThread extends Thread {
      private RESTRequestInterceptor interceptor;
      @Override public void run() {
        try {
          interceptor = pipeline();
        } catch (IOException | InterruptedException e) {
          e.printStackTrace();
        }
      }
      private RESTRequestInterceptor pipeline()
          throws IOException, InterruptedException {
        return UserGroupInformation.createRemoteUser(user).doAs(
            new PrivilegedExceptionAction<RESTRequestInterceptor>() {
              @Override
              public RESTRequestInterceptor run() throws Exception {
                RequestInterceptorChainWrapper wrapper =
                    getInterceptorChain(user);
                RESTRequestInterceptor interceptor =
                    wrapper.getRootInterceptor();
                Assert.assertNotNull(interceptor);
                LOG.info(""init web interceptor success for user"" + user);
                return interceptor;
              }
            });
      }
    }

    /*
     * We start the first thread. It should not finish initing a chainWrapper
     * before the other thread starts. In this way, the second thread can
     * init at the same time of the first one. In the end, we validate that
     * the 2 threads get the same chainWrapper without going into error.
     */
    ClientTestThread client1 = new ClientTestThread();
    ClientTestThread client2 = new ClientTestThread();
    client1.start();
    client2.start();
    client1.join();
    client2.join();

    Assert.assertNotNull(client1.interceptor);
    Assert.assertNotNull(client2.interceptor);
    Assert.assertSame(client1.interceptor, client2.interceptor);
  }

}
","/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * ""License""); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.hadoop.yarn.server.router.webapp;

import java.io.IOException;
import java.util.Collections;
import java.util.Map;
import java.util.Set;

import javax.servlet.http.HttpServletRequest;
import javax.servlet.http.HttpServletResponse;
import javax.ws.rs.Consumes;
import javax.ws.rs.DELETE;
import javax.ws.rs.DefaultValue;
import javax.ws.rs.FormParam;
import javax.ws.rs.GET;
import javax.ws.rs.POST;
import javax.ws.rs.PUT;
import javax.ws.rs.Path;
import javax.ws.rs.PathParam;
import javax.ws.rs.Produces;
import javax.ws.rs.QueryParam;
import javax.ws.rs.core.Context;
import javax.ws.rs.core.MediaType;
import javax.ws.rs.core.Response;

import org.apache.hadoop.classification.InterfaceAudience.Private;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.http.JettyUtils;
import org.apache.hadoop.security.UserGroupInformation;
import org.apache.hadoop.security.authorize.AuthorizationException;
import org.apache.hadoop.yarn.conf.YarnConfiguration;
import org.apache.hadoop.yarn.exceptions.YarnException;
import org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWSConsts;
import org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServiceProtocol;
import org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ActivitiesInfo;
import org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppActivitiesInfo;
import org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppAttemptsInfo;
import org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppInfo;
import org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppPriority;
import org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppQueue;
import org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppState;
import org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppTimeoutInfo;
import org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppTimeoutsInfo;
import org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ApplicationStatisticsInfo;
import org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ApplicationSubmissionContextInfo;
import org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppsInfo;
import org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ClusterInfo;
import org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ClusterMetricsInfo;
import org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ClusterUserInfo;
import org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.DelegationToken;
import org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.LabelsToNodesInfo;
import org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.NodeInfo;
import org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.NodeLabelsInfo;
import org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.NodeToLabelsEntryList;
import org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.NodeToLabelsInfo;
import org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.NodesInfo;
import org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.RMQueueAclInfo;
import org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ReservationDeleteRequestInfo;
import org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ReservationSubmissionRequestInfo;
import org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ReservationUpdateRequestInfo;
import org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ResourceInfo;
import org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ResourceOptionInfo;
import org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.BulkActivitiesInfo;
import org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.SchedulerTypeInfo;
import org.apache.hadoop.yarn.server.router.Router;
import org.apache.hadoop.yarn.server.router.RouterServerUtil;
import org.apache.hadoop.yarn.server.router.clientrm.RouterClientRMService;
import org.apache.hadoop.yarn.server.webapp.dao.ContainerInfo;
import org.apache.hadoop.yarn.server.webapp.dao.ContainersInfo;
import org.apache.hadoop.yarn.util.LRUCacheHashMap;
import org.apache.hadoop.yarn.webapp.dao.SchedConfUpdateInfo;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import org.apache.hadoop.classification.VisibleForTesting;
import com.google.inject.Inject;
import com.google.inject.Singleton;

import static org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices.DEFAULT_ACTIVITIES_COUNT;
import static org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices.DEFAULT_SUMMARIZE;

/**
 * RouterWebServices is a service that runs on each router that can be used to
 * intercept and inspect {@link RMWebServiceProtocol} messages from client to
 * the cluster resource manager. It listens {@link RMWebServiceProtocol} REST
 * messages from the client and creates a request intercepting pipeline instance
 * for each client. The pipeline is a chain of {@link RESTRequestInterceptor}
 * instances that can inspect and modify the request/response as needed. The
 * main difference with AMRMProxyService is the protocol they implement.
 **/
@Singleton
@Path(RMWSConsts.RM_WEB_SERVICE_PATH)
public class RouterWebServices implements RMWebServiceProtocol {

  private static final Logger LOG =
      LoggerFactory.getLogger(RouterWebServices.class);
  private final Router router;
  private final Configuration conf;
  private @Context HttpServletResponse response;

  private Map<String, RequestInterceptorChainWrapper> userPipelineMap;

  // -------Default values of QueryParams for RMWebServiceProtocol--------

  public static final String DEFAULT_QUEUE = ""default"";
  public static final String DEFAULT_RESERVATION_ID = """";
  public static final String DEFAULT_START_TIME = ""0"";
  public static final String DEFAULT_END_TIME = ""-1"";
  public static final String DEFAULT_INCLUDE_RESOURCE = ""false"";

  @Inject
  public RouterWebServices(final Router router, Configuration conf) {
    this.router = router;
    this.conf = conf;
    int maxCacheSize =
        conf.getInt(YarnConfiguration.ROUTER_PIPELINE_CACHE_MAX_SIZE,
            YarnConfiguration.DEFAULT_ROUTER_PIPELINE_CACHE_MAX_SIZE);
    this.userPipelineMap = Collections.synchronizedMap(new LRUCacheHashMap<>(maxCacheSize, true));
  }

  private void init() {
    // clear content type
    response.setContentType(null);
  }

  @VisibleForTesting
  protected RequestInterceptorChainWrapper getInterceptorChain(
      final HttpServletRequest hsr) {
    String user = """";
    if (hsr != null) {
      user = hsr.getRemoteUser();
    }
    try {
      if (user == null || user.equals("""")) {
        // Yarn Router user
        user = UserGroupInformation.getCurrentUser().getUserName();
      }
    } catch (IOException e) {
      LOG.error(""Cannot get user: {}"", e.getMessage());
    }
    RequestInterceptorChainWrapper chain = userPipelineMap.get(user);
    if (chain != null && chain.getRootInterceptor() != null) {
      return chain;
    }
    return initializePipeline(user);
  }

  /**
   * Gets the Request interceptor chains for all the users.
   *
   * @return the request interceptor chains.
   */
  @VisibleForTesting
  protected Map<String, RequestInterceptorChainWrapper> getPipelines() {
    return this.userPipelineMap;
  }

  /**
   * This method creates and returns reference of the first interceptor in the
   * chain of request interceptor instances.
   *
   * @return the reference of the first interceptor in the chain
   */
  @VisibleForTesting
  protected RESTRequestInterceptor createRequestInterceptorChain() {
    return RouterServerUtil.createRequestInterceptorChain(conf,
        YarnConfiguration.ROUTER_WEBAPP_INTERCEPTOR_CLASS_PIPELINE,
        YarnConfiguration.DEFAULT_ROUTER_WEBAPP_INTERCEPTOR_CLASS,
        RESTRequestInterceptor.class);
  }

  /**
   * Initializes the request interceptor pipeline for the specified user.
   *
   * @param user specified user.
   */
  private RequestInterceptorChainWrapper initializePipeline(String user) {
    synchronized (this.userPipelineMap) {
      if (this.userPipelineMap.containsKey(user)) {
        LOG.info(""Request to start an already existing user: {}""
            + "" was received, so ignoring."", user);
        return userPipelineMap.get(user);
      }

      RequestInterceptorChainWrapper chainWrapper =
          new RequestInterceptorChainWrapper();
      try {
        // We should init the pipeline instance after it is created and then
        // add to the map, to ensure thread safe.
        LOG.info(""Initializing request processing pipeline for user: {}."", user);

        RESTRequestInterceptor interceptorChain =
            this.createRequestInterceptorChain();
        interceptorChain.init(user);
        RouterClientRMService routerClientRMService = router.getClientRMProxyService();
        interceptorChain.setRouterClientRMService(routerClientRMService);
        chainWrapper.init(interceptorChain);
      } catch (Exception e) {
        LOG.error(""Init RESTRequestInterceptor error for user: {}"", user, e);
        throw e;
      }

      this.userPipelineMap.put(user, chainWrapper);
      return chainWrapper;
    }
  }

  /**
   * Private structure for encapsulating RequestInterceptor and user instances.
   *
   */
  @Private
  public static class RequestInterceptorChainWrapper {
    private RESTRequestInterceptor rootInterceptor;

    /**
     * Initializes the wrapper with the specified parameters.
     *
     * @param interceptor the first interceptor in the pipeline
     */
    public synchronized void init(RESTRequestInterceptor interceptor) {
      this.rootInterceptor = interceptor;
    }

    /**
     * Gets the root request interceptor.
     *
     * @return the root request interceptor
     */
    public synchronized RESTRequestInterceptor getRootInterceptor() {
      return rootInterceptor;
    }

    /**
     * Shutdown the chain of interceptors when the object is destroyed.
     */
    @Override
    protected void finalize() {
      rootInterceptor.shutdown();
    }
  }

  @GET
  @Produces({ MediaType.APPLICATION_JSON + ""; "" + JettyUtils.UTF_8,
      MediaType.APPLICATION_XML + ""; "" + JettyUtils.UTF_8 })
  @Override
  public ClusterInfo get() {
    return getClusterInfo();
  }

  @GET
  @Path(RMWSConsts.INFO)
  @Produces({ MediaType.APPLICATION_JSON + ""; "" + JettyUtils.UTF_8,
      MediaType.APPLICATION_XML + ""; "" + JettyUtils.UTF_8 })
  @Override
  public ClusterInfo getClusterInfo() {
    init();
    RequestInterceptorChainWrapper pipeline = getInterceptorChain(null);
    return pipeline.getRootInterceptor().getClusterInfo();
  }

  @GET
  @Path(RMWSConsts.CLUSTER_USER_INFO)
  @Produces({ MediaType.APPLICATION_JSON + ""; "" + JettyUtils.UTF_8,
      MediaType.APPLICATION_XML + ""; "" + JettyUtils.UTF_8 })
  @Override
  public ClusterUserInfo getClusterUserInfo(@Context HttpServletRequest hsr) {
    init();
    RequestInterceptorChainWrapper pipeline = getInterceptorChain(hsr);
    return pipeline.getRootInterceptor().getClusterUserInfo(hsr);
  }

  @GET
  @Path(RMWSConsts.METRICS)
  @Produces({ MediaType.APPLICATION_JSON + ""; "" + JettyUtils.UTF_8,
      MediaType.APPLICATION_XML + ""; "" + JettyUtils.UTF_8 })
  @Override
  public ClusterMetricsInfo getClusterMetricsInfo() {
    init();
    RequestInterceptorChainWrapper pipeline = getInterceptorChain(null);
    return pipeline.getRootInterceptor().getClusterMetricsInfo();
  }

  @GET
  @Path(RMWSConsts.SCHEDULER)
  @Produces({ MediaType.APPLICATION_JSON + ""; "" + JettyUtils.UTF_8,
      MediaType.APPLICATION_XML + ""; "" + JettyUtils.UTF_8 })
  @Override
  public SchedulerTypeInfo getSchedulerInfo() {
    init();
    RequestInterceptorChainWrapper pipeline = getInterceptorChain(null);
    return pipeline.getRootInterceptor().getSchedulerInfo();
  }

  @POST
  @Path(RMWSConsts.SCHEDULER_LOGS)
  @Produces({ MediaType.APPLICATION_JSON + ""; "" + JettyUtils.UTF_8,
      MediaType.APPLICATION_XML + ""; "" + JettyUtils.UTF_8 })
  @Override
  public String dumpSchedulerLogs(@FormParam(RMWSConsts.TIME) String time,
      @Context HttpServletRequest hsr) throws IOException {
    init();
    RequestInterceptorChainWrapper pipeline = getInterceptorChain(hsr);
    return pipeline.getRootInterceptor().dumpSchedulerLogs(time, hsr);
  }

  @GET
  @Path(RMWSConsts.NODES)
  @Produces({ MediaType.APPLICATION_JSON + ""; "" + JettyUtils.UTF_8,
      MediaType.APPLICATION_XML + ""; "" + JettyUtils.UTF_8 })
  @Override
  public NodesInfo getNodes(@QueryParam(RMWSConsts.STATES) String states) {
    init();
    RequestInterceptorChainWrapper pipeline = getInterceptorChain(null);
    return pipeline.getRootInterceptor().getNodes(states);
  }

  @GET
  @Path(RMWSConsts.NODES_NODEID)
  @Produces({ MediaType.APPLICATION_JSON + ""; "" + JettyUtils.UTF_8,
      MediaType.APPLICATION_XML + ""; "" + JettyUtils.UTF_8 })
  @Override
  public NodeInfo getNode(@PathParam(RMWSConsts.NODEID) String nodeId) {
    init();
    RequestInterceptorChainWrapper pipeline = getInterceptorChain(null);
    return pipeline.getRootInterceptor().getNode(nodeId);
  }

  @POST
  @Path(RMWSConsts.NODE_RESOURCE)
  @Consumes({ MediaType.APPLICATION_JSON, MediaType.APPLICATION_XML })
  @Produces({ MediaType.APPLICATION_JSON + ""; "" + JettyUtils.UTF_8,
      MediaType.APPLICATION_XML + ""; "" + JettyUtils.UTF_8 })
  @Override
  public ResourceInfo updateNodeResource(
      @Context HttpServletRequest hsr,
      @PathParam(RMWSConsts.NODEID) String nodeId,
      ResourceOptionInfo resourceOption) throws AuthorizationException {
    init();
    RequestInterceptorChainWrapper pipeline = getInterceptorChain(null);
    return pipeline.getRootInterceptor().updateNodeResource(
        hsr, nodeId, resourceOption);
  }

  @GET
  @Path(RMWSConsts.APPS)
  @Produces({ MediaType.APPLICATION_JSON + ""; "" + JettyUtils.UTF_8,
      MediaType.APPLICATION_XML + ""; "" + JettyUtils.UTF_8 })
  @Override
  public AppsInfo getApps(@Context HttpServletRequest hsr,
      @QueryParam(RMWSConsts.STATE) String stateQuery,
      @QueryParam(RMWSConsts.STATES) Set<String> statesQuery,
      @QueryParam(RMWSConsts.FINAL_STATUS) String finalStatusQuery,
      @QueryParam(RMWSConsts.USER) String userQuery,
      @QueryParam(RMWSConsts.QUEUE) String queueQuery,
      @QueryParam(RMWSConsts.LIMIT) String count,
      @QueryParam(RMWSConsts.STARTED_TIME_BEGIN) String startedBegin,
      @QueryParam(RMWSConsts.STARTED_TIME_END) String startedEnd,
      @QueryParam(RMWSConsts.FINISHED_TIME_BEGIN) String finishBegin,
      @QueryParam(RMWSConsts.FINISHED_TIME_END) String finishEnd,
      @QueryParam(RMWSConsts.APPLICATION_TYPES) Set<String> applicationTypes,
      @QueryParam(RMWSConsts.APPLICATION_TAGS) Set<String> applicationTags,
      @QueryParam(RMWSConsts.NAME) String name,
      @QueryParam(RMWSConsts.DESELECTS) Set<String> unselectedFields) {
    init();
    RequestInterceptorChainWrapper pipeline = getInterceptorChain(hsr);
    return pipeline.getRootInterceptor().getApps(hsr, stateQuery, statesQuery,
        finalStatusQuery, userQuery, queueQuery, count, startedBegin,
        startedEnd, finishBegin, finishEnd, applicationTypes, applicationTags,
        name, unselectedFields);
  }

  @GET
  @Path(RMWSConsts.SCHEDULER_ACTIVITIES)
  @Produces({ MediaType.APPLICATION_JSON + ""; "" + JettyUtils.UTF_8,
      MediaType.APPLICATION_XML + ""; "" + JettyUtils.UTF_8 })
  @Override
  public ActivitiesInfo getActivities(@Context HttpServletRequest hsr,
      @QueryParam(RMWSConsts.NODEID) String nodeId,
      @QueryParam(RMWSConsts.GROUP_BY) String groupBy) {
    init();
    RequestInterceptorChainWrapper pipeline = getInterceptorChain(hsr);
    return pipeline.getRootInterceptor()
        .getActivities(hsr, nodeId, groupBy);
  }

  @GET
  @Path(RMWSConsts.SCHEDULER_BULK_ACTIVITIES)
  @Produces({ MediaType.APPLICATION_JSON + ""; "" + JettyUtils.UTF_8,
      MediaType.APPLICATION_XML + ""; "" + JettyUtils.UTF_8 })
  @Override
  public BulkActivitiesInfo getBulkActivities(
      @Context HttpServletRequest hsr,
      @QueryParam(RMWSConsts.GROUP_BY) String groupBy,
      @QueryParam(RMWSConsts.ACTIVITIES_COUNT)
      @DefaultValue(DEFAULT_ACTIVITIES_COUNT) int activitiesCount)
      throws InterruptedException {
    init();
    RequestInterceptorChainWrapper pipeline = getInterceptorChain(hsr);
    return pipeline.getRootInterceptor().getBulkActivities(hsr, groupBy,
        activitiesCount);
  }

  @GET
  @Path(RMWSConsts.SCHEDULER_APP_ACTIVITIES)
  @Produces({ MediaType.APPLICATION_JSON + ""; "" + JettyUtils.UTF_8,
      MediaType.APPLICATION_XML + ""; "" + JettyUtils.UTF_8 })
  @Override
  public AppActivitiesInfo getAppActivities(@Context HttpServletRequest hsr,
      @PathParam(RMWSConsts.APPID) String appId,
      @QueryParam(RMWSConsts.MAX_TIME) String time,
      @QueryParam(RMWSConsts.REQUEST_PRIORITIES) Set<String> requestPriorities,
      @QueryParam(RMWSConsts.ALLOCATION_REQUEST_IDS)
          Set<String> allocationRequestIds,
      @QueryParam(RMWSConsts.GROUP_BY) String groupBy,
      @QueryParam(RMWSConsts.LIMIT) String limit,
      @QueryParam(RMWSConsts.ACTIONS) Set<String> actions,
      @QueryParam(RMWSConsts.SUMMARIZE) @DefaultValue(DEFAULT_SUMMARIZE)
          boolean summarize) {
    init();
    RequestInterceptorChainWrapper pipeline = getInterceptorChain(hsr);
    return pipeline.getRootInterceptor().getAppActivities(hsr, appId, time,
        requestPriorities, allocationRequestIds, groupBy, limit, actions,
        summarize);
  }

  @GET
  @Path(RMWSConsts.APP_STATISTICS)
  @Produces({ MediaType.APPLICATION_JSON + ""; "" + JettyUtils.UTF_8,
      MediaType.APPLICATION_XML + ""; "" + JettyUtils.UTF_8 })
  @Override
  public ApplicationStatisticsInfo getAppStatistics(
      @Context HttpServletRequest hsr,
      @QueryParam(RMWSConsts.STATES) Set<String> stateQueries,
      @QueryParam(RMWSConsts.APPLICATION_TYPES) Set<String> typeQueries) {
    init();
    RequestInterceptorChainWrapper pipeline = getInterceptorChain(hsr);
    return pipeline.getRootInterceptor().getAppStatistics(hsr, stateQueries,
        typeQueries);
  }

  @GET
  @Path(RMWSConsts.APPS_APPID)
  @Produces({ MediaType.APPLICATION_JSON + ""; "" + JettyUtils.UTF_8,
      MediaType.APPLICATION_XML + ""; "" + JettyUtils.UTF_8 })
  @Override
  public AppInfo getApp(@Context HttpServletRequest hsr,
      @PathParam(RMWSConsts.APPID) String appId,
      @QueryParam(RMWSConsts.DESELECTS) Set<String> unselectedFields) {
    init();
    RequestInterceptorChainWrapper pipeline = getInterceptorChain(hsr);
    return pipeline.getRootInterceptor().getApp(hsr, appId, unselectedFields);
  }

  @GET
  @Path(RMWSConsts.APPS_APPID_STATE)
  @Produces({ MediaType.APPLICATION_JSON + ""; "" + JettyUtils.UTF_8,
      MediaType.APPLICATION_XML + ""; "" + JettyUtils.UTF_8 })
  @Override
  public AppState getAppState(@Context HttpServletRequest hsr,
      @PathParam(RMWSConsts.APPID) String appId) throws AuthorizationException {
    init();
    RequestInterceptorChainWrapper pipeline = getInterceptorChain(hsr);
    return pipeline.getRootInterceptor().getAppState(hsr, appId);
  }

  @PUT
  @Path(RMWSConsts.APPS_APPID_STATE)
  @Produces({ MediaType.APPLICATION_JSON + ""; "" + JettyUtils.UTF_8,
      MediaType.APPLICATION_XML + ""; "" + JettyUtils.UTF_8 })
  @Override
  public Response updateAppState(AppState targetState,
      @Context HttpServletRequest hsr,
      @PathParam(RMWSConsts.APPID) String appId) throws AuthorizationException,
      YarnException, InterruptedException, IOException {
    init();
    RequestInterceptorChainWrapper pipeline = getInterceptorChain(hsr);
    return pipeline.getRootInterceptor().updateAppState(targetState, hsr,
        appId);
  }

  @GET
  @Path(RMWSConsts.GET_NODE_TO_LABELS)
  @Produces({ MediaType.APPLICATION_JSON + ""; "" + JettyUtils.UTF_8,
      MediaType.APPLICATION_XML + ""; "" + JettyUtils.UTF_8 })
  @Override
  public NodeToLabelsInfo getNodeToLabels(@Context HttpServletRequest hsr)
      throws IOException {
    init();
    RequestInterceptorChainWrapper pipeline = getInterceptorChain(hsr);
    return pipeline.getRootInterceptor().getNodeToLabels(hsr);
  }

  @GET
  @Path(RMWSConsts.LABEL_MAPPINGS)
  @Produces({ MediaType.APPLICATION_JSON + ""; "" + JettyUtils.UTF_8,
      MediaType.APPLICATION_XML + ""; "" + JettyUtils.UTF_8 })
  @Override
  public LabelsToNodesInfo getLabelsToNodes(
      @QueryParam(RMWSConsts.LABELS) Set<String> labels) throws IOException {
    init();
    RequestInterceptorChainWrapper pipeline = getInterceptorChain(null);
    return pipeline.getRootInterceptor().getLabelsToNodes(labels);
  }

  @POST
  @Path(RMWSConsts.REPLACE_NODE_TO_LABELS)
  @Produces({ MediaType.APPLICATION_JSON + ""; "" + JettyUtils.UTF_8,
      MediaType.APPLICATION_XML + ""; "" + JettyUtils.UTF_8 })
  @Override
  public Response replaceLabelsOnNodes(
      final NodeToLabelsEntryList newNodeToLabels,
      @Context HttpServletRequest hsr) throws Exception {
    init();
    RequestInterceptorChainWrapper pipeline = getInterceptorChain(hsr);
    return pipeline.getRootInterceptor().replaceLabelsOnNodes(newNodeToLabels,
        hsr);
  }

  @POST
  @Path(RMWSConsts.NODES_NODEID_REPLACE_LABELS)
  @Produces({ MediaType.APPLICATION_JSON + ""; "" + JettyUtils.UTF_8,
      MediaType.APPLICATION_XML + ""; "" + JettyUtils.UTF_8 })
  @Override
  public Response replaceLabelsOnNode(
      @QueryParam(RMWSConsts.LABELS) Set<String> newNodeLabelsName,
      @Context HttpServletRequest hsr,
      @PathParam(RMWSConsts.NODEID) String nodeId) throws Exception {
    init();
    RequestInterceptorChainWrapper pipeline = getInterceptorChain(hsr);
    return pipeline.getRootInterceptor().replaceLabelsOnNode(newNodeLabelsName,
        hsr, nodeId);
  }

  @GET
  @Path(RMWSConsts.GET_NODE_LABELS)
  @Produces({ MediaType.APPLICATION_JSON + ""; "" + JettyUtils.UTF_8,
      MediaType.APPLICATION_XML + ""; "" + JettyUtils.UTF_8 })
  @Override
  public NodeLabelsInfo getClusterNodeLabels(@Context HttpServletRequest hsr)
      throws IOException {
    init();
    RequestInterceptorChainWrapper pipeline = getInterceptorChain(hsr);
    return pipeline.getRootInterceptor().getClusterNodeLabels(hsr);
  }

  @POST
  @Path(RMWSConsts.ADD_NODE_LABELS)
  @Produces({ MediaType.APPLICATION_JSON + ""; "" + JettyUtils.UTF_8,
      MediaType.APPLICATION_XML + ""; "" + JettyUtils.UTF_8 })
  @Override
  public Response addToClusterNodeLabels(NodeLabelsInfo newNodeLabels,
      @Context HttpServletRequest hsr) throws Exception {
    init();
    RequestInterceptorChainWrapper pipeline = getInterceptorChain(hsr);
    return pipeline.getRootInterceptor().addToClusterNodeLabels(newNodeLabels,
        hsr);
  }

  @POST
  @Path(RMWSConsts.REMOVE_NODE_LABELS)
  @Produces({ MediaType.APPLICATION_JSON + ""; "" + JettyUtils.UTF_8,
      MediaType.APPLICATION_XML + ""; "" + JettyUtils.UTF_8 })
  @Override
  public Response removeFromClusterNodeLabels(
      @QueryParam(RMWSConsts.LABELS) Set<String> oldNodeLabels,
      @Context HttpServletRequest hsr) throws Exception {
    init();
    RequestInterceptorChainWrapper pipeline = getInterceptorChain(hsr);
    return pipeline.getRootInterceptor()
        .removeFromClusterNodeLabels(oldNodeLabels, hsr);
  }

  @GET
  @Path(RMWSConsts.NODES_NODEID_GETLABELS)
  @Produces({ MediaType.APPLICATION_JSON + ""; "" + JettyUtils.UTF_8,
      MediaType.APPLICATION_XML + ""; "" + JettyUtils.UTF_8 })
  @Override
  public NodeLabelsInfo getLabelsOnNode(@Context HttpServletRequest hsr,
      @PathParam(RMWSConsts.NODEID) String nodeId) throws IOException {
    init();
    RequestInterceptorChainWrapper pipeline = getInterceptorChain(hsr);
    return pipeline.getRootInterceptor().getLabelsOnNode(hsr, nodeId);
  }

  @GET
  @Path(RMWSConsts.APPS_APPID_PRIORITY)
  @Produces({ MediaType.APPLICATION_JSON + ""; "" + JettyUtils.UTF_8,
      MediaType.APPLICATION_XML + ""; "" + JettyUtils.UTF_8 })
  @Override
  public AppPriority getAppPriority(@Context HttpServletRequest hsr,
      @PathParam(RMWSConsts.APPID) String appId) throws AuthorizationException {
    init();
    RequestInterceptorChainWrapper pipeline = getInterceptorChain(hsr);
    return pipeline.getRootInterceptor().getAppPriority(hsr, appId);
  }

  @PUT
  @Path(RMWSConsts.APPS_APPID_PRIORITY)
  @Produces({ MediaType.APPLICATION_JSON + ""; "" + JettyUtils.UTF_8,
      MediaType.APPLICATION_XML + ""; "" + JettyUtils.UTF_8 })
  @Override
  public Response updateApplicationPriority(AppPriority targetPriority,
      @Context HttpServletRequest hsr,
      @PathParam(RMWSConsts.APPID) String appId) throws AuthorizationException,
      YarnException, InterruptedException, IOException {
    init();
    RequestInterceptorChainWrapper pipeline = getInterceptorChain(hsr);
    return pipeline.getRootInterceptor()
        .updateApplicationPriority(targetPriority, hsr, appId);
  }

  @GET
  @Path(RMWSConsts.APPS_APPID_QUEUE)
  @Produces({ MediaType.APPLICATION_JSON + ""; "" + JettyUtils.UTF_8,
      MediaType.APPLICATION_XML + ""; "" + JettyUtils.UTF_8 })
  @Override
  public AppQueue getAppQueue(@Context HttpServletRequest hsr,
      @PathParam(RMWSConsts.APPID) String appId) throws AuthorizationException {
    init();
    RequestInterceptorChainWrapper pipeline = getInterceptorChain(hsr);
    return pipeline.getRootInterceptor().getAppQueue(hsr, appId);
  }

  @PUT
  @Path(RMWSConsts.APPS_APPID_QUEUE)
  @Produces({ MediaType.APPLICATION_JSON + ""; "" + JettyUtils.UTF_8,
      MediaType.APPLICATION_XML + ""; "" + JettyUtils.UTF_8 })
  @Override
  public Response updateAppQueue(AppQueue targetQueue,
      @Context HttpServletRequest hsr,
      @PathParam(RMWSConsts.APPID) String appId) throws AuthorizationException,
      YarnException, InterruptedException, IOException {
    init();
    RequestInterceptorChainWrapper pipeline = getInterceptorChain(hsr);
    return pipeline.getRootInterceptor().updateAppQueue(targetQueue, hsr,
        appId);
  }

  @POST
  @Path(RMWSConsts.APPS_NEW_APPLICATION)
  @Produces({ MediaType.APPLICATION_JSON + ""; "" + JettyUtils.UTF_8,
      MediaType.APPLICATION_XML + ""; "" + JettyUtils.UTF_8 })
  @Override
  public Response createNewApplication(@Context HttpServletRequest hsr)
      throws AuthorizationException, IOException, InterruptedException {
    init();
    RequestInterceptorChainWrapper pipeline = getInterceptorChain(hsr);
    return pipeline.getRootInterceptor().createNewApplication(hsr);
  }

  @POST
  @Path(RMWSConsts.APPS)
  @Produces({ MediaType.APPLICATION_JSON + ""; "" + JettyUtils.UTF_8,
      MediaType.APPLICATION_XML + ""; "" + JettyUtils.UTF_8 })
  @Override
  public Response submitApplication(ApplicationSubmissionContextInfo newApp,
      @Context HttpServletRequest hsr)
      throws AuthorizationException, IOException, InterruptedException {
    init();
    RequestInterceptorChainWrapper pipeline = getInterceptorChain(hsr);
    return pipeline.getRootInterceptor().submitApplication(newApp, hsr);
  }

  @POST
  @Path(RMWSConsts.DELEGATION_TOKEN)
  @Produces({ MediaType.APPLICATION_JSON + ""; "" + JettyUtils.UTF_8,
      MediaType.APPLICATION_XML + ""; "" + JettyUtils.UTF_8 })
  @Override
  public Response postDelegationToken(DelegationToken tokenData,
      @Context HttpServletRequest hsr) throws AuthorizationException,
      IOException, InterruptedException, Exception {
    init();
    RequestInterceptorChainWrapper pipeline = getInterceptorChain(hsr);
    return pipeline.getRootInterceptor().postDelegationToken(tokenData, hsr);
  }

  @POST
  @Path(RMWSConsts.DELEGATION_TOKEN_EXPIRATION)
  @Produces({ MediaType.APPLICATION_JSON + ""; "" + JettyUtils.UTF_8,
      MediaType.APPLICATION_XML + ""; "" + JettyUtils.UTF_8 })
  @Override
  public Response postDelegationTokenExpiration(@Context HttpServletRequest hsr)
      throws AuthorizationException, IOException, Exception {
    init();
    RequestInterceptorChainWrapper pipeline = getInterceptorChain(hsr);
    return pipeline.getRootInterceptor().postDelegationTokenExpiration(hsr);
  }

  @DELETE
  @Path(RMWSConsts.DELEGATION_TOKEN)
  @Produces({ MediaType.APPLICATION_JSON + ""; "" + JettyUtils.UTF_8,
      MediaType.APPLICATION_XML + ""; "" + JettyUtils.UTF_8 })
  @Override
  public Response cancelDelegationToken(@Context HttpServletRequest hsr)
      throws AuthorizationException, IOException, InterruptedException,
      Exception {
    init();
    RequestInterceptorChainWrapper pipeline = getInterceptorChain(hsr);
    return pipeline.getRootInterceptor().cancelDelegationToken(hsr);
  }

  @POST
  @Path(RMWSConsts.RESERVATION_NEW)
  @Produces({ MediaType.APPLICATION_JSON + ""; "" + JettyUtils.UTF_8,
      MediaType.APPLICATION_XML + ""; "" + JettyUtils.UTF_8 })
  @Override
  public Response createNewReservation(@Context HttpServletRequest hsr)
      throws AuthorizationException, IOException, InterruptedException {
    init();
    RequestInterceptorChainWrapper pipeline = getInterceptorChain(hsr);
    return pipeline.getRootInterceptor().createNewReservation(hsr);
  }

  @POST
  @Path(RMWSConsts.RESERVATION_SUBMIT)
  @Produces({ MediaType.APPLICATION_JSON + ""; "" + JettyUtils.UTF_8,
      MediaType.APPLICATION_XML + ""; "" + JettyUtils.UTF_8 })
  @Override
  public Response submitReservation(ReservationSubmissionRequestInfo resContext,
      @Context HttpServletRequest hsr)
      throws AuthorizationException, IOException, InterruptedException {
    init();
    RequestInterceptorChainWrapper pipeline = getInterceptorChain(hsr);
    return pipeline.getRootInterceptor().submitReservation(resContext, hsr);
  }

  @POST
  @Path(RMWSConsts.RESERVATION_UPDATE)
  @Produces({ MediaType.APPLICATION_JSON + ""; "" + JettyUtils.UTF_8,
      MediaType.APPLICATION_XML + ""; "" + JettyUtils.UTF_8 })
  @Override
  public Response updateReservation(ReservationUpdateRequestInfo resContext,
      @Context HttpServletRequest hsr)
      throws AuthorizationException, IOException, InterruptedException {
    init();
    RequestInterceptorChainWrapper pipeline = getInterceptorChain(hsr);
    return pipeline.getRootInterceptor().updateReservation(resContext, hsr);
  }

  @POST
  @Path(RMWSConsts.RESERVATION_DELETE)
  @Produces({ MediaType.APPLICATION_JSON + ""; "" + JettyUtils.UTF_8,
      MediaType.APPLICATION_XML + ""; "" + JettyUtils.UTF_8 })
  @Override
  public Response deleteReservation(ReservationDeleteRequestInfo resContext,
      @Context HttpServletRequest hsr)
      throws AuthorizationException, IOException, InterruptedException {
    init();
    RequestInterceptorChainWrapper pipeline = getInterceptorChain(hsr);
    return pipeline.getRootInterceptor().deleteReservation(resContext, hsr);
  }

  @GET
  @Path(RMWSConsts.RESERVATION_LIST)
  @Produces({ MediaType.APPLICATION_JSON + ""; "" + JettyUtils.UTF_8,
      MediaType.APPLICATION_XML + ""; "" + JettyUtils.UTF_8 })
  @Override
  public Response listReservation(
      @QueryParam(RMWSConsts.QUEUE) @DefaultValue(DEFAULT_QUEUE) String queue,
      @QueryParam(RMWSConsts.RESERVATION_ID)
      @DefaultValue(DEFAULT_RESERVATION_ID) String reservationId,
      @QueryParam(RMWSConsts.START_TIME) @DefaultValue(DEFAULT_START_TIME) long startTime,
      @QueryParam(RMWSConsts.END_TIME) @DefaultValue(DEFAULT_END_TIME) long endTime,
      @QueryParam(RMWSConsts.INCLUDE_RESOURCE)
      @DefaultValue(DEFAULT_INCLUDE_RESOURCE) boolean includeResourceAllocations,
      @Context HttpServletRequest hsr) throws Exception {
    init();
    RequestInterceptorChainWrapper pipeline = getInterceptorChain(hsr);
    return pipeline.getRootInterceptor().listReservation(queue, reservationId,
        startTime, endTime, includeResourceAllocations, hsr);
  }

  @GET
  @Path(RMWSConsts.APPS_TIMEOUTS_TYPE)
  @Produces({ MediaType.APPLICATION_JSON + ""; "" + JettyUtils.UTF_8,
      MediaType.APPLICATION_XML + ""; "" + JettyUtils.UTF_8 })
  @Override
  public AppTimeoutInfo getAppTimeout(@Context HttpServletRequest hsr,
      @PathParam(RMWSConsts.APPID) String appId,
      @PathParam(RMWSConsts.TYPE) String type) throws AuthorizationException {
    init();
    RequestInterceptorChainWrapper pipeline = getInterceptorChain(hsr);
    return pipeline.getRootInterceptor().getAppTimeout(hsr, appId, type);
  }

  @GET
  @Path(RMWSConsts.APPS_TIMEOUTS)
  @Produces({ MediaType.APPLICATION_JSON + ""; "" + JettyUtils.UTF_8,
      MediaType.APPLICATION_XML + ""; "" + JettyUtils.UTF_8 })
  @Override
  public AppTimeoutsInfo getAppTimeouts(@Context HttpServletRequest hsr,
      @PathParam(RMWSConsts.APPID) String appId) throws AuthorizationException {
    init();
    RequestInterceptorChainWrapper pipeline = getInterceptorChain(hsr);
    return pipeline.getRootInterceptor().getAppTimeouts(hsr, appId);
  }

  @PUT
  @Path(RMWSConsts.APPS_TIMEOUT)
  @Produces({ MediaType.APPLICATION_JSON + ""; "" + JettyUtils.UTF_8,
      MediaType.APPLICATION_XML + ""; "" + JettyUtils.UTF_8 })
  @Override
  public Response updateApplicationTimeout(AppTimeoutInfo appTimeout,
      @Context HttpServletRequest hsr,
      @PathParam(RMWSConsts.APPID) String appId) throws AuthorizationException,
      YarnException, InterruptedException, IOException {
    init();
    RequestInterceptorChainWrapper pipeline = getInterceptorChain(hsr);
    return pipeline.getRootInterceptor().updateApplicationTimeout(appTimeout,
        hsr, appId);
  }

  @GET
  @Path(RMWSConsts.APPS_APPID_APPATTEMPTS)
  @Produces({ MediaType.APPLICATION_JSON + ""; "" + JettyUtils.UTF_8,
      MediaType.APPLICATION_XML + ""; "" + JettyUtils.UTF_8 })
  @Override
  public AppAttemptsInfo getAppAttempts(@Context HttpServletRequest hsr,
      @PathParam(RMWSConsts.APPID) String appId) {
    init();
    RequestInterceptorChainWrapper pipeline = getInterceptorChain(hsr);
    return pipeline.getRootInterceptor().getAppAttempts(hsr, appId);
  }

  @GET
  @Path(RMWSConsts.CHECK_USER_ACCESS_TO_QUEUE)
  @Produces({ MediaType.APPLICATION_JSON + ""; "" + JettyUtils.UTF_8,
                MediaType.APPLICATION_XML + ""; "" + JettyUtils.UTF_8 })
  @Override
  public RMQueueAclInfo checkUserAccessToQueue(
      @PathParam(RMWSConsts.QUEUE) String queue,
      @QueryParam(RMWSConsts.USER) String username,
      @QueryParam(RMWSConsts.QUEUE_ACL_TYPE)
      @DefaultValue(""SUBMIT_APPLICATIONS"") String queueAclType,
      @Context HttpServletRequest hsr) throws AuthorizationException {
    init();
    RequestInterceptorChainWrapper pipeline = getInterceptorChain(hsr);
    return pipeline.getRootInterceptor().checkUserAccessToQueue(queue,
        username, queueAclType, hsr);
  }

  @GET
  @Path(RMWSConsts.APPS_APPID_APPATTEMPTS_APPATTEMPTID)
  @Produces({ MediaType.APPLICATION_JSON + ""; "" + JettyUtils.UTF_8,
      MediaType.APPLICATION_XML + ""; "" + JettyUtils.UTF_8 })
  public org.apache.hadoop.yarn.server.webapp.dao.AppAttemptInfo getAppAttempt(
      @Context HttpServletRequest req, @Context HttpServletResponse res,
      @PathParam(RMWSConsts.APPID) String appId,
      @PathParam(RMWSConsts.APPATTEMPTID) String appAttemptId) {
    init();
    RequestInterceptorChainWrapper pipeline = getInterceptorChain(req);
    return pipeline.getRootInterceptor().getAppAttempt(req, res, appId,
        appAttemptId);
  }

  @GET
  @Path(RMWSConsts.APPS_APPID_APPATTEMPTS_APPATTEMPTID_CONTAINERS)
  @Produces({ MediaType.APPLICATION_JSON + ""; "" + JettyUtils.UTF_8,
      MediaType.APPLICATION_XML + ""; "" + JettyUtils.UTF_8 })
  public ContainersInfo getContainers(@Context HttpServletRequest req,
      @Context HttpServletResponse res,
      @PathParam(RMWSConsts.APPID) String appId,
      @PathParam(RMWSConsts.APPATTEMPTID) String appAttemptId) {
    init();
    RequestInterceptorChainWrapper pipeline = getInterceptorChain(req);
    return pipeline.getRootInterceptor().getContainers(req, res, appId,
        appAttemptId);
  }

  @GET
  @Path(RMWSConsts.GET_CONTAINER)
  @Produces({ MediaType.APPLICATION_JSON + ""; "" + JettyUtils.UTF_8,
      MediaType.APPLICATION_XML + ""; "" + JettyUtils.UTF_8 })
  public ContainerInfo getContainer(@Context HttpServletRequest req,
      @Context HttpServletResponse res,
      @PathParam(RMWSConsts.APPID) String appId,
      @PathParam(RMWSConsts.APPATTEMPTID) String appAttemptId,
      @PathParam(RMWSConsts.CONTAINERID) String containerId) {
    init();
    RequestInterceptorChainWrapper pipeline = getInterceptorChain(req);
    return pipeline.getRootInterceptor().getContainer(req, res, appId,
        appAttemptId, containerId);
  }

  @PUT
  @Path(RMWSConsts.SCHEDULER_CONF)
  @Produces({ MediaType.APPLICATION_JSON + ""; "" + JettyUtils.UTF_8,
      MediaType.APPLICATION_XML + ""; "" + JettyUtils.UTF_8 })
  @Consumes({ MediaType.APPLICATION_JSON, MediaType.APPLICATION_XML })
  @Override
  public Response updateSchedulerConfiguration(SchedConfUpdateInfo mutationInfo,
      HttpServletRequest hsr)
      throws AuthorizationException, InterruptedException {
    init();
    RequestInterceptorChainWrapper pipeline = getInterceptorChain(hsr);
    return pipeline.getRootInterceptor()
        .updateSchedulerConfiguration(mutationInfo, hsr);
  }

  @GET
  @Path(RMWSConsts.SCHEDULER_CONF)
  @Produces({ MediaType.APPLICATION_JSON + ""; "" + JettyUtils.UTF_8,
      MediaType.APPLICATION_XML + ""; "" + JettyUtils.UTF_8 })
  @Override
  public Response getSchedulerConfiguration(HttpServletRequest hsr)
      throws AuthorizationException {
    init();
    RequestInterceptorChainWrapper pipeline = getInterceptorChain(hsr);
    return pipeline.getRootInterceptor().getSchedulerConfiguration(hsr);
  }

  @VisibleForTesting
  protected void setResponse(HttpServletResponse response) {
    this.response = response;
  }

  @POST
  @Path(RMWSConsts.SIGNAL_TO_CONTAINER)
  @Produces({ MediaType.APPLICATION_JSON + ""; "" + JettyUtils.UTF_8,
      MediaType.APPLICATION_XML + ""; "" + JettyUtils.UTF_8 })
  public Response signalToContainer(
      @PathParam(RMWSConsts.CONTAINERID) String containerId,
      @PathParam(RMWSConsts.COMMAND) String command,
      @Context HttpServletRequest req)
      throws AuthorizationException {
    init();
    RequestInterceptorChainWrapper pipeline = getInterceptorChain(req);
    return pipeline.getRootInterceptor()
        .signalToContainer(containerId, command, req);
  }

  @GET
  @Path(RMWSConsts.GET_RM_NODE_LABELS)
  @Produces({ MediaType.APPLICATION_JSON + ""; "" + JettyUtils.UTF_8,
      MediaType.APPLICATION_XML + ""; "" + JettyUtils.UTF_8 })
  public NodeLabelsInfo getRMNodeLabels(@Context HttpServletRequest hsr)
      throws IOException {
    init();
    RequestInterceptorChainWrapper pipeline = getInterceptorChain(hsr);
    return pipeline.getRootInterceptor().getRMNodeLabels(hsr);
  }

  public Router getRouter() {
    return router;
  }
}
","['Assertion Roulette', 'Conditional Test Logic']",['Assertion Roulette'],0,1,1,15
41750_16.0_pippo_testwithpathwithsinglevalue,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/41750_test.java,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/41750_actual.java,"/*
 * Copyright (C) 2016-present the original author or authors.
 *
 * Licensed under the Apache License, Version 2.0 (the ""License"");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package ro.pippo.controller;

import org.junit.After;
import org.junit.Before;
import org.junit.Rule;
import org.junit.Test;
import org.junit.rules.ExpectedException;
import ro.pippo.core.ContentTypeEngines;
import ro.pippo.core.route.Route;

import java.util.List;
import java.util.stream.Collectors;
import java.util.stream.Stream;

import static org.hamcrest.MatcherAssert.assertThat;
import static org.hamcrest.collection.IsIterableContainingInAnyOrder.containsInAnyOrder;
import static org.junit.Assert.assertEquals;

/**
 * @author Dwouglas Mhagnum
 */
public class ControllerRegistryTest {

    private ControllerRegistry controllerRegistry;

    @Rule
    public ExpectedException thrown = ExpectedException.none();

    @Before
    public void before() {
        controllerRegistry = new ControllerRegistry();
    }

    @After
    public void after() {
        controllerRegistry = null;
    }

    @Test
    public void testWithoutPath() throws Exception {
        controllerRegistry.register(WithoutPathController.class);
        assertThat(getUriPatterns(controllerRegistry), containsInAnyOrder(WithoutPathController.expectedUriPatterns()));
    }

    @Test
    public void testWithPathWithoutValue() throws Exception {
        controllerRegistry.register(WithPathWithoutValueController.class);
        assertThat(getUriPatterns(controllerRegistry),
                containsInAnyOrder(WithPathWithoutValueController.expectedUriPatterns()));
    }

    @Test
    public void testWithPathWithSingleValue() throws Exception {
        controllerRegistry.register(WithPathWithSingleValueController.class);
        assertThat(getUriPatterns(controllerRegistry),
                containsInAnyOrder(WithPathWithSingleValueController.expectedUriPatterns()));
    }

    @Test
    public void testWithPathWithMultiValue() throws Exception {
        controllerRegistry.register(WithPathWithMultiValueController.class);
        assertThat(getUriPatterns(controllerRegistry),
                containsInAnyOrder(WithPathWithMultiValueController.expectedUriPatterns()));
    }

    @Test
    public void testRegisterPackage() throws Exception {
        controllerRegistry.register(WithoutPathController.class.getPackage());
        int expectedTotalRoutes = WithoutPathController.expectedUriPatterns().length
            + WithPathWithoutValueController.expectedUriPatterns().length
            + WithPathWithSingleValueController.expectedUriPatterns().length
            + WithPathWithMultiValueController.expectedUriPatterns().length
            + WithPathWithMultiValueAndInheritanceController.expectedUriPatterns().length
            + WithPathButWithEmptyMethodPathController.expectedUriPatterns().length;
        assertEquals(expectedTotalRoutes, controllerRegistry.getRoutes().size());
    }

    @Test
    public void testRegisterControllerClass() throws Exception {
        controllerRegistry.register(WithoutPathController.class, WithPathWithoutValueController.class,
                WithPathWithSingleValueController.class, WithPathWithMultiValueController.class);
        int expectedTotalRoutes = WithoutPathController.expectedUriPatterns().length
                + WithPathWithoutValueController.expectedUriPatterns().length
                + WithPathWithSingleValueController.expectedUriPatterns().length
                + WithPathWithMultiValueController.expectedUriPatterns().length;
        assertEquals(expectedTotalRoutes, controllerRegistry.getRoutes().size());
    }

    @Test
    public void testRegisterControllerInstance() throws Exception {
        controllerRegistry.register(new WithoutPathController(), new WithPathWithoutValueController(),
                new WithPathWithSingleValueController(), new WithPathWithMultiValueController());
        int expectedTotalRoutes = WithoutPathController.expectedUriPatterns().length
                + WithPathWithoutValueController.expectedUriPatterns().length
                + WithPathWithSingleValueController.expectedUriPatterns().length
                + WithPathWithMultiValueController.expectedUriPatterns().length;
        assertEquals(expectedTotalRoutes, controllerRegistry.getRoutes().size());
    }

    @Test
    public void testRegisterControllerWithoutRoute() throws Exception {
        controllerRegistry.register(new WithoutRouteController());
        assertEquals(0, controllerRegistry.getRoutes().size());
    }

    @Test
    public void testRegisterPackageThatDoNotExist() throws Exception {
        controllerRegistry.register(""ro.pippo.controller.nonexistent.package1"",
                ""ro.pippo.controller.nonexistent.package2"");
        assertEquals(0, controllerRegistry.getRoutes().size());
    }

    @Test
    public void testRegisterComplexControllerWithInheritance() throws Exception {
        controllerRegistry.register(WithPathWithMultiValueAndInheritanceController.class);
        assertThat(getUriPatterns(controllerRegistry),
                containsInAnyOrder(WithPathWithMultiValueAndInheritanceController.expectedUriPatterns()));
    }

    @Test
    public void testRegisterControllerWithMethodPath() throws Exception {
        controllerRegistry.register(new WithPathButWithEmptyMethodPathController());
        assertThat(getUriPatterns(controllerRegistry),
            containsInAnyOrder(WithPathButWithEmptyMethodPathController.expectedUriPatterns()));
    }

    public static class WithoutPathController extends Controller {

        @GET
        public void example0() {
        }

        @GET(""/example1"")
        public void example1() {
        }

        // without http method
        @Named(""withoutHttpMethod"") // to coverage branch
        public void withoutHttpMethod() {
        }

        public static String[] expectedUriPatterns() {
            return Stream.of(""/"", ""/example1"").toArray(String[]::new);
        }

    }

    @Path
    public static class WithPathWithoutValueController extends Controller {

        @GET
        public void example0() {
        }

        @GET(""/example1"")
        public void example1() {
        }

        @GET(""/example2/"")
        public void example2() {
        }

        @GET(""example3"")
        public void example3() {
        }

        @GET(""example4/"")
        public void example4() {
        }

        @GET({ ""example51/"", ""example52"", ""/example53"", ""/example54/"" })
        public void example5() {
        }

        // without http method
        public void withoutHttpMethod() {
        }

        public static String[] expectedUriPatterns() {
            return Stream.of(""/"", ""/example1"", ""/example2/"", ""/example3"", ""/example4/"", ""/example51/"", ""/example52"",
                    ""/example53"", ""/example54/"").toArray(String[]::new);
        }

    }

    @Path(""/"")
    public static class WithPathWithSingleValueController extends Controller {

        @GET
        public void example0() {
        }

        @GET(""/example1"")
        public void example1() {
        }

        @GET(""/example2/"")
        public void example2() {
        }

        @GET(""example3"")
        public void example3() {
        }

        @GET(""example4/"")
        public void example4() {
        }

        @GET({ ""example51/"", ""example52"", ""/example53"", ""/example54/"" })
        public void example5() {
        }

        // without http method
        public void withoutHttpMethod() {
        }

        public static String[] expectedUriPatterns() {
            return Stream.of(""/"", ""/example1"", ""/example2/"", ""/example3"", ""/example4/"", ""/example51/"", ""/example52"",
                    ""/example53"", ""/example54/"").toArray(String[]::new);
        }

    }

    @Path({ ""/"", ""/root1"", ""root2"", ""root3/"", ""/root4/"", ""root5/root6"" })
    public static class WithPathWithMultiValueController extends Controller {

        @GET
        public void example0() {
        }

        @GET(""/example1"")
        public void example1() {
        }

        @GET(""/example2/"")
        public void example2() {
        }

        @GET(""example3"")
        public void example3() {
        }

        @GET(""example4/"")
        public void example4() {
        }

        @GET({ ""example51/"", ""example52"", ""/example53"", ""/example54/"" })
        public void example5() {
        }

        // without http method
        public void withoutHttpMethod() {
        }

        public static String[] expectedUriPatterns() {
            // @formatter:off
            return Stream.of(
                    ""/"", ""/example1"", ""/example2/"", ""/example3"", ""/example4/"", ""/example51/"", ""/example52"", ""/example53"", ""/example54/"",
                    ""/root1"", ""/root1/example1"", ""/root1/example2/"", ""/root1/example3"", ""/root1/example4/"", ""/root1/example51/"", ""/root1/example52"", ""/root1/example53"", ""/root1/example54/"",
                    ""/root2"", ""/root2/example1"", ""/root2/example2/"", ""/root2/example3"", ""/root2/example4/"", ""/root2/example51/"", ""/root2/example52"", ""/root2/example53"", ""/root2/example54/"",
                    ""/root3/"", ""/root3/example1"", ""/root3/example2/"", ""/root3/example3"", ""/root3/example4/"", ""/root3/example51/"", ""/root3/example52"", ""/root3/example53"", ""/root3/example54/"",
                    ""/root4/"", ""/root4/example1"", ""/root4/example2/"", ""/root4/example3"", ""/root4/example4/"", ""/root4/example51/"", ""/root4/example52"", ""/root4/example53"", ""/root4/example54/"",
                    ""/root5/root6"", ""/root5/root6/example1"", ""/root5/root6/example2/"", ""/root5/root6/example3"", ""/root5/root6/example4/"", ""/root5/root6/example51/"", ""/root5/root6/example52"", ""/root5/root6/example53"", ""/root5/root6/example54/""
                ).toArray(String[]::new);
            // @formatter:on
        }

    }

    @Path({""/example1"", ""/"", ""example2"", ""/example3/""})
    public static class WithPathButWithEmptyMethodPathController extends Controller {
        @GET("""")
        public void empty() {
        }

        public static String[] expectedUriPatterns() {
            return new String[]{""/example1"", ""/"", ""/example2"", ""/example3/""};
        }
    }

    @Path({ ""/"", ""/root7"" })
    public static class WithPathWithMultiValueAndInheritanceController extends ControllerAbstract {

        @GET
        public void example0() {
        }

        @GET({ ""example11/"", ""example12"", ""/example13"", ""/example14/"" })
        public void example1() {
        }

        // without http method
        public void withoutHttpMethod() {
        }

        public static String[] expectedUriPatterns() {
            // @formatter:off
            return Stream.of(
                    ""/ControllerAbstract/"", ""/ControllerAbstract/example11/"", ""/ControllerAbstract/example12"", ""/ControllerAbstract/example13"", ""/ControllerAbstract/example14/"",
                    ""/ControllerAbstract/root7"", ""/ControllerAbstract/root7/example11/"", ""/ControllerAbstract/root7/example12"", ""/ControllerAbstract/root7/example13"", ""/ControllerAbstract/root7/example14/""
                ).toArray(String[]::new);
            // @formatter:on
        }

    }

    @Path(""/ControllerAbstract"")
    public static abstract class ControllerAbstract extends Controller {

    }

    @Path(""/WithoutRouteController/"")
    public static class WithoutRouteController extends Controller {

    }

    // for the test that scans the package
    public static class ArbitraryClass {

        @GET(""/example1"")
        public void example1() {
        }

    }

    private List<String> getUriPatterns(ControllerRegistry controllerRegistry) {
        return controllerRegistry.getRoutes().stream().map(Route::getUriPattern).collect(Collectors.toList());
    }

}
","/*
 * Copyright (C) 2016-present the original author or authors.
 *
 * Licensed under the Apache License, Version 2.0 (the ""License"");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package ro.pippo.controller;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import ro.pippo.controller.util.ClassUtils;
import ro.pippo.core.route.Route;
import ro.pippo.core.util.LangUtils;
import ro.pippo.core.util.StringUtils;

import java.lang.annotation.Annotation;
import java.lang.reflect.Method;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Collection;
import java.util.Collections;
import java.util.HashSet;
import java.util.LinkedHashMap;
import java.util.LinkedHashSet;
import java.util.List;
import java.util.Map;
import java.util.Set;

/**
 * Register annotated controller routes.
 * {@link DefaultControllerRouteFactory} is used if a custom {@link ControllerRouteFactory} is not supplied
 * via {@link ControllerRegistry::setControllerRouteFactory}.
 *
 * @author Decebal Suiu
 * @author James Moger
 */
public class ControllerRegistry {

    private static final Logger log = LoggerFactory.getLogger(ControllerRegistry.class);

    private final Set<Class<? extends Annotation>> httpMethodAnnotationClasses = new HashSet<>(Arrays.asList(
        DELETE.class, GET.class, HEAD.class, OPTIONS.class, PATCH.class, POST.class, PUT.class));

    private ControllerRouteFactory controllerRouteFactory;
    private List<Route> routes = new ArrayList<>();

    /**
     * Register all controller methods in the specified packages.
     *
     * @param packages
     */
    public void register(Package... packages) {
        String[] packageNames = Arrays.stream(packages)
            .map(Package::getName)
            .toArray(String[]::new);

        register(packageNames);
    }

    /**
     * Register all controller methods in the specified package names.
     *
     * @param packageNames
     */
    public void register(String... packageNames) {
        Collection<Class<? extends Controller>> classes = getControllerClasses(packageNames);
        if (classes.isEmpty()) {
            log.warn(""No annotated controllers found in package(s) '{}'"", Arrays.toString(packageNames));
            return;
        }

        log.debug(""Found {} controller classes in {} package(s)"", classes.size(), packageNames.length);

        for (Class<? extends Controller> controllerClass : classes) {
            register(controllerClass);
        }
    }

    /**
     * Register all controller methods in the specified controller classes.
     *
     * @param controllerClasses
     */
    public void register(Class<? extends Controller>... controllerClasses) {
        for (Class<? extends Controller> controllerClass : controllerClasses) {
            register(controllerClass);
        }
    }

    /**
     * Register all controller methods in the specified controllers.
     *
     * @param controllers
     */
    public void register(Controller... controllers) {
        for (Controller controller : controllers) {
            register(controller);
        }
    }

    /**
     * Return the collected routes.
     *
     * @return the routes
     */
    public List<Route> getRoutes() {
        return routes;
    }

    private void register(Class<? extends Controller> controllerClass) {
        Map<Method, Class<? extends Annotation>> controllerMethods = getControllerMethods(controllerClass);
        if (controllerMethods.isEmpty()) {
            // if we are using this registry we expect to discover controller routes!
            log.warn(""No annotated controller methods found in class '{}'"", controllerClass);
            return;
        }

        log.debug(""Found {} annotated controller method(s)"", controllerMethods.size());

        registerControllerMethods(controllerMethods, null);

        log.debug(""Added {} annotated routes from '{}'"", routes.size(), controllerClass);
    }

    public void register(Controller controller) {
        Class<? extends Controller> controllerClass = controller.getClass();
        Map<Method, Class<? extends Annotation>> controllerMethods = getControllerMethods(controllerClass);
        if (controllerMethods.isEmpty()) {
            // if we are using this registry we expect to discover controller routes!
            log.warn(""No annotated controller methods found in class '{}'"", controllerClass);
            return;
        }

        registerControllerMethods(controllerMethods, controller);

        log.debug(""Found {} annotated controller method(s)"", controllerMethods.size());
    }

    public ControllerRouteFactory getControllerRouteFactory() {
        if (controllerRouteFactory == null) {
            controllerRouteFactory = new DefaultControllerRouteFactory();
        }

        return controllerRouteFactory;
    }

    public ControllerRegistry setControllerRouteFactory(ControllerRouteFactory controllerRouteFactory) {
        this.controllerRouteFactory = controllerRouteFactory;

        return this;
    }

    /**
     * Register the controller methods as routes.
     *
     * @param controllerMethods
     * @param controller
     */
    private void registerControllerMethods(Map<Method, Class<? extends Annotation>> controllerMethods, Controller controller) {
        List<Route> controllerRoutes = createControllerRoutes(controllerMethods);
        for (Route controllerRoute : controllerRoutes) {
            if (controller != null) {
                ((ControllerHandler) controllerRoute.getRouteHandler()).setController(controller);
                controllerRoute.bind(""__controller"", controller);
            }
        }

        this.routes.addAll(controllerRoutes);
    }

    /**
     * Create controller routes from controller methods.
     *
     * @param controllerMethods
     * @return
     */
    @SuppressWarnings(""unchecked"")
    private List<Route> createControllerRoutes(Map<Method, Class<? extends Annotation>> controllerMethods) {
        List<Route> routes = new ArrayList<>();

        Class<? extends Controller> controllerClass = (Class<? extends Controller>) controllerMethods.keySet().iterator().next().getDeclaringClass();
        Set<String> controllerPaths = getControllerPaths(controllerClass);
        Collection<Method> methods = sortControllerMethods(controllerMethods.keySet());
        for (Method method : methods) {
            Class<? extends Annotation> httpMethodAnnotationClass = controllerMethods.get(method);
            Annotation httpMethodAnnotation = method.getAnnotation(httpMethodAnnotationClass);

            String httpMethod = httpMethodAnnotation.annotationType().getAnnotation(HttpMethod.class).value();
            String[] methodPaths = ClassUtils.executeDeclaredMethod(httpMethodAnnotation, ""value"");

            if (controllerPaths.isEmpty()) {
                // add an empty string to allow controllerPaths iteration
                controllerPaths.add("""");
            }
            if (methodPaths.length == 0) {
                // add an empty string to allow method iteration
                methodPaths = new String[]{""""};
            }

            for (String controllerPath : controllerPaths) {
                // controllerMethod specifies one or more paths, concatenate with controller paths
                for (String methodPath : methodPaths) {
                    String fullPath = null;
                    boolean isPathEmpty = controllerPath.isEmpty();
                    boolean isMethodPathEmpty = methodPath.isEmpty();
                    if (isPathEmpty && isMethodPathEmpty) {
                        // not sure if this is correct behavior
                        // maintaining the same behavior as of now
                        fullPath = ""/"";
                    } else if (isPathEmpty) {
                        fullPath = StringUtils.addStart(methodPath, ""/"");
                    } else if (isMethodPathEmpty) {
                        fullPath = StringUtils.addStart(controllerPath, ""/"");
                    } else {
                        fullPath = String.join(""/"",
                            StringUtils.removeEnd(StringUtils.addStart(controllerPath, ""/""), ""/""),
                            StringUtils.removeStart(methodPath, ""/"")
                        );
                    }

                    // create controller method route
                    Route route = getControllerRouteFactory().createRoute(httpMethod, fullPath, method);

                    // add the route to the list of routes
                    routes.add(route);
                }
            }
        }

        return routes;
    }

    /**
     * Discover Controller classes.
     *
     * @param packageNames
     * @return controller classes
     */
    private Collection<Class<? extends Controller>> getControllerClasses(String... packageNames) {
        log.debug(""Discovering annotated controller in package(s) '{}'"", Arrays.toString(packageNames));
        return ClassUtils.getSubTypesOf(Controller.class, packageNames);
    }

    /**
     * Discover Route methods.
     *
     * @param controllerClass
     * @return discovered methods
     */
    private Map<Method, Class<? extends Annotation>> getControllerMethods(Class<? extends Controller> controllerClass) {
        // collect the annotated methods
        Map<Method, Class<? extends Annotation>> controllerMethods = new LinkedHashMap<>();

        // discover all annotated controllers methods
        for (Method method : ClassUtils.getDeclaredMethods(controllerClass)) {
            for (Annotation annotation : method.getAnnotations()) {
                Class<? extends Annotation> annotationClass = annotation.annotationType();
                if (httpMethodAnnotationClasses.contains(annotationClass)) {
                    controllerMethods.put(method, annotationClass);
                    break;
                }
            }
        }

        return controllerMethods;
    }

    /**
     * Recursively builds the paths for the controller class.
     *
     * @param controllerClass
     * @return the paths for the controller
     */
    private Set<String> getControllerPaths(Class<?> controllerClass) {
        Set<String> parentPaths = Collections.emptySet();
        if (controllerClass.getSuperclass() != null) {
            parentPaths = getControllerPaths(controllerClass.getSuperclass());
        }

        Set<String> paths = new LinkedHashSet<>();
        Path controllerPath = controllerClass.getAnnotation(Path.class);

        if (controllerPath != null && controllerPath.value().length > 0) {
            if (parentPaths.isEmpty()) {
                // add all controller paths
                paths.addAll(Arrays.asList(controllerPath.value()));
            } else {
                // create controller paths based on the parent paths
                for (String parentPath : parentPaths) {
                    for (String path : controllerPath.value()) {
                        paths.add(StringUtils.removeEnd(parentPath, ""/"") + ""/"" + StringUtils.removeStart(path, ""/""));
                    }
                }
            }
        } else {
            // add all parent paths
            paths.addAll(parentPaths);
        }

        return paths;
    }

    /**
     * Sort the controller's methods by their preferred order, if specified.
     *
     * @param controllerMethods
     * @return a sorted list of methods
     */
    private Collection<Method> sortControllerMethods(Set<Method> controllerMethods) {
        List<Method> list = new ArrayList<>(controllerMethods);
        list.sort((m1, m2) -> {
            int o1 = Integer.MAX_VALUE;
            Order order1 = ClassUtils.getAnnotation(m1, Order.class);
            if (order1 != null) {
                o1 = order1.value();
            }

            int o2 = Integer.MAX_VALUE;
            Order order2 = ClassUtils.getAnnotation(m2, Order.class);
            if (order2 != null) {
                o2 = order2.value();
            }

            if (o1 == o2) {
                // same or unsorted, compare controller+controllerMethod
                String s1 = LangUtils.toString(m1);
                String s2 = LangUtils.toString(m2);

                return s1.compareTo(s2);
            }

            return (o1 < o2) ? -1 : 1;
        });

        return list;
    }

}
",[],"['Assertion Roulette', 'Eager Test', 'Magic Number Test']",3,0,0,15
36830_45.0_hbase_testclassmethodsarebuilderstyle,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/36830_test.java,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/36830_actual.java,"/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * ""License""); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hadoop.hbase.client;

import static org.junit.Assert.assertEquals;
import static org.junit.Assert.assertNull;
import static org.junit.Assert.assertThrows;
import static org.junit.Assert.assertTrue;

import java.util.Map;
import org.apache.hadoop.hbase.HBaseClassTestRule;
import org.apache.hadoop.hbase.HConstants;
import org.apache.hadoop.hbase.KeepDeletedCells;
import org.apache.hadoop.hbase.exceptions.DeserializationException;
import org.apache.hadoop.hbase.exceptions.HBaseException;
import org.apache.hadoop.hbase.io.compress.Compression;
import org.apache.hadoop.hbase.io.compress.Compression.Algorithm;
import org.apache.hadoop.hbase.io.encoding.DataBlockEncoding;
import org.apache.hadoop.hbase.io.encoding.IndexBlockEncoding;
import org.apache.hadoop.hbase.regionserver.BloomType;
import org.apache.hadoop.hbase.testclassification.MiscTests;
import org.apache.hadoop.hbase.testclassification.SmallTests;
import org.apache.hadoop.hbase.util.BuilderStyleTest;
import org.apache.hadoop.hbase.util.Bytes;
import org.apache.hadoop.hbase.util.PrettyPrinter;
import org.junit.Assert;
import org.junit.ClassRule;
import org.junit.Test;
import org.junit.experimental.categories.Category;

@Category({ MiscTests.class, SmallTests.class })
public class TestColumnFamilyDescriptorBuilder {
  @ClassRule
  public static final HBaseClassTestRule CLASS_RULE =
    HBaseClassTestRule.forClass(TestColumnFamilyDescriptorBuilder.class);

  @Test
  public void testBuilder() throws DeserializationException {
    ColumnFamilyDescriptorBuilder builder =
      ColumnFamilyDescriptorBuilder.newBuilder(HConstants.CATALOG_FAMILY).setInMemory(true)
        .setScope(HConstants.REPLICATION_SCOPE_LOCAL).setBloomFilterType(BloomType.NONE);
    final int v = 123;
    builder.setBlocksize(v);
    builder.setTimeToLive(v);
    builder.setBlockCacheEnabled(!ColumnFamilyDescriptorBuilder.DEFAULT_BLOCKCACHE);
    builder.setValue(Bytes.toBytes(""a""), Bytes.toBytes(""b""));
    builder.setMaxVersions(v);
    assertEquals(v, builder.build().getMaxVersions());
    builder.setMinVersions(v);
    assertEquals(v, builder.build().getMinVersions());
    builder.setKeepDeletedCells(KeepDeletedCells.TRUE);
    builder.setInMemory(!ColumnFamilyDescriptorBuilder.DEFAULT_IN_MEMORY);
    boolean inmemory = builder.build().isInMemory();
    builder.setScope(v);
    builder.setDataBlockEncoding(DataBlockEncoding.FAST_DIFF);
    builder.setBloomFilterType(BloomType.ROW);
    builder.setCompressionType(Algorithm.SNAPPY);
    builder.setMobEnabled(true);
    builder.setMobThreshold(1000L);
    builder.setDFSReplication((short) v);

    ColumnFamilyDescriptor hcd = builder.build();
    byte[] bytes = ColumnFamilyDescriptorBuilder.toByteArray(hcd);
    ColumnFamilyDescriptor deserializedHcd = ColumnFamilyDescriptorBuilder.parseFrom(bytes);
    assertTrue(hcd.equals(deserializedHcd));
    assertEquals(v, hcd.getBlocksize());
    assertEquals(v, hcd.getTimeToLive());
    assertTrue(
      Bytes.equals(hcd.getValue(Bytes.toBytes(""a"")), deserializedHcd.getValue(Bytes.toBytes(""a""))));
    assertEquals(hcd.getMaxVersions(), deserializedHcd.getMaxVersions());
    assertEquals(hcd.getMinVersions(), deserializedHcd.getMinVersions());
    assertEquals(hcd.getKeepDeletedCells(), deserializedHcd.getKeepDeletedCells());
    assertEquals(inmemory, deserializedHcd.isInMemory());
    assertEquals(hcd.getScope(), deserializedHcd.getScope());
    assertTrue(deserializedHcd.getCompressionType().equals(Compression.Algorithm.SNAPPY));
    assertTrue(deserializedHcd.getDataBlockEncoding().equals(DataBlockEncoding.FAST_DIFF));
    assertTrue(deserializedHcd.getBloomFilterType().equals(BloomType.ROW));
    assertEquals(hcd.isMobEnabled(), deserializedHcd.isMobEnabled());
    assertEquals(hcd.getMobThreshold(), deserializedHcd.getMobThreshold());
    assertEquals(v, deserializedHcd.getDFSReplication());
  }

  /**
   * Tests HColumnDescriptor with empty familyName
   */
  @Test
  public void testHColumnDescriptorShouldThrowIAEWhenFamilyNameEmpty() throws Exception {
    assertThrows(""Column Family name can not be empty"", IllegalArgumentException.class,
      () -> ColumnFamilyDescriptorBuilder.of(""""));
  }

  /**
   * Test that we add and remove strings from configuration properly.
   */
  @Test
  public void testAddGetRemoveConfiguration() {
    ColumnFamilyDescriptorBuilder builder =
      ColumnFamilyDescriptorBuilder.newBuilder(Bytes.toBytes(""foo""));
    String key = ""Some"";
    String value = ""value"";
    builder.setConfiguration(key, value);
    assertEquals(value, builder.build().getConfigurationValue(key));
    builder.removeConfiguration(key);
    assertEquals(null, builder.build().getConfigurationValue(key));
  }

  @Test
  public void testMobValuesInHColumnDescriptorShouldReadable() {
    boolean isMob = true;
    long threshold = 1000;
    String policy = ""weekly"";
    // We unify the format of all values saved in the descriptor.
    // Each value is stored as bytes of string.
    String isMobString = PrettyPrinter.format(String.valueOf(isMob),
      ColumnFamilyDescriptorBuilder.getUnit(ColumnFamilyDescriptorBuilder.IS_MOB));
    String thresholdString = PrettyPrinter.format(String.valueOf(threshold),
      ColumnFamilyDescriptorBuilder.getUnit(ColumnFamilyDescriptorBuilder.MOB_THRESHOLD));
    String policyString = PrettyPrinter.format(Bytes.toStringBinary(Bytes.toBytes(policy)),
      ColumnFamilyDescriptorBuilder
        .getUnit(ColumnFamilyDescriptorBuilder.MOB_COMPACT_PARTITION_POLICY));
    assertEquals(String.valueOf(isMob), isMobString);
    assertEquals(String.valueOf(threshold), thresholdString);
    assertEquals(String.valueOf(policy), policyString);
  }

  @Test
  public void testClassMethodsAreBuilderStyle() {
    /*
     * ColumnFamilyDescriptorBuilder should have a builder style setup where setXXX/addXXX methods
     * can be chainable together: . For example: ColumnFamilyDescriptorBuilder builder =
     * ColumnFamilyDescriptorBuilder.newBuilder() .setFoo(foo) .setBar(bar) .setBuz(buz) This test
     * ensures that all methods starting with ""set"" returns the declaring object
     */

    BuilderStyleTest.assertClassesAreBuilderStyle(ColumnFamilyDescriptorBuilder.class);
  }

  @Test
  public void testSetTimeToLive() throws HBaseException {
    String ttl;
    ColumnFamilyDescriptorBuilder builder =
      ColumnFamilyDescriptorBuilder.newBuilder(Bytes.toBytes(""foo""));

    ttl = ""50000"";
    builder.setTimeToLive(ttl);
    Assert.assertEquals(50000, builder.build().getTimeToLive());

    ttl = ""50000 seconds"";
    builder.setTimeToLive(ttl);
    Assert.assertEquals(50000, builder.build().getTimeToLive());

    ttl = """";
    builder.setTimeToLive(ttl);
    Assert.assertEquals(0, builder.build().getTimeToLive());

    ttl = ""FOREVER"";
    builder.setTimeToLive(ttl);
    Assert.assertEquals(HConstants.FOREVER, builder.build().getTimeToLive());

    ttl = ""1 HOUR 10 minutes 1 second"";
    builder.setTimeToLive(ttl);
    Assert.assertEquals(4201, builder.build().getTimeToLive());

    ttl = ""500 Days 23 HOURS"";
    builder.setTimeToLive(ttl);
    Assert.assertEquals(43282800, builder.build().getTimeToLive());

    ttl = ""43282800 SECONDS (500 Days 23 hours)"";
    builder.setTimeToLive(ttl);
    Assert.assertEquals(43282800, builder.build().getTimeToLive());
  }

  @Test
  public void testSetBlocksize() throws HBaseException {
    String blocksize;
    ColumnFamilyDescriptorBuilder builder =
      ColumnFamilyDescriptorBuilder.newBuilder(Bytes.toBytes(""foo""));

    blocksize = ""131072"";
    builder.setBlocksize(blocksize);
    assertEquals(131072, builder.build().getBlocksize());

    blocksize = ""100KB"";
    builder.setBlocksize(blocksize);
    assertEquals(102400, builder.build().getBlocksize());

    blocksize = ""1MB"";
    builder.setBlocksize(blocksize);
    assertEquals(1048576, builder.build().getBlocksize());

    // ignore case
    blocksize = ""64kb 512B"";
    builder.setBlocksize(blocksize);
    assertEquals(66048, builder.build().getBlocksize());

    blocksize = ""66048 B (64KB 512B)"";
    builder.setBlocksize(blocksize);
    assertEquals(66048, builder.build().getBlocksize());
  }

  /**
   * Test for verifying the ColumnFamilyDescriptorBuilder's default values so that backward
   * compatibility with hbase-1.x can be mantained (see HBASE-24981).
   */
  @Test
  public void testDefaultBuilder() {
    final Map<String, String> defaultValueMap = ColumnFamilyDescriptorBuilder.getDefaultValues();
    assertEquals(defaultValueMap.size(), 12);
    assertEquals(defaultValueMap.get(ColumnFamilyDescriptorBuilder.BLOOMFILTER),
      BloomType.ROW.toString());
    assertEquals(defaultValueMap.get(ColumnFamilyDescriptorBuilder.REPLICATION_SCOPE), ""0"");
    assertEquals(defaultValueMap.get(ColumnFamilyDescriptorBuilder.MAX_VERSIONS), ""1"");
    assertEquals(defaultValueMap.get(ColumnFamilyDescriptorBuilder.MIN_VERSIONS), ""0"");
    assertEquals(defaultValueMap.get(ColumnFamilyDescriptorBuilder.COMPRESSION),
      Compression.Algorithm.NONE.toString());
    assertEquals(defaultValueMap.get(ColumnFamilyDescriptorBuilder.TTL),
      Integer.toString(Integer.MAX_VALUE));
    assertEquals(defaultValueMap.get(ColumnFamilyDescriptorBuilder.BLOCKSIZE),
      Integer.toString(64 * 1024));
    assertEquals(defaultValueMap.get(ColumnFamilyDescriptorBuilder.IN_MEMORY),
      Boolean.toString(false));
    assertEquals(defaultValueMap.get(ColumnFamilyDescriptorBuilder.BLOCKCACHE),
      Boolean.toString(true));
    assertEquals(defaultValueMap.get(ColumnFamilyDescriptorBuilder.KEEP_DELETED_CELLS),
      KeepDeletedCells.FALSE.toString());
    assertEquals(defaultValueMap.get(ColumnFamilyDescriptorBuilder.DATA_BLOCK_ENCODING),
      DataBlockEncoding.NONE.toString());
    assertEquals(defaultValueMap.get(ColumnFamilyDescriptorBuilder.INDEX_BLOCK_ENCODING),
      IndexBlockEncoding.NONE.toString());
  }

  @Test
  public void testSetEmptyValue() {
    ColumnFamilyDescriptorBuilder builder =
      ColumnFamilyDescriptorBuilder.newBuilder(HConstants.CATALOG_FAMILY);
    String testConf = ""TestConfiguration"";
    String testValue = ""TestValue"";
    // test set value
    builder.setValue(testValue, ""2"");
    assertEquals(""2"", Bytes.toString(builder.build().getValue(Bytes.toBytes(testValue))));
    builder.setValue(testValue, """");
    assertNull(builder.build().getValue(Bytes.toBytes(testValue)));

    // test set configuration
    builder.setConfiguration(testConf, ""1"");
    assertEquals(""1"", builder.build().getConfigurationValue(testConf));
    builder.setConfiguration(testConf, """");
    assertNull(builder.build().getConfigurationValue(testConf));
  }
}
","/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * ""License""); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hadoop.hbase.client;

import java.io.IOException;
import java.util.Collections;
import java.util.HashMap;
import java.util.HashSet;
import java.util.Map;
import java.util.Set;
import java.util.function.Function;
import org.apache.hadoop.hbase.HConstants;
import org.apache.hadoop.hbase.KeepDeletedCells;
import org.apache.hadoop.hbase.MemoryCompactionPolicy;
import org.apache.hadoop.hbase.exceptions.DeserializationException;
import org.apache.hadoop.hbase.exceptions.HBaseException;
import org.apache.hadoop.hbase.io.compress.Compression;
import org.apache.hadoop.hbase.io.encoding.DataBlockEncoding;
import org.apache.hadoop.hbase.io.encoding.IndexBlockEncoding;
import org.apache.hadoop.hbase.regionserver.BloomType;
import org.apache.hadoop.hbase.util.Bytes;
import org.apache.hadoop.hbase.util.PrettyPrinter;
import org.apache.hadoop.hbase.util.PrettyPrinter.Unit;
import org.apache.yetus.audience.InterfaceAudience;

import org.apache.hbase.thirdparty.com.google.common.base.Preconditions;

import org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil;
import org.apache.hadoop.hbase.shaded.protobuf.generated.HBaseProtos.ColumnFamilySchema;

@InterfaceAudience.Public
public class ColumnFamilyDescriptorBuilder {
  // For future backward compatibility

  // Version 3 was when column names become byte arrays and when we picked up
  // Time-to-live feature. Version 4 was when we moved to byte arrays, HBASE-82.
  // Version 5 was when bloom filter descriptors were removed.
  // Version 6 adds metadata as a map where keys and values are byte[].
  // Version 7 -- add new compression and hfile blocksize to HColumnDescriptor (HBASE-1217)
  // Version 8 -- reintroduction of bloom filters, changed from boolean to enum
  // Version 9 -- add data block encoding
  // Version 10 -- change metadata to standard type.
  // Version 11 -- add column family level configuration.
  private static final byte COLUMN_DESCRIPTOR_VERSION = (byte) 11;

  @InterfaceAudience.Private
  public static final String IN_MEMORY_COMPACTION = ""IN_MEMORY_COMPACTION"";
  private static final Bytes IN_MEMORY_COMPACTION_BYTES =
    new Bytes(Bytes.toBytes(IN_MEMORY_COMPACTION));

  @InterfaceAudience.Private
  public static final String IN_MEMORY = HConstants.IN_MEMORY;
  private static final Bytes IN_MEMORY_BYTES = new Bytes(Bytes.toBytes(IN_MEMORY));

  // These constants are used as FileInfo keys
  @InterfaceAudience.Private
  public static final String COMPRESSION = ""COMPRESSION"";
  private static final Bytes COMPRESSION_BYTES = new Bytes(Bytes.toBytes(COMPRESSION));
  @InterfaceAudience.Private
  public static final String COMPRESSION_COMPACT = ""COMPRESSION_COMPACT"";
  private static final Bytes COMPRESSION_COMPACT_BYTES =
    new Bytes(Bytes.toBytes(COMPRESSION_COMPACT));
  public static final String COMPRESSION_COMPACT_MAJOR = ""COMPRESSION_COMPACT_MAJOR"";
  private static final Bytes COMPRESSION_COMPACT_MAJOR_BYTES =
    new Bytes(Bytes.toBytes(COMPRESSION_COMPACT_MAJOR));
  public static final String COMPRESSION_COMPACT_MINOR = ""COMPRESSION_COMPACT_MINOR"";
  private static final Bytes COMPRESSION_COMPACT_MINOR_BYTES =
    new Bytes(Bytes.toBytes(COMPRESSION_COMPACT_MINOR));
  @InterfaceAudience.Private
  public static final String DATA_BLOCK_ENCODING = ""DATA_BLOCK_ENCODING"";
  private static final Bytes DATA_BLOCK_ENCODING_BYTES =
    new Bytes(Bytes.toBytes(DATA_BLOCK_ENCODING));
  @InterfaceAudience.Private
  public static final String INDEX_BLOCK_ENCODING = ""INDEX_BLOCK_ENCODING"";
  private static final Bytes INDEX_BLOCK_ENCODING_BYTES =
    new Bytes(Bytes.toBytes(INDEX_BLOCK_ENCODING));
  /**
   * Key for the BLOCKCACHE attribute. A more exact name would be CACHE_DATA_ON_READ because this
   * flag sets whether or not we cache DATA blocks. We always cache INDEX and BLOOM blocks; caching
   * these blocks cannot be disabled.
   */
  @InterfaceAudience.Private
  public static final String BLOCKCACHE = ""BLOCKCACHE"";
  private static final Bytes BLOCKCACHE_BYTES = new Bytes(Bytes.toBytes(BLOCKCACHE));
  @InterfaceAudience.Private
  public static final String CACHE_DATA_ON_WRITE = ""CACHE_DATA_ON_WRITE"";
  private static final Bytes CACHE_DATA_ON_WRITE_BYTES =
    new Bytes(Bytes.toBytes(CACHE_DATA_ON_WRITE));
  @InterfaceAudience.Private
  public static final String CACHE_INDEX_ON_WRITE = ""CACHE_INDEX_ON_WRITE"";
  private static final Bytes CACHE_INDEX_ON_WRITE_BYTES =
    new Bytes(Bytes.toBytes(CACHE_INDEX_ON_WRITE));
  @InterfaceAudience.Private
  public static final String CACHE_BLOOMS_ON_WRITE = ""CACHE_BLOOMS_ON_WRITE"";
  private static final Bytes CACHE_BLOOMS_ON_WRITE_BYTES =
    new Bytes(Bytes.toBytes(CACHE_BLOOMS_ON_WRITE));
  @InterfaceAudience.Private
  public static final String EVICT_BLOCKS_ON_CLOSE = ""EVICT_BLOCKS_ON_CLOSE"";
  private static final Bytes EVICT_BLOCKS_ON_CLOSE_BYTES =
    new Bytes(Bytes.toBytes(EVICT_BLOCKS_ON_CLOSE));

  /**
   * Key for the PREFETCH_BLOCKS_ON_OPEN attribute. If set, all INDEX, BLOOM, and DATA blocks of
   * HFiles belonging to this family will be loaded into the cache as soon as the file is opened.
   * These loads will not count as cache misses.
   */
  @InterfaceAudience.Private
  public static final String PREFETCH_BLOCKS_ON_OPEN = ""PREFETCH_BLOCKS_ON_OPEN"";
  private static final Bytes PREFETCH_BLOCKS_ON_OPEN_BYTES =
    new Bytes(Bytes.toBytes(PREFETCH_BLOCKS_ON_OPEN));

  /**
   * Size of storefile/hfile 'blocks'. Default is {@link #DEFAULT_BLOCKSIZE}. Use smaller block
   * sizes for faster random-access at expense of larger indices (more memory consumption). Note
   * that this is a soft limit and that blocks have overhead (metadata, CRCs) so blocks will tend to
   * be the size specified here and then some; i.e. don't expect that setting BLOCKSIZE=4k means
   * hbase data will align with an SSDs 4k page accesses (TODO).
   */
  @InterfaceAudience.Private
  public static final String BLOCKSIZE = ""BLOCKSIZE"";
  private static final Bytes BLOCKSIZE_BYTES = new Bytes(Bytes.toBytes(BLOCKSIZE));

  @InterfaceAudience.Private
  public static final String TTL = ""TTL"";
  private static final Bytes TTL_BYTES = new Bytes(Bytes.toBytes(TTL));
  @InterfaceAudience.Private
  public static final String BLOOMFILTER = ""BLOOMFILTER"";
  private static final Bytes BLOOMFILTER_BYTES = new Bytes(Bytes.toBytes(BLOOMFILTER));
  @InterfaceAudience.Private
  public static final String REPLICATION_SCOPE = ""REPLICATION_SCOPE"";
  @InterfaceAudience.Private
  public static final String MAX_VERSIONS = HConstants.VERSIONS;
  private static final Bytes MAX_VERSIONS_BYTES = new Bytes(Bytes.toBytes(MAX_VERSIONS));
  @InterfaceAudience.Private
  public static final String MIN_VERSIONS = ""MIN_VERSIONS"";
  private static final Bytes MIN_VERSIONS_BYTES = new Bytes(Bytes.toBytes(MIN_VERSIONS));
  /**
   * Retain all cells across flushes and compactions even if they fall behind a delete tombstone. To
   * see all retained cells, do a 'raw' scan; see Scan#setRaw or pass RAW =&gt; true attribute in
   * the shell.
   */
  @InterfaceAudience.Private
  public static final String KEEP_DELETED_CELLS = ""KEEP_DELETED_CELLS"";
  private static final Bytes KEEP_DELETED_CELLS_BYTES =
    new Bytes(Bytes.toBytes(KEEP_DELETED_CELLS));
  @InterfaceAudience.Private
  public static final String COMPRESS_TAGS = ""COMPRESS_TAGS"";
  private static final Bytes COMPRESS_TAGS_BYTES = new Bytes(Bytes.toBytes(COMPRESS_TAGS));
  @InterfaceAudience.Private
  public static final String ENCRYPTION = ""ENCRYPTION"";
  private static final Bytes ENCRYPTION_BYTES = new Bytes(Bytes.toBytes(ENCRYPTION));
  @InterfaceAudience.Private
  public static final String ENCRYPTION_KEY = ""ENCRYPTION_KEY"";
  private static final Bytes ENCRYPTION_KEY_BYTES = new Bytes(Bytes.toBytes(ENCRYPTION_KEY));

  private static final boolean DEFAULT_MOB = false;
  @InterfaceAudience.Private
  public static final String IS_MOB = ""IS_MOB"";
  private static final Bytes IS_MOB_BYTES = new Bytes(Bytes.toBytes(IS_MOB));
  @InterfaceAudience.Private
  public static final String MOB_THRESHOLD = ""MOB_THRESHOLD"";
  private static final Bytes MOB_THRESHOLD_BYTES = new Bytes(Bytes.toBytes(MOB_THRESHOLD));
  public static final long DEFAULT_MOB_THRESHOLD = 100 * 1024; // 100k
  @InterfaceAudience.Private
  public static final String MOB_COMPACT_PARTITION_POLICY = ""MOB_COMPACT_PARTITION_POLICY"";
  private static final Bytes MOB_COMPACT_PARTITION_POLICY_BYTES =
    new Bytes(Bytes.toBytes(MOB_COMPACT_PARTITION_POLICY));
  public static final MobCompactPartitionPolicy DEFAULT_MOB_COMPACT_PARTITION_POLICY =
    MobCompactPartitionPolicy.DAILY;
  @InterfaceAudience.Private
  public static final String DFS_REPLICATION = ""DFS_REPLICATION"";
  private static final Bytes DFS_REPLICATION_BYTES = new Bytes(Bytes.toBytes(DFS_REPLICATION));
  public static final short DEFAULT_DFS_REPLICATION = 0;
  @InterfaceAudience.Private
  public static final String STORAGE_POLICY = ""STORAGE_POLICY"";
  private static final Bytes STORAGE_POLICY_BYTES = new Bytes(Bytes.toBytes(STORAGE_POLICY));

  public static final String NEW_VERSION_BEHAVIOR = ""NEW_VERSION_BEHAVIOR"";
  private static final Bytes NEW_VERSION_BEHAVIOR_BYTES =
    new Bytes(Bytes.toBytes(NEW_VERSION_BEHAVIOR));
  public static final boolean DEFAULT_NEW_VERSION_BEHAVIOR = false;
  /**
   * Default compression type.
   */
  public static final Compression.Algorithm DEFAULT_COMPRESSION = Compression.Algorithm.NONE;

  /**
   * Default data block encoding algorithm.
   */
  public static final DataBlockEncoding DEFAULT_DATA_BLOCK_ENCODING = DataBlockEncoding.NONE;

  /**
   * Default index block encoding algorithm.
   */
  public static final IndexBlockEncoding DEFAULT_INDEX_BLOCK_ENCODING = IndexBlockEncoding.NONE;

  /**
   * Default number of versions of a record to keep.
   */
  public static final int DEFAULT_MAX_VERSIONS = 1;

  /**
   * Default is not to keep a minimum of versions.
   */
  public static final int DEFAULT_MIN_VERSIONS = 0;

  /**
   * Default setting for whether to try and serve this column family from memory or not.
   */
  public static final boolean DEFAULT_IN_MEMORY = false;

  /**
   * Default setting for preventing deleted from being collected immediately.
   */
  public static final KeepDeletedCells DEFAULT_KEEP_DELETED = KeepDeletedCells.FALSE;

  /**
   * Default setting for whether to use a block cache or not.
   */
  public static final boolean DEFAULT_BLOCKCACHE = true;

  /**
   * Default setting for whether to cache data blocks on write if block caching is enabled.
   */
  public static final boolean DEFAULT_CACHE_DATA_ON_WRITE = false;

  /**
   * Default setting for whether to cache index blocks on write if block caching is enabled.
   */
  public static final boolean DEFAULT_CACHE_INDEX_ON_WRITE = false;

  /**
   * Default size of blocks in files stored to the filesytem (hfiles).
   */
  public static final int DEFAULT_BLOCKSIZE = HConstants.DEFAULT_BLOCKSIZE;

  /**
   * Default setting for whether or not to use bloomfilters.
   */
  public static final BloomType DEFAULT_BLOOMFILTER = BloomType.ROW;

  /**
   * Default setting for whether to cache bloom filter blocks on write if block caching is enabled.
   */
  public static final boolean DEFAULT_CACHE_BLOOMS_ON_WRITE = false;

  /**
   * Default time to live of cell contents.
   */
  public static final int DEFAULT_TTL = HConstants.FOREVER;

  /**
   * Default scope.
   */
  public static final int DEFAULT_REPLICATION_SCOPE = HConstants.REPLICATION_SCOPE_LOCAL;

  /**
   * Default setting for whether to evict cached blocks from the blockcache on close.
   */
  public static final boolean DEFAULT_EVICT_BLOCKS_ON_CLOSE = false;

  /**
   * Default compress tags along with any type of DataBlockEncoding.
   */
  public static final boolean DEFAULT_COMPRESS_TAGS = true;

  /*
   * Default setting for whether to prefetch blocks into the blockcache on open.
   */
  public static final boolean DEFAULT_PREFETCH_BLOCKS_ON_OPEN = false;

  private final static Map<String, String> DEFAULT_VALUES = new HashMap<>();

  private static Map<Bytes, Bytes> getDefaultValuesBytes() {
    Map<Bytes, Bytes> values = new HashMap<>();
    DEFAULT_VALUES
      .forEach((k, v) -> values.put(new Bytes(Bytes.toBytes(k)), new Bytes(Bytes.toBytes(v))));
    return values;
  }

  public static Map<String, String> getDefaultValues() {
    return Collections.unmodifiableMap(DEFAULT_VALUES);
  }

  private final static Set<Bytes> RESERVED_KEYWORDS = new HashSet<>();

  static {
    DEFAULT_VALUES.put(BLOOMFILTER, DEFAULT_BLOOMFILTER.name());
    DEFAULT_VALUES.put(REPLICATION_SCOPE, String.valueOf(DEFAULT_REPLICATION_SCOPE));
    DEFAULT_VALUES.put(MAX_VERSIONS, String.valueOf(DEFAULT_MAX_VERSIONS));
    DEFAULT_VALUES.put(MIN_VERSIONS, String.valueOf(DEFAULT_MIN_VERSIONS));
    DEFAULT_VALUES.put(COMPRESSION, DEFAULT_COMPRESSION.name());
    DEFAULT_VALUES.put(TTL, String.valueOf(DEFAULT_TTL));
    DEFAULT_VALUES.put(BLOCKSIZE, String.valueOf(DEFAULT_BLOCKSIZE));
    DEFAULT_VALUES.put(IN_MEMORY, String.valueOf(DEFAULT_IN_MEMORY));
    DEFAULT_VALUES.put(BLOCKCACHE, String.valueOf(DEFAULT_BLOCKCACHE));
    DEFAULT_VALUES.put(KEEP_DELETED_CELLS, String.valueOf(DEFAULT_KEEP_DELETED));
    DEFAULT_VALUES.put(DATA_BLOCK_ENCODING, String.valueOf(DEFAULT_DATA_BLOCK_ENCODING));
    DEFAULT_VALUES.put(INDEX_BLOCK_ENCODING, String.valueOf(DEFAULT_INDEX_BLOCK_ENCODING));
    // Do NOT add this key/value by default. NEW_VERSION_BEHAVIOR is NOT defined in hbase1 so
    // it is not possible to make an hbase1 HCD the same as an hbase2 HCD and so the replication
    // compare of schemas will fail. It is OK not adding the below to the initial map because of
    // fetch of this value, we will check for null and if null will return the default.
    // DEFAULT_VALUES.put(NEW_VERSION_BEHAVIOR, String.valueOf(DEFAULT_NEW_VERSION_BEHAVIOR));
    DEFAULT_VALUES.keySet().forEach(s -> RESERVED_KEYWORDS.add(new Bytes(Bytes.toBytes(s))));
    RESERVED_KEYWORDS.add(new Bytes(Bytes.toBytes(ENCRYPTION)));
    RESERVED_KEYWORDS.add(new Bytes(Bytes.toBytes(ENCRYPTION_KEY)));
    RESERVED_KEYWORDS.add(new Bytes(Bytes.toBytes(IS_MOB)));
    RESERVED_KEYWORDS.add(new Bytes(Bytes.toBytes(MOB_THRESHOLD)));
    RESERVED_KEYWORDS.add(new Bytes(Bytes.toBytes(MOB_COMPACT_PARTITION_POLICY)));
  }

  public static Unit getUnit(String key) {
    /* TTL for now, we can add more as we need */
    switch (key) {
      case TTL:
        return Unit.TIME_INTERVAL;
      case BLOCKSIZE:
        return Unit.BYTE;
      default:
        return Unit.NONE;
    }
  }

  /**
   * Check if the column family name is legal.
   * @param b Family name.
   * @return <code>b</code>
   * @throws IllegalArgumentException If not null and not a legitimate family name: i.e. 'printable'
   *                                  and ends in a ':' (Null passes are allowed because
   *                                  <code>b</code> can be null when deserializing). Cannot start
   *                                  with a '.' either. Also Family can not be an empty value or
   *                                  equal ""recovered.edits"".
   */
  public static byte[] isLegalColumnFamilyName(final byte[] b) {
    if (b == null) {
      return null;
    }
    Preconditions.checkArgument(b.length != 0, ""Column Family name can not be empty"");
    if (b[0] == '.') {
      throw new IllegalArgumentException(
        ""Column Family names cannot start with a "" + ""period: "" + Bytes.toString(b));
    }
    for (int i = 0; i < b.length; i++) {
      if (Character.isISOControl(b[i]) || b[i] == ':' || b[i] == '\\' || b[i] == '/') {
        throw new IllegalArgumentException(""Illegal character <"" + b[i]
          + "">. Column Family names cannot contain control characters or colons: ""
          + Bytes.toString(b));
      }
    }
    byte[] recoveredEdit = Bytes.toBytes(HConstants.RECOVERED_EDITS_DIR);
    if (Bytes.equals(recoveredEdit, b)) {
      throw new IllegalArgumentException(
        ""Column Family name cannot be: "" + HConstants.RECOVERED_EDITS_DIR);
    }
    return b;
  }

  private final ModifyableColumnFamilyDescriptor desc;

  public static ColumnFamilyDescriptor parseFrom(final byte[] pbBytes)
    throws DeserializationException {
    return ModifyableColumnFamilyDescriptor.parseFrom(pbBytes);
  }

  public static ColumnFamilyDescriptorBuilder newBuilder(final byte[] name) {
    return new ColumnFamilyDescriptorBuilder(name);
  }

  public static ColumnFamilyDescriptorBuilder newBuilder(final ColumnFamilyDescriptor desc) {
    return new ColumnFamilyDescriptorBuilder(desc);
  }

  public static ColumnFamilyDescriptor copy(ColumnFamilyDescriptor desc) {
    return new ModifyableColumnFamilyDescriptor(desc);
  }

  public static ColumnFamilyDescriptor of(String name) {
    return of(Bytes.toBytes(name));
  }

  public static ColumnFamilyDescriptor of(byte[] name) {
    return newBuilder(name).build();
  }

  private ColumnFamilyDescriptorBuilder(final byte[] name) {
    this.desc = new ModifyableColumnFamilyDescriptor(name);
  }

  private ColumnFamilyDescriptorBuilder(final ColumnFamilyDescriptor desc) {
    this.desc = new ModifyableColumnFamilyDescriptor(desc);
  }

  /**
   * Serialize the table descriptor to a byte array.
   * @param desc The table descriptor to serialize
   * @return This instance serialized with pb with pb magic prefix
   */
  public static byte[] toByteArray(ColumnFamilyDescriptor desc) {
    if (desc instanceof ModifyableColumnFamilyDescriptor) {
      return ((ModifyableColumnFamilyDescriptor) desc).toByteArray();
    }
    return new ModifyableColumnFamilyDescriptor(desc).toByteArray();
  }

  public ColumnFamilyDescriptor build() {
    return new ModifyableColumnFamilyDescriptor(desc);
  }

  public ColumnFamilyDescriptorBuilder removeConfiguration(String key) {
    desc.removeConfiguration(key);
    return this;
  }

  public String getNameAsString() {
    return desc.getNameAsString();
  }

  public ColumnFamilyDescriptorBuilder setBlockCacheEnabled(boolean value) {
    desc.setBlockCacheEnabled(value);
    return this;
  }

  public ColumnFamilyDescriptorBuilder setBlocksize(int value) {
    desc.setBlocksize(value);
    return this;
  }

  public ColumnFamilyDescriptorBuilder setBlocksize(String value) throws HBaseException {
    desc.setBlocksize(value);
    return this;
  }

  public ColumnFamilyDescriptorBuilder setBloomFilterType(final BloomType value) {
    desc.setBloomFilterType(value);
    return this;
  }

  public ColumnFamilyDescriptorBuilder setCacheBloomsOnWrite(boolean value) {
    desc.setCacheBloomsOnWrite(value);
    return this;
  }

  public ColumnFamilyDescriptorBuilder setCacheDataOnWrite(boolean value) {
    desc.setCacheDataOnWrite(value);
    return this;
  }

  public ColumnFamilyDescriptorBuilder setCacheIndexesOnWrite(final boolean value) {
    desc.setCacheIndexesOnWrite(value);
    return this;
  }

  public ColumnFamilyDescriptorBuilder setCompactionCompressionType(Compression.Algorithm value) {
    desc.setCompactionCompressionType(value);
    return this;
  }

  public ColumnFamilyDescriptorBuilder
    setMajorCompactionCompressionType(Compression.Algorithm value) {
    desc.setMajorCompactionCompressionType(value);
    return this;
  }

  public ColumnFamilyDescriptorBuilder
    setMinorCompactionCompressionType(Compression.Algorithm value) {
    desc.setMinorCompactionCompressionType(value);
    return this;
  }

  public ColumnFamilyDescriptorBuilder setCompressTags(boolean value) {
    desc.setCompressTags(value);
    return this;
  }

  public ColumnFamilyDescriptorBuilder setCompressionType(Compression.Algorithm value) {
    desc.setCompressionType(value);
    return this;
  }

  public Compression.Algorithm getCompressionType() {
    return desc.getCompressionType();
  }

  public ColumnFamilyDescriptorBuilder setConfiguration(final String key, final String value) {
    desc.setConfiguration(key, value);
    return this;
  }

  public ColumnFamilyDescriptorBuilder setDFSReplication(short value) {
    desc.setDFSReplication(value);
    return this;
  }

  public ColumnFamilyDescriptorBuilder setDataBlockEncoding(DataBlockEncoding value) {
    desc.setDataBlockEncoding(value);
    return this;
  }

  public ColumnFamilyDescriptorBuilder setIndexBlockEncoding(IndexBlockEncoding value) {
    desc.setIndexBlockEncoding(value);
    return this;
  }

  public ColumnFamilyDescriptorBuilder setEncryptionKey(final byte[] value) {
    desc.setEncryptionKey(value);
    return this;
  }

  public ColumnFamilyDescriptorBuilder setEncryptionType(String value) {
    desc.setEncryptionType(value);
    return this;
  }

  public ColumnFamilyDescriptorBuilder setEvictBlocksOnClose(boolean value) {
    desc.setEvictBlocksOnClose(value);
    return this;
  }

  public ColumnFamilyDescriptorBuilder setInMemory(final boolean value) {
    desc.setInMemory(value);
    return this;
  }

  public ColumnFamilyDescriptorBuilder setInMemoryCompaction(final MemoryCompactionPolicy value) {
    desc.setInMemoryCompaction(value);
    return this;
  }

  public ColumnFamilyDescriptorBuilder setKeepDeletedCells(KeepDeletedCells value) {
    desc.setKeepDeletedCells(value);
    return this;
  }

  public ColumnFamilyDescriptorBuilder setMaxVersions(final int value) {
    desc.setMaxVersions(value);
    return this;
  }

  public ColumnFamilyDescriptorBuilder setMinVersions(final int value) {
    desc.setMinVersions(value);
    return this;
  }

  public ColumnFamilyDescriptorBuilder
    setMobCompactPartitionPolicy(final MobCompactPartitionPolicy value) {
    desc.setMobCompactPartitionPolicy(value);
    return this;
  }

  public ColumnFamilyDescriptorBuilder setMobEnabled(final boolean value) {
    desc.setMobEnabled(value);
    return this;
  }

  public ColumnFamilyDescriptorBuilder setMobThreshold(final long value) {
    desc.setMobThreshold(value);
    return this;
  }

  public ColumnFamilyDescriptorBuilder setPrefetchBlocksOnOpen(final boolean value) {
    desc.setPrefetchBlocksOnOpen(value);
    return this;
  }

  public ColumnFamilyDescriptorBuilder setScope(final int value) {
    desc.setScope(value);
    return this;
  }

  public ColumnFamilyDescriptorBuilder setStoragePolicy(final String value) {
    desc.setStoragePolicy(value);
    return this;
  }

  public ColumnFamilyDescriptorBuilder setTimeToLive(final int value) {
    desc.setTimeToLive(value);
    return this;
  }

  public ColumnFamilyDescriptorBuilder setTimeToLive(final String value) throws HBaseException {
    desc.setTimeToLive(value);
    return this;
  }

  public ColumnFamilyDescriptorBuilder setNewVersionBehavior(final boolean value) {
    desc.setNewVersionBehavior(value);
    return this;
  }

  public ColumnFamilyDescriptorBuilder setValue(final Bytes key, final Bytes value) {
    desc.setValue(key, value);
    return this;
  }

  public ColumnFamilyDescriptorBuilder setValue(final byte[] key, final byte[] value) {
    desc.setValue(key, value);
    return this;
  }

  public ColumnFamilyDescriptorBuilder setValue(final String key, final String value) {
    desc.setValue(key, value);
    return this;
  }

  public ColumnFamilyDescriptorBuilder setVersionsWithTimeToLive(final int retentionInterval,
    final int versionAfterInterval) {
    desc.setVersionsWithTimeToLive(retentionInterval, versionAfterInterval);
    return this;
  }

  /**
   * An ModifyableFamilyDescriptor contains information about a column family such as the number of
   * versions, compression settings, etc. It is used as input when creating a table or adding a
   * column.
   */
  private static final class ModifyableColumnFamilyDescriptor
    implements ColumnFamilyDescriptor, Comparable<ModifyableColumnFamilyDescriptor> {

    // Column family name
    private final byte[] name;

    // Column metadata
    private final Map<Bytes, Bytes> values = new HashMap<>();

    /**
     * A map which holds the configuration specific to the column family. The keys of the map have
     * the same names as config keys and override the defaults with cf-specific settings. Example
     * usage may be for compactions, etc.
     */
    private final Map<String, String> configuration = new HashMap<>();

    /**
     * Construct a column descriptor specifying only the family name The other attributes are
     * defaulted.
     * @param name Column family name. Must be 'printable' -- digit or letter -- and may not contain
     *             a <code>:</code> TODO: make this private after the HCD is removed.
     */
    @InterfaceAudience.Private
    public ModifyableColumnFamilyDescriptor(final byte[] name) {
      this(isLegalColumnFamilyName(name), getDefaultValuesBytes(), Collections.emptyMap());
    }

    /**
     * Constructor. Makes a deep copy of the supplied descriptor. TODO: make this private after the
     * HCD is removed.
     * @param desc The descriptor.
     */
    @InterfaceAudience.Private
    public ModifyableColumnFamilyDescriptor(ColumnFamilyDescriptor desc) {
      this(desc.getName(), desc.getValues(), desc.getConfiguration());
    }

    private ModifyableColumnFamilyDescriptor(byte[] name, Map<Bytes, Bytes> values,
      Map<String, String> config) {
      this.name = name;
      this.values.putAll(values);
      this.configuration.putAll(config);
    }

    @Override
    public byte[] getName() {
      return Bytes.copy(name);
    }

    @Override
    public String getNameAsString() {
      return Bytes.toString(name);
    }

    @Override
    public Bytes getValue(Bytes key) {
      return values.get(key);
    }

    @Override
    public byte[] getValue(byte[] key) {
      Bytes value = values.get(new Bytes(key));
      return value == null ? null : value.get();
    }

    @Override
    public String getValue(String key) {
      Bytes rval = values.get(new Bytes(Bytes.toBytes(key)));
      return rval == null ? null : Bytes.toString(rval.get(), rval.getOffset(), rval.getLength());
    }

    @Override
    public Map<Bytes, Bytes> getValues() {
      return Collections.unmodifiableMap(values);
    }

    public ModifyableColumnFamilyDescriptor setValue(byte[] key, byte[] value) {
      return setValue(toBytesOrNull(key, Function.identity()),
        toBytesOrNull(value, Function.identity()));
    }

    public ModifyableColumnFamilyDescriptor setValue(String key, String value) {
      return setValue(toBytesOrNull(key, Bytes::toBytes), toBytesOrNull(value, Bytes::toBytes));
    }

    private ModifyableColumnFamilyDescriptor setValue(Bytes key, String value) {
      return setValue(key, toBytesOrNull(value, Bytes::toBytes));
    }

    private ModifyableColumnFamilyDescriptor setValue(Bytes key, Bytes value) {
      if (value == null || value.getLength() == 0) {
        values.remove(key);
      } else {
        values.put(key, value);
      }
      return this;
    }

    private static <T> Bytes toBytesOrNull(T t, Function<T, byte[]> f) {
      if (t == null) {
        return null;
      } else {
        return new Bytes(f.apply(t));
      }
    }

    private <T> T getStringOrDefault(Bytes key, Function<String, T> function, T defaultValue) {
      return getOrDefault(key, b -> function.apply(Bytes.toString(b)), defaultValue);
    }

    private <T> T getOrDefault(Bytes key, Function<byte[], T> function, T defaultValue) {
      Bytes value = values.get(key);
      if (value == null) {
        return defaultValue;
      } else {
        return function.apply(value.get());
      }
    }

    @Override
    public int getMaxVersions() {
      return getStringOrDefault(MAX_VERSIONS_BYTES, Integer::parseInt, DEFAULT_MAX_VERSIONS);
    }

    /**
     * Set the maximum number of versions to retain.
     * @param maxVersions maximum number of versions
     * @return this (for chained invocation)
     */
    public ModifyableColumnFamilyDescriptor setMaxVersions(int maxVersions) {
      if (maxVersions <= 0) {
        // TODO: Allow maxVersion of 0 to be the way you say ""Keep all versions"".
        // Until there is support, consider 0 or < 0 -- a configuration error.
        throw new IllegalArgumentException(""Maximum versions must be positive"");
      }
      if (maxVersions < this.getMinVersions()) {
        throw new IllegalArgumentException(
          ""Set MaxVersion to "" + maxVersions + "" while minVersion is "" + this.getMinVersions()
            + "". Maximum versions must be >= minimum versions "");
      }
      setValue(MAX_VERSIONS_BYTES, Integer.toString(maxVersions));
      return this;
    }

    /**
     * Set minimum and maximum versions to keep.
     * @param minVersions minimal number of versions
     * @param maxVersions maximum number of versions
     * @return this (for chained invocation)
     */
    public ModifyableColumnFamilyDescriptor setVersions(int minVersions, int maxVersions) {
      if (minVersions <= 0) {
        // TODO: Allow minVersion and maxVersion of 0 to be the way you say ""Keep all versions"".
        // Until there is support, consider 0 or < 0 -- a configuration error.
        throw new IllegalArgumentException(""Minimum versions must be positive"");
      }

      if (maxVersions < minVersions) {
        throw new IllegalArgumentException(
          ""Unable to set MaxVersion to "" + maxVersions + "" and set MinVersion to "" + minVersions
            + "", as maximum versions must be >= minimum versions."");
      }
      setMinVersions(minVersions);
      setMaxVersions(maxVersions);
      return this;
    }

    @Override
    public int getBlocksize() {
      return getStringOrDefault(BLOCKSIZE_BYTES, Integer::valueOf, DEFAULT_BLOCKSIZE);
    }

    public ModifyableColumnFamilyDescriptor setBlocksize(int s) {
      return setValue(BLOCKSIZE_BYTES, Integer.toString(s));
    }

    public ModifyableColumnFamilyDescriptor setBlocksize(String blocksize) throws HBaseException {
      return setBlocksize(
        Integer.parseInt(PrettyPrinter.valueOf(blocksize, PrettyPrinter.Unit.BYTE)));
    }

    @Override
    public Compression.Algorithm getCompressionType() {
      return getStringOrDefault(COMPRESSION_BYTES,
        n -> Compression.Algorithm.valueOf(n.toUpperCase()), DEFAULT_COMPRESSION);
    }

    /**
     * Compression types supported in hbase. LZO is not bundled as part of the hbase distribution.
     * See See <a href=""http://hbase.apache.org/book.html#lzo.compression"">LZO Compression</a> for
     * how to enable it.
     * @param type Compression type setting.
     * @return this (for chained invocation)
     */
    public ModifyableColumnFamilyDescriptor setCompressionType(Compression.Algorithm type) {
      return setValue(COMPRESSION_BYTES, type.name());
    }

    @Override
    public DataBlockEncoding getDataBlockEncoding() {
      return getStringOrDefault(DATA_BLOCK_ENCODING_BYTES,
        n -> DataBlockEncoding.valueOf(n.toUpperCase()), DataBlockEncoding.NONE);
    }

    /**
     * Set data block encoding algorithm used in block cache.
     * @param type What kind of data block encoding will be used.
     * @return this (for chained invocation)
     */
    public ModifyableColumnFamilyDescriptor setDataBlockEncoding(DataBlockEncoding type) {
      return setValue(DATA_BLOCK_ENCODING_BYTES,
        type == null ? DataBlockEncoding.NONE.name() : type.name());
    }

    @Override
    public IndexBlockEncoding getIndexBlockEncoding() {
      return getStringOrDefault(INDEX_BLOCK_ENCODING_BYTES,
        n -> IndexBlockEncoding.valueOf(n.toUpperCase()), IndexBlockEncoding.NONE);
    }

    /**
     * Set index block encoding algorithm used in block cache.
     * @param type What kind of index block encoding will be used.
     * @return this (for chained invocation)
     */
    public ModifyableColumnFamilyDescriptor setIndexBlockEncoding(IndexBlockEncoding type) {
      return setValue(INDEX_BLOCK_ENCODING_BYTES,
        type == null ? IndexBlockEncoding.NONE.name() : type.name());
    }

    /**
     * Set whether the tags should be compressed along with DataBlockEncoding. When no
     * DataBlockEncoding is been used, this is having no effect.
     * @return this (for chained invocation)
     */
    public ModifyableColumnFamilyDescriptor setCompressTags(boolean compressTags) {
      return setValue(COMPRESS_TAGS_BYTES, String.valueOf(compressTags));
    }

    @Override
    public boolean isCompressTags() {
      return getStringOrDefault(COMPRESS_TAGS_BYTES, Boolean::valueOf, DEFAULT_COMPRESS_TAGS);
    }

    @Override
    public Compression.Algorithm getCompactionCompressionType() {
      return getStringOrDefault(COMPRESSION_COMPACT_BYTES,
        n -> Compression.Algorithm.valueOf(n.toUpperCase()), getCompressionType());
    }

    @Override
    public Compression.Algorithm getMajorCompactionCompressionType() {
      return getStringOrDefault(COMPRESSION_COMPACT_MAJOR_BYTES,
        n -> Compression.Algorithm.valueOf(n.toUpperCase()), getCompactionCompressionType());
    }

    @Override
    public Compression.Algorithm getMinorCompactionCompressionType() {
      return getStringOrDefault(COMPRESSION_COMPACT_MINOR_BYTES,
        n -> Compression.Algorithm.valueOf(n.toUpperCase()), getCompactionCompressionType());
    }

    /**
     * Compression types supported in hbase. LZO is not bundled as part of the hbase distribution.
     * See See <a href=""http://hbase.apache.org/book.html#lzo.compression"">LZO Compression</a> for
     * how to enable it.
     * @param type Compression type setting.
     * @return this (for chained invocation)
     */
    public ModifyableColumnFamilyDescriptor
      setCompactionCompressionType(Compression.Algorithm type) {
      return setValue(COMPRESSION_COMPACT_BYTES, type.name());
    }

    public ModifyableColumnFamilyDescriptor
      setMajorCompactionCompressionType(Compression.Algorithm type) {
      return setValue(COMPRESSION_COMPACT_MAJOR_BYTES, type.name());
    }

    public ModifyableColumnFamilyDescriptor
      setMinorCompactionCompressionType(Compression.Algorithm type) {
      return setValue(COMPRESSION_COMPACT_MINOR_BYTES, type.name());
    }

    @Override
    public boolean isInMemory() {
      return getStringOrDefault(IN_MEMORY_BYTES, Boolean::valueOf, DEFAULT_IN_MEMORY);
    }

    /**
     * Set the inMemory flag
     * @param inMemory True if we are to favor keeping all values for this column family in the
     *                 HRegionServer cache
     * @return this (for chained invocation)
     */
    public ModifyableColumnFamilyDescriptor setInMemory(boolean inMemory) {
      return setValue(IN_MEMORY_BYTES, Boolean.toString(inMemory));
    }

    @Override
    public MemoryCompactionPolicy getInMemoryCompaction() {
      return getStringOrDefault(IN_MEMORY_COMPACTION_BYTES,
        n -> MemoryCompactionPolicy.valueOf(n.toUpperCase()), null);
    }

    public ModifyableColumnFamilyDescriptor
      setInMemoryCompaction(MemoryCompactionPolicy inMemoryCompaction) {
      return setValue(IN_MEMORY_COMPACTION_BYTES, inMemoryCompaction.name());
    }

    @Override
    public KeepDeletedCells getKeepDeletedCells() {
      return getStringOrDefault(KEEP_DELETED_CELLS_BYTES, KeepDeletedCells::getValue,
        DEFAULT_KEEP_DELETED);
    }

    public ModifyableColumnFamilyDescriptor setKeepDeletedCells(KeepDeletedCells keepDeletedCells) {
      return setValue(KEEP_DELETED_CELLS_BYTES, keepDeletedCells.name());
    }

    /**
     * By default, HBase only consider timestamp in versions. So a previous Delete with higher ts
     * will mask a later Put with lower ts. Set this to true to enable new semantics of versions. We
     * will also consider mvcc in versions. See HBASE-15968 for details.
     */
    @Override
    public boolean isNewVersionBehavior() {
      return getStringOrDefault(NEW_VERSION_BEHAVIOR_BYTES, Boolean::parseBoolean,
        DEFAULT_NEW_VERSION_BEHAVIOR);
    }

    public ModifyableColumnFamilyDescriptor setNewVersionBehavior(boolean newVersionBehavior) {
      return setValue(NEW_VERSION_BEHAVIOR_BYTES, Boolean.toString(newVersionBehavior));
    }

    @Override
    public int getTimeToLive() {
      return getStringOrDefault(TTL_BYTES, Integer::parseInt, DEFAULT_TTL);
    }

    /**
     * Set the time to live
     * @param timeToLive Time-to-live of cell contents, in seconds.
     * @return this (for chained invocation)
     */
    public ModifyableColumnFamilyDescriptor setTimeToLive(int timeToLive) {
      return setValue(TTL_BYTES, Integer.toString(timeToLive));
    }

    /**
     * Set the time to live
     * @param timeToLive Time-to-live of cell contents, in seconds.
     * @return this (for chained invocation)
     * @throws org.apache.hadoop.hbase.exceptions.HBaseException exception
     */
    public ModifyableColumnFamilyDescriptor setTimeToLive(String timeToLive) throws HBaseException {
      return setTimeToLive(Integer.parseInt(PrettyPrinter.valueOf(timeToLive, Unit.TIME_INTERVAL)));
    }

    @Override
    public int getMinVersions() {
      return getStringOrDefault(MIN_VERSIONS_BYTES, Integer::valueOf, DEFAULT_MIN_VERSIONS);
    }

    /**
     * Set minimum versions to retain.
     * @param minVersions The minimum number of versions to keep. (used when timeToLive is set)
     * @return this (for chained invocation)
     */
    public ModifyableColumnFamilyDescriptor setMinVersions(int minVersions) {
      return setValue(MIN_VERSIONS_BYTES, Integer.toString(minVersions));
    }

    /**
     * Retain all versions for a given TTL(retentionInterval), and then only a specific number of
     * versions(versionAfterInterval) after that interval elapses.
     * @param retentionInterval    Retain all versions for this interval
     * @param versionAfterInterval Retain no of versions to retain after retentionInterval
     * @return this (for chained invocation)
     */
    public ModifyableColumnFamilyDescriptor setVersionsWithTimeToLive(final int retentionInterval,
      final int versionAfterInterval) {
      ModifyableColumnFamilyDescriptor modifyableColumnFamilyDescriptor =
        setVersions(versionAfterInterval, Integer.MAX_VALUE);
      modifyableColumnFamilyDescriptor.setTimeToLive(retentionInterval);
      modifyableColumnFamilyDescriptor.setKeepDeletedCells(KeepDeletedCells.TTL);
      return modifyableColumnFamilyDescriptor;
    }

    @Override
    public boolean isBlockCacheEnabled() {
      return getStringOrDefault(BLOCKCACHE_BYTES, Boolean::valueOf, DEFAULT_BLOCKCACHE);
    }

    /**
     * Set the blockCacheEnabled flag
     * @param blockCacheEnabled True if hfile DATA type blocks should be cached (We always cache
     *                          INDEX and BLOOM blocks; you cannot turn this off).
     * @return this (for chained invocation)
     */
    public ModifyableColumnFamilyDescriptor setBlockCacheEnabled(boolean blockCacheEnabled) {
      return setValue(BLOCKCACHE_BYTES, Boolean.toString(blockCacheEnabled));
    }

    @Override
    public BloomType getBloomFilterType() {
      return getStringOrDefault(BLOOMFILTER_BYTES, n -> BloomType.valueOf(n.toUpperCase()),
        DEFAULT_BLOOMFILTER);
    }

    public ModifyableColumnFamilyDescriptor setBloomFilterType(final BloomType bt) {
      return setValue(BLOOMFILTER_BYTES, bt.name());
    }

    @Override
    public int getScope() {
      return getStringOrDefault(REPLICATION_SCOPE_BYTES, Integer::valueOf,
        DEFAULT_REPLICATION_SCOPE);
    }

    public ModifyableColumnFamilyDescriptor setScope(int scope) {
      return setValue(REPLICATION_SCOPE_BYTES, Integer.toString(scope));
    }

    @Override
    public boolean isCacheDataOnWrite() {
      return getStringOrDefault(CACHE_DATA_ON_WRITE_BYTES, Boolean::valueOf,
        DEFAULT_CACHE_DATA_ON_WRITE);
    }

    /**
     * Set the setCacheDataOnWrite flag
     * @param value true if we should cache data blocks on write
     * @return this (for chained invocation)
     */
    public ModifyableColumnFamilyDescriptor setCacheDataOnWrite(boolean value) {
      return setValue(CACHE_DATA_ON_WRITE_BYTES, Boolean.toString(value));
    }

    @Override
    public boolean isCacheIndexesOnWrite() {
      return getStringOrDefault(CACHE_INDEX_ON_WRITE_BYTES, Boolean::valueOf,
        DEFAULT_CACHE_INDEX_ON_WRITE);
    }

    /**
     * Set the setCacheIndexesOnWrite flag
     * @param value true if we should cache index blocks on write
     * @return this (for chained invocation)
     */
    public ModifyableColumnFamilyDescriptor setCacheIndexesOnWrite(boolean value) {
      return setValue(CACHE_INDEX_ON_WRITE_BYTES, Boolean.toString(value));
    }

    @Override
    public boolean isCacheBloomsOnWrite() {
      return getStringOrDefault(CACHE_BLOOMS_ON_WRITE_BYTES, Boolean::valueOf,
        DEFAULT_CACHE_BLOOMS_ON_WRITE);
    }

    /**
     * Set the setCacheBloomsOnWrite flag.
     * @param value true if we should cache bloomfilter blocks on write
     * @return this (for chained invocation)
     */
    public ModifyableColumnFamilyDescriptor setCacheBloomsOnWrite(boolean value) {
      return setValue(CACHE_BLOOMS_ON_WRITE_BYTES, Boolean.toString(value));
    }

    @Override
    public boolean isEvictBlocksOnClose() {
      return getStringOrDefault(EVICT_BLOCKS_ON_CLOSE_BYTES, Boolean::valueOf,
        DEFAULT_EVICT_BLOCKS_ON_CLOSE);
    }

    /**
     * Set the setEvictBlocksOnClose flag.
     * @param value true if we should evict cached blocks from the blockcache on close
     * @return this (for chained invocation)
     */
    public ModifyableColumnFamilyDescriptor setEvictBlocksOnClose(boolean value) {
      return setValue(EVICT_BLOCKS_ON_CLOSE_BYTES, Boolean.toString(value));
    }

    @Override
    public boolean isPrefetchBlocksOnOpen() {
      return getStringOrDefault(PREFETCH_BLOCKS_ON_OPEN_BYTES, Boolean::valueOf,
        DEFAULT_PREFETCH_BLOCKS_ON_OPEN);
    }

    /**
     * Set the setPrefetchBlocksOnOpen flag
     * @param value true if we should prefetch blocks into the blockcache on open
     * @return this (for chained invocation)
     */
    public ModifyableColumnFamilyDescriptor setPrefetchBlocksOnOpen(boolean value) {
      return setValue(PREFETCH_BLOCKS_ON_OPEN_BYTES, Boolean.toString(value));
    }

    @Override
    public String toString() {
      StringBuilder s = new StringBuilder();
      s.append('{');
      s.append(HConstants.NAME);
      s.append("" => '"");
      s.append(getNameAsString());
      s.append(""'"");
      s.append(getValues(true));
      s.append('}');
      return s.toString();
    }

    @Override
    public String toStringCustomizedValues() {
      StringBuilder s = new StringBuilder();
      s.append('{');
      s.append(HConstants.NAME);
      s.append("" => '"");
      s.append(getNameAsString());
      s.append(""'"");
      s.append(getValues(false));
      s.append('}');
      return s.toString();
    }

    private StringBuilder getValues(boolean printDefaults) {
      StringBuilder s = new StringBuilder();

      boolean hasConfigKeys = false;

      // print all reserved keys first
      for (Map.Entry<Bytes, Bytes> entry : values.entrySet()) {
        if (!RESERVED_KEYWORDS.contains(entry.getKey())) {
          hasConfigKeys = true;
          continue;
        }
        String key = Bytes.toString(entry.getKey().get());
        String value = Bytes.toStringBinary(entry.getValue().get());
        if (
          printDefaults || !DEFAULT_VALUES.containsKey(key)
            || !DEFAULT_VALUES.get(key).equalsIgnoreCase(value)
        ) {
          s.append("", "");
          s.append(key);
          s.append("" => "");
          s.append('\'').append(PrettyPrinter.format(value, getUnit(key))).append('\'');
        }
      }

      // print all non-reserved, advanced config keys as a separate subset
      if (hasConfigKeys) {
        s.append("", "");
        s.append(HConstants.METADATA).append("" => "");
        s.append('{');
        boolean printComma = false;
        for (Map.Entry<Bytes, Bytes> entry : values.entrySet()) {
          Bytes k = entry.getKey();
          if (RESERVED_KEYWORDS.contains(k)) {
            continue;
          }
          String key = Bytes.toString(k.get());
          String value = Bytes.toStringBinary(entry.getValue().get());
          if (printComma) {
            s.append("", "");
          }
          printComma = true;
          s.append('\'').append(key).append('\'');
          s.append("" => "");
          s.append('\'').append(PrettyPrinter.format(value, getUnit(key))).append('\'');
        }
        s.append('}');
      }

      if (!configuration.isEmpty()) {
        s.append("", "");
        s.append(HConstants.CONFIGURATION).append("" => "");
        s.append('{');
        boolean printCommaForConfiguration = false;
        for (Map.Entry<String, String> e : configuration.entrySet()) {
          if (printCommaForConfiguration) {
            s.append("", "");
          }
          printCommaForConfiguration = true;
          s.append('\'').append(e.getKey()).append('\'');
          s.append("" => "");
          s.append('\'').append(PrettyPrinter.format(e.getValue(), getUnit(e.getKey())))
            .append('\'');
        }
        s.append(""}"");
      }
      return s;
    }

    @Override
    public boolean equals(Object obj) {
      if (this == obj) {
        return true;
      }
      if (obj instanceof ModifyableColumnFamilyDescriptor) {
        return ColumnFamilyDescriptor.COMPARATOR.compare(this,
          (ModifyableColumnFamilyDescriptor) obj) == 0;
      }
      return false;
    }

    @Override
    public int hashCode() {
      int result = Bytes.hashCode(name);
      result ^= (int) COLUMN_DESCRIPTOR_VERSION;
      result ^= values.hashCode();
      result ^= configuration.hashCode();
      return result;
    }

    @Override
    public int compareTo(ModifyableColumnFamilyDescriptor other) {
      return COMPARATOR.compare(this, other);
    }

    /** Returns This instance serialized with pb with pb magic prefix */
    private byte[] toByteArray() {
      return ProtobufUtil.prependPBMagic(ProtobufUtil.toColumnFamilySchema(this).toByteArray());
    }

    /**
     * Parse the serialized representation of a {@link ModifyableColumnFamilyDescriptor}
     * @param bytes A pb serialized {@link ModifyableColumnFamilyDescriptor} instance with pb magic
     *              prefix
     * @return An instance of {@link ModifyableColumnFamilyDescriptor} made from <code>bytes</code>
     * @see #toByteArray()
     */
    private static ColumnFamilyDescriptor parseFrom(final byte[] bytes)
      throws DeserializationException {
      if (!ProtobufUtil.isPBMagicPrefix(bytes)) {
        throw new DeserializationException(""No magic"");
      }
      int pblen = ProtobufUtil.lengthOfPBMagic();
      ColumnFamilySchema.Builder builder = ColumnFamilySchema.newBuilder();
      ColumnFamilySchema cfs = null;
      try {
        ProtobufUtil.mergeFrom(builder, bytes, pblen, bytes.length - pblen);
        cfs = builder.build();
      } catch (IOException e) {
        throw new DeserializationException(e);
      }
      return ProtobufUtil.toColumnFamilyDescriptor(cfs);
    }

    @Override
    public String getConfigurationValue(String key) {
      return configuration.get(key);
    }

    @Override
    public Map<String, String> getConfiguration() {
      // shallow pointer copy
      return Collections.unmodifiableMap(configuration);
    }

    /**
     * Setter for storing a configuration setting in {@link #configuration} map.
     * @param key   Config key. Same as XML config key e.g. hbase.something.or.other.
     * @param value String value. If null, removes the configuration.
     * @return this (for chained invocation)
     */
    public ModifyableColumnFamilyDescriptor setConfiguration(String key, String value) {
      if (value == null || value.length() == 0) {
        configuration.remove(key);
      } else {
        configuration.put(key, value);
      }
      return this;
    }

    /**
     * Remove a configuration setting represented by the key from the {@link #configuration} map.
     * @return this (for chained invocation)
     */
    public ModifyableColumnFamilyDescriptor removeConfiguration(final String key) {
      return setConfiguration(key, null);
    }

    @Override
    public String getEncryptionType() {
      return getStringOrDefault(ENCRYPTION_BYTES, Function.identity(), null);
    }

    /**
     * Set the encryption algorithm for use with this family
     * @return this (for chained invocation)
     */
    public ModifyableColumnFamilyDescriptor setEncryptionType(String algorithm) {
      return setValue(ENCRYPTION_BYTES, algorithm);
    }

    @Override
    public byte[] getEncryptionKey() {
      return getOrDefault(ENCRYPTION_KEY_BYTES, Bytes::copy, null);
    }

    /**
     * Set the raw crypto key attribute for the family
     * @return this (for chained invocation)
     */
    public ModifyableColumnFamilyDescriptor setEncryptionKey(byte[] keyBytes) {
      return setValue(ENCRYPTION_KEY_BYTES, new Bytes(keyBytes));
    }

    @Override
    public long getMobThreshold() {
      return getStringOrDefault(MOB_THRESHOLD_BYTES, Long::valueOf, DEFAULT_MOB_THRESHOLD);
    }

    /**
     * Sets the mob threshold of the family.
     * @param threshold The mob threshold.
     * @return this (for chained invocation)
     */
    public ModifyableColumnFamilyDescriptor setMobThreshold(long threshold) {
      return setValue(MOB_THRESHOLD_BYTES, String.valueOf(threshold));
    }

    @Override
    public boolean isMobEnabled() {
      return getStringOrDefault(IS_MOB_BYTES, Boolean::valueOf, DEFAULT_MOB);
    }

    /**
     * Enables the mob for the family.
     * @param isMobEnabled Whether to enable the mob for the family.
     * @return this (for chained invocation)
     */
    public ModifyableColumnFamilyDescriptor setMobEnabled(boolean isMobEnabled) {
      return setValue(IS_MOB_BYTES, String.valueOf(isMobEnabled));
    }

    @Override
    public MobCompactPartitionPolicy getMobCompactPartitionPolicy() {
      return getStringOrDefault(MOB_COMPACT_PARTITION_POLICY_BYTES,
        n -> MobCompactPartitionPolicy.valueOf(n.toUpperCase()),
        DEFAULT_MOB_COMPACT_PARTITION_POLICY);
    }

    /**
     * Set the mob compact partition policy for the family.
     * @param policy policy type
     * @return this (for chained invocation)
     */
    public ModifyableColumnFamilyDescriptor
      setMobCompactPartitionPolicy(MobCompactPartitionPolicy policy) {
      return setValue(MOB_COMPACT_PARTITION_POLICY_BYTES, policy.name());
    }

    @Override
    public short getDFSReplication() {
      return getStringOrDefault(DFS_REPLICATION_BYTES, Short::valueOf, DEFAULT_DFS_REPLICATION);
    }

    /**
     * Set the replication factor to hfile(s) belonging to this family
     * @param replication number of replicas the blocks(s) belonging to this CF should have, or
     *                    {@link #DEFAULT_DFS_REPLICATION} for the default replication factor set in
     *                    the filesystem
     * @return this (for chained invocation)
     */
    public ModifyableColumnFamilyDescriptor setDFSReplication(short replication) {
      if (replication < 1 && replication != DEFAULT_DFS_REPLICATION) {
        throw new IllegalArgumentException(
          ""DFS replication factor cannot be less than 1 if explicitly set."");
      }
      return setValue(DFS_REPLICATION_BYTES, Short.toString(replication));
    }

    @Override
    public String getStoragePolicy() {
      return getStringOrDefault(STORAGE_POLICY_BYTES, Function.identity(), null);
    }

    /**
     * Set the storage policy for use with this family
     * @param policy the policy to set, valid setting includes: <i>""LAZY_PERSIST""</i>,
     *               <i>""ALL_SSD""</i>, <i>""ONE_SSD""</i>, <i>""HOT""</i>, <i>""WARM""</i>, <i>""COLD""</i>
     * @return this (for chained invocation)
     */
    public ModifyableColumnFamilyDescriptor setStoragePolicy(String policy) {
      return setValue(STORAGE_POLICY_BYTES, policy);
    }

  }
}
","['Assertion Roulette', 'Sensitive Equality']","['Assertion Roulette', 'Magic Number Test', 'Eager Test', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette']",2,1,1,14
35288_30_joda-time_test_getvalue_long_long,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/35288_test.java,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/35288_actual.java,"/*
 *  Copyright 2001-2009 Stephen Colebourne
 *
 *  Licensed under the Apache License, Version 2.0 (the ""License"");
 *  you may not use this file except in compliance with the License.
 *  You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 *  Unless required by applicable law or agreed to in writing, software
 *  distributed under the License is distributed on an ""AS IS"" BASIS,
 *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 *  See the License for the specific language governing permissions and
 *  limitations under the License.
 */
package org.joda.time.field;

import java.io.ByteArrayInputStream;
import java.io.ByteArrayOutputStream;
import java.io.ObjectInputStream;
import java.io.ObjectOutputStream;

import junit.framework.TestCase;
import junit.framework.TestSuite;

import org.joda.time.DurationField;
import org.joda.time.DurationFieldType;
import org.joda.time.chrono.ISOChronology;

/**
 * This class is a Junit unit test for PreciseDurationField.
 *
 * @author Stephen Colebourne
 */
public class TestScaledDurationField extends TestCase {
    
    private static final long LONG_INTEGER_MAX = Integer.MAX_VALUE;
    private static final int INTEGER_MAX = Integer.MAX_VALUE;
    private static final long LONG_MAX = Long.MAX_VALUE;
    
    private ScaledDurationField iField;

    public static void main(String[] args) {
        junit.textui.TestRunner.run(suite());
    }

    public static TestSuite suite() {
        return new TestSuite(TestScaledDurationField.class);
    }

    public TestScaledDurationField(String name) {
        super(name);
    }

    @Override
    protected void setUp() throws Exception {
        DurationField base = MillisDurationField.INSTANCE;
        iField = new ScaledDurationField(base, DurationFieldType.minutes(), 90);
    }

    @Override
    protected void tearDown() throws Exception {
        iField = null;
    }

    //-----------------------------------------------------------------------
    public void test_constructor() {
        try {
            new ScaledDurationField(null, DurationFieldType.minutes(), 10);
            fail();
        } catch (IllegalArgumentException ex) {}
        try {
            new ScaledDurationField(MillisDurationField.INSTANCE, null, 10);
            fail();
        } catch (IllegalArgumentException ex) {}
        try {
            new ScaledDurationField(MillisDurationField.INSTANCE, DurationFieldType.minutes(), 0);
            fail();
        } catch (IllegalArgumentException ex) {}
        try {
            new ScaledDurationField(MillisDurationField.INSTANCE, DurationFieldType.minutes(), 1);
            fail();
        } catch (IllegalArgumentException ex) {}
    }

    public void test_getScalar() {
        assertEquals(90, iField.getScalar());
    }

    //-----------------------------------------------------------------------
    public void test_getType() {
        assertEquals(DurationFieldType.minutes(), iField.getType());
    }

    public void test_getName() {
        assertEquals(""minutes"", iField.getName());
    }
    
    public void test_isSupported() {
        assertEquals(true, iField.isSupported());
    }

    public void test_isPrecise() {
        assertEquals(true, iField.isPrecise());
    }

    public void test_getUnitMillis() {
        assertEquals(90, iField.getUnitMillis());
    }

    public void test_toString() {
        assertEquals(""DurationField[minutes]"", iField.toString());
    }

    //-----------------------------------------------------------------------
    public void test_getValue_long() {
        assertEquals(0, iField.getValue(0L));
        assertEquals(12345678 / 90, iField.getValue(12345678L));
        assertEquals(-1234 / 90, iField.getValue(-1234L));
        assertEquals(INTEGER_MAX / 90, iField.getValue(LONG_INTEGER_MAX));
        try {
            iField.getValue(LONG_INTEGER_MAX + 1L);
            fail();
        } catch (ArithmeticException ex) {}
    }

    public void test_getValueAsLong_long() {
        assertEquals(0L, iField.getValueAsLong(0L));
        assertEquals(12345678L / 90, iField.getValueAsLong(12345678L));
        assertEquals(-1234 / 90L, iField.getValueAsLong(-1234L));
        assertEquals(LONG_INTEGER_MAX + 1L, iField.getValueAsLong(LONG_INTEGER_MAX * 90L + 90L));
    }

    public void test_getValue_long_long() {
        assertEquals(0, iField.getValue(0L, 567L));
        assertEquals(12345678 / 90, iField.getValue(12345678L, 567L));
        assertEquals(-1234 / 90, iField.getValue(-1234L, 567L));
        assertEquals(INTEGER_MAX / 90, iField.getValue(LONG_INTEGER_MAX, 567L));
        try {
            iField.getValue(LONG_INTEGER_MAX + 1L, 567L);
            fail();
        } catch (ArithmeticException ex) {}
    }

    public void test_getValueAsLong_long_long() {
        assertEquals(0L, iField.getValueAsLong(0L, 567L));
        assertEquals(12345678 / 90L, iField.getValueAsLong(12345678L, 567L));
        assertEquals(-1234 / 90L, iField.getValueAsLong(-1234L, 567L));
        assertEquals(LONG_INTEGER_MAX + 1L, iField.getValueAsLong(LONG_INTEGER_MAX * 90L + 90L, 567L));
    }

    //-----------------------------------------------------------------------
    public void test_getMillis_int() {
        assertEquals(0, iField.getMillis(0));
        assertEquals(1234L * 90L, iField.getMillis(1234));
        assertEquals(-1234L * 90L, iField.getMillis(-1234));
        assertEquals(LONG_INTEGER_MAX * 90L, iField.getMillis(INTEGER_MAX));
    }

    public void test_getMillis_long() {
        assertEquals(0L, iField.getMillis(0L));
        assertEquals(1234L * 90L, iField.getMillis(1234L));
        assertEquals(-1234L * 90L, iField.getMillis(-1234L));
        try {
            iField.getMillis(LONG_MAX);
            fail();
        } catch (ArithmeticException ex) {}
    }

    public void test_getMillis_int_long() {
        assertEquals(0L, iField.getMillis(0, 567L));
        assertEquals(1234L * 90L, iField.getMillis(1234, 567L));
        assertEquals(-1234L * 90L, iField.getMillis(-1234, 567L));
        assertEquals(LONG_INTEGER_MAX * 90L, iField.getMillis(INTEGER_MAX, 567L));
    }

    public void test_getMillis_long_long() {
        assertEquals(0L, iField.getMillis(0L, 567L));
        assertEquals(1234L * 90L, iField.getMillis(1234L, 567L));
        assertEquals(-1234L * 90L, iField.getMillis(-1234L, 567L));
        try {
            iField.getMillis(LONG_MAX, 567L);
            fail();
        } catch (ArithmeticException ex) {}
    }

    //-----------------------------------------------------------------------
    public void test_add_long_int() {
        assertEquals(567L, iField.add(567L, 0));
        assertEquals(567L + 1234L * 90L, iField.add(567L, 1234));
        assertEquals(567L - 1234L * 90L, iField.add(567L, -1234));
        try {
            iField.add(LONG_MAX, 1);
            fail();
        } catch (ArithmeticException ex) {}
    }

    public void test_add_long_long() {
        assertEquals(567L, iField.add(567L, 0L));
        assertEquals(567L + 1234L * 90L, iField.add(567L, 1234L));
        assertEquals(567L - 1234L * 90L, iField.add(567L, -1234L));
        try {
            iField.add(LONG_MAX, 1L);
            fail();
        } catch (ArithmeticException ex) {}
        try {
            iField.add(1L, LONG_MAX);
            fail();
        } catch (ArithmeticException ex) {}
    }

    //-----------------------------------------------------------------------
    public void test_getDifference_long_int() {
        assertEquals(0, iField.getDifference(1L, 0L));
        assertEquals(567, iField.getDifference(567L * 90L, 0L));
        assertEquals(567 - 1234, iField.getDifference(567L * 90L, 1234L * 90L));
        assertEquals(567 + 1234, iField.getDifference(567L * 90L, -1234L * 90L));
        try {
            iField.getDifference(LONG_MAX, -1L);
            fail();
        } catch (ArithmeticException ex) {}
    }

    public void test_getDifferenceAsLong_long_long() {
        assertEquals(0L, iField.getDifferenceAsLong(1L, 0L));
        assertEquals(567L, iField.getDifferenceAsLong(567L * 90L, 0L));
        assertEquals(567L - 1234L, iField.getDifferenceAsLong(567L * 90L, 1234L * 90L));
        assertEquals(567L + 1234L, iField.getDifferenceAsLong(567L * 90L, -1234L * 90L));
        try {
            iField.getDifferenceAsLong(LONG_MAX, -1L);
            fail();
        } catch (ArithmeticException ex) {}
    }

    //-----------------------------------------------------------------------
    public void test_equals() {
        assertEquals(true, iField.equals(iField));
        assertEquals(false, iField.equals(ISOChronology.getInstance().minutes()));
        DurationField dummy = new ScaledDurationField(MillisDurationField.INSTANCE, DurationFieldType.minutes(), 2);
        assertEquals(false, iField.equals(dummy));
        dummy = new ScaledDurationField(MillisDurationField.INSTANCE, DurationFieldType.minutes(), 90);
        assertEquals(true, iField.equals(dummy));
        dummy = new ScaledDurationField(MillisDurationField.INSTANCE, DurationFieldType.millis(), 90);
        assertEquals(false, iField.equals(dummy));
        assertEquals(false, iField.equals(""""));
        assertEquals(false, iField.equals(null));
    }

    public void test_hashCode() {
        assertEquals(iField.hashCode(), iField.hashCode());
        assertEquals(false, iField.hashCode() == ISOChronology.getInstance().minutes().hashCode());
        DurationField dummy = new ScaledDurationField(MillisDurationField.INSTANCE, DurationFieldType.minutes(), 2);
        assertEquals(false, iField.hashCode() == dummy.hashCode());
        dummy = new ScaledDurationField(MillisDurationField.INSTANCE, DurationFieldType.minutes(), 90);
        assertEquals(true, iField.hashCode() == dummy.hashCode());
        dummy = new ScaledDurationField(MillisDurationField.INSTANCE, DurationFieldType.millis(), 90);
        assertEquals(false, iField.hashCode() == dummy.hashCode());
    }

    //-----------------------------------------------------------------------
    public void test_compareTo() {
        assertEquals(0, iField.compareTo(iField));
        assertEquals(-1, iField.compareTo(ISOChronology.getInstance().minutes()));
        DurationField dummy = new PreciseDurationField(DurationFieldType.minutes(), 0);
        assertEquals(1, iField.compareTo(dummy));
//        try {
//            iField.compareTo("""");
//            fail();
//        } catch (ClassCastException ex) {}
        try {
            iField.compareTo(null);
            fail();
        } catch (NullPointerException ex) {}
    }

    //-----------------------------------------------------------------------
    public void testSerialization() throws Exception {
        DurationField test = iField;
        
        ByteArrayOutputStream baos = new ByteArrayOutputStream();
        ObjectOutputStream oos = new ObjectOutputStream(baos);
        oos.writeObject(test);
        oos.close();
        byte[] bytes = baos.toByteArray();
        
        ByteArrayInputStream bais = new ByteArrayInputStream(bytes);
        ObjectInputStream ois = new ObjectInputStream(bais);
        DurationField result = (DurationField) ois.readObject();
        ois.close();
        
        assertEquals(test, result);
    }

}
","/*
 *  Copyright 2001-2005 Stephen Colebourne
 *
 *  Licensed under the Apache License, Version 2.0 (the ""License"");
 *  you may not use this file except in compliance with the License.
 *  You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 *  Unless required by applicable law or agreed to in writing, software
 *  distributed under the License is distributed on an ""AS IS"" BASIS,
 *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 *  See the License for the specific language governing permissions and
 *  limitations under the License.
 */
package org.joda.time.field;

import org.joda.time.DurationField;
import org.joda.time.DurationFieldType;

/**
 * Scales a DurationField such that it's unit millis becomes larger in
 * magnitude.
 * <p>
 * ScaledDurationField is thread-safe and immutable.
 *
 * @see PreciseDurationField
 *
 * @author Brian S O'Neill
 * @since 1.0
 */
public class ScaledDurationField extends DecoratedDurationField {

    private static final long serialVersionUID = -3205227092378684157L;

    private final int iScalar;

    /**
     * Constructor
     * 
     * @param field  the field to wrap, like ""year()"".
     * @param type  the type this field will actually use
     * @param scalar  scalar, such as 100 years in a century
     * @throws IllegalArgumentException if scalar is zero or one.
     */
    public ScaledDurationField(DurationField field, DurationFieldType type, int scalar) {
        super(field, type);
        if (scalar == 0 || scalar == 1) {
            throw new IllegalArgumentException(""The scalar must not be 0 or 1"");
        }
        iScalar = scalar;
    }

    @Override
    public int getValue(long duration) {
        return getWrappedField().getValue(duration) / iScalar;
    }

    @Override
    public long getValueAsLong(long duration) {
        return getWrappedField().getValueAsLong(duration) / iScalar;
    }

    @Override
    public int getValue(long duration, long instant) {
        return getWrappedField().getValue(duration, instant) / iScalar;
    }

    @Override
    public long getValueAsLong(long duration, long instant) {
        return getWrappedField().getValueAsLong(duration, instant) / iScalar;
    }

    @Override
    public long getMillis(int value) {
        long scaled = ((long) value) * ((long) iScalar);
        return getWrappedField().getMillis(scaled);
    }

    @Override
    public long getMillis(long value) {
        long scaled = FieldUtils.safeMultiply(value, iScalar);
        return getWrappedField().getMillis(scaled);
    }

    @Override
    public long getMillis(int value, long instant) {
        long scaled = ((long) value) * ((long) iScalar);
        return getWrappedField().getMillis(scaled, instant);
    }

    @Override
    public long getMillis(long value, long instant) {
        long scaled = FieldUtils.safeMultiply(value, iScalar);
        return getWrappedField().getMillis(scaled, instant);
    }

    @Override
    public long add(long instant, int value) {
        long scaled = ((long) value) * ((long) iScalar);
        return getWrappedField().add(instant, scaled);
    }

    @Override
    public long add(long instant, long value) {
        long scaled = FieldUtils.safeMultiply(value, iScalar);
        return getWrappedField().add(instant, scaled);
    }

    @Override
    public int getDifference(long minuendInstant, long subtrahendInstant) {
        return getWrappedField().getDifference(minuendInstant, subtrahendInstant) / iScalar;
    }

    @Override
    public long getDifferenceAsLong(long minuendInstant, long subtrahendInstant) {
        return getWrappedField().getDifferenceAsLong(minuendInstant, subtrahendInstant) / iScalar;
    }

    @Override
    public long getUnitMillis() {
        return getWrappedField().getUnitMillis() * iScalar;
    }

    //-----------------------------------------------------------------------
    /**
     * Returns the scalar applied, in the field's units.
     * 
     * @return the scalar
     */
    public int getScalar() {
        return iScalar;
    }

    /**
     * Compares this duration field to another.
     * Two fields are equal if of the same type and duration.
     * 
     * @param obj  the object to compare to
     * @return if equal
     */
    @Override
    public boolean equals(Object obj) {
        if (this == obj) {
            return true;
        } else if (obj instanceof ScaledDurationField) {
            ScaledDurationField other = (ScaledDurationField) obj;
            return (getWrappedField().equals(other.getWrappedField())) &&
                   (getType() == other.getType()) &&
                   (iScalar == other.iScalar);
        }
        return false;
    }

    /**
     * Gets a hash code for this instance.
     * 
     * @return a suitable hashcode
     */
    @Override
    public int hashCode() {
        long scalar = iScalar;
        int hash = (int) (scalar ^ (scalar >>> 32));
        hash += getType().hashCode();
        hash += getWrappedField().hashCode();
        return hash;
    }

}
","['Assertion Roulette', 'Exception Catching Throwing', 'General Fixture', 'Redundant Assertion', 'Sensitive Equality']","['Assertion Roulette', 'Eager Test', 'Magic Number Test', 'Mystery Guest', 'Sensitive Equality']",3,3,2,10
35289_30_joda-time_test_getvalueaslong_long_long,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/35289_test.java,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/35289_actual.java,"/*
 *  Copyright 2001-2009 Stephen Colebourne
 *
 *  Licensed under the Apache License, Version 2.0 (the ""License"");
 *  you may not use this file except in compliance with the License.
 *  You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 *  Unless required by applicable law or agreed to in writing, software
 *  distributed under the License is distributed on an ""AS IS"" BASIS,
 *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 *  See the License for the specific language governing permissions and
 *  limitations under the License.
 */
package org.joda.time.field;

import java.io.ByteArrayInputStream;
import java.io.ByteArrayOutputStream;
import java.io.ObjectInputStream;
import java.io.ObjectOutputStream;

import junit.framework.TestCase;
import junit.framework.TestSuite;

import org.joda.time.DurationField;
import org.joda.time.DurationFieldType;
import org.joda.time.chrono.ISOChronology;

/**
 * This class is a Junit unit test for PreciseDurationField.
 *
 * @author Stephen Colebourne
 */
public class TestScaledDurationField extends TestCase {
    
    private static final long LONG_INTEGER_MAX = Integer.MAX_VALUE;
    private static final int INTEGER_MAX = Integer.MAX_VALUE;
    private static final long LONG_MAX = Long.MAX_VALUE;
    
    private ScaledDurationField iField;

    public static void main(String[] args) {
        junit.textui.TestRunner.run(suite());
    }

    public static TestSuite suite() {
        return new TestSuite(TestScaledDurationField.class);
    }

    public TestScaledDurationField(String name) {
        super(name);
    }

    @Override
    protected void setUp() throws Exception {
        DurationField base = MillisDurationField.INSTANCE;
        iField = new ScaledDurationField(base, DurationFieldType.minutes(), 90);
    }

    @Override
    protected void tearDown() throws Exception {
        iField = null;
    }

    //-----------------------------------------------------------------------
    public void test_constructor() {
        try {
            new ScaledDurationField(null, DurationFieldType.minutes(), 10);
            fail();
        } catch (IllegalArgumentException ex) {}
        try {
            new ScaledDurationField(MillisDurationField.INSTANCE, null, 10);
            fail();
        } catch (IllegalArgumentException ex) {}
        try {
            new ScaledDurationField(MillisDurationField.INSTANCE, DurationFieldType.minutes(), 0);
            fail();
        } catch (IllegalArgumentException ex) {}
        try {
            new ScaledDurationField(MillisDurationField.INSTANCE, DurationFieldType.minutes(), 1);
            fail();
        } catch (IllegalArgumentException ex) {}
    }

    public void test_getScalar() {
        assertEquals(90, iField.getScalar());
    }

    //-----------------------------------------------------------------------
    public void test_getType() {
        assertEquals(DurationFieldType.minutes(), iField.getType());
    }

    public void test_getName() {
        assertEquals(""minutes"", iField.getName());
    }
    
    public void test_isSupported() {
        assertEquals(true, iField.isSupported());
    }

    public void test_isPrecise() {
        assertEquals(true, iField.isPrecise());
    }

    public void test_getUnitMillis() {
        assertEquals(90, iField.getUnitMillis());
    }

    public void test_toString() {
        assertEquals(""DurationField[minutes]"", iField.toString());
    }

    //-----------------------------------------------------------------------
    public void test_getValue_long() {
        assertEquals(0, iField.getValue(0L));
        assertEquals(12345678 / 90, iField.getValue(12345678L));
        assertEquals(-1234 / 90, iField.getValue(-1234L));
        assertEquals(INTEGER_MAX / 90, iField.getValue(LONG_INTEGER_MAX));
        try {
            iField.getValue(LONG_INTEGER_MAX + 1L);
            fail();
        } catch (ArithmeticException ex) {}
    }

    public void test_getValueAsLong_long() {
        assertEquals(0L, iField.getValueAsLong(0L));
        assertEquals(12345678L / 90, iField.getValueAsLong(12345678L));
        assertEquals(-1234 / 90L, iField.getValueAsLong(-1234L));
        assertEquals(LONG_INTEGER_MAX + 1L, iField.getValueAsLong(LONG_INTEGER_MAX * 90L + 90L));
    }

    public void test_getValue_long_long() {
        assertEquals(0, iField.getValue(0L, 567L));
        assertEquals(12345678 / 90, iField.getValue(12345678L, 567L));
        assertEquals(-1234 / 90, iField.getValue(-1234L, 567L));
        assertEquals(INTEGER_MAX / 90, iField.getValue(LONG_INTEGER_MAX, 567L));
        try {
            iField.getValue(LONG_INTEGER_MAX + 1L, 567L);
            fail();
        } catch (ArithmeticException ex) {}
    }

    public void test_getValueAsLong_long_long() {
        assertEquals(0L, iField.getValueAsLong(0L, 567L));
        assertEquals(12345678 / 90L, iField.getValueAsLong(12345678L, 567L));
        assertEquals(-1234 / 90L, iField.getValueAsLong(-1234L, 567L));
        assertEquals(LONG_INTEGER_MAX + 1L, iField.getValueAsLong(LONG_INTEGER_MAX * 90L + 90L, 567L));
    }

    //-----------------------------------------------------------------------
    public void test_getMillis_int() {
        assertEquals(0, iField.getMillis(0));
        assertEquals(1234L * 90L, iField.getMillis(1234));
        assertEquals(-1234L * 90L, iField.getMillis(-1234));
        assertEquals(LONG_INTEGER_MAX * 90L, iField.getMillis(INTEGER_MAX));
    }

    public void test_getMillis_long() {
        assertEquals(0L, iField.getMillis(0L));
        assertEquals(1234L * 90L, iField.getMillis(1234L));
        assertEquals(-1234L * 90L, iField.getMillis(-1234L));
        try {
            iField.getMillis(LONG_MAX);
            fail();
        } catch (ArithmeticException ex) {}
    }

    public void test_getMillis_int_long() {
        assertEquals(0L, iField.getMillis(0, 567L));
        assertEquals(1234L * 90L, iField.getMillis(1234, 567L));
        assertEquals(-1234L * 90L, iField.getMillis(-1234, 567L));
        assertEquals(LONG_INTEGER_MAX * 90L, iField.getMillis(INTEGER_MAX, 567L));
    }

    public void test_getMillis_long_long() {
        assertEquals(0L, iField.getMillis(0L, 567L));
        assertEquals(1234L * 90L, iField.getMillis(1234L, 567L));
        assertEquals(-1234L * 90L, iField.getMillis(-1234L, 567L));
        try {
            iField.getMillis(LONG_MAX, 567L);
            fail();
        } catch (ArithmeticException ex) {}
    }

    //-----------------------------------------------------------------------
    public void test_add_long_int() {
        assertEquals(567L, iField.add(567L, 0));
        assertEquals(567L + 1234L * 90L, iField.add(567L, 1234));
        assertEquals(567L - 1234L * 90L, iField.add(567L, -1234));
        try {
            iField.add(LONG_MAX, 1);
            fail();
        } catch (ArithmeticException ex) {}
    }

    public void test_add_long_long() {
        assertEquals(567L, iField.add(567L, 0L));
        assertEquals(567L + 1234L * 90L, iField.add(567L, 1234L));
        assertEquals(567L - 1234L * 90L, iField.add(567L, -1234L));
        try {
            iField.add(LONG_MAX, 1L);
            fail();
        } catch (ArithmeticException ex) {}
        try {
            iField.add(1L, LONG_MAX);
            fail();
        } catch (ArithmeticException ex) {}
    }

    //-----------------------------------------------------------------------
    public void test_getDifference_long_int() {
        assertEquals(0, iField.getDifference(1L, 0L));
        assertEquals(567, iField.getDifference(567L * 90L, 0L));
        assertEquals(567 - 1234, iField.getDifference(567L * 90L, 1234L * 90L));
        assertEquals(567 + 1234, iField.getDifference(567L * 90L, -1234L * 90L));
        try {
            iField.getDifference(LONG_MAX, -1L);
            fail();
        } catch (ArithmeticException ex) {}
    }

    public void test_getDifferenceAsLong_long_long() {
        assertEquals(0L, iField.getDifferenceAsLong(1L, 0L));
        assertEquals(567L, iField.getDifferenceAsLong(567L * 90L, 0L));
        assertEquals(567L - 1234L, iField.getDifferenceAsLong(567L * 90L, 1234L * 90L));
        assertEquals(567L + 1234L, iField.getDifferenceAsLong(567L * 90L, -1234L * 90L));
        try {
            iField.getDifferenceAsLong(LONG_MAX, -1L);
            fail();
        } catch (ArithmeticException ex) {}
    }

    //-----------------------------------------------------------------------
    public void test_equals() {
        assertEquals(true, iField.equals(iField));
        assertEquals(false, iField.equals(ISOChronology.getInstance().minutes()));
        DurationField dummy = new ScaledDurationField(MillisDurationField.INSTANCE, DurationFieldType.minutes(), 2);
        assertEquals(false, iField.equals(dummy));
        dummy = new ScaledDurationField(MillisDurationField.INSTANCE, DurationFieldType.minutes(), 90);
        assertEquals(true, iField.equals(dummy));
        dummy = new ScaledDurationField(MillisDurationField.INSTANCE, DurationFieldType.millis(), 90);
        assertEquals(false, iField.equals(dummy));
        assertEquals(false, iField.equals(""""));
        assertEquals(false, iField.equals(null));
    }

    public void test_hashCode() {
        assertEquals(iField.hashCode(), iField.hashCode());
        assertEquals(false, iField.hashCode() == ISOChronology.getInstance().minutes().hashCode());
        DurationField dummy = new ScaledDurationField(MillisDurationField.INSTANCE, DurationFieldType.minutes(), 2);
        assertEquals(false, iField.hashCode() == dummy.hashCode());
        dummy = new ScaledDurationField(MillisDurationField.INSTANCE, DurationFieldType.minutes(), 90);
        assertEquals(true, iField.hashCode() == dummy.hashCode());
        dummy = new ScaledDurationField(MillisDurationField.INSTANCE, DurationFieldType.millis(), 90);
        assertEquals(false, iField.hashCode() == dummy.hashCode());
    }

    //-----------------------------------------------------------------------
    public void test_compareTo() {
        assertEquals(0, iField.compareTo(iField));
        assertEquals(-1, iField.compareTo(ISOChronology.getInstance().minutes()));
        DurationField dummy = new PreciseDurationField(DurationFieldType.minutes(), 0);
        assertEquals(1, iField.compareTo(dummy));
//        try {
//            iField.compareTo("""");
//            fail();
//        } catch (ClassCastException ex) {}
        try {
            iField.compareTo(null);
            fail();
        } catch (NullPointerException ex) {}
    }

    //-----------------------------------------------------------------------
    public void testSerialization() throws Exception {
        DurationField test = iField;
        
        ByteArrayOutputStream baos = new ByteArrayOutputStream();
        ObjectOutputStream oos = new ObjectOutputStream(baos);
        oos.writeObject(test);
        oos.close();
        byte[] bytes = baos.toByteArray();
        
        ByteArrayInputStream bais = new ByteArrayInputStream(bytes);
        ObjectInputStream ois = new ObjectInputStream(bais);
        DurationField result = (DurationField) ois.readObject();
        ois.close();
        
        assertEquals(test, result);
    }

}
","/*
 *  Copyright 2001-2005 Stephen Colebourne
 *
 *  Licensed under the Apache License, Version 2.0 (the ""License"");
 *  you may not use this file except in compliance with the License.
 *  You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 *  Unless required by applicable law or agreed to in writing, software
 *  distributed under the License is distributed on an ""AS IS"" BASIS,
 *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 *  See the License for the specific language governing permissions and
 *  limitations under the License.
 */
package org.joda.time.field;

import org.joda.time.DurationField;
import org.joda.time.DurationFieldType;

/**
 * Scales a DurationField such that it's unit millis becomes larger in
 * magnitude.
 * <p>
 * ScaledDurationField is thread-safe and immutable.
 *
 * @see PreciseDurationField
 *
 * @author Brian S O'Neill
 * @since 1.0
 */
public class ScaledDurationField extends DecoratedDurationField {

    private static final long serialVersionUID = -3205227092378684157L;

    private final int iScalar;

    /**
     * Constructor
     * 
     * @param field  the field to wrap, like ""year()"".
     * @param type  the type this field will actually use
     * @param scalar  scalar, such as 100 years in a century
     * @throws IllegalArgumentException if scalar is zero or one.
     */
    public ScaledDurationField(DurationField field, DurationFieldType type, int scalar) {
        super(field, type);
        if (scalar == 0 || scalar == 1) {
            throw new IllegalArgumentException(""The scalar must not be 0 or 1"");
        }
        iScalar = scalar;
    }

    @Override
    public int getValue(long duration) {
        return getWrappedField().getValue(duration) / iScalar;
    }

    @Override
    public long getValueAsLong(long duration) {
        return getWrappedField().getValueAsLong(duration) / iScalar;
    }

    @Override
    public int getValue(long duration, long instant) {
        return getWrappedField().getValue(duration, instant) / iScalar;
    }

    @Override
    public long getValueAsLong(long duration, long instant) {
        return getWrappedField().getValueAsLong(duration, instant) / iScalar;
    }

    @Override
    public long getMillis(int value) {
        long scaled = ((long) value) * ((long) iScalar);
        return getWrappedField().getMillis(scaled);
    }

    @Override
    public long getMillis(long value) {
        long scaled = FieldUtils.safeMultiply(value, iScalar);
        return getWrappedField().getMillis(scaled);
    }

    @Override
    public long getMillis(int value, long instant) {
        long scaled = ((long) value) * ((long) iScalar);
        return getWrappedField().getMillis(scaled, instant);
    }

    @Override
    public long getMillis(long value, long instant) {
        long scaled = FieldUtils.safeMultiply(value, iScalar);
        return getWrappedField().getMillis(scaled, instant);
    }

    @Override
    public long add(long instant, int value) {
        long scaled = ((long) value) * ((long) iScalar);
        return getWrappedField().add(instant, scaled);
    }

    @Override
    public long add(long instant, long value) {
        long scaled = FieldUtils.safeMultiply(value, iScalar);
        return getWrappedField().add(instant, scaled);
    }

    @Override
    public int getDifference(long minuendInstant, long subtrahendInstant) {
        return getWrappedField().getDifference(minuendInstant, subtrahendInstant) / iScalar;
    }

    @Override
    public long getDifferenceAsLong(long minuendInstant, long subtrahendInstant) {
        return getWrappedField().getDifferenceAsLong(minuendInstant, subtrahendInstant) / iScalar;
    }

    @Override
    public long getUnitMillis() {
        return getWrappedField().getUnitMillis() * iScalar;
    }

    //-----------------------------------------------------------------------
    /**
     * Returns the scalar applied, in the field's units.
     * 
     * @return the scalar
     */
    public int getScalar() {
        return iScalar;
    }

    /**
     * Compares this duration field to another.
     * Two fields are equal if of the same type and duration.
     * 
     * @param obj  the object to compare to
     * @return if equal
     */
    @Override
    public boolean equals(Object obj) {
        if (this == obj) {
            return true;
        } else if (obj instanceof ScaledDurationField) {
            ScaledDurationField other = (ScaledDurationField) obj;
            return (getWrappedField().equals(other.getWrappedField())) &&
                   (getType() == other.getType()) &&
                   (iScalar == other.iScalar);
        }
        return false;
    }

    /**
     * Gets a hash code for this instance.
     * 
     * @return a suitable hashcode
     */
    @Override
    public int hashCode() {
        long scalar = iScalar;
        int hash = (int) (scalar ^ (scalar >>> 32));
        hash += getType().hashCode();
        hash += getWrappedField().hashCode();
        return hash;
    }

}
","['Assertion Roulette', 'Exception Catching Throwing', 'General Fixture', 'Redundant Assertion', 'Sensitive Equality']","['Assertion Roulette', 'Magic Number Test']",1,4,1,12
8134_27.0_querydsl_union5,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/8134_test.java,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/8134_actual.java,"package com.querydsl.sql;

import static com.querydsl.core.Target.*;
import static com.querydsl.sql.Constants.employee;
import static org.junit.Assert.*;

import java.io.IOException;
import java.sql.SQLException;
import java.util.Arrays;
import java.util.Collections;
import java.util.List;

import org.junit.Ignore;
import org.junit.Test;

import com.mysema.commons.lang.CloseableIterator;
import com.querydsl.core.Tuple;
import com.querydsl.core.testutil.ExcludeIn;
import com.querydsl.core.types.Expression;
import com.querydsl.core.types.Projections;
import com.querydsl.core.types.SubQueryExpression;
import com.querydsl.core.types.dsl.Expressions;
import com.querydsl.core.types.dsl.NumberPath;
import com.querydsl.sql.domain.Employee;
import com.querydsl.sql.domain.QEmployee;

public class UnionBase extends AbstractBaseTest {

    @SuppressWarnings(""unchecked"")
    @Test
    @ExcludeIn({MYSQL, TERADATA})
    public void in_union() {
        assertNotNull(query().from(employee)
                .where(employee.id.in(
                        query().union(query().select(Expressions.ONE),
                                query().select(Expressions.TWO))))
                .select(Expressions.ONE).fetchFirst());
    }

    @Test
    @SuppressWarnings(""unchecked"")
    @ExcludeIn(FIREBIRD) // order is not properly supported
    public void union() throws SQLException {
        SubQueryExpression<Integer> sq1 = query().from(employee).select(employee.id.max().as(""ID""));
        SubQueryExpression<Integer> sq2 = query().from(employee).select(employee.id.min().as(""ID""));
        assertEquals(
                Arrays.asList(query().select(employee.id.min()).from(employee).fetchFirst(),
                        query().select(employee.id.max()).from(employee).fetchFirst()),
                query().union(sq1, sq2).orderBy(employee.id.asc()).fetch());
    }

    @Test
    @SuppressWarnings(""unchecked"")
    public void union_list() throws SQLException {
        SubQueryExpression<Integer> sq1 = query().from(employee).select(employee.id.max());
        SubQueryExpression<Integer> sq2 = query().from(employee).select(employee.id.min());
        assertEquals(
                query().union(sq1, sq2).fetch(),
                query().union(sq1, sq2).list());
    }

    @Test
    @SuppressWarnings(""unchecked"")
    public void union_all() {
        SubQueryExpression<Integer> sq1 = query().from(employee).select(employee.id.max());
        SubQueryExpression<Integer> sq2 = query().from(employee).select(employee.id.min());
        List<Integer> list = query().unionAll(sq1, sq2).fetch();
        assertFalse(list.isEmpty());
    }

    @SuppressWarnings(""unchecked"")
    @Test
    public void union_multiple_columns() throws SQLException {
        SubQueryExpression<Tuple> sq1 = query().from(employee).select(employee.firstname, employee.lastname);
        SubQueryExpression<Tuple> sq2 = query().from(employee).select(employee.lastname, employee.firstname);
        List<Tuple> list = query().union(sq1, sq2).fetch();
        assertFalse(list.isEmpty());
        for (Tuple row : list) {
            assertNotNull(row.get(0, Object.class));
            assertNotNull(row.get(1, Object.class));
        }
    }

    @SuppressWarnings(""unchecked"")
    @Test
    @ExcludeIn(DERBY)
    public void union_multiple_columns2() throws SQLException {
        SubQueryExpression<Tuple> sq1 = query().from(employee).select(employee.firstname, employee.lastname);
        SubQueryExpression<Tuple> sq2 = query().from(employee).select(employee.firstname, employee.lastname);
        SQLQuery<?> query = query();
        query.union(sq1, sq2);
        List<String> list = query.select(employee.firstname).fetch();
        assertFalse(list.isEmpty());
        for (String row : list) {
            assertNotNull(row);
        }
    }

    @SuppressWarnings(""unchecked"")
    @Test
    @ExcludeIn(DERBY)
    public void union_multiple_columns3() throws SQLException {
        SubQueryExpression<Tuple> sq1 = query().from(employee).select(employee.firstname, employee.lastname);
        SubQueryExpression<Tuple> sq2 = query().from(employee).select(employee.firstname, employee.lastname);
        SQLQuery<?> query = query();
        query.union(sq1, sq2);
        List<Tuple> list = query.select(employee.lastname, employee.firstname).fetch();
        assertFalse(list.isEmpty());
        for (Tuple row : list) {
            System.out.println(row.get(0, String.class) + "" "" + row.get(1, String.class));
        }
    }

    @Test
    @SuppressWarnings(""unchecked"")
    public void union_empty_result() throws SQLException {
        SubQueryExpression<Integer> sq1 = query().from(employee).where(employee.firstname.eq(""XXX"")).select(employee.id);
        SubQueryExpression<Integer> sq2 = query().from(employee).where(employee.firstname.eq(""YYY"")).select(employee.id);
        List<Integer> list = query().union(sq1, sq2).fetch();
        assertTrue(list.isEmpty());
    }

    @Test
    @SuppressWarnings(""unchecked"")
    public void union2() throws SQLException {
        List<Integer> list = query().union(
                query().from(employee).select(employee.id.max()),
                query().from(employee).select(employee.id.min())).fetch();
        assertFalse(list.isEmpty());

    }

    @Test
    @SuppressWarnings(""unchecked"")
    public void union3() throws SQLException {
        SubQueryExpression<Tuple> sq3 = query().from(employee).select(new Expression[]{employee.id.max()});
        SubQueryExpression<Tuple> sq4 = query().from(employee).select(new Expression[]{employee.id.min()});
        List<Tuple> list2 = query().union(sq3, sq4).fetch();
        assertFalse(list2.isEmpty());
    }

    @SuppressWarnings(""unchecked"")
    @Test
    @ExcludeIn({DERBY})
    public void union4() {
        SubQueryExpression<Tuple> sq1 = query().from(employee).select(employee.id, employee.firstname);
        SubQueryExpression<Tuple> sq2 = query().from(employee).select(employee.id, employee.firstname);
        assertEquals(1, query().union(employee, sq1, sq2).select(employee.id.count()).fetch().size());
    }

    // FIXME for CUBRID
    // Teradata: The ORDER BY clause must contain only integer constants.
    @SuppressWarnings(""unchecked"")
    @Test
    @ExcludeIn({DERBY, CUBRID, FIREBIRD, TERADATA})
    @Ignore // FIXME
    public void union5() {
        /* (select e.ID, e.FIRSTNAME, superior.ID as sup_id, superior.FIRSTNAME as sup_name
         * from EMPLOYEE e join EMPLOYEE superior on e.SUPERIOR_ID = superior.ID)
         * union
         * (select e.ID, e.FIRSTNAME, null, null from EMPLOYEE e)
         * order by ID asc
         */
        QEmployee superior = new QEmployee(""superior"");
        SubQueryExpression<Tuple> sq1 = query().from(employee)
                .join(employee.superiorIdKey, superior)
                .select(employee.id, employee.firstname, superior.id.as(""sup_id""), superior.firstname.as(""sup_name""));
        SubQueryExpression<Tuple> sq2 = query().from(employee)
                .select(employee.id, employee.firstname, null, null);
        List<Tuple> results = query().union(sq1, sq2).orderBy(employee.id.asc()).fetch();
        for (Tuple result : results) {
            System.err.println(Collections.singletonList(result));
        }
    }

    @Test
    @ExcludeIn({FIREBIRD, TERADATA}) // The ORDER BY clause must contain only integer constants.
    @SuppressWarnings(""unchecked"")
    public void union_with_order() throws SQLException {
        SubQueryExpression<Integer> sq1 = query().from(employee).select(employee.id);
        SubQueryExpression<Integer> sq2 = query().from(employee).select(employee.id);
        List<Integer> list = query().union(sq1, sq2).orderBy(employee.id.asc()).fetch();
        assertFalse(list.isEmpty());
    }

    @SuppressWarnings(""unchecked"")
    @Test
    @ExcludeIn(FIREBIRD)
    public void union_multi_column_projection_list() throws IOException {
        SubQueryExpression<Tuple> sq1 = query().from(employee).select(employee.id.max(), employee.id.max().subtract(1));
        SubQueryExpression<Tuple> sq2 = query().from(employee).select(employee.id.min(), employee.id.min().subtract(1));

        List<Tuple> list = query().union(sq1, sq2).list();
        assertEquals(2, list.size());
        assertTrue(list.get(0) != null);
        assertTrue(list.get(1) != null);
    }

    @SuppressWarnings(""unchecked"")
    @Test
    @ExcludeIn(FIREBIRD)
    public void union_multi_column_projection_iterate() throws IOException {
        SubQueryExpression<Tuple> sq1 = query().from(employee).select(employee.id.max(), employee.id.max().subtract(1));
        SubQueryExpression<Tuple> sq2 = query().from(employee).select(employee.id.min(), employee.id.min().subtract(1));

        try (CloseableIterator<Tuple> iterator = query().union(sq1, sq2).iterate()) {
            assertTrue(iterator.hasNext());
            assertTrue(iterator.next() != null);
            assertTrue(iterator.next() != null);
            assertFalse(iterator.hasNext());
        }
    }

    @SuppressWarnings(""unchecked"")
    @Test
    public void union_single_column_projections_list() throws IOException {
        SubQueryExpression<Integer> sq1 = query().from(employee).select(employee.id.max());
        SubQueryExpression<Integer> sq2 = query().from(employee).select(employee.id.min());

        List<Integer> list = query().union(sq1, sq2).list();
        assertEquals(2, list.size());
        assertTrue(list.get(0) != null);
        assertTrue(list.get(1) != null);
    }

    @SuppressWarnings(""unchecked"")
    @Test
    public void union_single_column_projections_iterate() throws IOException {
        SubQueryExpression<Integer> sq1 = query().from(employee).select(employee.id.max());
        SubQueryExpression<Integer> sq2 = query().from(employee).select(employee.id.min());

        try (CloseableIterator<Integer> iterator = query().union(sq1, sq2).iterate()) {
            assertTrue(iterator.hasNext());
            assertTrue(iterator.next() != null);
            assertTrue(iterator.next() != null);
            assertFalse(iterator.hasNext());
        }
    }

    @SuppressWarnings(""unchecked"")
    @Test
    public void union_factoryExpression() {
        SubQueryExpression<Employee> sq1 = query().from(employee)
                .select(Projections.constructor(Employee.class, employee.id));
        SubQueryExpression<Employee> sq2 = query().from(employee)
                .select(Projections.constructor(Employee.class, employee.id));
        List<Employee> employees = query().union(sq1, sq2).list();
        for (Employee employee : employees) {
            assertNotNull(employee);
        }
    }

    @SuppressWarnings(""unchecked"")
    @Test
    @ExcludeIn({DERBY, CUBRID})
    public void union_clone() {
        NumberPath<Integer> idAlias = Expressions.numberPath(Integer.class, ""id"");
        SubQueryExpression<Employee> sq1 = query().from(employee)
                .select(Projections.constructor(Employee.class, employee.id.as(idAlias)));
        SubQueryExpression<Employee> sq2 = query().from(employee)
                .select(Projections.constructor(Employee.class, employee.id.as(idAlias)));

        SQLQuery<?> query = query();
        query.union(sq1, sq2);
        assertEquals(10, query.clone().select(idAlias).fetch().size());
    }

}
","package com.querydsl.sql;

import static com.querydsl.core.Target.*;
import static com.querydsl.sql.Constants.employee;
import static org.junit.Assert.*;

import java.io.IOException;
import java.sql.SQLException;
import java.util.Arrays;
import java.util.Collections;
import java.util.List;

import org.junit.Ignore;
import org.junit.Test;

import com.mysema.commons.lang.CloseableIterator;
import com.querydsl.core.Tuple;
import com.querydsl.core.testutil.ExcludeIn;
import com.querydsl.core.types.Expression;
import com.querydsl.core.types.Projections;
import com.querydsl.core.types.SubQueryExpression;
import com.querydsl.core.types.dsl.Expressions;
import com.querydsl.core.types.dsl.NumberPath;
import com.querydsl.sql.domain.Employee;
import com.querydsl.sql.domain.QEmployee;

public class UnionBase extends AbstractBaseTest {

    @SuppressWarnings(""unchecked"")
    @Test
    @ExcludeIn({MYSQL, TERADATA})
    public void in_union() {
        assertNotNull(query().from(employee)
                .where(employee.id.in(
                        query().union(query().select(Expressions.ONE),
                                query().select(Expressions.TWO))))
                .select(Expressions.ONE).fetchFirst());
    }

    @Test
    @SuppressWarnings(""unchecked"")
    @ExcludeIn(FIREBIRD) // order is not properly supported
    public void union() throws SQLException {
        SubQueryExpression<Integer> sq1 = query().from(employee).select(employee.id.max().as(""ID""));
        SubQueryExpression<Integer> sq2 = query().from(employee).select(employee.id.min().as(""ID""));
        assertEquals(
                Arrays.asList(query().select(employee.id.min()).from(employee).fetchFirst(),
                        query().select(employee.id.max()).from(employee).fetchFirst()),
                query().union(sq1, sq2).orderBy(employee.id.asc()).fetch());
    }

    @Test
    @SuppressWarnings(""unchecked"")
    public void union_list() throws SQLException {
        SubQueryExpression<Integer> sq1 = query().from(employee).select(employee.id.max());
        SubQueryExpression<Integer> sq2 = query().from(employee).select(employee.id.min());
        assertEquals(
                query().union(sq1, sq2).fetch(),
                query().union(sq1, sq2).list());
    }

    @Test
    @SuppressWarnings(""unchecked"")
    public void union_all() {
        SubQueryExpression<Integer> sq1 = query().from(employee).select(employee.id.max());
        SubQueryExpression<Integer> sq2 = query().from(employee).select(employee.id.min());
        List<Integer> list = query().unionAll(sq1, sq2).fetch();
        assertFalse(list.isEmpty());
    }

    @SuppressWarnings(""unchecked"")
    @Test
    public void union_multiple_columns() throws SQLException {
        SubQueryExpression<Tuple> sq1 = query().from(employee).select(employee.firstname, employee.lastname);
        SubQueryExpression<Tuple> sq2 = query().from(employee).select(employee.lastname, employee.firstname);
        List<Tuple> list = query().union(sq1, sq2).fetch();
        assertFalse(list.isEmpty());
        for (Tuple row : list) {
            assertNotNull(row.get(0, Object.class));
            assertNotNull(row.get(1, Object.class));
        }
    }

    @SuppressWarnings(""unchecked"")
    @Test
    @ExcludeIn(DERBY)
    public void union_multiple_columns2() throws SQLException {
        SubQueryExpression<Tuple> sq1 = query().from(employee).select(employee.firstname, employee.lastname);
        SubQueryExpression<Tuple> sq2 = query().from(employee).select(employee.firstname, employee.lastname);
        SQLQuery<?> query = query();
        query.union(sq1, sq2);
        List<String> list = query.select(employee.firstname).fetch();
        assertFalse(list.isEmpty());
        for (String row : list) {
            assertNotNull(row);
        }
    }

    @SuppressWarnings(""unchecked"")
    @Test
    @ExcludeIn(DERBY)
    public void union_multiple_columns3() throws SQLException {
        SubQueryExpression<Tuple> sq1 = query().from(employee).select(employee.firstname, employee.lastname);
        SubQueryExpression<Tuple> sq2 = query().from(employee).select(employee.firstname, employee.lastname);
        SQLQuery<?> query = query();
        query.union(sq1, sq2);
        List<Tuple> list = query.select(employee.lastname, employee.firstname).fetch();
        assertFalse(list.isEmpty());
        for (Tuple row : list) {
            System.out.println(row.get(0, String.class) + "" "" + row.get(1, String.class));
        }
    }

    @Test
    @SuppressWarnings(""unchecked"")
    public void union_empty_result() throws SQLException {
        SubQueryExpression<Integer> sq1 = query().from(employee).where(employee.firstname.eq(""XXX"")).select(employee.id);
        SubQueryExpression<Integer> sq2 = query().from(employee).where(employee.firstname.eq(""YYY"")).select(employee.id);
        List<Integer> list = query().union(sq1, sq2).fetch();
        assertTrue(list.isEmpty());
    }

    @Test
    @SuppressWarnings(""unchecked"")
    public void union2() throws SQLException {
        List<Integer> list = query().union(
                query().from(employee).select(employee.id.max()),
                query().from(employee).select(employee.id.min())).fetch();
        assertFalse(list.isEmpty());

    }

    @Test
    @SuppressWarnings(""unchecked"")
    public void union3() throws SQLException {
        SubQueryExpression<Tuple> sq3 = query().from(employee).select(new Expression[]{employee.id.max()});
        SubQueryExpression<Tuple> sq4 = query().from(employee).select(new Expression[]{employee.id.min()});
        List<Tuple> list2 = query().union(sq3, sq4).fetch();
        assertFalse(list2.isEmpty());
    }

    @SuppressWarnings(""unchecked"")
    @Test
    @ExcludeIn({DERBY})
    public void union4() {
        SubQueryExpression<Tuple> sq1 = query().from(employee).select(employee.id, employee.firstname);
        SubQueryExpression<Tuple> sq2 = query().from(employee).select(employee.id, employee.firstname);
        assertEquals(1, query().union(employee, sq1, sq2).select(employee.id.count()).fetch().size());
    }

    // FIXME for CUBRID
    // Teradata: The ORDER BY clause must contain only integer constants.
    @SuppressWarnings(""unchecked"")
    @Test
    @ExcludeIn({DERBY, CUBRID, FIREBIRD, TERADATA})
    @Ignore // FIXME
    public void union5() {
        /* (select e.ID, e.FIRSTNAME, superior.ID as sup_id, superior.FIRSTNAME as sup_name
         * from EMPLOYEE e join EMPLOYEE superior on e.SUPERIOR_ID = superior.ID)
         * union
         * (select e.ID, e.FIRSTNAME, null, null from EMPLOYEE e)
         * order by ID asc
         */
        QEmployee superior = new QEmployee(""superior"");
        SubQueryExpression<Tuple> sq1 = query().from(employee)
                .join(employee.superiorIdKey, superior)
                .select(employee.id, employee.firstname, superior.id.as(""sup_id""), superior.firstname.as(""sup_name""));
        SubQueryExpression<Tuple> sq2 = query().from(employee)
                .select(employee.id, employee.firstname, null, null);
        List<Tuple> results = query().union(sq1, sq2).orderBy(employee.id.asc()).fetch();
        for (Tuple result : results) {
            System.err.println(Collections.singletonList(result));
        }
    }

    @Test
    @ExcludeIn({FIREBIRD, TERADATA}) // The ORDER BY clause must contain only integer constants.
    @SuppressWarnings(""unchecked"")
    public void union_with_order() throws SQLException {
        SubQueryExpression<Integer> sq1 = query().from(employee).select(employee.id);
        SubQueryExpression<Integer> sq2 = query().from(employee).select(employee.id);
        List<Integer> list = query().union(sq1, sq2).orderBy(employee.id.asc()).fetch();
        assertFalse(list.isEmpty());
    }

    @SuppressWarnings(""unchecked"")
    @Test
    @ExcludeIn(FIREBIRD)
    public void union_multi_column_projection_list() throws IOException {
        SubQueryExpression<Tuple> sq1 = query().from(employee).select(employee.id.max(), employee.id.max().subtract(1));
        SubQueryExpression<Tuple> sq2 = query().from(employee).select(employee.id.min(), employee.id.min().subtract(1));

        List<Tuple> list = query().union(sq1, sq2).list();
        assertEquals(2, list.size());
        assertTrue(list.get(0) != null);
        assertTrue(list.get(1) != null);
    }

    @SuppressWarnings(""unchecked"")
    @Test
    @ExcludeIn(FIREBIRD)
    public void union_multi_column_projection_iterate() throws IOException {
        SubQueryExpression<Tuple> sq1 = query().from(employee).select(employee.id.max(), employee.id.max().subtract(1));
        SubQueryExpression<Tuple> sq2 = query().from(employee).select(employee.id.min(), employee.id.min().subtract(1));

        try (CloseableIterator<Tuple> iterator = query().union(sq1, sq2).iterate()) {
            assertTrue(iterator.hasNext());
            assertTrue(iterator.next() != null);
            assertTrue(iterator.next() != null);
            assertFalse(iterator.hasNext());
        }
    }

    @SuppressWarnings(""unchecked"")
    @Test
    public void union_single_column_projections_list() throws IOException {
        SubQueryExpression<Integer> sq1 = query().from(employee).select(employee.id.max());
        SubQueryExpression<Integer> sq2 = query().from(employee).select(employee.id.min());

        List<Integer> list = query().union(sq1, sq2).list();
        assertEquals(2, list.size());
        assertTrue(list.get(0) != null);
        assertTrue(list.get(1) != null);
    }

    @SuppressWarnings(""unchecked"")
    @Test
    public void union_single_column_projections_iterate() throws IOException {
        SubQueryExpression<Integer> sq1 = query().from(employee).select(employee.id.max());
        SubQueryExpression<Integer> sq2 = query().from(employee).select(employee.id.min());

        try (CloseableIterator<Integer> iterator = query().union(sq1, sq2).iterate()) {
            assertTrue(iterator.hasNext());
            assertTrue(iterator.next() != null);
            assertTrue(iterator.next() != null);
            assertFalse(iterator.hasNext());
        }
    }

    @SuppressWarnings(""unchecked"")
    @Test
    public void union_factoryExpression() {
        SubQueryExpression<Employee> sq1 = query().from(employee)
                .select(Projections.constructor(Employee.class, employee.id));
        SubQueryExpression<Employee> sq2 = query().from(employee)
                .select(Projections.constructor(Employee.class, employee.id));
        List<Employee> employees = query().union(sq1, sq2).list();
        for (Employee employee : employees) {
            assertNotNull(employee);
        }
    }

    @SuppressWarnings(""unchecked"")
    @Test
    @ExcludeIn({DERBY, CUBRID})
    public void union_clone() {
        NumberPath<Integer> idAlias = Expressions.numberPath(Integer.class, ""id"");
        SubQueryExpression<Employee> sq1 = query().from(employee)
                .select(Projections.constructor(Employee.class, employee.id.as(idAlias)));
        SubQueryExpression<Employee> sq2 = query().from(employee)
                .select(Projections.constructor(Employee.class, employee.id.as(idAlias)));

        SQLQuery<?> query = query();
        query.union(sq1, sq2);
        assertEquals(10, query.clone().select(idAlias).fetch().size());
    }

}
","['Assertion Roulette', 'Conditional Test Logic', 'Print Statement', 'Unknown Test']","['Assertion Roulette', 'Magic Number Test', 'Lazy Test', 'Redundant Print', 'Resource Optimism', 'Sleepy Test', 'Sensitive Equality', 'Unknown Test']",6,2,2,11
10684_66.0_sawmill_testparseprocessorexecutionstep,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/10684_test.java,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/10684_actual.java,"package io.logz.sawmill;

import io.logz.sawmill.conditions.AndCondition;
import io.logz.sawmill.conditions.TestCondition;
import io.logz.sawmill.parser.ConditionDefinition;
import io.logz.sawmill.parser.ConditionalExecutionStepDefinition;
import io.logz.sawmill.parser.ExecutionStepDefinition;
import io.logz.sawmill.parser.ExecutionStepsParser;
import io.logz.sawmill.parser.ProcessorDefinition;
import io.logz.sawmill.parser.ProcessorExecutionStepDefinition;
import io.logz.sawmill.processors.AddTagProcessor;
import io.logz.sawmill.processors.TestProcessor;
import org.apache.commons.lang3.RandomStringUtils;
import org.junit.Before;
import org.junit.Test;

import java.util.Arrays;
import java.util.Collections;
import java.util.List;
import java.util.Optional;

import static io.logz.sawmill.utilities.JsonUtils.createList;
import static io.logz.sawmill.utilities.JsonUtils.createMap;
import static io.logz.sawmill.utils.FactoryUtils.defaultConditionFactoryRegistry;
import static io.logz.sawmill.utils.FactoryUtils.defaultProcessorFactoryRegistry;
import static org.assertj.core.api.AssertionsForInterfaceTypes.assertThat;

public class ExecutionStepsParserTest {

    private ExecutionStepsParser executionStepsParser;

    @Before
    public void init() {
        defaultProcessorFactoryRegistry.register(""test"", new TestProcessor.Factory());
        defaultConditionFactoryRegistry.register(""testCondition"", new TestCondition.Factory());

        executionStepsParser = new ExecutionStepsParser(defaultProcessorFactoryRegistry, defaultConditionFactoryRegistry);
    }

    @Test
    public void testParseProcessorExecutionStep() {
        List<ExecutionStepDefinition> executionStepDefinitionList = Collections.singletonList(
                createAddTagStepDefinition()
        );

        List<ExecutionStep> executionSteps = executionStepsParser.parse(executionStepDefinitionList);
        assertThat(executionSteps.size()).isEqualTo(1);

        ProcessorExecutionStep processorExecutionStep = (ProcessorExecutionStep) executionSteps.get(0);
        assertThat(processorExecutionStep.getProcessor()).isInstanceOf(AddTagProcessor.class);

        Optional<List<ExecutionStep>> onFailureExecutionSteps = processorExecutionStep.getOnFailureExecutionSteps();
        assertThat(onFailureExecutionSteps.isPresent()).isFalse();
    }

    @Test
    public void testParseOnFailureExecutionStep() {
        List<ExecutionStepDefinition> executionStepDefinitionList = Collections.singletonList(
                createAddTagStepDefinition(Collections.singletonList(
                        createAddTagStepDefinition()
                ))
        );

        List<ExecutionStep> executionSteps = executionStepsParser.parse(executionStepDefinitionList);

        ProcessorExecutionStep processorExecutionStep = (ProcessorExecutionStep) executionSteps.get(0);
        List<ExecutionStep> onFailureExecutionSteps = processorExecutionStep.getOnFailureExecutionSteps().get();

        assertThat(onFailureExecutionSteps.size()).isEqualTo(1);

        ProcessorExecutionStep onFailureExecutionStep = (ProcessorExecutionStep) onFailureExecutionSteps.get(0);
        assertThat(onFailureExecutionStep.getProcessor()).isInstanceOf(AddTagProcessor.class);
    }

    @Test
    public void testParseOnSuccessExecutionStep() {
        List<ExecutionStepDefinition> executionStepDefinitionList = Collections.singletonList(
                createAddTagStepDefinition(null, null, Collections.singletonList(createAddTagStepDefinition()))
        );

        List<ExecutionStep> executionSteps = executionStepsParser.parse(executionStepDefinitionList);

        ProcessorExecutionStep processorExecutionStep = (ProcessorExecutionStep) executionSteps.get(0);
        List<ExecutionStep> onSuccessExecutionSteps = processorExecutionStep.getOnSuccessExecutionSteps().get();

        assertThat(onSuccessExecutionSteps.size()).isEqualTo(1);

        ProcessorExecutionStep onSuccessExecutionStep = (ProcessorExecutionStep) onSuccessExecutionSteps.get(0);
        assertThat(onSuccessExecutionStep.getProcessor()).isInstanceOf(AddTagProcessor.class);
    }

    @Test
    public void testParseConditionalExecutionStep() {
        List<ExecutionStepDefinition> executionStepDefinitionList = Collections.singletonList(
                new ConditionalExecutionStepDefinition(
                        createAndExistsConditionDefinition(),
                        Collections.singletonList(
                                createAddTagStepDefinition()
                        ),
                        Collections.singletonList(
                                createAddTagStepDefinition()
                        )));

        List<ExecutionStep> executionSteps = executionStepsParser.parse(executionStepDefinitionList);
        assertThat(executionSteps.size()).isEqualTo(1);

        ConditionalExecutionStep conditionalExecutionStep = (ConditionalExecutionStep) executionSteps.get(0);

        assertThat(conditionalExecutionStep.getCondition()).isInstanceOf(AndCondition.class);

        ProcessorExecutionStep onTrueExecutionStep = (ProcessorExecutionStep) conditionalExecutionStep.getOnTrue().get(0);
        assertThat(onTrueExecutionStep.getProcessor()).isInstanceOf(AddTagProcessor.class);

        ProcessorExecutionStep onFalseExecutionStep = (ProcessorExecutionStep) conditionalExecutionStep.getOnFalse().get(0);
        assertThat(onFalseExecutionStep.getProcessor()).isInstanceOf(AddTagProcessor.class);
    }

    @Test
    public void testDefaultProcessorName() {
        List<ExecutionStepDefinition> executionStepDefinitionList = Arrays.asList(
                new ConditionalExecutionStepDefinition(
                        createAndExistsConditionDefinition(),
                        Collections.singletonList(
                                createAddTagStepDefinition()
                        ),
                        Collections.singletonList(
                                createAddTagStepDefinition()
                        )),
                createAddTagStepDefinition(Collections.singletonList(
                        createAddTagStepDefinition()
                ))
        );

        List<ExecutionStep> executionSteps = executionStepsParser.parse(executionStepDefinitionList);
        ConditionalExecutionStep conditionalExecutionStep = (ConditionalExecutionStep) executionSteps.get(0);

        ProcessorExecutionStep onTrueExecutionStep = (ProcessorExecutionStep) conditionalExecutionStep.getOnTrue().get(0);
        assertThat(onTrueExecutionStep.getProcessorName()).isEqualTo(""[addTag1]"");

        ProcessorExecutionStep onFalseExecutionStep = (ProcessorExecutionStep) conditionalExecutionStep.getOnFalse().get(0);
        assertThat(onFalseExecutionStep.getProcessorName()).isEqualTo(""[addTag2]"");

        ProcessorExecutionStep processorExecutionStep = (ProcessorExecutionStep) executionSteps.get(1);
        assertThat(processorExecutionStep.getProcessorName()).isEqualTo(""[addTag3]"");

        ProcessorExecutionStep onFailureExecutionStep = (ProcessorExecutionStep) processorExecutionStep.getOnFailureExecutionSteps().get().get(0);
        assertThat(onFailureExecutionStep.getProcessorName()).isEqualTo(""[addTag4]"");
    }

    @Test
    public void testProcessorName() {
        String processorName1 = RandomStringUtils.randomAlphanumeric(10);
        String processorName2 = RandomStringUtils.randomAlphanumeric(10);
        String processorName3 = RandomStringUtils.randomAlphanumeric(10);
        String processorName4 = RandomStringUtils.randomAlphanumeric(10);

        List<ExecutionStepDefinition> executionStepDefinitionList = Arrays.asList(
                new ConditionalExecutionStepDefinition(
                        createAndExistsConditionDefinition(),
                        Collections.singletonList(
                                createAddTagStepDefinition(processorName1)
                        ),
                        Collections.singletonList(
                                createAddTagStepDefinition(processorName2)
                        )),
                createAddTagStepDefinition(processorName3, Collections.singletonList(
                        createAddTagStepDefinition(processorName4)
                ))
        );

        List<ExecutionStep> executionSteps = executionStepsParser.parse(executionStepDefinitionList);
        ConditionalExecutionStep conditionalExecutionStep = (ConditionalExecutionStep) executionSteps.get(0);

        ProcessorExecutionStep onTrueExecutionStep = (ProcessorExecutionStep) conditionalExecutionStep.getOnTrue().get(0);
        assertThat(onTrueExecutionStep.getProcessorName()).isEqualTo(""[addTag1]"" + processorName1);

        ProcessorExecutionStep onFalseExecutionStep = (ProcessorExecutionStep) conditionalExecutionStep.getOnFalse().get(0);
        assertThat(onFalseExecutionStep.getProcessorName()).isEqualTo(""[addTag2]"" + processorName2);

        ProcessorExecutionStep processorExecutionStep = (ProcessorExecutionStep) executionSteps.get(1);
        assertThat(processorExecutionStep.getProcessorName()).isEqualTo(""[addTag3]"" + processorName3);

        ProcessorExecutionStep onFailureExecutionStep = (ProcessorExecutionStep) processorExecutionStep.getOnFailureExecutionSteps().get().get(0);
        assertThat(onFailureExecutionStep.getProcessorName()).isEqualTo(""[addTag4]"" + processorName4);
    }

    private ConditionDefinition createAndExistsConditionDefinition() {
        return new ConditionDefinition(""and"", createMap(""conditions"", createList(
                new ConditionDefinition(""exists"", createMap(
                        ""field"", ""field1""
                )),
                new ConditionDefinition(""exists"", createMap(
                        ""field"", ""field2""
                ))
        )));
    }

    private ProcessorExecutionStepDefinition createAddTagStepDefinition() {
        return createAddTagStepDefinition(null, null);
    }

    private ProcessorExecutionStepDefinition createAddTagStepDefinition(String name) {
        return createAddTagStepDefinition(name, null);
    }

    private ProcessorExecutionStepDefinition createAddTagStepDefinition(List<ExecutionStepDefinition> onFailureExecutionStepDefinitions) {
        return createAddTagStepDefinition(null, onFailureExecutionStepDefinitions);
    }

    private ProcessorExecutionStepDefinition createAddTagStepDefinition(String name, List<ExecutionStepDefinition> onFailureExecutionStepDefinitions) {
        return createAddTagStepDefinition(name, onFailureExecutionStepDefinitions, null);
    }

    private ProcessorExecutionStepDefinition createAddTagStepDefinition(String name,
                                                                        List<ExecutionStepDefinition> onFailureExecutionStepDefinitions,
                                                                        List<ExecutionStepDefinition> onSuccessExecutionStepDefinitions) {
        return new ProcessorExecutionStepDefinition(createAddTagProcessorDefinition(), name,
                onFailureExecutionStepDefinitions, onSuccessExecutionStepDefinitions);
    }

    private ProcessorDefinition createAddTagProcessorDefinition() {
        return new ProcessorDefinition(""addTag"", createMap(""tags"", createList(""tag1"", ""tag2"")));
    }

}","package io.logz.sawmill.parser;

import io.logz.sawmill.Condition;
import io.logz.sawmill.ConditionFactoryRegistry;
import io.logz.sawmill.ConditionalExecutionStep;
import io.logz.sawmill.ExecutionStep;
import io.logz.sawmill.Processor;
import io.logz.sawmill.ProcessorExecutionStep;
import io.logz.sawmill.ProcessorFactoryRegistry;

import java.util.Collections;
import java.util.List;
import java.util.Optional;
import java.util.stream.Collectors;

public class ExecutionStepsParser {
    private final ProcessorParser processorParser;
    private final ConditionParser conditionParser;

    public ExecutionStepsParser(ProcessorFactoryRegistry processorFactoryRegistry, ConditionFactoryRegistry conditionFactoryRegistry) {
        this.processorParser = new ProcessorParser(processorFactoryRegistry);
        this.conditionParser = new ConditionParser(conditionFactoryRegistry);
    }

    public List<ExecutionStep> parse(List<ExecutionStepDefinition> executionStepDefinitionList) {
        return parse(executionStepDefinitionList, new IdGenerator());
    }

    private List<ExecutionStep> parse(List<ExecutionStepDefinition> executionStepDefinitionList, IdGenerator idGenerator) {
        return executionStepDefinitionList.stream().map(stepDefinition -> parse(stepDefinition, idGenerator)).collect(Collectors.toList());
    }

    private ExecutionStep parse(ExecutionStepDefinition executionStepDefinition, IdGenerator idGenerator) {
        if (executionStepDefinition instanceof ConditionalExecutionStepDefinition) {
            ConditionalExecutionStepDefinition conditionalExecutionStep = (ConditionalExecutionStepDefinition) executionStepDefinition;
            return parseConditionalExecutionStep(conditionalExecutionStep, idGenerator);
        } else if (executionStepDefinition instanceof ProcessorExecutionStepDefinition) {
            ProcessorExecutionStepDefinition processorExecutionStep = (ProcessorExecutionStepDefinition) executionStepDefinition;
            return parseProcessorExecutionStep(processorExecutionStep, idGenerator);
        }

        throw new RuntimeException(""Unsupported execution step definition: "" + executionStepDefinition.getClass());
    }

    private ConditionalExecutionStep parseConditionalExecutionStep(ConditionalExecutionStepDefinition conditionalExecutionStepDefinition, IdGenerator idGenerator) {
        Condition parsedCondition = conditionParser.parse(conditionalExecutionStepDefinition.getConditionDefinition());

        List<ExecutionStep> parsedOnTrue = parse(conditionalExecutionStepDefinition.getOnTrue(), idGenerator);

        Optional<List<ExecutionStepDefinition>> optionalOnFalse = conditionalExecutionStepDefinition.getOnFalse();
        List<ExecutionStep> parsedOnFalse = optionalOnFalse.isPresent() ? parse(optionalOnFalse.get(), idGenerator) : Collections.emptyList();

        return new ConditionalExecutionStep(parsedCondition, parsedOnTrue, parsedOnFalse);
    }

    private ProcessorExecutionStep parseProcessorExecutionStep(ProcessorExecutionStepDefinition processorExecutionStepDefinition, IdGenerator idGenerator) {
        String processorId = getProcessorName(processorExecutionStepDefinition, idGenerator);
        Processor processor = processorParser.parse(processorExecutionStepDefinition.getProcessorDefinition());
        Optional<List<ExecutionStepDefinition>> onFailureExecutionStepDefinitions =
                processorExecutionStepDefinition.getOnFailureExecutionStepDefinitionList();
        List<ExecutionStep> failureExecutionSteps = onFailureExecutionStepDefinitions.isPresent() ? parse(onFailureExecutionStepDefinitions.get(), idGenerator) : null;

        Optional<List<ExecutionStepDefinition>> onSuccessExecutionStepDefinitions =
                processorExecutionStepDefinition.getOnSuccessExecutionStepDefinitionList();
        List<ExecutionStep> successExecutionSteps = onSuccessExecutionStepDefinitions.isPresent() ? parse(onSuccessExecutionStepDefinitions.get(), idGenerator) : null;

        return new ProcessorExecutionStep(processorId, processor, failureExecutionSteps, successExecutionSteps);
    }

    private String getProcessorName(ProcessorExecutionStepDefinition processorExecutionStepDefinition, IdGenerator idGenerator) {
        String type = processorExecutionStepDefinition.getProcessorDefinition().getType();
        String prefix = ""["" + type + idGenerator.getNextId() + ""]"";
        Optional<String> optionalName = processorExecutionStepDefinition.getName();

        return prefix + optionalName.orElse("""");
    }

    private class IdGenerator {
        private int id;

        public IdGenerator() {
            id = 1;
        }

        public int getNextId() {
            return id++;
        }
    }

}
",['Assertion Roulette'],"['Assertion Roulette', 'Eager Test']",1,0,1,15
51070_43.0_timely_testdownsamplecombining,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/51070_test.java,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/51070_actual.java,"package timely.server.sample.iterators;

import static org.junit.Assert.assertEquals;
import static org.junit.Assert.assertNotNull;
import static org.junit.Assert.assertTrue;

import java.util.*;
import java.util.Map.Entry;

import org.apache.accumulo.core.client.IteratorSetting;
import org.apache.accumulo.core.data.ColumnUpdate;
import org.apache.accumulo.core.data.Key;
import org.apache.accumulo.core.data.Mutation;
import org.apache.accumulo.core.data.Range;
import org.apache.accumulo.core.data.Value;
import org.apache.accumulo.core.iterators.SortedKeyValueIterator;
import org.apache.accumulo.core.iteratorsImpl.system.SortedMapIterator;
import org.junit.Before;
import org.junit.Test;

import timely.accumulo.MetricAdapter;
import timely.model.Metric;
import timely.model.ObjectSizeOf;
import timely.model.Tag;
import timely.server.sample.Downsample;
import timely.server.sample.Sample;
import timely.server.sample.aggregators.Avg;

public class DownsampleIteratorTest {

    final private SortedMap<Key,Value> testData1 = new TreeMap<>();
    final private SortedMap<Key,Value> testData2 = new TreeMap<>();

    @Before
    public void createTestData() {
        createTestData1();
        createTestData2();
    }

    private void createTestData1() {
        List<Tag> tags = Collections.singletonList(new Tag(""host"", ""host1""));

        for (long i = 0; i < 1000; i += 100) {
            Metric m = new Metric(""sys.loadAvg"", i, .2, tags);
            Mutation mutation = MetricAdapter.toMutation(m);
            for (ColumnUpdate cu : mutation.getUpdates()) {
                Key key = new Key(mutation.getRow(), cu.getColumnFamily(), cu.getColumnQualifier(), cu.getColumnVisibility(), cu.getTimestamp());
                testData1.put(key, new Value(cu.getValue()));
            }
        }
    }

    void put(Map<Key,Value> testData, Metric m) {
        Mutation mutation = MetricAdapter.toMutation(m);
        for (ColumnUpdate cu : mutation.getUpdates()) {
            Key key = new Key(mutation.getRow(), cu.getColumnFamily(), cu.getColumnQualifier(), cu.getColumnVisibility(), cu.getTimestamp());
            testData.put(key, new Value(cu.getValue()));
        }
    }

    private void createTestData2() {
        List<Tag> tags = Collections.singletonList(new Tag(""host"", ""host1""));
        List<Tag> tags2 = Collections.singletonList(new Tag(""host"", ""host2""));

        for (long i = 0; i < 1000; i += 100) {
            put(testData2, new Metric(""sys.loadAvg"", i, .2, tags));
            put(testData2, new Metric(""sys.loadAvg"", i + 50, .5, tags2));
        }
    }

    @Test
    public void simpleGetOneSample() throws Exception {
        // check that data gets pulled out
        DownsampleIterator iter = new DownsampleIterator();
        Map<Set<Tag>,Downsample> samples = runQuery(iter, testData1, 100, -1);
        assertEquals(1, samples.size());
        for (Entry<Set<Tag>,Downsample> entry : samples.entrySet()) {
            Set<Tag> tags = entry.getKey();
            assertEquals(1, tags.size());
            assertEquals(Collections.singleton(new Tag(""host"", ""host1"")), tags);
            long ts = 0;
            for (Sample sample : entry.getValue()) {
                assertEquals(ts, sample.getTimestamp());
                ts += 100;
                assertEquals(0.2, sample.getValue(), 0.0001);
            }
            assertEquals(1000, ts);
        }
    }

    @Test
    public void simpleGetTwoSamples() throws Exception {
        DownsampleIterator iter = new DownsampleIterator();
        Map<Set<Tag>,Downsample> samples = runQuery(iter, testData2, 100, -1);
        assertEquals(2, samples.size());
        for (Tag tag : new Tag[] {new Tag(""host"", ""host1""), new Tag(""host"", ""host2"")}) {
            Downsample dsample = samples.get(Collections.singleton(tag));
            assertNotNull(dsample);
            long ts = 0;
            double value = .2;
            if (tag.getValue().equals(""host2"")) {
                value = .5;
            }
            int count = 0;
            for (Sample sample : dsample) {
                assertEquals(ts, sample.getTimestamp());
                ts += 100;
                assertEquals(value, sample.getValue(), 0.0001);
                count++;
            }
            assertEquals(10, count);
        }
    }

    @Test
    public void simpleTestDownsampling() throws Exception {
        DownsampleIterator iter = new DownsampleIterator();
        Map<Set<Tag>,Downsample> samples = runQuery(iter, testData2, 200, -1);
        assertEquals(2, samples.size());
        for (Tag tag : new Tag[] {new Tag(""host"", ""host1""), new Tag(""host"", ""host2"")}) {
            Downsample dsample = samples.get(Collections.singleton(tag));
            assertNotNull(dsample);
            long ts = 0;
            double value = .2;
            if (tag.getValue().equals(""host2"")) {
                value = .5;
            }
            int count = 0;
            for (Sample sample : dsample) {
                assertEquals(ts, sample.getTimestamp());
                ts += 200;
                assertEquals(value, sample.getValue(), 0.0001);
                count++;
            }
            assertEquals(5, count);
        }
    }

    static public class SampleObject implements ObjectSizeOf {

        private long sizeInBytes = 0;

        public void setSizeInBytes(long sizeInBytes) {
            this.sizeInBytes = sizeInBytes;
        }

        @Override
        public long sizeInBytes() {
            return sizeInBytes;
        }
    }

    @Test
    public void memoryEstimatorTestSmallObjects() {
        long maxMemory = 1000;
        long start = System.currentTimeMillis();
        long period = 500l;
        long sizeOfObjects = 20;
        SampleObject o = new SampleObject();
        DownsampleMemoryEstimator memoryEstimator = new DownsampleMemoryEstimator(maxMemory, start, period);
        boolean shouldReturn = false;
        for (long x = 100; x <= 5000; x += 100) {
            long timestamp = start + x;
            o.setSizeInBytes(o.sizeInBytes() + sizeOfObjects);
            shouldReturn = memoryEstimator.shouldReturnBasedOnMemoryUsage(timestamp, o);
            if (memoryEstimator.isNewBucket()) {
                long memoryPercentageUsedCalculated = Math.round((double) o.sizeInBytes() / maxMemory * 100);
                long memoryPercentageUsedEstimate = Math.round(memoryEstimator.getMemoryUsedPercentage());
                long percentError = Math.round(Math.abs(memoryPercentageUsedCalculated - memoryPercentageUsedEstimate) / memoryPercentageUsedCalculated * 100);
                assertTrue(percentError == 0);
            }

            if (shouldReturn) {
                o.setSizeInBytes(0);
                memoryEstimator.reset();
            }
        }
        assertTrue(shouldReturn);
    }

    @Test
    public void memoryEstimatorTestLargeObjects() {
        long maxMemory = 10000;
        long start = System.currentTimeMillis();
        long period = 500l;
        long sizeOfObjects = 200;
        SampleObject o = new SampleObject();
        DownsampleMemoryEstimator memoryEstimator = new DownsampleMemoryEstimator(maxMemory, start, period);
        boolean shouldReturn = false;
        for (long x = 100; x <= 5000; x += 100) {
            long timestamp = start + x;
            o.setSizeInBytes(o.sizeInBytes() + sizeOfObjects);
            shouldReturn = memoryEstimator.shouldReturnBasedOnMemoryUsage(timestamp, o);
            if (memoryEstimator.isNewBucket()) {
                long memoryPercentageUsedCalculated = Math.round((double) o.sizeInBytes() / maxMemory * 100);
                long memoryPercentageUsedEstimate = Math.round(memoryEstimator.getMemoryUsedPercentage());
                long percentError = Math.round(Math.abs(memoryPercentageUsedCalculated - memoryPercentageUsedEstimate) / memoryPercentageUsedCalculated * 100);
                assertTrue(percentError == 0);
                assertTrue(memoryEstimator.isHighVolumeBuckets());
            }

            if (shouldReturn) {
                o.setSizeInBytes(0);
                memoryEstimator.reset();
            }
        }
        assertTrue(shouldReturn);
    }

    private SortedMap<Key,Value> createTestData3(int elapsedTime, int skipInterval, int numTagVariations) {
        SortedMap<Key,Value> testData3 = new TreeMap<>();
        List<List<Tag>> listOfTagVariations = new ArrayList<>();
        for (int x = 1; x <= numTagVariations; x++) {
            listOfTagVariations.add(Collections.singletonList(new Tag(""instance"", Integer.toString(x))));
        }

        for (long i = 0; i < elapsedTime; i += skipInterval) {
            for (List<Tag> tags : listOfTagVariations) {
                put(testData3, new Metric(""sys.loadAvg"", i, i * 2, tags));
            }
        }
        return testData3;
    }

    @Test
    public void testDownsampleCombining() throws Exception {

        int numTagVariations = 2;
        int sampleInterval = 50;
        int elapsedTime = 100;
        int skipInterval = 10;
        SortedMap<Key,Value> testData3 = createTestData3(elapsedTime, skipInterval, numTagVariations);
        DownsampleIterator iter = new DownsampleIterator();
        Map<Set<Tag>,Downsample> samples = runQuery(iter, testData3, sampleInterval, 1000);
        assertEquals(numTagVariations, samples.size());
        long totalBuckets = 0;
        for (Entry<Set<Tag>,Downsample> entry : samples.entrySet()) {
            totalBuckets = totalBuckets + entry.getValue().getNumBuckets();
        }
        assertEquals((elapsedTime / sampleInterval) * numTagVariations, totalBuckets);
    }

    private Map<Set<Tag>,Downsample> runQuery(SortedKeyValueIterator<Key,Value> iter, SortedMap<Key,Value> testData, long period, long maxDownsampleMemory)
                    throws Exception {
        IteratorSetting is = new IteratorSetting(100, DownsampleIterator.class);
        DownsampleIterator.setDownsampleOptions(is, 0, 1000, period, maxDownsampleMemory, Avg.class.getName());
        SortedKeyValueIterator<Key,Value> source = new SortedMapIterator(testData);
        iter.init(source, is.getOptions(), null);
        iter.seek(new Range(), Collections.emptyList(), true);
        boolean hasTop = iter.hasTop();
        assertTrue(hasTop);
        Key key;
        Map<Set<Tag>,Downsample> samples = new HashMap<>();
        do {
            Map<Set<Tag>,Downsample> currentSamples = DownsampleIterator.decodeValue(iter.getTopValue());
            List<Downsample> downsampleArray = new ArrayList<>();
            for (Entry<Set<Tag>,Downsample> entry : currentSamples.entrySet()) {
                Downsample downsample = samples.get(entry.getKey());
                if (downsample == null) {
                    samples.put(entry.getKey(), entry.getValue());
                } else {
                    downsampleArray.clear();
                    downsampleArray.add(downsample);
                    downsampleArray.add(entry.getValue());
                    samples.put(entry.getKey(), Downsample.combineDownsample(downsampleArray, null));
                }
            }
            key = iter.getTopKey();
            iter.next();
        } while (iter.hasTop());

        assertEquals(testData.lastKey(), key);
        return samples;
    }

}
","package timely.server.sample.iterators;

import static org.apache.accumulo.core.conf.ConfigurationTypeHelper.getTimeInMillis;

import java.io.ByteArrayInputStream;
import java.io.ByteArrayOutputStream;
import java.io.IOException;
import java.io.ObjectInputStream;
import java.io.ObjectOutputStream;
import java.util.HashMap;
import java.util.HashSet;
import java.util.Map;
import java.util.Set;

import org.apache.accumulo.core.client.IteratorSetting;
import org.apache.accumulo.core.data.Key;
import org.apache.accumulo.core.data.Value;
import org.apache.accumulo.core.iterators.IteratorEnvironment;
import org.apache.accumulo.core.iterators.SortedKeyValueIterator;
import org.apache.accumulo.core.iterators.WrappingIterator;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import timely.accumulo.MetricAdapter;
import timely.api.request.timeseries.QueryRequest;
import timely.api.response.TimelyException;
import timely.model.Metric;
import timely.model.Tag;
import timely.server.sample.Aggregator;
import timely.server.sample.Downsample;
import timely.server.sample.DownsampleFactory;
import timely.server.sample.aggregators.Avg;

public class DownsampleIterator extends WrappingIterator {

    private static final Logger log = LoggerFactory.getLogger(DownsampleIterator.class);
    private static final String START = ""downsample.start"";
    private static final String END = ""downsample.end"";
    private static final String PERIOD = ""downsample.period"";
    private static final String MAX_DOWNSAMPLE_MEMORY = ""downsample.maxDownsampleMemory"";
    private static final String AGGCLASS = ""downsample.aggclass"";

    private static final long DEFAULT_DOWNSAMPLE_MS = 1;
    private static final String DEFAULT_DOWNSAMPLE_AGGREGATOR = Avg.class.getSimpleName().toLowerCase();

    private DownsampleFactory factory;
    private final Map<Set<Tag>,Downsample> value = new HashMap<>();
    private long start;
    private long end;
    private long period;
    private Key last;

    private DownsampleMemoryEstimator memoryEstimator = null;

    @Override
    public void init(SortedKeyValueIterator<Key,Value> source, Map<String,String> options, IteratorEnvironment env) throws IOException {
        super.init(source, options, env);
        start = Long.parseLong(options.get(START));
        end = Long.parseLong(options.get(END));
        period = Long.parseLong(options.get(PERIOD));
        // default = 100 MB
        long maxDownsampleMemory = -1;
        if (options.containsKey(MAX_DOWNSAMPLE_MEMORY)) {
            maxDownsampleMemory = Long.parseLong(options.get(MAX_DOWNSAMPLE_MEMORY));
        }
        memoryEstimator = new DownsampleMemoryEstimator(maxDownsampleMemory, start, period);

        String aggClassname = options.get(AGGCLASS);
        Class<?> aggClass;
        try {
            aggClass = this.getClass().getClassLoader().loadClass(aggClassname);
        } catch (ClassNotFoundException e) {
            throw new RuntimeException(e);
        }
        @SuppressWarnings(""unchecked"")
        Class<? extends Aggregator> uncheckedAggClass = (Class<? extends Aggregator>) aggClass;
        factory = new DownsampleFactory(start, end, period, uncheckedAggClass);
    }

    @Override
    public boolean hasTop() {

        if (last == null && super.hasTop()) {
            while (super.hasTop()) {
                Key topKey = super.getTopKey();
                Value topValue = super.getTopValue();
                try {
                    Metric metric = MetricAdapter.parse(topKey, topValue);
                    long timestamp = metric.getValue().getTimestamp();
                    if (memoryEstimator.shouldReturnBasedOnMemoryUsage(timestamp, value)) {
                        log.trace(""returning current values - memory usage > "" + memoryEstimator.maxDownsampleMemory + "" for metric="" + metric);
                        break;
                    }
                    last = topKey;

                    Set<Tag> tags = new HashSet<>(metric.getTags());
                    Downsample sample = value.get(tags);
                    if (sample == null) {
                        try {
                            value.put(tags, sample = factory.create());
                        } catch (TimelyException e) {
                            throw new RuntimeException(e);
                        }
                    }
                    sample.add(metric.getValue().getTimestamp(), metric.getValue().getMeasure());
                } catch (Exception e) {
                    log.error(""Error: {} parsing metric at key: {}"", e.getMessage(), topKey.toString());
                }
                try {
                    super.next();
                } catch (IOException e) {
                    throw new RuntimeException(""Downstream next() failed"", e);
                }
            }
        }
        return last != null;
    }

    @Override
    public Key getTopKey() {
        return last;
    }

    @Override
    public Value getTopValue() {
        try (ByteArrayOutputStream bos = new ByteArrayOutputStream(); ObjectOutputStream out = new ObjectOutputStream(bos)) {
            out.writeObject(value);
            out.flush();
            // empty for next batch of downsamples
            value.clear();
            memoryEstimator.reset();
            return new Value(bos.toByteArray());
        } catch (IOException e) {
            throw new RuntimeException(e);
        }
    }

    @Override
    public void next() throws IOException {
        if (!super.hasTop()) {
            last = null;
        }
    }

    public static void setDownsampleOptions(IteratorSetting is, long start, long end, long period, long maxDownsampleMemory, String classname) {
        is.addOption(START, """" + start);
        is.addOption(END, """" + end);
        is.addOption(PERIOD, """" + period);
        is.addOption(MAX_DOWNSAMPLE_MEMORY, """" + maxDownsampleMemory);
        is.addOption(AGGCLASS, classname);
    }

    public static Map<Set<Tag>,Downsample> decodeValue(Value value) throws IOException, ClassNotFoundException {
        ByteArrayInputStream bis = new ByteArrayInputStream(value.get());
        ObjectInputStream ois = new ObjectInputStream(bis);
        @SuppressWarnings(""unchecked"")
        Map<Set<Tag>,Downsample> unchecked = (Map<Set<Tag>,Downsample>) ois.readObject();
        return unchecked;
    }

    public static long getDownsamplePeriod(QueryRequest.SubQuery query) {
        // disabling the downsampling OR setting the aggregation to none are
        // both considered to be disabling
        if (!query.getDownsample().isPresent() || query.getDownsample().get().endsWith(""-none"")) {
            return DEFAULT_DOWNSAMPLE_MS;
        }
        String parts[] = query.getDownsample().get().split(""-"");
        return getTimeInMillis(parts[0]);
    }

    public static Class<? extends Aggregator> getDownsampleAggregator(QueryRequest.SubQuery query) {
        String aggregatorName = Aggregator.NONE;
        if (query.getDownsample().isPresent()) {
            String parts[] = query.getDownsample().get().split(""-"");
            aggregatorName = parts[1];
        }
        // disabling the downsampling OR setting the aggregation to none are
        // both considered to be disabling
        if (aggregatorName.equals(Aggregator.NONE)) {
            // we need a downsampling iterator, so default to max to ensure we
            // return something
            aggregatorName = DEFAULT_DOWNSAMPLE_AGGREGATOR;
        }
        return Aggregator.getAggregator(aggregatorName);
    }
}
","['Assertion Roulette', 'Conditional Test Logic']",['Assertion Roulette'],0,1,1,15
51065_43.0_timely_simplegettwosamples,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/51065_test.java,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/51065_actual.java,"package timely.server.sample.iterators;

import static org.junit.Assert.assertEquals;
import static org.junit.Assert.assertNotNull;
import static org.junit.Assert.assertTrue;

import java.util.*;
import java.util.Map.Entry;

import org.apache.accumulo.core.client.IteratorSetting;
import org.apache.accumulo.core.data.ColumnUpdate;
import org.apache.accumulo.core.data.Key;
import org.apache.accumulo.core.data.Mutation;
import org.apache.accumulo.core.data.Range;
import org.apache.accumulo.core.data.Value;
import org.apache.accumulo.core.iterators.SortedKeyValueIterator;
import org.apache.accumulo.core.iteratorsImpl.system.SortedMapIterator;
import org.junit.Before;
import org.junit.Test;

import timely.accumulo.MetricAdapter;
import timely.model.Metric;
import timely.model.ObjectSizeOf;
import timely.model.Tag;
import timely.server.sample.Downsample;
import timely.server.sample.Sample;
import timely.server.sample.aggregators.Avg;

public class DownsampleIteratorTest {

    final private SortedMap<Key,Value> testData1 = new TreeMap<>();
    final private SortedMap<Key,Value> testData2 = new TreeMap<>();

    @Before
    public void createTestData() {
        createTestData1();
        createTestData2();
    }

    private void createTestData1() {
        List<Tag> tags = Collections.singletonList(new Tag(""host"", ""host1""));

        for (long i = 0; i < 1000; i += 100) {
            Metric m = new Metric(""sys.loadAvg"", i, .2, tags);
            Mutation mutation = MetricAdapter.toMutation(m);
            for (ColumnUpdate cu : mutation.getUpdates()) {
                Key key = new Key(mutation.getRow(), cu.getColumnFamily(), cu.getColumnQualifier(), cu.getColumnVisibility(), cu.getTimestamp());
                testData1.put(key, new Value(cu.getValue()));
            }
        }
    }

    void put(Map<Key,Value> testData, Metric m) {
        Mutation mutation = MetricAdapter.toMutation(m);
        for (ColumnUpdate cu : mutation.getUpdates()) {
            Key key = new Key(mutation.getRow(), cu.getColumnFamily(), cu.getColumnQualifier(), cu.getColumnVisibility(), cu.getTimestamp());
            testData.put(key, new Value(cu.getValue()));
        }
    }

    private void createTestData2() {
        List<Tag> tags = Collections.singletonList(new Tag(""host"", ""host1""));
        List<Tag> tags2 = Collections.singletonList(new Tag(""host"", ""host2""));

        for (long i = 0; i < 1000; i += 100) {
            put(testData2, new Metric(""sys.loadAvg"", i, .2, tags));
            put(testData2, new Metric(""sys.loadAvg"", i + 50, .5, tags2));
        }
    }

    @Test
    public void simpleGetOneSample() throws Exception {
        // check that data gets pulled out
        DownsampleIterator iter = new DownsampleIterator();
        Map<Set<Tag>,Downsample> samples = runQuery(iter, testData1, 100, -1);
        assertEquals(1, samples.size());
        for (Entry<Set<Tag>,Downsample> entry : samples.entrySet()) {
            Set<Tag> tags = entry.getKey();
            assertEquals(1, tags.size());
            assertEquals(Collections.singleton(new Tag(""host"", ""host1"")), tags);
            long ts = 0;
            for (Sample sample : entry.getValue()) {
                assertEquals(ts, sample.getTimestamp());
                ts += 100;
                assertEquals(0.2, sample.getValue(), 0.0001);
            }
            assertEquals(1000, ts);
        }
    }

    @Test
    public void simpleGetTwoSamples() throws Exception {
        DownsampleIterator iter = new DownsampleIterator();
        Map<Set<Tag>,Downsample> samples = runQuery(iter, testData2, 100, -1);
        assertEquals(2, samples.size());
        for (Tag tag : new Tag[] {new Tag(""host"", ""host1""), new Tag(""host"", ""host2"")}) {
            Downsample dsample = samples.get(Collections.singleton(tag));
            assertNotNull(dsample);
            long ts = 0;
            double value = .2;
            if (tag.getValue().equals(""host2"")) {
                value = .5;
            }
            int count = 0;
            for (Sample sample : dsample) {
                assertEquals(ts, sample.getTimestamp());
                ts += 100;
                assertEquals(value, sample.getValue(), 0.0001);
                count++;
            }
            assertEquals(10, count);
        }
    }

    @Test
    public void simpleTestDownsampling() throws Exception {
        DownsampleIterator iter = new DownsampleIterator();
        Map<Set<Tag>,Downsample> samples = runQuery(iter, testData2, 200, -1);
        assertEquals(2, samples.size());
        for (Tag tag : new Tag[] {new Tag(""host"", ""host1""), new Tag(""host"", ""host2"")}) {
            Downsample dsample = samples.get(Collections.singleton(tag));
            assertNotNull(dsample);
            long ts = 0;
            double value = .2;
            if (tag.getValue().equals(""host2"")) {
                value = .5;
            }
            int count = 0;
            for (Sample sample : dsample) {
                assertEquals(ts, sample.getTimestamp());
                ts += 200;
                assertEquals(value, sample.getValue(), 0.0001);
                count++;
            }
            assertEquals(5, count);
        }
    }

    static public class SampleObject implements ObjectSizeOf {

        private long sizeInBytes = 0;

        public void setSizeInBytes(long sizeInBytes) {
            this.sizeInBytes = sizeInBytes;
        }

        @Override
        public long sizeInBytes() {
            return sizeInBytes;
        }
    }

    @Test
    public void memoryEstimatorTestSmallObjects() {
        long maxMemory = 1000;
        long start = System.currentTimeMillis();
        long period = 500l;
        long sizeOfObjects = 20;
        SampleObject o = new SampleObject();
        DownsampleMemoryEstimator memoryEstimator = new DownsampleMemoryEstimator(maxMemory, start, period);
        boolean shouldReturn = false;
        for (long x = 100; x <= 5000; x += 100) {
            long timestamp = start + x;
            o.setSizeInBytes(o.sizeInBytes() + sizeOfObjects);
            shouldReturn = memoryEstimator.shouldReturnBasedOnMemoryUsage(timestamp, o);
            if (memoryEstimator.isNewBucket()) {
                long memoryPercentageUsedCalculated = Math.round((double) o.sizeInBytes() / maxMemory * 100);
                long memoryPercentageUsedEstimate = Math.round(memoryEstimator.getMemoryUsedPercentage());
                long percentError = Math.round(Math.abs(memoryPercentageUsedCalculated - memoryPercentageUsedEstimate) / memoryPercentageUsedCalculated * 100);
                assertTrue(percentError == 0);
            }

            if (shouldReturn) {
                o.setSizeInBytes(0);
                memoryEstimator.reset();
            }
        }
        assertTrue(shouldReturn);
    }

    @Test
    public void memoryEstimatorTestLargeObjects() {
        long maxMemory = 10000;
        long start = System.currentTimeMillis();
        long period = 500l;
        long sizeOfObjects = 200;
        SampleObject o = new SampleObject();
        DownsampleMemoryEstimator memoryEstimator = new DownsampleMemoryEstimator(maxMemory, start, period);
        boolean shouldReturn = false;
        for (long x = 100; x <= 5000; x += 100) {
            long timestamp = start + x;
            o.setSizeInBytes(o.sizeInBytes() + sizeOfObjects);
            shouldReturn = memoryEstimator.shouldReturnBasedOnMemoryUsage(timestamp, o);
            if (memoryEstimator.isNewBucket()) {
                long memoryPercentageUsedCalculated = Math.round((double) o.sizeInBytes() / maxMemory * 100);
                long memoryPercentageUsedEstimate = Math.round(memoryEstimator.getMemoryUsedPercentage());
                long percentError = Math.round(Math.abs(memoryPercentageUsedCalculated - memoryPercentageUsedEstimate) / memoryPercentageUsedCalculated * 100);
                assertTrue(percentError == 0);
                assertTrue(memoryEstimator.isHighVolumeBuckets());
            }

            if (shouldReturn) {
                o.setSizeInBytes(0);
                memoryEstimator.reset();
            }
        }
        assertTrue(shouldReturn);
    }

    private SortedMap<Key,Value> createTestData3(int elapsedTime, int skipInterval, int numTagVariations) {
        SortedMap<Key,Value> testData3 = new TreeMap<>();
        List<List<Tag>> listOfTagVariations = new ArrayList<>();
        for (int x = 1; x <= numTagVariations; x++) {
            listOfTagVariations.add(Collections.singletonList(new Tag(""instance"", Integer.toString(x))));
        }

        for (long i = 0; i < elapsedTime; i += skipInterval) {
            for (List<Tag> tags : listOfTagVariations) {
                put(testData3, new Metric(""sys.loadAvg"", i, i * 2, tags));
            }
        }
        return testData3;
    }

    @Test
    public void testDownsampleCombining() throws Exception {

        int numTagVariations = 2;
        int sampleInterval = 50;
        int elapsedTime = 100;
        int skipInterval = 10;
        SortedMap<Key,Value> testData3 = createTestData3(elapsedTime, skipInterval, numTagVariations);
        DownsampleIterator iter = new DownsampleIterator();
        Map<Set<Tag>,Downsample> samples = runQuery(iter, testData3, sampleInterval, 1000);
        assertEquals(numTagVariations, samples.size());
        long totalBuckets = 0;
        for (Entry<Set<Tag>,Downsample> entry : samples.entrySet()) {
            totalBuckets = totalBuckets + entry.getValue().getNumBuckets();
        }
        assertEquals((elapsedTime / sampleInterval) * numTagVariations, totalBuckets);
    }

    private Map<Set<Tag>,Downsample> runQuery(SortedKeyValueIterator<Key,Value> iter, SortedMap<Key,Value> testData, long period, long maxDownsampleMemory)
                    throws Exception {
        IteratorSetting is = new IteratorSetting(100, DownsampleIterator.class);
        DownsampleIterator.setDownsampleOptions(is, 0, 1000, period, maxDownsampleMemory, Avg.class.getName());
        SortedKeyValueIterator<Key,Value> source = new SortedMapIterator(testData);
        iter.init(source, is.getOptions(), null);
        iter.seek(new Range(), Collections.emptyList(), true);
        boolean hasTop = iter.hasTop();
        assertTrue(hasTop);
        Key key;
        Map<Set<Tag>,Downsample> samples = new HashMap<>();
        do {
            Map<Set<Tag>,Downsample> currentSamples = DownsampleIterator.decodeValue(iter.getTopValue());
            List<Downsample> downsampleArray = new ArrayList<>();
            for (Entry<Set<Tag>,Downsample> entry : currentSamples.entrySet()) {
                Downsample downsample = samples.get(entry.getKey());
                if (downsample == null) {
                    samples.put(entry.getKey(), entry.getValue());
                } else {
                    downsampleArray.clear();
                    downsampleArray.add(downsample);
                    downsampleArray.add(entry.getValue());
                    samples.put(entry.getKey(), Downsample.combineDownsample(downsampleArray, null));
                }
            }
            key = iter.getTopKey();
            iter.next();
        } while (iter.hasTop());

        assertEquals(testData.lastKey(), key);
        return samples;
    }

}
","package timely.server.sample.iterators;

import static org.apache.accumulo.core.conf.ConfigurationTypeHelper.getTimeInMillis;

import java.io.ByteArrayInputStream;
import java.io.ByteArrayOutputStream;
import java.io.IOException;
import java.io.ObjectInputStream;
import java.io.ObjectOutputStream;
import java.util.HashMap;
import java.util.HashSet;
import java.util.Map;
import java.util.Set;

import org.apache.accumulo.core.client.IteratorSetting;
import org.apache.accumulo.core.data.Key;
import org.apache.accumulo.core.data.Value;
import org.apache.accumulo.core.iterators.IteratorEnvironment;
import org.apache.accumulo.core.iterators.SortedKeyValueIterator;
import org.apache.accumulo.core.iterators.WrappingIterator;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import timely.accumulo.MetricAdapter;
import timely.api.request.timeseries.QueryRequest;
import timely.api.response.TimelyException;
import timely.model.Metric;
import timely.model.Tag;
import timely.server.sample.Aggregator;
import timely.server.sample.Downsample;
import timely.server.sample.DownsampleFactory;
import timely.server.sample.aggregators.Avg;

public class DownsampleIterator extends WrappingIterator {

    private static final Logger log = LoggerFactory.getLogger(DownsampleIterator.class);
    private static final String START = ""downsample.start"";
    private static final String END = ""downsample.end"";
    private static final String PERIOD = ""downsample.period"";
    private static final String MAX_DOWNSAMPLE_MEMORY = ""downsample.maxDownsampleMemory"";
    private static final String AGGCLASS = ""downsample.aggclass"";

    private static final long DEFAULT_DOWNSAMPLE_MS = 1;
    private static final String DEFAULT_DOWNSAMPLE_AGGREGATOR = Avg.class.getSimpleName().toLowerCase();

    private DownsampleFactory factory;
    private final Map<Set<Tag>,Downsample> value = new HashMap<>();
    private long start;
    private long end;
    private long period;
    private Key last;

    private DownsampleMemoryEstimator memoryEstimator = null;

    @Override
    public void init(SortedKeyValueIterator<Key,Value> source, Map<String,String> options, IteratorEnvironment env) throws IOException {
        super.init(source, options, env);
        start = Long.parseLong(options.get(START));
        end = Long.parseLong(options.get(END));
        period = Long.parseLong(options.get(PERIOD));
        // default = 100 MB
        long maxDownsampleMemory = -1;
        if (options.containsKey(MAX_DOWNSAMPLE_MEMORY)) {
            maxDownsampleMemory = Long.parseLong(options.get(MAX_DOWNSAMPLE_MEMORY));
        }
        memoryEstimator = new DownsampleMemoryEstimator(maxDownsampleMemory, start, period);

        String aggClassname = options.get(AGGCLASS);
        Class<?> aggClass;
        try {
            aggClass = this.getClass().getClassLoader().loadClass(aggClassname);
        } catch (ClassNotFoundException e) {
            throw new RuntimeException(e);
        }
        @SuppressWarnings(""unchecked"")
        Class<? extends Aggregator> uncheckedAggClass = (Class<? extends Aggregator>) aggClass;
        factory = new DownsampleFactory(start, end, period, uncheckedAggClass);
    }

    @Override
    public boolean hasTop() {

        if (last == null && super.hasTop()) {
            while (super.hasTop()) {
                Key topKey = super.getTopKey();
                Value topValue = super.getTopValue();
                try {
                    Metric metric = MetricAdapter.parse(topKey, topValue);
                    long timestamp = metric.getValue().getTimestamp();
                    if (memoryEstimator.shouldReturnBasedOnMemoryUsage(timestamp, value)) {
                        log.trace(""returning current values - memory usage > "" + memoryEstimator.maxDownsampleMemory + "" for metric="" + metric);
                        break;
                    }
                    last = topKey;

                    Set<Tag> tags = new HashSet<>(metric.getTags());
                    Downsample sample = value.get(tags);
                    if (sample == null) {
                        try {
                            value.put(tags, sample = factory.create());
                        } catch (TimelyException e) {
                            throw new RuntimeException(e);
                        }
                    }
                    sample.add(metric.getValue().getTimestamp(), metric.getValue().getMeasure());
                } catch (Exception e) {
                    log.error(""Error: {} parsing metric at key: {}"", e.getMessage(), topKey.toString());
                }
                try {
                    super.next();
                } catch (IOException e) {
                    throw new RuntimeException(""Downstream next() failed"", e);
                }
            }
        }
        return last != null;
    }

    @Override
    public Key getTopKey() {
        return last;
    }

    @Override
    public Value getTopValue() {
        try (ByteArrayOutputStream bos = new ByteArrayOutputStream(); ObjectOutputStream out = new ObjectOutputStream(bos)) {
            out.writeObject(value);
            out.flush();
            // empty for next batch of downsamples
            value.clear();
            memoryEstimator.reset();
            return new Value(bos.toByteArray());
        } catch (IOException e) {
            throw new RuntimeException(e);
        }
    }

    @Override
    public void next() throws IOException {
        if (!super.hasTop()) {
            last = null;
        }
    }

    public static void setDownsampleOptions(IteratorSetting is, long start, long end, long period, long maxDownsampleMemory, String classname) {
        is.addOption(START, """" + start);
        is.addOption(END, """" + end);
        is.addOption(PERIOD, """" + period);
        is.addOption(MAX_DOWNSAMPLE_MEMORY, """" + maxDownsampleMemory);
        is.addOption(AGGCLASS, classname);
    }

    public static Map<Set<Tag>,Downsample> decodeValue(Value value) throws IOException, ClassNotFoundException {
        ByteArrayInputStream bis = new ByteArrayInputStream(value.get());
        ObjectInputStream ois = new ObjectInputStream(bis);
        @SuppressWarnings(""unchecked"")
        Map<Set<Tag>,Downsample> unchecked = (Map<Set<Tag>,Downsample>) ois.readObject();
        return unchecked;
    }

    public static long getDownsamplePeriod(QueryRequest.SubQuery query) {
        // disabling the downsampling OR setting the aggregation to none are
        // both considered to be disabling
        if (!query.getDownsample().isPresent() || query.getDownsample().get().endsWith(""-none"")) {
            return DEFAULT_DOWNSAMPLE_MS;
        }
        String parts[] = query.getDownsample().get().split(""-"");
        return getTimeInMillis(parts[0]);
    }

    public static Class<? extends Aggregator> getDownsampleAggregator(QueryRequest.SubQuery query) {
        String aggregatorName = Aggregator.NONE;
        if (query.getDownsample().isPresent()) {
            String parts[] = query.getDownsample().get().split(""-"");
            aggregatorName = parts[1];
        }
        // disabling the downsampling OR setting the aggregation to none are
        // both considered to be disabling
        if (aggregatorName.equals(Aggregator.NONE)) {
            // we need a downsampling iterator, so default to max to ensure we
            // return something
            aggregatorName = DEFAULT_DOWNSAMPLE_AGGREGATOR;
        }
        return Aggregator.getAggregator(aggregatorName);
    }
}
","['Assertion Roulette', 'Conditional Test Logic']",['Assertion Roulette'],0,1,1,15
17104_21_dubbo_testonremove,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/17104_test.java,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/17104_actual.java,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the ""License""); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.dubbo.common.threadlocal;

import java.lang.reflect.Field;
import java.util.concurrent.CountDownLatch;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.concurrent.atomic.AtomicReference;
import java.util.concurrent.locks.LockSupport;

import org.junit.jupiter.api.AfterEach;
import org.junit.jupiter.api.Assertions;
import org.junit.jupiter.api.Test;

import static org.awaitility.Awaitility.await;
import static org.hamcrest.CoreMatchers.instanceOf;
import static org.hamcrest.CoreMatchers.is;
import static org.hamcrest.CoreMatchers.not;
import static org.hamcrest.MatcherAssert.assertThat;
import static org.junit.jupiter.api.Assertions.assertEquals;

class InternalThreadLocalTest {

    private static final int THREADS = 10;

    private static final int PERFORMANCE_THREAD_COUNT = 1000;

    private static final int GET_COUNT = 1000000;

    @AfterEach
    public void setup() {
        InternalThreadLocalMap.remove();
    }

    @Test
    void testInternalThreadLocal() {
        final AtomicInteger index = new AtomicInteger(0);

        final InternalThreadLocal<Integer> internalThreadLocal = new InternalThreadLocal<Integer>() {

            @Override
            protected Integer initialValue() {
                Integer v = index.getAndIncrement();
                System.out.println(""thread : "" + Thread.currentThread().getName() + "" init value : "" + v);
                return v;
            }
        };

        for (int i = 0; i < THREADS; i++) {
            Thread t = new Thread(internalThreadLocal::get);
            t.start();
        }

        await().until(index::get, is(THREADS));
    }

    @Test
    void testRemoveAll() {
        final InternalThreadLocal<Integer> internalThreadLocal = new InternalThreadLocal<Integer>();
        internalThreadLocal.set(1);
        Assertions.assertEquals(1, (int) internalThreadLocal.get(), ""set failed"");

        final InternalThreadLocal<String> internalThreadLocalString = new InternalThreadLocal<String>();
        internalThreadLocalString.set(""value"");
        Assertions.assertEquals(""value"", internalThreadLocalString.get(), ""set failed"");

        InternalThreadLocal.removeAll();
        Assertions.assertNull(internalThreadLocal.get(), ""removeAll failed!"");
        Assertions.assertNull(internalThreadLocalString.get(), ""removeAll failed!"");
    }

    @Test
    void testSize() {
        final InternalThreadLocal<Integer> internalThreadLocal = new InternalThreadLocal<Integer>();
        internalThreadLocal.set(1);
        Assertions.assertEquals(1, InternalThreadLocal.size(), ""size method is wrong!"");

        final InternalThreadLocal<String> internalThreadLocalString = new InternalThreadLocal<String>();
        internalThreadLocalString.set(""value"");
        Assertions.assertEquals(2, InternalThreadLocal.size(), ""size method is wrong!"");
        InternalThreadLocal.removeAll();
    }

    @Test
    void testSetAndGet() {
        final Integer testVal = 10;
        final InternalThreadLocal<Integer> internalThreadLocal = new InternalThreadLocal<Integer>();
        internalThreadLocal.set(testVal);
        Assertions.assertEquals(testVal, internalThreadLocal.get(), ""set is not equals get"");
    }

    @Test
    void testRemove() {
        final InternalThreadLocal<Integer> internalThreadLocal = new InternalThreadLocal<Integer>();
        internalThreadLocal.set(1);
        Assertions.assertEquals(1, (int) internalThreadLocal.get(), ""get method false!"");

        internalThreadLocal.remove();
        Assertions.assertNull(internalThreadLocal.get(), ""remove failed!"");
    }

    @Test
    void testOnRemove() {
        final Integer[] valueToRemove = {null};
        final InternalThreadLocal<Integer> internalThreadLocal = new InternalThreadLocal<Integer>() {
            @Override
            protected void onRemoval(Integer value) {
                // value calculate
                valueToRemove[0] = value + 1;
            }
        };
        internalThreadLocal.set(1);
        Assertions.assertEquals(1, (int) internalThreadLocal.get(), ""get method false!"");

        internalThreadLocal.remove();
        Assertions.assertEquals(2, (int) valueToRemove[0], ""onRemove method failed!"");
    }

    @Test
    void testMultiThreadSetAndGet() throws InterruptedException {
        final Integer testVal1 = 10;
        final Integer testVal2 = 20;
        final InternalThreadLocal<Integer> internalThreadLocal = new InternalThreadLocal<Integer>();
        final CountDownLatch countDownLatch = new CountDownLatch(2);
        Thread t1 = new Thread(new Runnable() {
            @Override
            public void run() {

                internalThreadLocal.set(testVal1);
                Assertions.assertEquals(testVal1, internalThreadLocal.get(), ""set is not equals get"");
                countDownLatch.countDown();
            }
        });
        t1.start();

        Thread t2 = new Thread(new Runnable() {
            @Override
            public void run() {
                internalThreadLocal.set(testVal2);
                Assertions.assertEquals(testVal2, internalThreadLocal.get(), ""set is not equals get"");
                countDownLatch.countDown();
            }
        });
        t2.start();
        countDownLatch.await();
    }

    /**
     * print
     * take[2689]ms
     * <p></p>
     * This test is based on a Machine with 4 core and 16g memory.
     */
    @Test
    void testPerformanceTradition() {
        final ThreadLocal<String>[] caches1 = new ThreadLocal[PERFORMANCE_THREAD_COUNT];
        final Thread mainThread = Thread.currentThread();
        for (int i = 0; i < PERFORMANCE_THREAD_COUNT; i++) {
            caches1[i] = new ThreadLocal<String>();
        }
        Thread t1 = new Thread(new Runnable() {
            @Override
            public void run() {
                for (int i = 0; i < PERFORMANCE_THREAD_COUNT; i++) {
                    caches1[i].set(""float.lu"");
                }
                long start = System.nanoTime();
                for (int i = 0; i < PERFORMANCE_THREAD_COUNT; i++) {
                    for (int j = 0; j < GET_COUNT; j++) {
                        caches1[i].get();
                    }
                }
                long end = System.nanoTime();
                System.out.println(""take["" + TimeUnit.NANOSECONDS.toMillis(end - start) + ""]ms"");
                LockSupport.unpark(mainThread);
            }
        });
        t1.start();
        LockSupport.park(mainThread);
    }

    /**
     * print
     * take[14]ms
     * <p></p>
     * This test is based on a Machine with 4 core and 16g memory.
     */
    @Test
    void testPerformance() {
        final InternalThreadLocal<String>[] caches = new InternalThreadLocal[PERFORMANCE_THREAD_COUNT];
        final Thread mainThread = Thread.currentThread();
        for (int i = 0; i < PERFORMANCE_THREAD_COUNT; i++) {
            caches[i] = new InternalThreadLocal<String>();
        }
        Thread t = new InternalThread(new Runnable() {
            @Override
            public void run() {
                for (int i = 0; i < PERFORMANCE_THREAD_COUNT; i++) {
                    caches[i].set(""float.lu"");
                }
                long start = System.nanoTime();
                for (int i = 0; i < PERFORMANCE_THREAD_COUNT; i++) {
                    for (int j = 0; j < GET_COUNT; j++) {
                        caches[i].get();
                    }
                }
                long end = System.nanoTime();
                System.out.println(""take["" + TimeUnit.NANOSECONDS.toMillis(end - start) + ""]ms"");
                LockSupport.unpark(mainThread);
            }
        });
        t.start();
        LockSupport.park(mainThread);
    }

    @Test
    void testConstructionWithIndex() throws Exception {
        // reset ARRAY_LIST_CAPACITY_MAX_SIZE to speed up
        int NEW_ARRAY_LIST_CAPACITY_MAX_SIZE = 8;
        Field nextIndexField = InternalThreadLocalMap.class.getDeclaredField(""NEXT_INDEX"");

        nextIndexField.setAccessible(true);
        AtomicInteger nextIndex = (AtomicInteger) nextIndexField.get(AtomicInteger.class);
        int arrayListCapacityMaxSize = InternalThreadLocalMap.ARRAY_LIST_CAPACITY_MAX_SIZE;
        int nextIndex_before = nextIndex.incrementAndGet();
        nextIndex.set(0);
        final AtomicReference<Throwable> throwable = new AtomicReference<Throwable>();
        try {
            InternalThreadLocalMap.ARRAY_LIST_CAPACITY_MAX_SIZE = NEW_ARRAY_LIST_CAPACITY_MAX_SIZE;
            while (nextIndex.get() < NEW_ARRAY_LIST_CAPACITY_MAX_SIZE) {
                new InternalThreadLocal<Boolean>();
            }
            assertEquals(NEW_ARRAY_LIST_CAPACITY_MAX_SIZE - 1, InternalThreadLocalMap.lastVariableIndex());
            try {
                new InternalThreadLocal<Boolean>();
            } catch (Throwable t) {
                throwable.set(t);
            }
            // Assert the max index cannot greater than (ARRAY_LIST_CAPACITY_MAX_SIZE - 1)
            assertThat(throwable.get(), is(instanceOf(IllegalStateException.class)));
            // Assert the index was reset to ARRAY_LIST_CAPACITY_MAX_SIZE after it reaches ARRAY_LIST_CAPACITY_MAX_SIZE
            assertEquals(NEW_ARRAY_LIST_CAPACITY_MAX_SIZE - 1, InternalThreadLocalMap.lastVariableIndex());
        } finally {
            // Restore the index
            nextIndex.set(nextIndex_before);
            InternalThreadLocalMap.ARRAY_LIST_CAPACITY_MAX_SIZE = arrayListCapacityMaxSize;
        }
    }

    @Test
    void testInternalThreadLocalMapExpand() throws Exception {
        final AtomicReference<Throwable> throwable = new AtomicReference<Throwable>();
        Runnable runnable = new Runnable() {
            @Override
            public void run() {
                int expand_threshold = 1 << 30;
                try {
                    InternalThreadLocalMap.get().setIndexedVariable(expand_threshold, null);
                } catch (Throwable t) {
                    throwable.set(t);
                }
            }
        };
        InternalThread internalThread = new InternalThread(runnable);
        internalThread.start();
        internalThread.join();
        // Assert the expanded size is not overflowed to negative value
        assertThat(throwable.get(), is(not(instanceOf(NegativeArraySizeException.class))));
    }
}
","/*
 * Copyright 2014 The Netty Project
 *
 * The Netty Project licenses this file to you under the Apache License,
 * version 2.0 (the ""License""); you may not use this file except in compliance
 * with the License. You may obtain a copy of the License at:
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 * License for the specific language governing permissions and limitations
 * under the License.
 */

package org.apache.dubbo.common.threadlocal;

import java.util.Collections;
import java.util.IdentityHashMap;
import java.util.Set;

/**
 * InternalThreadLocal
 * A special variant of {@link ThreadLocal} that yields higher access performance when accessed from a
 * {@link InternalThread}.
 * <p></p>
 * Internally, a {@link InternalThread} uses a constant index in an array, instead of using hash code and hash table,
 * to look for a variable.  Although seemingly very subtle, it yields slight performance advantage over using a hash
 * table, and it is useful when accessed frequently.
 * <p></p>
 * This design is learning from {@see io.netty.util.concurrent.FastThreadLocal} which is in Netty.
 */
public class InternalThreadLocal<V> extends ThreadLocal<V> {

    private static final int VARIABLES_TO_REMOVE_INDEX = InternalThreadLocalMap.nextVariableIndex();

    private final int index;

    public InternalThreadLocal() {
        index = InternalThreadLocalMap.nextVariableIndex();
    }

    /**
     * Removes all {@link InternalThreadLocal} variables bound to the current thread.  This operation is useful when you
     * are in a container environment, and you don't want to leave the thread local variables in the threads you do not
     * manage.
     */
    @SuppressWarnings(""unchecked"")
    public static void removeAll() {
        InternalThreadLocalMap threadLocalMap = InternalThreadLocalMap.getIfSet();
        if (threadLocalMap == null) {
            return;
        }

        try {
            Object v = threadLocalMap.indexedVariable(VARIABLES_TO_REMOVE_INDEX);
            if (v != null && v != InternalThreadLocalMap.UNSET) {
                Set<InternalThreadLocal<?>> variablesToRemove = (Set<InternalThreadLocal<?>>) v;
                InternalThreadLocal<?>[] variablesToRemoveArray =
                        variablesToRemove.toArray(new InternalThreadLocal[0]);
                for (InternalThreadLocal<?> tlv : variablesToRemoveArray) {
                    tlv.remove(threadLocalMap);
                }
            }
        } finally {
            InternalThreadLocalMap.remove();
        }
    }

    /**
     * Returns the number of thread local variables bound to the current thread.
     */
    public static int size() {
        InternalThreadLocalMap threadLocalMap = InternalThreadLocalMap.getIfSet();
        if (threadLocalMap == null) {
            return 0;
        } else {
            return threadLocalMap.size();
        }
    }

    public static void destroy() {
        InternalThreadLocalMap.destroy();
    }

    @SuppressWarnings(""unchecked"")
    private static void addToVariablesToRemove(InternalThreadLocalMap threadLocalMap, InternalThreadLocal<?> variable) {
        Object v = threadLocalMap.indexedVariable(VARIABLES_TO_REMOVE_INDEX);
        Set<InternalThreadLocal<?>> variablesToRemove;
        if (v == InternalThreadLocalMap.UNSET || v == null) {
            variablesToRemove = Collections.newSetFromMap(new IdentityHashMap<InternalThreadLocal<?>, Boolean>());
            threadLocalMap.setIndexedVariable(VARIABLES_TO_REMOVE_INDEX, variablesToRemove);
        } else {
            variablesToRemove = (Set<InternalThreadLocal<?>>) v;
        }

        variablesToRemove.add(variable);
    }

    @SuppressWarnings(""unchecked"")
    private static void removeFromVariablesToRemove(InternalThreadLocalMap threadLocalMap, InternalThreadLocal<?> variable) {

        Object v = threadLocalMap.indexedVariable(VARIABLES_TO_REMOVE_INDEX);

        if (v == InternalThreadLocalMap.UNSET || v == null) {
            return;
        }

        Set<InternalThreadLocal<?>> variablesToRemove = (Set<InternalThreadLocal<?>>) v;
        variablesToRemove.remove(variable);
    }

    /**
     * Returns the current value for the current thread
     */
    @SuppressWarnings(""unchecked"")
    @Override
    public final V get() {
        InternalThreadLocalMap threadLocalMap = InternalThreadLocalMap.get();
        Object v = threadLocalMap.indexedVariable(index);
        if (v != InternalThreadLocalMap.UNSET) {
            return (V) v;
        }

        return initialize(threadLocalMap);
    }

    public final V getWithoutInitialize() {
        InternalThreadLocalMap threadLocalMap = InternalThreadLocalMap.get();
        Object v = threadLocalMap.indexedVariable(index);
        if (v != InternalThreadLocalMap.UNSET) {
            return (V) v;
        }

        return null;
    }

    private V initialize(InternalThreadLocalMap threadLocalMap) {
        V v = null;
        try {
            v = initialValue();
        } catch (Exception e) {
            throw new RuntimeException(e);
        }

        threadLocalMap.setIndexedVariable(index, v);
        addToVariablesToRemove(threadLocalMap, this);
        return v;
    }

    /**
     * Sets the value for the current thread.
     */
    @Override
    public final void set(V value) {
        if (value == null || value == InternalThreadLocalMap.UNSET) {
            remove();
        } else {
            InternalThreadLocalMap threadLocalMap = InternalThreadLocalMap.get();
            if (threadLocalMap.setIndexedVariable(index, value)) {
                addToVariablesToRemove(threadLocalMap, this);
            }
        }
    }

    /**
     * Sets the value to uninitialized; a proceeding call to get() will trigger a call to initialValue().
     */
    @SuppressWarnings(""unchecked"")
    @Override
    public final void remove() {
        remove(InternalThreadLocalMap.getIfSet());
    }

    /**
     * Sets the value to uninitialized for the specified thread local map;
     * a proceeding call to get() will trigger a call to initialValue().
     * The specified thread local map must be for the current thread.
     */
    @SuppressWarnings(""unchecked"")
    public final void remove(InternalThreadLocalMap threadLocalMap) {
        if (threadLocalMap == null) {
            return;
        }

        Object v = threadLocalMap.removeIndexedVariable(index);
        removeFromVariablesToRemove(threadLocalMap, this);

        if (v != InternalThreadLocalMap.UNSET) {
            try {
                onRemoval((V) v);
            } catch (Exception e) {
                throw new RuntimeException(e);
            }
        }
    }

    /**
     * Returns the initial value for this thread-local variable.
     */
    @Override
    protected V initialValue() {
        return null;
    }

    /**
     * Invoked when this thread local variable is removed by {@link #remove()}.
     */
    protected void onRemoval(@SuppressWarnings(""unused"") V value) throws Exception {
    }
}
","['Assertion Roulette', 'Conditional Test Logic', 'Exception Catching Throwing', 'Eager Test', 'Lazy Test', 'Unknown Test']","['Assertion Roulette', 'Eager Test', 'Redundant Print', 'Sleepy Test', 'Sensitive Equality', 'Magic Number Test', 'Resource Optimism', 'Lazy Test', 'Mystery Guest']",6,3,3,9
2281_47.0_rxjava2-extras_testerrorpostchaining,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/2281_test.java,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/2281_actual.java,"package com.github.davidmoten.rx2.internal.flowable;

import static org.junit.Assert.assertTrue;

import java.util.List;
import java.util.concurrent.atomic.AtomicBoolean;
import java.util.concurrent.atomic.AtomicInteger;

import org.junit.Assert;
import org.junit.FixMethodOrder;
import org.junit.Ignore;
import org.junit.Test;
import org.junit.runners.MethodSorters;
import org.mockito.Mockito;

import com.github.davidmoten.rx2.Actions;
import com.github.davidmoten.rx2.exceptions.ThrowingException;
import com.github.davidmoten.rx2.flowable.Transformers;

import io.reactivex.BackpressureStrategy;
import io.reactivex.Flowable;
import io.reactivex.Notification;
import io.reactivex.Observable;
import io.reactivex.functions.Function;
import io.reactivex.schedulers.Schedulers;
import io.reactivex.subjects.PublishSubject;
import io.reactivex.subscribers.TestSubscriber;

@FixMethodOrder(MethodSorters.NAME_ASCENDING)
public final class FlowableRepeatingTransformTest {

    private static final Function<List<Integer>, Integer> sum = (new Function<List<Integer>, Integer>() {
        @Override
        public Integer apply(List<Integer> list) throws Exception {
            int sum = 0;
            for (int value : list) {
                sum += value;
            }
            return sum;
        }
    });

    private static final Function<Flowable<Integer>, Flowable<Integer>> reducer = new Function<Flowable<Integer>, Flowable<Integer>>() {

        @Override
        public Flowable<Integer> apply(Flowable<Integer> f) throws Exception {
            return f.buffer(2).map(sum);
        }
    };

    private static final Function<Flowable<Integer>, Flowable<Integer>> plusOne = new Function<Flowable<Integer>, Flowable<Integer>>() {

        @Override
        public Flowable<Integer> apply(Flowable<Integer> f) throws Exception {
            return f.map(new Function<Integer, Integer>() {

                @Override
                public Integer apply(Integer t) throws Exception {
                    return t + 1;
                }
            });
        }
    };

    private static final Function<Flowable<Integer>, Flowable<Integer>> reducerThrows = new Function<Flowable<Integer>, Flowable<Integer>>() {

        @Override
        public Flowable<Integer> apply(Flowable<Integer> f) throws Exception {
            throw new ThrowingException();
        }
    };

    private static final Function<Flowable<Integer>, Flowable<Integer>> reducerThrowsOnThird = new Function<Flowable<Integer>, Flowable<Integer>>() {
        final AtomicInteger count = new AtomicInteger();

        @Override
        public Flowable<Integer> apply(Flowable<Integer> f) throws Exception {
            if (count.incrementAndGet() >= 3) {
                throw new ThrowingException();
            } else {
                return reducer.apply(f);
            }
        }
    };

    private static final Function<Flowable<Integer>, Flowable<Integer>> reducerAsync = new Function<Flowable<Integer>, Flowable<Integer>>() {

        @Override
        public Flowable<Integer> apply(Flowable<Integer> f) throws Exception {
            return f.subscribeOn(Schedulers.computation()).buffer(2).map(sum);
        }
    };

    @Test
    public void testEmpty() {
        int result = Flowable.<Integer>empty() //
                .to(Transformers.reduce(reducer, 2)) //
                .single(-1) //
                .blockingGet();
        Assert.assertEquals(-1, result);
    }

    @Test
    public void testOne() {
        check(1, 2);
    }

    @Test
    public void testOneAsync() {
        checkAsync(1, 2);
    }

    @Test
    public void testCompletesFirstLevel() {
        check(2, 2);
    }

    @Test
    public void testCompletesSecondLevel() {
        check(3, 2);
    }

    @Test(timeout = 1000)
    public void testCompletesThirdLevel() {
        check(4, 2);
    }

    @Test
    public void testCompletesThirdLevelWithOneLeftOver() {
        check(5, 2);
    }

    @Test
    public void testCompletesFourLevels() {
        check(8, 2);
    }

    @Test
    public void testMany() {
        for (int n = 5; n <= 100; n++) {
            int m = (int) Math.round(Math.floor(Math.log(n) / Math.log(2))) - 1;
            for (int maxChained = Math.max(3, m); maxChained < 6; maxChained++) {
//                System.out.println(""maxChained="" + maxChained + "",n="" + n);
                check(n, maxChained);
            }
        }
    }

    @Test
    @Ignore
    public void testManyAsync() {
        for (int n = 5; n <= 100; n++) {
            int m = (int) Math.round(Math.floor(Math.log(n) / Math.log(2))) - 1;
            for (int maxChained = Math.max(3, m); maxChained < 6; maxChained++) {
//                System.out.println(""maxChained="" + maxChained + "",n="" + n);
                checkAsync(n, maxChained);
            }
        }
    }

    @Test(expected = IllegalArgumentException.class)
    public void testMaxChainedGreaterThanZero() {
        check(10, 0);
    }

    @Test
    public void testReducerThrows() {
        Flowable.range(1, 10) //
                .to(Transformers.reduce(reducerThrows, 2)) //
                .test() //
                .assertNoValues() //
                .assertError(ThrowingException.class);
    }

    @Test
    public void testReducerThrowsOnThirdCall() {
        Flowable.range(1, 128) //
                .to(Transformers.reduce(reducerThrowsOnThird, 2)) //
                .test() //
                .assertNoValues() //
                .assertError(ThrowingException.class);
    }

    @Test
    public void testUpstreamCancelled() {
        AtomicBoolean cancelled = new AtomicBoolean();
        Flowable.<Integer>never() //
                .doOnCancel(Actions.setToTrue(cancelled)) //
                .to(Transformers.reduce(reducer, 2)) //
                .test().cancel();
        assertTrue(cancelled.get());
    }

    @Test
    public void testErrorPreChaining() {
        Flowable.<Integer>error(new ThrowingException()) //
                .to(Transformers.reduce(reducer, 2)) //
                .test() //
                .assertNoValues() //
                .assertError(ThrowingException.class);
    }

    @Test
    @Ignore
    public void testErrorPreChainingCausesCancel() {
        AtomicBoolean cancelled = new AtomicBoolean();
        Flowable.<Integer>error(new ThrowingException()) //
                .doOnCancel(Actions.setToTrue(cancelled)) //
                .to(Transformers.reduce(reducer, 2)) //
                .test() //
                .assertNoValues() //
                .assertError(ThrowingException.class);
        assertTrue(cancelled.get());
    }

    @Test
    public void testErrorPostChaining() {
        Flowable.range(1, 100) //
                .concatWith(Flowable.<Integer>error(new ThrowingException())) //
                .to(Transformers.reduce(reducer, 2)) //
                .test() //
                .assertNoValues() //
                .assertError(ThrowingException.class);
    }

    @Test
    public void testMaxIterationsOne() {
        Function<Observable<Integer>, Observable<?>> tester = new Function<Observable<Integer>, Observable<?>>() {
            @Override
            public Observable<?> apply(Observable<Integer> o) throws Exception {
                return o.concatWith(Observable.<Integer>never());
            }
        };
        Flowable.just(1, 5) //
                .to(Transformers.repeat(plusOne, 3, 1, tester)) //
                .test() //
                .assertValues(2, 6) //
                .assertComplete();
    }

    @Test(timeout = 20000)
    public void testMaxIterationsTwoMaxChainedThree() {
        Flowable.just(1, 5) //
                .to(Transformers.reduce(plusOne, 3, 2)) //
                .test() //
                .assertValues(3, 7) //
                .assertComplete();
    }

    @Test(timeout = 20000)
    public void testMaxIterations() {
        Flowable.range(1, 2) //
                .to(Transformers.reduce(plusOne, 2, 3)) //
                .test() //
                .assertValues(4, 5) //
                .assertComplete();
    }

    @Test
    public void testRequestOverflow() {
        PublishSubject<Integer> subject = PublishSubject.create();

        TestSubscriber<Integer> sub = subject.toFlowable(BackpressureStrategy.BUFFER) //
                .to(Transformers.reduce(reducer, 2, 5)) //
                .test(Long.MAX_VALUE - 2) //
                .requestMore(Long.MAX_VALUE - 2);
        subject.onNext(1);
        subject.onNext(2);
        subject.onComplete();
        sub.assertValues(3);
    }

    @Test
    public void testDematerialize() {
        Flowable.just(Notification.createOnNext(1)).dematerialize().count().blockingGet();
        Flowable.empty().dematerialize().count().blockingGet();
    }

    @Test
    public void testBackpressure() {
        Flowable.range(1, 4) //
                .to(Transformers.reduce(plusOne, 2, 3)) //
                .test(0) //
                .assertNoValues() //
                .requestMore(1) //
                .assertValue(4) //
                .requestMore(1) //
                .assertValues(4, 5) //
                .requestMore(2) //
                .assertValues(4, 5, 6, 7) //
                .assertComplete();
    }

    @Test
    public void testBackpressureOnErrorNoRequests() {
        Flowable.<Integer>error(new ThrowingException())//
                .to(Transformers.reduce(plusOne, 2, 3)) //
                .test(0) //
                .assertNoValues() //
                .assertError(ThrowingException.class);
    }

    @Test(expected = IllegalArgumentException.class)
    public void testMaxIterationsZeroThrowsIAE() {
        @SuppressWarnings(""unchecked"")
        Function<Observable<Integer>, Observable<?>> tester = Mockito.mock(Function.class);
        Transformers.<Integer>repeat(plusOne, 2, 0, tester);
    }
    
    @Test
    public void testStackOverflowDoesNotHappen() {
        Flowable.range(1, 3) //
        .to(Transformers.reduce(plusOne, 2, 10000)) //
        .test();
    }

    private static void check(int n, int maxChained) {
        int result = Flowable.range(1, n) //
                .to(Transformers.reduce(reducer, maxChained)) //
                .single(-1) //
                .blockingGet();
        Assert.assertEquals(sum(n), result);
    }

    private static void checkAsync(int n, int maxChained) {
        int result = Flowable.range(1, n) //
                .to(Transformers.reduce(reducerAsync, maxChained)) //
                .single(-1) //
                .blockingGet();
        Assert.assertEquals(sum(n), result);
    }

    private static int sum(int n) {
        int sum = 0;
        for (int i = 1; i <= n; i++) {
            sum += i;
        }
        return sum;
    }

    public static void main(String[] args) {
        Flowable.just(1, 2, 3) //
                .to(Transformers.reduce(plusOne, 2, Long.MAX_VALUE)) //
                .test();
    }

}
","package com.github.davidmoten.rx2.internal.flowable;

import java.util.concurrent.atomic.AtomicInteger;
import java.util.concurrent.atomic.AtomicLong;
import java.util.concurrent.atomic.AtomicReference;

import org.reactivestreams.Subscriber;
import org.reactivestreams.Subscription;

import com.github.davidmoten.guavamini.Preconditions;

import io.reactivex.Flowable;
import io.reactivex.FlowableSubscriber;
import io.reactivex.Observable;
import io.reactivex.Observer;
import io.reactivex.disposables.Disposable;
import io.reactivex.disposables.Disposables;
import io.reactivex.exceptions.Exceptions;
import io.reactivex.functions.Function;
import io.reactivex.internal.fuseable.SimplePlainQueue;
import io.reactivex.internal.queue.SpscLinkedArrayQueue;
import io.reactivex.internal.subscriptions.SubscriptionHelper;
import io.reactivex.internal.util.BackpressureHelper;
import io.reactivex.plugins.RxJavaPlugins;

public final class FlowableRepeatingTransform<T> extends Flowable<T> {

    private final Flowable<T> source;
    private final Function<? super Flowable<T>, ? extends Flowable<T>> transform;
    private final int maxChained;
    private final long maxIterations;
    private final Function<Observable<T>, ? extends Observable<?>> tester;

    public FlowableRepeatingTransform(Flowable<T> source,
            Function<? super Flowable<T>, ? extends Flowable<T>> transform, int maxChained,
            long maxIterations, Function<Observable<T>, Observable<?>> tester) {
        Preconditions.checkArgument(maxChained > 0, ""maxChained must be > 0"");
        Preconditions.checkArgument(maxIterations > 0, ""maxIterations must be > 0"");
        Preconditions.checkNotNull(transform, ""transform must not be null"");
        Preconditions.checkNotNull(tester, ""tester must not be null"");
        this.source = source;
        this.transform = transform;
        this.maxChained = maxChained;
        this.maxIterations = maxIterations;
        this.tester = tester;
    }

    @Override
    protected void subscribeActual(Subscriber<? super T> child) {

        Flowable<T> f;
        try {
            f = transform.apply(source);
        } catch (Exception e) {
            Exceptions.throwIfFatal(e);
            child.onSubscribe(SubscriptionHelper.CANCELLED);
            child.onError(e);
            return;
        }
        AtomicReference<Chain<T>> chainRef = new AtomicReference<Chain<T>>();
        DestinationSerializedSubject<T> destination = new DestinationSerializedSubject<T>(child,
                chainRef);
        Chain<T> chain = new Chain<T>(transform, destination, maxIterations, maxChained, tester);
        chainRef.set(chain);
        // destination is not initially subscribed to the chain but will be when
        // tester function result completes
        destination.subscribe(child);
        ChainedReplaySubject<T> sub = ChainedReplaySubject.create(destination, chain, tester);
        chain.initialize(sub);
        f.onTerminateDetach() //
                .subscribe(sub);
    }

    private static enum EventType {
        TESTER_ADD, TESTER_DONE, TESTER_COMPLETE_OR_CANCEL, NEXT, ERROR, COMPLETE;
    }

    private static final class Event<T> {

        final EventType eventType;
        final ChainedReplaySubject<T> subject;
        final Subscriber<? super T> subscriber;
        final T t;
        final Throwable error;

        Event(EventType eventType, ChainedReplaySubject<T> subject,
                Subscriber<? super T> subscriber, T t, Throwable error) {
            this.eventType = eventType;
            this.subject = subject;
            this.subscriber = subscriber;
            this.t = t;
            this.error = error;
        }
    }

    @SuppressWarnings(""serial"")
    private static final class Chain<T> extends AtomicInteger implements Subscription {

        private final Function<? super Flowable<T>, ? extends Flowable<T>> transform;
        private final SimplePlainQueue<Event<T>> queue;
        private final DestinationSerializedSubject<T> destination;
        private final long maxIterations;
        private final int maxChained;
        private final Function<Observable<T>, ? extends Observable<?>> test;

        // state
        private int iteration = 1;
        private int length;
        private ChainedReplaySubject<T> finalSubscriber;
        private boolean destinationAttached;
        private volatile boolean cancelled;

        Chain(Function<? super Flowable<T>, ? extends Flowable<T>> transform,
                DestinationSerializedSubject<T> destination, long maxIterations, int maxChained,
                Function<Observable<T>, ? extends Observable<?>> test) {
            this.transform = transform;
            this.destination = destination;
            this.maxIterations = maxIterations;
            this.maxChained = maxChained;
            this.test = test;
            this.queue = new SpscLinkedArrayQueue<Event<T>>(16);
        }

        void initialize(ChainedReplaySubject<T> subject) {
            finalSubscriber = subject;
            if (maxIterations == 1) {
                finalSubscriber.subscribe(destination);
                destinationAttached = true;
            }
        }

        void tryAddSubscriber(ChainedReplaySubject<T> subject) {
            queue.offer(new Event<T>(EventType.TESTER_ADD, subject, null, null, null));
            drain();
        }

        void done(ChainedReplaySubject<T> subject) {
            queue.offer(new Event<T>(EventType.TESTER_DONE, subject, null, null, null));
            drain();
        }

        void completeOrCancel(ChainedReplaySubject<T> subject) {
            queue.offer(
                    new Event<T>(EventType.TESTER_COMPLETE_OR_CANCEL, subject, null, null, null));
            drain();
        }

        public void onError(Subscriber<? super T> child, Throwable err) {
            queue.offer(new Event<T>(EventType.ERROR, null, child, null, err));
            drain();

        }

        public void onCompleted(Subscriber<? super T> child) {
            queue.offer(new Event<T>(EventType.COMPLETE, null, child, null, null));
            drain();

        }

        public void onNext(Subscriber<? super T> child, T t) {
            queue.offer(new Event<T>(EventType.NEXT, null, child, t, null));
            drain();
        }

        void drain() {
            if (getAndIncrement() == 0) {
                if (cancelled) {
                    finalSubscriber.cancel();
                    queue.clear();
                    return;
                }
                int missed = 1;
                while (true) {
                    while (true) {
                        Event<T> v = queue.poll();
                        if (v == null) {
                            break;
                        } else if (v.eventType == EventType.TESTER_ADD) {
                            handleAdd(v);
                        } else if (v.eventType == EventType.TESTER_DONE) {
                            handleDone();
                        } else if (v.eventType == EventType.NEXT) {
                            v.subscriber.onNext(v.t);
                        } else if (v.eventType == EventType.COMPLETE) {
                            v.subscriber.onComplete();
                        } else if (v.eventType == EventType.ERROR) {
                            v.subscriber.onError(v.error);
                        } else {
                            handleCompleteOrCancel(v);
                        }
                    }
                    missed = addAndGet(-missed);
                    if (missed == 0) {
                        break;
                    }
                }
            }
        }

        private void handleAdd(Event<T> v) {
            debug(""ADD "" + v.subject);
            if (!destinationAttached && v.subject == finalSubscriber && length < maxChained
                    && !destinationAttached) {
                if (iteration <= maxIterations - 1) {
                    // ok to add another subject to the chain
                    ChainedReplaySubject<T> sub = ChainedReplaySubject.create(destination, this,
                            test);
                    if (iteration == maxIterations - 1) {
                        sub.subscribe(destination);
                        debug(sub + ""subscribed to by destination"");
                        destinationAttached = true;
                    }
                    addToChain(sub);
                    finalSubscriber = sub;
                    iteration++;
                    length += 1;
                }
            }
        }

        private void handleDone() {
            debug(""DONE"");
            if (!destinationAttached) {
                destinationAttached = true;
                finalSubscriber.subscribe(destination);
            }
        }

        private void handleCompleteOrCancel(Event<T> v) {
            debug(""COMPLETE/CANCEL "" + v.subject);
            if (destinationAttached) {
                return;
            }
            if (v.subject == finalSubscriber) {
                // TODO what to do here?
                // cancelWholeChain();
            } else if (iteration < maxIterations - 1) {
                ChainedReplaySubject<T> sub = ChainedReplaySubject.create(destination, this, test);
                addToChain(sub);
                finalSubscriber = sub;
                iteration++;
            } else if (iteration == maxIterations - 1) {
                ChainedReplaySubject<T> sub = ChainedReplaySubject.create(destination, this, test);
                destinationAttached = true;
                sub.subscribe(destination);
                addToChain(sub);
                debug(sub + ""subscribed to by destination"");
                finalSubscriber = sub;
                iteration++;
            } else {
                length--;
            }
        }

        private void addToChain(final Subscriber<T> sub) {
            Flowable<T> f;
            try {
                f = transform.apply(finalSubscriber);
            } catch (Exception e) {
                Exceptions.throwIfFatal(e);
                cancelWholeChain();
                destination.onError(e);
                return;
            }
            log(""adding subscriber to "" + finalSubscriber);
            f.onTerminateDetach().subscribe(sub);
            debug(finalSubscriber + "" subscribed to by "" + sub);
        }

        private void cancelWholeChain() {
            cancelled = true;
            drain();
        }

        @Override
        public void request(long n) {
            // ignore, just want to be able to cancel
        }

        @Override
        public void cancel() {
            cancelled = true;
            cancelWholeChain();
        }

    }

    private static class DestinationSerializedSubject<T> extends Flowable<T>
            implements FlowableSubscriber<T>, Subscription {

        private final Subscriber<? super T> child;
        private final AtomicReference<Chain<T>> chain;

        private final AtomicInteger wip = new AtomicInteger();
        private final AtomicReference<Subscription> parent = new AtomicReference<Subscription>();
        private final AtomicLong requested = new AtomicLong();
        private final SimplePlainQueue<T> queue = new SpscLinkedArrayQueue<T>(16);
        private final AtomicLong deferredRequests = new AtomicLong();

        private Throwable error;
        private volatile boolean done;
        private volatile boolean cancelled;

        DestinationSerializedSubject(Subscriber<? super T> child, AtomicReference<Chain<T>> chain) {
            this.child = child;
            this.chain = chain;
        }

        @Override
        protected void subscribeActual(Subscriber<? super T> child) {
            debug(this + "" subscribed to by "" + child);
            child.onSubscribe(new MultiSubscription(this, chain.get()));
            // don't need to drain because destination is always subscribed to
            // this before this is subscribed to parent
        }

        @Override
        public void onSubscribe(Subscription pr) {
            parent.set(pr);
            long r = deferredRequests.getAndSet(-1);
            if (r > 0L) {
                debug(this + "" requesting of parent "" + r);
                pr.request(r);
            }
            drain();
        }

        @Override
        public void request(long n) {
            debug(this + "" request "" + n);
            if (SubscriptionHelper.validate(n)) {
                BackpressureHelper.add(requested, n);
                while (true) {
                    Subscription p = parent.get();
                    long d = deferredRequests.get();
                    if (d == -1) {
                        // parent exists so can request of it
                        debug(this + "" requesting from parent "" + n);
                        p.request(n);
                        break;
                    } else {
                        long d2 = d + n;
                        if (d2 < 0) {
                            d2 = Long.MAX_VALUE;
                        }
                        if (deferredRequests.compareAndSet(d, d2)) {
                            break;
                        }
                    }
                }
                drain();
            }
        }

        @Override
        public void cancel() {
            cancelled = true;
            SubscriptionHelper.cancel(this.parent);
            chain.get().cancel();
        }

        @Override
        public void onNext(T t) {
            queue.offer(t);
            drain();
        }

        @Override
        public void onError(Throwable e) {
            error = e;
            done = true;
            drain();
        }

        @Override
        public void onComplete() {
            debug(""final complete"");
            done = true;
            drain();
        }

        private void drain() {
            // this is a pretty standard drain loop
            // default is to shortcut errors (don't delay them)
            if (wip.getAndIncrement() == 0) {
                int missed = 1;
                while (true) {
                    long r = requested.get();
                    long e = 0;
                    boolean d = done;
                    while (e != r) {
                        if (cancelled) {
                            queue.clear();
                            return;
                        }
                        if (d && terminate()) {
                            return;
                        }
                        T t = queue.poll();
                        if (t == null) {
                            if (d) {
                                cancel();
                                child.onComplete();
                                return;
                            } else {
                                break;
                            }
                        } else {
                            child.onNext(t);
                            e++;
                        }
                        d = done;
                    }
                    if (d && terminate()) {
                        return;
                    }
                    if (e != 0 && r != Long.MAX_VALUE) {
                        r = requested.addAndGet(-e);
                    }
                    missed = wip.addAndGet(-missed);
                    if (missed == 0) {
                        return;
                    }
                }
            }
        }

        private boolean terminate() {
            // done is true at this point
            Throwable err = error;
            if (err != null) {
                queue.clear();
                error = null;
                cancel();
                child.onError(err);
                return true;
            } else if (queue.isEmpty()) {
                cancel();
                child.onComplete();
                return true;
            } else {
                return false;
            }
        }

    }

    private static final class Tester<T> extends Observable<T> implements Observer<T> {

        private Observer<? super T> observer;

        @Override
        protected void subscribeActual(Observer<? super T> observer) {
            observer.onSubscribe(Disposables.empty());
            this.observer = observer;
        }

        @Override
        public void onSubscribe(Disposable d) {
            throw new RuntimeException(""unexpected"");
        }

        @Override
        public void onNext(T t) {
            observer.onNext(t);
        }

        @Override
        public void onError(Throwable e) {
            observer.onError(e);
        }

        @Override
        public void onComplete() {
            observer.onComplete();
        }
    }

    private static final class TesterObserver<T> implements Observer<Object> {

        private final Chain<T> chain;
        private final ChainedReplaySubject<T> subject;

        TesterObserver(Chain<T> chain, ChainedReplaySubject<T> subject) {
            this.chain = chain;
            this.subject = subject;
        }

        @Override
        public void onSubscribe(Disposable d) {
            // ignore
        }

        @Override
        public void onNext(Object t) {
            debug(subject + "" TestObserver emits add "" + t);
            chain.tryAddSubscriber(subject);
        }

        @Override
        public void onError(Throwable e) {
            chain.cancel();
            subject.destination().onError(e);
        }

        @Override
        public void onComplete() {
            debug(subject + "" TestObserver emits done"");
            chain.done(subject);
        }
    }

    /**
     * Requests minimally of upstream and buffers until this subscriber itself
     * is subscribed to. A maximum of {@code maxDepthConcurrent} subscribers can
     * be chained together at any one time.
     * 
     * @param <T>
     *            generic type
     */
    private static final class ChainedReplaySubject<T> extends Flowable<T>
            implements FlowableSubscriber<T>, Subscription {

        // assigned in constructor
        private final DestinationSerializedSubject<T> destination;
        private final Chain<T> chain;

        // assigned here
        private final SimplePlainQueue<T> queue = new SpscLinkedArrayQueue<T>(16);
        private final AtomicLong requested = new AtomicLong();
        private final AtomicReference<Requests<T>> requests = new AtomicReference<Requests<T>>(
                new Requests<T>(null, 0, 0, null));
        private final AtomicInteger wip = new AtomicInteger();
        private final Tester<T> tester;

        // mutable
        private volatile boolean done;
        // visibility controlled by `done`
        private Throwable error;
        private volatile boolean cancelled;
        private final Function<Observable<T>, ? extends Observable<?>> test;

        static <T> ChainedReplaySubject<T> create(DestinationSerializedSubject<T> destination,
                Chain<T> chain, Function<Observable<T>, ? extends Observable<?>> test) {
            ChainedReplaySubject<T> c = new ChainedReplaySubject<T>(destination, chain, test);
            c.init();
            return c;
        }

        private ChainedReplaySubject(DestinationSerializedSubject<T> destination, Chain<T> chain,
                Function<Observable<T>, ? extends Observable<?>> test) {
            this.destination = destination;
            this.chain = chain;
            this.test = test;
            this.tester = new Tester<T>();
        }

        private static final class Requests<T> {
            final Subscription parent;
            final long unreconciled;
            final long deferred;
            final Subscriber<? super T> child;

            Requests(Subscription parent, long unreconciled, long deferred,
                    Subscriber<? super T> child) {
                this.parent = parent;
                this.unreconciled = unreconciled;
                this.deferred = deferred;
                this.child = child;
            }
        }

        private void init() {
            Observable<?> o;
            try {
                o = test.apply(tester);
            } catch (Exception e) {
                // TODO
                throw new RuntimeException(e);
            }
            o.subscribe(new TesterObserver<T>(chain, this));
        }

        DestinationSerializedSubject<T> destination() {
            return destination;
        }

        @Override
        public void onSubscribe(Subscription parent) {
            while (true) {
                Requests<T> r = requests.get();
                Requests<T> r2;
                if (r.deferred == 0) {
                    r2 = new Requests<T>(parent, r.unreconciled + 1, 0, r.child);
                    if (requests.compareAndSet(r, r2)) {
                        parent.request(1);
                        break;
                    }
                } else {
                    r2 = new Requests<T>(parent, r.unreconciled, 0, r.child);
                    if (requests.compareAndSet(r, r2)) {
                        parent.request(r.deferred);
                        break;
                    }
                }
            }
            drain();
        }

        @Override
        protected void subscribeActual(Subscriber<? super T> child) {
            debug(this + "" subscribed with "" + child);
            while (true) {
                Requests<T> r = requests.get();
                Requests<T> r2 = new Requests<T>(r.parent, r.unreconciled, r.deferred, child);
                if (requests.compareAndSet(r, r2)) {
                    break;
                }
            }
            child.onSubscribe(this);
            drain();
        }

        @Override
        public void request(long n) {
            debug(this + "" request "" + n);
            if (SubscriptionHelper.validate(n)) {
                BackpressureHelper.add(requested, n);
                while (true) {
                    Requests<T> r = requests.get();
                    Requests<T> r2;
                    if (r.parent == null) {
                        long d = r.deferred + n;
                        if (d < 0) {
                            d = Long.MAX_VALUE;
                        }
                        r2 = new Requests<T>(r.parent, r.unreconciled, d, r.child);
                        if (requests.compareAndSet(r, r2)) {
                            break;
                        }
                    } else {
                        long x = n + r.deferred - r.unreconciled;
                        long u = Math.max(0, -x);
                        r2 = new Requests<T>(r.parent, u, 0, r.child);
                        if (requests.compareAndSet(r, r2)) {
                            if (x > 0) {
                                r.parent.request(x);
                            }
                            break;
                        }
                    }
                }
                drain();
            }
        }

        @Override
        public void onNext(T t) {
            debug(this + "" arrived "" + t);
            if (done) {
                return;
            }
            queue.offer(t);
            tester.onNext(t);
            while (true) {
                Requests<T> r = requests.get();
                Requests<T> r2;
                if (r.child == null) {
                    r2 = new Requests<T>(r.parent, r.unreconciled + 1, r.deferred, r.child);
                    if (requests.compareAndSet(r, r2)) {
                        // make minimal request to keep upstream producing
                        r.parent.request(1);
                        break;
                    }
                } else {
                    r2 = new Requests<T>(r.parent, r.unreconciled, 0, r.child);
                    if (requests.compareAndSet(r, r2)) {
                        break;
                    }
                }
            }
            drain();
        }

        @Override
        public void onComplete() {
            debug(this + "" complete"");
            if (done) {
                return;
            }
            done = true;
            cancelParent();
            debug(this + "" emits complete to tester"");
            tester.onComplete();
            drain();
        }

        @Override
        public void onError(Throwable t) {
            debug(this + "" error "" + t);
            if (done) {
                RxJavaPlugins.onError(t);
                return;
            }
            error = t;
            done = true;
            tester.onError(t);
            drain();
        }

        private void drain() {
            // this is a pretty standard drain loop
            // default is to shortcut errors (don't delay them)
            if (wip.getAndIncrement() == 0) {
                int missed = 1;
                while (true) {
                    long r = requested.get();
                    long e = 0;
                    boolean d = done;
                    while (e != r) {
                        if (cancelled) {
                            queue.clear();
                            return;
                        }
                        Subscriber<? super T> child = requests.get().child;
                        if (child == null) {
                            break;
                        }
                        Throwable err = error;
                        if (err != null) {
                            queue.clear();
                            error = null;
                            cancel();
                            chain.onError(child, err);
                            return;
                        }

                        T t = queue.poll();
                        if (t == null) {
                            if (d) {
                                cancel();
                                chain.onCompleted(child);
                                return;
                            } else {
                                break;
                            }
                        } else {
                            debug(this + "" emitting "" + t + "" to "" + requests.get().child + "":""
                                    + requests.get().child.getClass().getSimpleName());
                            chain.onNext(child, t);
                            e++;
                        }
                        d = done;
                    }
                    if (d && queue.isEmpty() && terminate()) {
                        return;
                    }
                    if (e != 0 && r != Long.MAX_VALUE) {
                        r = requested.addAndGet(-e);
                    }
                    missed = wip.addAndGet(-missed);
                    if (missed == 0) {
                        return;
                    }
                }
            }
        }

        private boolean terminate() {
            Subscriber<? super T> child = requests.get().child;
            if (child != null) {
                Throwable err = error;
                if (err != null) {
                    queue.clear();
                    error = null;
                    cancel();
                    chain.onError(child, err);
                    return true;
                } else {
                    cancel();
                    chain.onCompleted(child);
                    return true;
                }
            }
            return false;
        }

        @Override
        public void cancel() {
            if (!cancelled) {
                cancelled = true;
                cancelParentTryToAddSubscriberToChain();
            }
        }

        private void cancelParentTryToAddSubscriberToChain() {
            cancelParent();
            chain.completeOrCancel(this);
        }

        private void cancelParent() {
            Subscription par = requests.get().parent;
            if (par != null) {
                par.cancel();
            }
        }

    }

    private static final class MultiSubscription implements Subscription {

        private final Subscription primary;
        private final Subscription secondary;

        MultiSubscription(Subscription primary, Subscription secondary) {
            this.primary = primary;
            this.secondary = secondary;
        }

        @Override
        public void request(long n) {
            primary.request(n);
        }

        @Override
        public void cancel() {
            primary.cancel();
            secondary.cancel();
        }

    }

    static void debug(String message) {
        // System.out.println(message);
    }

    static void log(String message) {
        // System.out.println(message);
    }

}
","['Assertion Roulette', 'Conditional Test Logic', 'Unknown Test']","['Assertion Roulette', 'Eager Test', 'Lazy Test', 'Redundant Print', 'Redundant Assertion', 'Sensitive Equality', 'Sleepy Test']",6,2,1,10
24503_4.0_hadoop_testopportunistic,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/24503_test.java,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/24503_actual.java,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * ""License""); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.yarn.api.records.ContainerId;
import org.apache.hadoop.yarn.api.records.ExecutionType;
import org.apache.hadoop.yarn.api.records.Resource;
import org.apache.hadoop.yarn.conf.YarnConfiguration;
import org.apache.hadoop.yarn.security.ContainerTokenIdentifier;
import org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container;
import org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation;
import org.junit.Before;
import org.junit.Test;
import org.junit.Assert;

import java.util.List;
import static org.mockito.Mockito.*;

/**
 * Unit test for CGroupsMemoryResourceHandlerImpl.
 */
public class TestCGroupsMemoryResourceHandlerImpl {

  private CGroupsHandler mockCGroupsHandler;
  private CGroupsMemoryResourceHandlerImpl cGroupsMemoryResourceHandler;

  @Before
  public void setup() {
    mockCGroupsHandler = mock(CGroupsHandler.class);
    when(mockCGroupsHandler.getPathForCGroup(any(), any())).thenReturn(""."");
    cGroupsMemoryResourceHandler =
        new CGroupsMemoryResourceHandlerImpl(mockCGroupsHandler);
  }

  @Test
  public void testBootstrap() throws Exception {
    Configuration conf = new YarnConfiguration();
    conf.setBoolean(YarnConfiguration.NM_PMEM_CHECK_ENABLED, false);
    conf.setBoolean(YarnConfiguration.NM_VMEM_CHECK_ENABLED, false);
    List<PrivilegedOperation> ret =
        cGroupsMemoryResourceHandler.bootstrap(conf);
    verify(mockCGroupsHandler, times(1))
        .initializeCGroupController(CGroupsHandler.CGroupController.MEMORY);
    Assert.assertNull(ret);
    Assert.assertEquals(""Default swappiness value incorrect"", 0,
        cGroupsMemoryResourceHandler.getSwappiness());
    conf.setBoolean(YarnConfiguration.NM_PMEM_CHECK_ENABLED, true);
    try {
      cGroupsMemoryResourceHandler.bootstrap(conf);
    } catch(ResourceHandlerException re) {
      Assert.fail(""Pmem check should be allowed to run with cgroups"");
    }
    conf.setBoolean(YarnConfiguration.NM_PMEM_CHECK_ENABLED, false);
    conf.setBoolean(YarnConfiguration.NM_VMEM_CHECK_ENABLED, true);
    try {
      cGroupsMemoryResourceHandler.bootstrap(conf);
    } catch(ResourceHandlerException re) {
      Assert.fail(""Vmem check should be allowed to run with cgroups"");
    }
  }

  @Test
  public void testSwappinessValues() throws Exception {
    Configuration conf = new YarnConfiguration();
    conf.setBoolean(YarnConfiguration.NM_PMEM_CHECK_ENABLED, false);
    conf.setBoolean(YarnConfiguration.NM_VMEM_CHECK_ENABLED, false);
    conf.setInt(YarnConfiguration.NM_MEMORY_RESOURCE_CGROUPS_SWAPPINESS, -1);
    try {
      cGroupsMemoryResourceHandler.bootstrap(conf);
      Assert.fail(""Negative values for swappiness should not be allowed."");
    } catch (ResourceHandlerException re) {
      // do nothing
    }
    try {
      conf.setInt(YarnConfiguration.NM_MEMORY_RESOURCE_CGROUPS_SWAPPINESS, 101);
      cGroupsMemoryResourceHandler.bootstrap(conf);
      Assert.fail(""Values greater than 100 for swappiness""
          + "" should not be allowed."");
    } catch (ResourceHandlerException re) {
      // do nothing
    }
    conf.setInt(YarnConfiguration.NM_MEMORY_RESOURCE_CGROUPS_SWAPPINESS, 60);
    cGroupsMemoryResourceHandler.bootstrap(conf);
    Assert.assertEquals(""Swappiness value incorrect"", 60,
        cGroupsMemoryResourceHandler.getSwappiness());
  }

  @Test
  public void testPreStart() throws Exception {
    Configuration conf = new Configuration();
    conf.setBoolean(YarnConfiguration.NM_PMEM_CHECK_ENABLED, false);
    conf.setBoolean(YarnConfiguration.NM_VMEM_CHECK_ENABLED, false);
    cGroupsMemoryResourceHandler.bootstrap(conf);
    String id = ""container_01_01"";
    String path = ""test-path/"" + id;
    ContainerId mockContainerId = mock(ContainerId.class);
    when(mockContainerId.toString()).thenReturn(id);
    Container mockContainer = mock(Container.class);
    when(mockContainer.getContainerId()).thenReturn(mockContainerId);
    when(mockCGroupsHandler
        .getPathForCGroupTasks(CGroupsHandler.CGroupController.MEMORY, id))
        .thenReturn(path);
    int memory = 1024;
    when(mockContainer.getResource())
        .thenReturn(Resource.newInstance(memory, 1));
    List<PrivilegedOperation> ret =
        cGroupsMemoryResourceHandler.preStart(mockContainer);
    verify(mockCGroupsHandler, times(1))
        .createCGroup(CGroupsHandler.CGroupController.MEMORY, id);
    verify(mockCGroupsHandler, times(1))
        .updateCGroupParam(CGroupsHandler.CGroupController.MEMORY, id,
            CGroupsHandler.CGROUP_PARAM_MEMORY_HARD_LIMIT_BYTES,
            String.valueOf(memory) + ""M"");
    verify(mockCGroupsHandler, times(1))
        .updateCGroupParam(CGroupsHandler.CGroupController.MEMORY, id,
            CGroupsHandler.CGROUP_PARAM_MEMORY_SOFT_LIMIT_BYTES,
            String.valueOf((int) (memory * 0.9)) + ""M"");
    verify(mockCGroupsHandler, times(1))
        .updateCGroupParam(CGroupsHandler.CGroupController.MEMORY, id,
            CGroupsHandler.CGROUP_PARAM_MEMORY_SWAPPINESS, String.valueOf(0));
    Assert.assertNotNull(ret);
    Assert.assertEquals(1, ret.size());
    PrivilegedOperation op = ret.get(0);
    Assert.assertEquals(PrivilegedOperation.OperationType.ADD_PID_TO_CGROUP,
        op.getOperationType());
    List<String> args = op.getArguments();
    Assert.assertEquals(1, args.size());
    Assert.assertEquals(PrivilegedOperation.CGROUP_ARG_PREFIX + path,
        args.get(0));
  }

  @Test
  public void testPreStartNonEnforced() throws Exception {
    Configuration conf = new Configuration();
    conf.setBoolean(YarnConfiguration.NM_PMEM_CHECK_ENABLED, false);
    conf.setBoolean(YarnConfiguration.NM_VMEM_CHECK_ENABLED, false);
    conf.setBoolean(YarnConfiguration.NM_MEMORY_RESOURCE_ENFORCED, false);
    cGroupsMemoryResourceHandler.bootstrap(conf);
    String id = ""container_01_01"";
    String path = ""test-path/"" + id;
    ContainerId mockContainerId = mock(ContainerId.class);
    when(mockContainerId.toString()).thenReturn(id);
    Container mockContainer = mock(Container.class);
    when(mockContainer.getContainerId()).thenReturn(mockContainerId);
    when(mockCGroupsHandler
        .getPathForCGroupTasks(CGroupsHandler.CGroupController.MEMORY, id))
        .thenReturn(path);
    int memory = 1024;
    when(mockContainer.getResource())
        .thenReturn(Resource.newInstance(memory, 1));
    List<PrivilegedOperation> ret =
        cGroupsMemoryResourceHandler.preStart(mockContainer);
    verify(mockCGroupsHandler, times(1))
        .createCGroup(CGroupsHandler.CGroupController.MEMORY, id);
    verify(mockCGroupsHandler, times(0))
        .updateCGroupParam(CGroupsHandler.CGroupController.MEMORY, id,
            CGroupsHandler.CGROUP_PARAM_MEMORY_HARD_LIMIT_BYTES,
            String.valueOf(memory) + ""M"");
    verify(mockCGroupsHandler, times(0))
        .updateCGroupParam(CGroupsHandler.CGroupController.MEMORY, id,
            CGroupsHandler.CGROUP_PARAM_MEMORY_SOFT_LIMIT_BYTES,
            String.valueOf((int) (memory * 0.9)) + ""M"");
    verify(mockCGroupsHandler, times(0))
        .updateCGroupParam(CGroupsHandler.CGroupController.MEMORY, id,
            CGroupsHandler.CGROUP_PARAM_MEMORY_SWAPPINESS, String.valueOf(0));
    Assert.assertNotNull(ret);
    Assert.assertEquals(1, ret.size());
    PrivilegedOperation op = ret.get(0);
    Assert.assertEquals(PrivilegedOperation.OperationType.ADD_PID_TO_CGROUP,
        op.getOperationType());
    List<String> args = op.getArguments();
    Assert.assertEquals(1, args.size());
    Assert.assertEquals(PrivilegedOperation.CGROUP_ARG_PREFIX + path,
        args.get(0));
  }

  @Test
  public void testReacquireContainer() throws Exception {
    ContainerId containerIdMock = mock(ContainerId.class);
    Assert.assertNull(
        cGroupsMemoryResourceHandler.reacquireContainer(containerIdMock));
  }

  @Test
  public void testPostComplete() throws Exception {
    String id = ""container_01_01"";
    ContainerId mockContainerId = mock(ContainerId.class);
    when(mockContainerId.toString()).thenReturn(id);
    Assert
        .assertNull(cGroupsMemoryResourceHandler.postComplete(mockContainerId));
    verify(mockCGroupsHandler, times(1))
        .deleteCGroup(CGroupsHandler.CGroupController.MEMORY, id);
  }

  @Test
  public void testTeardown() throws Exception {
    Assert.assertNull(cGroupsMemoryResourceHandler.teardown());
  }

  @Test
  public void testOpportunistic() throws Exception {
    Configuration conf = new YarnConfiguration();
    conf.setBoolean(YarnConfiguration.NM_PMEM_CHECK_ENABLED, false);
    conf.setBoolean(YarnConfiguration.NM_VMEM_CHECK_ENABLED, false);

    cGroupsMemoryResourceHandler.bootstrap(conf);
    ContainerTokenIdentifier tokenId = mock(ContainerTokenIdentifier.class);
    when(tokenId.getExecutionType()).thenReturn(ExecutionType.OPPORTUNISTIC);
    Container container = mock(Container.class);
    String id = ""container_01_01"";
    ContainerId mockContainerId = mock(ContainerId.class);
    when(mockContainerId.toString()).thenReturn(id);
    when(container.getContainerId()).thenReturn(mockContainerId);
    when(container.getContainerTokenIdentifier()).thenReturn(tokenId);
    when(container.getResource()).thenReturn(Resource.newInstance(1024, 2));
    cGroupsMemoryResourceHandler.preStart(container);
    verify(mockCGroupsHandler, times(1))
        .updateCGroupParam(CGroupsHandler.CGroupController.MEMORY, id,
            CGroupsHandler.CGROUP_PARAM_MEMORY_SOFT_LIMIT_BYTES, ""0M"");
    verify(mockCGroupsHandler, times(1))
        .updateCGroupParam(CGroupsHandler.CGroupController.MEMORY, id,
            CGroupsHandler.CGROUP_PARAM_MEMORY_SWAPPINESS, ""100"");
    verify(mockCGroupsHandler, times(1))
        .updateCGroupParam(CGroupsHandler.CGroupController.MEMORY, id,
            CGroupsHandler.CGROUP_PARAM_MEMORY_HARD_LIMIT_BYTES, ""1024M"");
  }
}
","/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * ""License""); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources;

import org.apache.hadoop.classification.VisibleForTesting;
import org.apache.hadoop.classification.InterfaceAudience;
import org.apache.hadoop.classification.InterfaceStability;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.yarn.conf.YarnConfiguration;
import org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation;

import java.util.List;

/**
 * Handler class to handle the memory controller. YARN already ships a
 * physical memory monitor in Java but it isn't as
 * good as CGroups. This handler sets the soft and hard memory limits. The soft
 * limit is set to 90% of the hard limit.
 */
@InterfaceAudience.Private
@InterfaceStability.Unstable
public class CGroupsMemoryResourceHandlerImpl extends AbstractCGroupsMemoryResourceHandler {

  private static final int OPPORTUNISTIC_SWAPPINESS = 100;
  private int swappiness = 0;

  CGroupsMemoryResourceHandlerImpl(CGroupsHandler cGroupsHandler) {
    super(cGroupsHandler);
  }

  @Override
  public List<PrivilegedOperation> bootstrap(Configuration conf)
      throws ResourceHandlerException {
    super.bootstrap(conf);
    swappiness = conf
        .getInt(YarnConfiguration.NM_MEMORY_RESOURCE_CGROUPS_SWAPPINESS,
            YarnConfiguration.DEFAULT_NM_MEMORY_RESOURCE_CGROUPS_SWAPPINESS);
    if (swappiness < 0 || swappiness > 100) {
      throw new ResourceHandlerException(
          ""Illegal value '"" + swappiness + ""' for ""
              + YarnConfiguration.NM_MEMORY_RESOURCE_CGROUPS_SWAPPINESS
              + "". Value must be between 0 and 100."");
    }
    return null;
  }

  @VisibleForTesting
  int getSwappiness() {
    return swappiness;
  }

  @Override
  protected void updateMemoryHardLimit(String cgroupId, long containerHardLimit)
      throws ResourceHandlerException {
    getCGroupsHandler().updateCGroupParam(MEMORY, cgroupId,
        CGroupsHandler.CGROUP_PARAM_MEMORY_HARD_LIMIT_BYTES,
        String.valueOf(containerHardLimit) + ""M"");
  }

  @Override
  protected void updateOpportunisticMemoryLimits(String cgroupId) throws ResourceHandlerException {
    getCGroupsHandler().updateCGroupParam(MEMORY, cgroupId,
        CGroupsHandler.CGROUP_PARAM_MEMORY_SOFT_LIMIT_BYTES,
        String.valueOf(OPPORTUNISTIC_SOFT_LIMIT) + ""M"");
    getCGroupsHandler().updateCGroupParam(MEMORY, cgroupId,
        CGroupsHandler.CGROUP_PARAM_MEMORY_SWAPPINESS,
        String.valueOf(OPPORTUNISTIC_SWAPPINESS));
  }

  @Override
  protected void updateGuaranteedMemoryLimits(String cgroupId, long containerSoftLimit)
      throws ResourceHandlerException {
    getCGroupsHandler().updateCGroupParam(MEMORY, cgroupId,
        CGroupsHandler.CGROUP_PARAM_MEMORY_SOFT_LIMIT_BYTES,
        String.valueOf(containerSoftLimit) + ""M"");
    getCGroupsHandler().updateCGroupParam(MEMORY, cgroupId,
        CGroupsHandler.CGROUP_PARAM_MEMORY_SWAPPINESS,
        String.valueOf(swappiness));
  }
}
","['Assertion Roulette', 'Exception Catching Throwing', 'General Fixture', 'Unknown Test']",['Assertion Roulette'],0,3,1,13
4538_18.0_admiral_testfetchlogs,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/4538_test.java,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/4538_actual.java,"/*
 * Copyright (c) 2017 VMware, Inc. All Rights Reserved.
 *
 * This product is licensed to you under the Apache License, Version 2.0 (the ""License"").
 * You may not use this product except in compliance with the License.
 *
 * This product may include a number of subcomponents with separate copyright notices
 * and license terms. Your use of these subcomponents is subject to the terms and
 * conditions of the subcomponent's license, as noted in the LICENSE file.
 */

package com.vmware.admiral.adapter.kubernetes.service;

import static org.junit.Assert.assertEquals;
import static org.junit.Assert.assertTrue;
import static org.junit.Assert.fail;

import java.util.ArrayList;

import org.junit.After;
import org.junit.Before;
import org.junit.Test;

import com.vmware.admiral.adapter.common.AdapterRequest;
import com.vmware.admiral.adapter.common.KubernetesOperationType;
import com.vmware.admiral.adapter.common.service.mock.MockTaskService.MockTaskState;
import com.vmware.admiral.adapter.kubernetes.mock.BaseKubernetesMockTest;
import com.vmware.admiral.adapter.kubernetes.mock.MockKubernetesHost;
import com.vmware.admiral.adapter.kubernetes.mock.MockKubernetesHostService;
import com.vmware.admiral.compute.kubernetes.entities.common.ObjectMeta;
import com.vmware.admiral.compute.kubernetes.entities.pods.Container;
import com.vmware.admiral.compute.kubernetes.entities.pods.Pod;
import com.vmware.admiral.compute.kubernetes.entities.pods.PodSpec;
import com.vmware.admiral.compute.kubernetes.service.PodFactoryService;
import com.vmware.admiral.compute.kubernetes.service.PodService.PodState;
import com.vmware.admiral.service.common.LogService;
import com.vmware.admiral.service.common.LogService.LogServiceState;
import com.vmware.admiral.service.common.ServiceTaskCallback;
import com.vmware.photon.controller.model.resources.ComputeService.ComputeState;
import com.vmware.xenon.common.Operation;
import com.vmware.xenon.common.Service.ServiceOption;
import com.vmware.xenon.common.TaskState;
import com.vmware.xenon.common.UriUtils;

public class KubernetesAdapterServiceTest extends BaseKubernetesMockTest {
    private MockKubernetesHost service;
    private ComputeState kubernetesHostState;
    private String provisioningTaskLink;
    private String testKubernetesCredentialsLink;

    @Before
    public void startServices() throws Throwable {
        service = new MockKubernetesHost();
        // Set the service to handle all subpaths of its main path
        service.toggleOption(ServiceOption.URI_NAMESPACE_OWNER, true);
        mockKubernetesHost.startService(
                Operation.createPost(UriUtils.buildUri(
                        mockKubernetesHost, MockKubernetesHostService.SELF_LINK)),
                service);

        testKubernetesCredentialsLink = createTestKubernetesAuthCredentials();
        kubernetesHostState = createKubernetesHostComputeState(testKubernetesCredentialsLink);
    }

    @After
    public void stopServices() {
        mockKubernetesHost.stopService(service);
    }

    @Test
    public void testCreate() throws Throwable {
        PodState podState = new PodState();
        podState.pod = new Pod();
        podState.pod.spec = new PodSpec();
        podState.pod.spec.containers = new ArrayList<>();
        Container container1 = new Container();
        podState.pod.spec.containers.add(container1);
        podState.pod.metadata = new ObjectMeta();
        podState.pod.metadata.selfLink = ""/api/v1/namespaces/default/pods/test-pod"";
        podState.parentLink = kubernetesHostState.documentSelfLink;
        podState = doPost(podState, PodFactoryService.SELF_LINK);

        provisioningTaskLink = createProvisioningTask();

        AdapterRequest request = new AdapterRequest();
        request.resourceReference = UriUtils.buildUri(host, podState.documentSelfLink);
        request.serviceTaskCallback = ServiceTaskCallback.create(provisioningTaskLink);
        request.operationTypeId = KubernetesOperationType.CREATE.id;

        doOperation(KubernetesAdapterService.SELF_LINK, request);

        waitForPropertyValue(provisioningTaskLink, MockTaskState.class, ""taskInfo.stage"",
                TaskState.TaskStage.FAILED);
    }

    @Test
    public void testFetchLogs() throws Throwable {
        service.containerNamesToLogs.put(""container1"", ""test-log-1"");
        service.containerNamesToLogs.put(""container2"", ""test-log-2"");
        service.containerNamesToLogs.put(""container3"", ""test-log-3"");

        PodState podState = new PodState();
        podState.pod = new Pod();
        podState.pod.spec = new PodSpec();
        podState.pod.spec.containers = new ArrayList<>();
        Container container1 = new Container();
        container1.name = ""container1"";
        Container container2 = new Container();
        container2.name = ""container2"";
        Container container3 = new Container();
        container3.name = ""container3"";
        podState.pod.spec.containers.add(container1);
        podState.pod.spec.containers.add(container2);
        podState.pod.spec.containers.add(container3);
        podState.pod.metadata = new ObjectMeta();
        podState.pod.metadata.selfLink = ""/api/v1/namespaces/default/pods/test-pod"";
        podState.parentLink = kubernetesHostState.documentSelfLink;

        podState = doPost(podState, PodFactoryService.SELF_LINK);

        provisioningTaskLink = createProvisioningTask();

        AdapterRequest request = new AdapterRequest();
        request.resourceReference = UriUtils.buildUri(host, podState.documentSelfLink);
        request.serviceTaskCallback = ServiceTaskCallback.create(provisioningTaskLink);
        request.operationTypeId = KubernetesOperationType.FETCH_LOGS.id;

        doOperation(KubernetesAdapterService.SELF_LINK, request);

        waitForPropertyValue(provisioningTaskLink, MockTaskState.class, ""taskInfo.stage"",
                TaskState.TaskStage.FINISHED);

        for (Container container : podState.pod.spec.containers) {
            LogServiceState logState = getDocument(LogServiceState.class, LogService
                    .FACTORY_LINK + ""/"" + UriUtils.getLastPathSegment(podState.documentSelfLink)
                    + ""-""
                    + container.name);
            assertEquals(service.containerNamesToLogs.get(container.name), new String(logState
                    .logs, ""UTF-8""));
        }
    }

    @Test
    public void testInspect() throws Throwable {
        PodState podState = new PodState();
        podState.pod = new Pod();
        podState.pod.spec = new PodSpec();
        podState.pod.spec.containers = new ArrayList<>();
        Container container1 = new Container();
        container1.name = ""container1"";
        container1.image = ""test-image"";
        podState.pod.spec.containers.add(container1);
        podState.pod.metadata = new ObjectMeta();
        podState.pod.metadata.selfLink = ""/api/v1/namespaces/default/pods/test-pod"";
        podState.pod.metadata.name = ""test-pod"";
        podState.parentLink = kubernetesHostState.documentSelfLink;
        podState.kubernetesSelfLink = podState.pod.metadata.selfLink;
        podState = doPost(podState, PodFactoryService.SELF_LINK);

        Pod updatedPod = new Pod();
        updatedPod.metadata = new ObjectMeta();
        updatedPod.metadata.name = ""test-pod"";
        updatedPod.metadata.selfLink = ""/api/v1/namespaces/default/pods/test-pod"";
        updatedPod.spec = new PodSpec();
        updatedPod.spec.containers = new ArrayList<>();
        Container updatedContainer = new Container();
        updatedContainer.name = ""new-container1"";
        updatedContainer.image = ""new-test-image"";
        updatedPod.spec.containers.add(updatedContainer);

        service.inspectMap.put(podState.pod, updatedPod);

        provisioningTaskLink = createProvisioningTask();

        AdapterRequest request = new AdapterRequest();
        request.resourceReference = UriUtils.buildUri(host, podState.documentSelfLink);
        request.serviceTaskCallback = ServiceTaskCallback.create(provisioningTaskLink);
        request.operationTypeId = KubernetesOperationType.INSPECT.id;
        doOperation(KubernetesAdapterService.SELF_LINK, request);

        waitForPropertyValue(provisioningTaskLink, MockTaskState.class, ""taskInfo.stage"",
                TaskState.TaskStage.FINISHED);

        PodState patchedPod = getDocument(PodState.class, podState.documentSelfLink);

        assertEquals(podState.descriptionLink, patchedPod.descriptionLink);
        assertEquals(podState.compositeComponentLink, patchedPod.compositeComponentLink);
        assertEquals(podState.parentLink, patchedPod.parentLink);

        assertEquals(updatedContainer.name, patchedPod.pod.spec.containers.get(0).name);
        assertEquals(updatedContainer.image, patchedPod.pod.spec.containers.get(0).image);
    }

    @Test
    public void testDelete() throws Throwable {
        PodState podState = new PodState();
        podState.pod = new Pod();
        podState.pod.spec = new PodSpec();
        podState.pod.spec.containers = new ArrayList<>();
        Container container1 = new Container();
        container1.name = ""container1"";
        container1.image = ""test-image"";
        podState.pod.spec.containers.add(container1);
        podState.pod.metadata = new ObjectMeta();
        podState.pod.metadata.selfLink = ""/api/v1/namespaces/default/pods/test-pod"";
        podState.pod.metadata.name = ""test-pod"";
        podState.parentLink = kubernetesHostState.documentSelfLink;
        podState.kubernetesSelfLink = podState.pod.metadata.selfLink;
        podState = doPost(podState, PodFactoryService.SELF_LINK);

        service.deployedElementsMap.put(""test-pod"", podState.pod);
        assertTrue(service.deployedElementsMap.size() == 1);

        provisioningTaskLink = createProvisioningTask();

        AdapterRequest request = new AdapterRequest();
        request.resourceReference = UriUtils.buildUri(host, podState.documentSelfLink);
        request.serviceTaskCallback = ServiceTaskCallback.create(provisioningTaskLink);
        request.operationTypeId = KubernetesOperationType.DELETE.id;

        doOperation(KubernetesAdapterService.SELF_LINK, request);

        waitForPropertyValue(provisioningTaskLink, MockTaskState.class, ""taskInfo.stage"",
                TaskState.TaskStage.FINISHED);

        final String selfLink = podState.documentSelfLink;
        final long timeoutInMillis = 5000; // 5sec
        long startTime = System.currentTimeMillis();

        waitFor(() -> {
            if (System.currentTimeMillis() - startTime > timeoutInMillis) {
                fail(String.format(""Entity [%s] not deletes within %s ms"", selfLink, timeoutInMillis));
            }

            return service.deployedElementsMap.size() == 0;
        });
    }
}
","/*
 * Copyright (c) 2017 VMware, Inc. All Rights Reserved.
 *
 * This product is licensed to you under the Apache License, Version 2.0 (the ""License"").
 * You may not use this product except in compliance with the License.
 *
 * This product may include a number of subcomponents with separate copyright notices
 * and license terms. Your use of these subcomponents is subject to the terms and
 * conditions of the subcomponent's license, as noted in the LICENSE file.
 */

package com.vmware.admiral.adapter.kubernetes.service;

import static com.vmware.admiral.compute.content.kubernetes.KubernetesUtil.fromResourceStateToBaseKubernetesState;

import java.util.HashMap;
import java.util.Map;
import java.util.Map.Entry;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.atomic.AtomicBoolean;
import java.util.concurrent.atomic.AtomicInteger;

import com.vmware.admiral.adapter.common.AdapterRequest;
import com.vmware.admiral.adapter.common.KubernetesOperationType;
import com.vmware.admiral.adapter.kubernetes.KubernetesRemoteApiClient;
import com.vmware.admiral.common.ManagementUriParts;
import com.vmware.admiral.compute.container.CompositeComponentRegistry;
import com.vmware.admiral.compute.content.kubernetes.KubernetesUtil;
import com.vmware.admiral.compute.kubernetes.entities.pods.Container;
import com.vmware.admiral.compute.kubernetes.service.BaseKubernetesState;
import com.vmware.admiral.compute.kubernetes.service.KubernetesDescriptionService.KubernetesDescription;
import com.vmware.admiral.compute.kubernetes.service.PodFactoryService;
import com.vmware.admiral.compute.kubernetes.service.PodService.PodState;
import com.vmware.admiral.service.common.LogService;
import com.vmware.admiral.service.common.LogService.LogServiceState;
import com.vmware.xenon.common.Operation;
import com.vmware.xenon.common.Operation.CompletionHandler;
import com.vmware.xenon.common.TaskState.TaskStage;

public class KubernetesAdapterService extends AbstractKubernetesAdapterService {
    public static final String SELF_LINK = ManagementUriParts.ADAPTER_KUBERNETES;

    private static final String LOG_FETCH_FAILED_FORMAT = ""Unable to fetch logs for container: %s""
            + "" error: %s"";

    private static class RequestContext {
        public AdapterRequest request;
        public BaseKubernetesState kubernetesState;
        public KubernetesDescription kubernetesDescription;
        public KubernetesContext k8sContext;
        public KubernetesRemoteApiClient executor;
    }

    @Override
    public void handlePatch(Operation op) {
        RequestContext context = new RequestContext();
        context.request = op.getBody(AdapterRequest.class);
        context.request.validate();

        KubernetesOperationType operationType = KubernetesOperationType.instanceById(context.request
                .operationTypeId);

        op.complete();

        logInfo(""Processing kubernetes operation request %s for resource %s."",
                operationType, context.request.resourceReference);

        processKubernetesRequest(context);
    }

    private void processKubernetesRequest(RequestContext context) {

        sendRequest(Operation
                .createGet(context.request.resourceReference)
                .setCompletion((o, ex) -> {
                    if (ex != null) {
                        fail(context.request, ex);
                    } else {
                        Class<? extends BaseKubernetesState> stateType = null;
                        try {
                            stateType = fromResourceStateToBaseKubernetesState(
                                    CompositeComponentRegistry.metaByStateLink(
                                            context.request.resourceReference
                                                    .getPath()).stateClass);
                            context.kubernetesState = o.getBody(stateType);
                            processKubernetesState(context);
                        } catch (IllegalArgumentException iae) {
                            fail(context.request, iae);
                        }
                    }
                }));
    }

    private void processKubernetesState(RequestContext context) {
        if (context.kubernetesState.parentLink == null) {
            fail(context.request, new IllegalArgumentException(""parentLink missing""));
            return;
        }

        getComputeHost(
                context.request,
                null,
                context.request.resolve(context.kubernetesState.parentLink),
                (k8sContext) -> {
                    context.k8sContext = k8sContext;
                    context.executor = getApiClient();

                    processOperation(context);
                });
    }

    private void processOperation(RequestContext context) {
        try {
            KubernetesOperationType operationType = KubernetesOperationType.instanceById(context
                    .request.operationTypeId);
            switch (operationType) {
            case CREATE:
                fail(context.request,
                        new IllegalArgumentException(""Unsupported request type: "" + operationType));
                break;

            case DELETE:
                processDeleteKubernetesEntity(context);
                break;

            case FETCH_LOGS:
                processFetchPodLogs(context);
                break;

            case INSPECT:
                inspectKubernetesEntity(context);
                break;

            default:
                fail(context.request,
                        new IllegalArgumentException(""Unexpected request type: "" + operationType));
            }
        } catch (Throwable e) {
            fail(context.request, e);
        }
    }

    private void processDeleteKubernetesEntity(RequestContext context) {
        context.executor
                .deleteEntity(context.kubernetesState.kubernetesSelfLink, context.k8sContext,
                        (o, ex) -> {
                            if (ex != null) {
                                fail(context.request, ex);
                            } else {
                                patchTaskStage(context.request, TaskStage.FINISHED, null);
                            }
                        });
    }

    private void processFetchPodLogs(RequestContext context) {
        if (!context.kubernetesState.documentSelfLink.startsWith(PodFactoryService.SELF_LINK)) {
            throw new IllegalArgumentException(""Cannot fetch logs for types that are not pods."");
        }

        PodState podState = (PodState) context.kubernetesState;

        Map<String, String> containerNamesToLogLinks = new HashMap<>();
        for (Container container : podState.pod.spec.containers) {
            String logLink = podState.kubernetesSelfLink + ""/log?container="" + container.name;
            containerNamesToLogLinks.put(container.name, logLink);
        }

        Map<String, String> containerNameToLogOutput = new ConcurrentHashMap<>();

        AtomicInteger counter = new AtomicInteger(containerNamesToLogLinks.size());

        for (Entry<String, String> containerNameToLogLink : containerNamesToLogLinks.entrySet()) {
            String name = containerNameToLogLink.getKey();
            String logLink = containerNameToLogLink.getValue();
            context.executor.fetchLogs(logLink, context.k8sContext, (o, ex) -> {
                if (ex != null) {
                    containerNameToLogOutput.put(name, String.format(LOG_FETCH_FAILED_FORMAT,
                            name, ex.getMessage()));
                } else {
                    String log = o.getBody(String.class);
                    if (log == null || log.isEmpty()) {
                        log = ""--"";
                    }
                    containerNameToLogOutput.put(name, log);
                }
                if (counter.decrementAndGet() == 0) {
                    processFetchedLogs(context, containerNameToLogOutput);
                }
            });
        }

    }

    private void processFetchedLogs(RequestContext context, Map<String, String> logs) {
        AtomicInteger counter = new AtomicInteger(logs.size());
        AtomicBoolean hasError = new AtomicBoolean(false);

        for (Entry<String, String> log : logs.entrySet()) {
            LogServiceState logServiceState = new LogServiceState();
            logServiceState.documentSelfLink = KubernetesUtil.buildLogUriPath(context
                    .kubernetesState, log.getKey());
            logServiceState.logs = log.getValue().getBytes();
            logServiceState.tenantLinks = context.kubernetesState.tenantLinks;

            sendRequest(Operation.createPost(this, LogService.FACTORY_LINK)
                    .setBody(logServiceState)
                    .setContextId(context.request.getRequestId())
                    .setCompletion((o, ex) -> {
                        if (ex != null) {
                            if (hasError.compareAndSet(false, true)) {
                                fail(context.request, ex);
                            }
                        } else {
                            if (counter.decrementAndGet() == 0 && !hasError.get()) {
                                patchTaskStage(context.request, TaskStage.FINISHED, null);
                            }
                        }
                    }));
        }
    }

    private void inspectKubernetesEntity(RequestContext context) {
        context.executor.inspectEntity(context.kubernetesState.kubernetesSelfLink,
                context.k8sContext, (o, ex) -> {
                    if (ex != null) {
                        fail(context.request, ex);
                    } else {
                        patchKubernetesEntity(context, o, (op, err) -> {
                            if (err != null) {
                                fail(context.request, err);
                            } else {
                                patchTaskStage(context.request, TaskStage.FINISHED, null);
                            }
                        });
                    }
                });
    }

    private void patchKubernetesEntity(RequestContext context, Operation inspectResponse,
            CompletionHandler handler) {
        String jsonResponse = inspectResponse.getBody(String.class);
        BaseKubernetesState newState = null;
        try {
            newState = context.kubernetesState.getClass().newInstance();
            newState.setKubernetesEntityFromJson(jsonResponse);
        } catch (Throwable ex) {
            fail(context.request, ex);
        }
        sendRequest(Operation.createPatch(this, context.kubernetesState.documentSelfLink)
                .setBody(newState)
                .setCompletion(handler));
    }
}
","['Assertion Roulette', 'Conditional Test Logic', 'Unknown Test']","['Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette']",0,2,1,14
11377_69.0_search-highlighter_commontermsquerynoremove,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/11377_test.java,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/11377_actual.java,"package org.wikimedia.highlighter.experimental.lucene;

import static org.hamcrest.Matchers.not;
import static org.mockito.Matchers.any;
import static org.mockito.Matchers.anyFloat;
import static org.mockito.Matchers.anyInt;
import static org.mockito.Matchers.anyString;
import static org.mockito.Matchers.eq;
import static org.mockito.Matchers.isNull;
import static org.mockito.Mockito.mock;
import static org.mockito.Mockito.never;
import static org.mockito.Mockito.times;
import static org.mockito.Mockito.verify;
import static org.mockito.Mockito.when;
import static org.wikimedia.highlighter.experimental.lucene.LuceneMatchers.recognises;

import java.io.Closeable;
import java.io.IOException;
import java.util.ArrayList;
import java.util.Collections;
import java.util.List;

import org.apache.lucene.analysis.core.KeywordAnalyzer;
import org.apache.lucene.document.Field;
import org.apache.lucene.document.TextField;
import org.apache.lucene.index.DirectoryReader;
import org.apache.lucene.index.IndexReader;
import org.apache.lucene.index.IndexWriter;
import org.apache.lucene.index.Term;
import org.apache.lucene.queries.CommonTermsQuery;
import org.apache.lucene.search.BooleanClause;
import org.apache.lucene.search.BooleanClause.Occur;
import org.apache.lucene.search.BooleanQuery;
import org.apache.lucene.search.BoostQuery;
import org.apache.lucene.search.FuzzyQuery;
import org.apache.lucene.search.PhraseQuery;
import org.apache.lucene.search.PrefixQuery;
import org.apache.lucene.search.Query;
import org.apache.lucene.search.RegexpQuery;
import org.apache.lucene.search.SynonymQuery;
import org.apache.lucene.search.TermQuery;
import org.apache.lucene.search.WildcardQuery;
import org.apache.lucene.store.Directory;
import org.apache.lucene.util.LuceneTestCase;
import org.apache.lucene.util.TestUtil;
import org.apache.lucene.util.automaton.Automaton;
import org.hamcrest.Matcher;
import org.junit.Test;
import org.mockito.ArgumentCaptor;
import org.wikimedia.highlighter.experimental.lucene.QueryFlattener.Callback;

import com.google.common.collect.Lists;

@SuppressWarnings(""checkstyle:classfanoutcomplexity"") // do not care too much about complexity of test classes
public class QueryFlattenerTest extends LuceneTestCase {
    private final List<Closeable> toClose = new ArrayList<>();
    private final Term bar = new Term(""foo"", ""bar"");
    private final Term baz = new Term(""foo"", ""baz"");

    @Test
    public void termQuery() {
        Callback callback = mock(Callback.class);
        new QueryFlattener().flatten(new TermQuery(bar), null, callback);
        verify(callback).flattened(bar.bytes(), 1f, null);
    }

    @Test
    public void phraseQueryPhraseAsPhrase() {
        phraseQueryTestCase(false);
    }

    @Test
    public void phraseQueryPhraseAsTerms() {
        phraseQueryTestCase(true);
    }

    private void phraseQueryTestCase(boolean phraseAsTerms) {
        Callback callback = mock(Callback.class);
        PhraseQuery.Builder q = new PhraseQuery.Builder();
        q.add(bar);
        q.add(baz);
        new QueryFlattener(1, phraseAsTerms, true).flatten(q.build(), null, callback);
        verify(callback).flattened(bar.bytes(), phraseAsTerms ? 1f : 0, null);
        verify(callback).flattened(baz.bytes(), phraseAsTerms ? 1f : 0, null);
        if (phraseAsTerms) {
            verify(callback, never()).startPhrase(anyInt(), anyFloat());
            verify(callback, never()).startPhrasePosition(anyInt());
            verify(callback, never()).endPhrasePosition();
            verify(callback, never()).endPhrase(anyString(), anyInt(), anyFloat());
        } else {
            verify(callback).startPhrase(2, 1);
            verify(callback, times(2)).startPhrasePosition(1);
            verify(callback, times(2)).endPhrasePosition();
            verify(callback).endPhrase(""foo"", 0, 1);
        }
    }

    @Test
    public void booleanQuery() {
        Callback callback = mock(Callback.class);
        BooleanQuery.Builder bq = new BooleanQuery.Builder();
        bq.add(new BooleanClause(new TermQuery(bar), Occur.MUST));
        bq.add(new BooleanClause(new TermQuery(baz), Occur.MUST_NOT));
        new QueryFlattener().flatten(bq.build(), null, callback);
        verify(callback).flattened(bar.bytes(), 1f, null);
        verify(callback, never()).flattened(eq(baz.bytes()), anyFloat(), isNull(Query.class));
    }

    @Test
    public void boostQuery() {
        Callback callback = mock(Callback.class);
        BoostQuery bq = new BoostQuery(new TermQuery(bar), 2f);
        new QueryFlattener().flatten(bq, null, callback);
        verify(callback).flattened(bar.bytes(), 2f, null);
    }

    @Test
    public void rewritten() throws IOException {
        Callback callback = mock(Callback.class);
        Query rewritten = mock(Query.class);
        when(rewritten.rewrite(null)).thenReturn(new TermQuery(bar));
        new QueryFlattener().flatten(rewritten, null, callback);
        verify(callback).flattened(bar.bytes(), 1f, rewritten);
    }

    @Test
    public void fuzzyQuery() {
        flattenedToAutomatonThatMatches(new FuzzyQuery(bar), recognises(bar), recognises(baz), recognises(""barr""), recognises(""bor""),
                not(recognises(""barrrr"")));
    }

    @Test
    public void fuzzyQueryShorterThenPrefix() {
        Callback callback = mock(Callback.class);
        new QueryFlattener().flatten(new FuzzyQuery(bar, 2, 100), null, callback);
        verify(callback).flattened(bar.bytes(), 1f, null);
        verify(callback, never()).flattened(any(Automaton.class), anyFloat(), anyInt());
    }

    @Test
    public void regexpQuery() {
        flattenedToAutomatonThatMatches(new RegexpQuery(new Term(""test"", ""ba[zr]"")), recognises(bar), recognises(baz),
                not(recognises(""barr"")));
    }

    @Test
    public void wildcardQuery() {
        flattenedToAutomatonThatMatches(new WildcardQuery(new Term(""test"", ""ba?"")), recognises(bar), recognises(baz),
                not(recognises(""barr"")));

        flattenedToAutomatonThatMatches(new WildcardQuery(new Term(""test"", ""ba*"")), recognises(bar), recognises(baz), recognises(""barr""),
                not(recognises(""bor"")));
    }

    @Test
    public void prefixQuery() {
        flattenedToAutomatonThatMatches(new PrefixQuery(new Term(""test"", ""ba"")), recognises(bar), recognises(baz), recognises(""barr""),
                not(recognises(""bor"")));
    }

    @Test
    public void commonTermsQueryNoRemove() {
        IndexReader reader = readerWithTerms(bar, randomIntBetween(1, 20), baz, randomIntBetween(1, 20));
        Callback callback = mock(Callback.class);
        CommonTermsQuery q = new CommonTermsQuery(Occur.SHOULD, Occur.MUST, 10f);
        q.add(bar);
        q.add(baz);
        new QueryFlattener(100, false, false).flatten(q, reader, callback);
        verify(callback).flattened(bar.bytes(), 1f, null);
        verify(callback).flattened(baz.bytes(), 1f, null);
    }

    @Test
    public void commonTermsQueryAllCommon() {
        IndexReader reader = readerWithTerms(bar, randomIntBetween(11, 20), baz, randomIntBetween(11, 20));
        Callback callback = mock(Callback.class);
        CommonTermsQuery q = new CommonTermsQuery(Occur.SHOULD, Occur.MUST, 10f);
        q.add(bar);
        q.add(baz);
        new QueryFlattener().flatten(q, reader, callback);
        verify(callback).flattened(bar.bytes(), 1f, null);
        verify(callback).flattened(baz.bytes(), 1f, null);
    }

    @Test
    public void commonTermsQueryAllUncommon() {
        IndexReader reader = readerWithTerms(bar, randomIntBetween(1, 10), baz, randomIntBetween(1, 10));
        Callback callback = mock(Callback.class);
        CommonTermsQuery q = new CommonTermsQuery(Occur.SHOULD, Occur.MUST, 10f);
        q.add(bar);
        q.add(baz);
        new QueryFlattener().flatten(q, reader, callback);
        verify(callback).flattened(bar.bytes(), 1f, null);
        verify(callback).flattened(baz.bytes(), 1f, null);
    }

    @Test
    public void commonTermsQueryOneUncommon() {
        IndexReader reader = readerWithTerms(bar, randomIntBetween(1, 10), baz, randomIntBetween(11, 20));
        Callback callback = mock(Callback.class);
        CommonTermsQuery q = new CommonTermsQuery(Occur.SHOULD, Occur.MUST, 10f);
        q.add(bar);
        q.add(baz);
        new QueryFlattener().flatten(q, reader, callback);
        verify(callback).flattened(bar.bytes(), 1f, null);
        verify(callback, never()).flattened(eq(baz.bytes()), anyFloat(), any(Object.class));
    }

    @Test
    public void testSynonym() {
        Callback callback = mock(Callback.class);
        SynonymQuery.Builder synonymBuilder = new SynonymQuery.Builder(""foo"");
        new QueryFlattener().flatten(synonymBuilder.addTerm(bar).addTerm(baz).build(), null, callback);
        verify(callback).flattened(bar.bytes(), 1f, null);
        verify(callback).flattened(baz.bytes(), 1f, null);
    }

    @SafeVarargs
    private final void flattenedToAutomatonThatMatches(Query query, Matcher<Automaton>... matchers) {
        Callback callback = mock(Callback.class);
        new QueryFlattener().flatten(query, null, callback);
        ArgumentCaptor<Automaton> a = ArgumentCaptor.forClass(Automaton.class);
        verify(callback).flattened(a.capture(), eq(1f), anyInt());
        for (Matcher<Automaton> matcher : matchers) {
            assertThat(a.getValue(), matcher);
        }
    }

    private IndexReader readerWithTerms(Object... termsAndFreqs) {
        try {
            assertEquals(""Expected an even number of terms and freqs"", 0, termsAndFreqs.length % 2);
            Directory dir = newDirectory();
            toClose.add(dir);
            try (IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new KeywordAnalyzer()))) {
                for (int i = 0; i < termsAndFreqs.length; i += 2) {
                    Term term = (Term) termsAndFreqs[i];
                    int freq = ((Number) termsAndFreqs[i + 1]).intValue();
                    for (int f = 0; f < freq; f++) {
                        writer.addDocument(Collections.singleton(new TextField(term.field(), term.text(), Field.Store.NO)));
                    }
                }
            }
            IndexReader reader = DirectoryReader.open(dir);
            toClose.add(reader);
            return reader;
        } catch (IOException e) {
            throw new RuntimeException(e);
        }
    }

    private int randomIntBetween(int min, int max) {
        return TestUtil.nextInt(random(), min, max);
    }

    @Override
    public void tearDown() throws Exception {
        super.tearDown();

        for (Closeable c : Lists.reverse(toClose)) {
            c.close();
        }
        toClose.clear();
    }
}
","package org.wikimedia.highlighter.experimental.lucene;

import java.io.IOException;
import java.util.HashSet;
import java.util.List;
import java.util.Objects;
import java.util.Set;

import org.apache.lucene.index.IndexReader;
import org.apache.lucene.index.Term;
import org.apache.lucene.queries.CommonTermsQuery;
import org.apache.lucene.search.BooleanClause;
import org.apache.lucene.search.BooleanClause.Occur;
import org.apache.lucene.search.BooleanQuery;
import org.apache.lucene.search.BoostQuery;
import org.apache.lucene.search.ConstantScoreQuery;
import org.apache.lucene.search.DisjunctionMaxQuery;
import org.apache.lucene.search.FuzzyQuery;
import org.apache.lucene.search.MultiPhraseQuery;
import org.apache.lucene.search.MultiTermQuery;
import org.apache.lucene.search.MultiTermQuery.TopTermsScoringBooleanQueryRewrite;
import org.apache.lucene.search.PhraseQuery;
import org.apache.lucene.search.PrefixQuery;
import org.apache.lucene.search.Query;
import org.apache.lucene.search.RegexpQuery;
import org.apache.lucene.search.SynonymQuery;
import org.apache.lucene.search.TermQuery;
import org.apache.lucene.search.WildcardQuery;
import org.apache.lucene.search.spans.SpanNearQuery;
import org.apache.lucene.search.spans.SpanNotQuery;
import org.apache.lucene.search.spans.SpanOrQuery;
import org.apache.lucene.search.spans.SpanPositionCheckQuery;
import org.apache.lucene.search.spans.SpanQuery;
import org.apache.lucene.search.spans.SpanTermQuery;
import org.apache.lucene.util.BytesRef;
import org.apache.lucene.util.UnicodeUtil;
import org.apache.lucene.util.automaton.Automata;
import org.apache.lucene.util.automaton.Automaton;
import org.apache.lucene.util.automaton.LevenshteinAutomata;
import org.apache.lucene.util.automaton.Operations;

import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;

/**
 * Flattens {@link Query}s similarly to Lucene's FieldQuery.
 */
@SuppressWarnings(""checkstyle:classfanoutcomplexity"") // should be fixed at some point
@SuppressFBWarnings(value = ""UCC_UNRELATED_COLLECTION_CONTENTS"", justification = ""sentAutomata is used to check different kinds objects"")
public class QueryFlattener {
    /**
     * Some queries are inefficient to rebuild multiple times so we store some
     * information about them here and check if we've already seen them.
     */
    private final Set<Object> sentAutomata = new HashSet<>();
    private final int maxMultiTermQueryTerms;
    private final boolean phraseAsTerms;
    private final boolean removeHighFrequencyTermsFromCommonTerms;

    /**
     * Default configuration.
     */
    public QueryFlattener() {
        this(1000, false, true);
    }

    public QueryFlattener(int maxMultiTermQueryTerms, boolean phraseAsTerms, boolean removeHighFrequencyTermsFromCommonTerms) {
        this.maxMultiTermQueryTerms = maxMultiTermQueryTerms;
        this.phraseAsTerms = phraseAsTerms;
        this.removeHighFrequencyTermsFromCommonTerms = removeHighFrequencyTermsFromCommonTerms;
    }

    public interface Callback {
        /**
         * Called once per query containing the term.
         *
         * @param term the term
         * @param boost weight of the term
         * @param sourceOverride null if the source of the term is the query
         *            containing it, not null if the term query came from some
         *            rewritten query
         */
        void flattened(BytesRef term, float boost, Object sourceOverride);

        /**
         * Called with each new automaton. QueryFlattener makes an effort to
         * only let the first copy of any duplicate automata through.
         *
         * @param automaton automaton from the query
         * @param boost weight of terms matchign the automaton
         * @param source hashcode of the source. Automata don't have a hashcode
         *            so this will always provide the source.
         */
        void flattened(Automaton automaton, float boost, int source);

        /**
         * Called to mark the start of a phrase.
         */
        void startPhrase(int positionCount, float boost);

        void startPhrasePosition(int termCount);

        void endPhrasePosition();

        /**
         * Called to mark the end of a phrase.
         */
        void endPhrase(String field, int slop, float boost);
    }

    public void flatten(Query query, IndexReader reader, Callback callback) {
        flatten(query, 1f, null, reader, callback);
    }

    /**
     * Should phrase queries be returned as terms?
     *
     * @return true mean skip startPhrase and endPhrase and give the terms in a
     *         phrase the weight of the whole phrase
     */
    protected boolean phraseAsTerms() {
        return phraseAsTerms;
    }

    @SuppressWarnings(""checkstyle:CyclomaticComplexity"") // cyclomatic complexity is high, but the code is simple to read
    protected void flatten(Query query, float pathBoost, Object sourceOverride, IndexReader reader,
            Callback callback) {
        if (query instanceof TermQuery) {
            flattenQuery((TermQuery) query, pathBoost, sourceOverride, reader, callback);
        } else if (query instanceof BoostQuery) {
            flattenQuery((BoostQuery) query, pathBoost, sourceOverride, reader, callback);
        } else if (query instanceof PhraseQuery) {
            flattenQuery((PhraseQuery) query, pathBoost, sourceOverride, reader, callback);
        } else if (query instanceof BooleanQuery) {
            flattenQuery((BooleanQuery) query, pathBoost, sourceOverride, reader, callback);
        } else if (query instanceof DisjunctionMaxQuery) {
            flattenQuery((DisjunctionMaxQuery) query, pathBoost, sourceOverride, reader, callback);
        } else if (query instanceof ConstantScoreQuery) {
            flattenQuery((ConstantScoreQuery) query, pathBoost, sourceOverride, reader, callback);
        } else if (query instanceof MultiPhraseQuery) {
            flattenQuery((MultiPhraseQuery) query, pathBoost, sourceOverride, reader, callback);
        } else if (query instanceof SpanQuery
                && flattenSpan((SpanQuery) query, pathBoost, sourceOverride, reader, callback)) {
            // Actually nothing to do here, but it keeps the code lining up to
            // have it.
        } else if (query instanceof FuzzyQuery) {
            flattenQuery((FuzzyQuery) query, pathBoost, sourceOverride, reader, callback);
        } else if (query instanceof RegexpQuery) {
            flattenQuery((RegexpQuery) query, pathBoost, sourceOverride, reader, callback);
        } else if (query instanceof WildcardQuery) {
            flattenQuery((WildcardQuery) query, pathBoost, sourceOverride, reader, callback);
        } else if (query instanceof PrefixQuery) {
            flattenQuery((PrefixQuery) query, pathBoost, sourceOverride, reader, callback);
        } else if (query instanceof CommonTermsQuery) {
            flattenQuery((CommonTermsQuery) query, pathBoost, sourceOverride, reader, callback);
        } else if (query instanceof SynonymQuery) {
            flattenQuery((SynonymQuery) query, pathBoost, sourceOverride, reader, callback);
        } else if (!flattenUnknown(query, pathBoost, sourceOverride, reader, callback)) {
            Query newRewritten = rewriteQuery(query, pathBoost, sourceOverride, reader);
            if (newRewritten != query) {
                // only rewrite once and then flatten again - the rewritten
                // query could have a special treatment
                flatten(newRewritten, pathBoost, query, reader, callback);
            }
        }
    }

    protected boolean flattenSpan(SpanQuery query, float pathBoost, Object sourceOverride,
            IndexReader reader, Callback callback) {
        if (query instanceof SpanTermQuery) {
            flattenQuery((SpanTermQuery) query, pathBoost, sourceOverride, reader, callback);
            return true;
        } else if (query instanceof SpanPositionCheckQuery) {
            flattenQuery((SpanPositionCheckQuery) query, pathBoost, sourceOverride, reader,
                    callback);
            return true;
        } else if (query instanceof SpanNearQuery) {
            flattenQuery((SpanNearQuery) query, pathBoost, sourceOverride, reader, callback);
            return true;
        } else if (query instanceof SpanNotQuery) {
            flattenQuery((SpanNotQuery) query, pathBoost, sourceOverride, reader, callback);
            return true;
        } else if (query instanceof SpanOrQuery) {
            flattenQuery((SpanOrQuery) query, pathBoost, sourceOverride, reader, callback);
            return true;
        }
        return false;
    }

    protected boolean flattenUnknown(Query query, float pathBoost, Object sourceOverride,
            IndexReader reader, Callback callback) {
        return false;
    }

    protected void flattenQuery(TermQuery query, float pathBoost, Object sourceOverride,
            IndexReader reader, Callback callback) {
        callback.flattened(query.getTerm().bytes(), pathBoost, sourceOverride);
    }

    protected void flattenQuery(BoostQuery query, float pathBoost, Object sourceOverride,
            IndexReader reader, Callback callback) {
        flatten(query.getQuery(), query.getBoost() * pathBoost, sourceOverride, reader, callback);
    }

    protected void flattenQuery(PhraseQuery query, float pathBoost, Object sourceOverride,
            IndexReader reader, Callback callback) {
        Term[] terms = query.getTerms();
        if (terms.length == 0) {
            return;
        }
        if (phraseAsTerms) {
            for (Term term : terms) {
                callback.flattened(term.bytes(), pathBoost, sourceOverride);
            }
        } else {
            callback.startPhrase(terms.length, pathBoost);
            for (Term term : terms) {
                callback.startPhrasePosition(1);
                callback.flattened(term.bytes(), 0, sourceOverride);
                callback.endPhrasePosition();
            }
            callback.endPhrase(terms[0].field(), query.getSlop(), pathBoost);
        }
    }

    @SuppressFBWarnings(
            value = ""OCP_OVERLY_CONCRETE_PARAMETER"",
            justification = ""Using a specific type is required as different behaviour are expected"")
    protected void flattenQuery(BooleanQuery query, float pathBoost, Object sourceOverride,
            IndexReader reader, Callback callback) {
        for (BooleanClause clause : query) {
            // Exclude FILTER clauses with isScoring(), before lucene 5 most of
            // these queries were wrapped inside a FitleredQuery
            // but now the prefered way is to add a boolean clause with
            // Occur.FILTER
            // e.g. the _type filter with elasticsearch now uses this type of
            // construct.
            if (!clause.isProhibited() && clause.isScoring()) {
                flatten(clause.getQuery(), pathBoost, sourceOverride, reader,
                        callback);
            }
        }
    }

    @SuppressFBWarnings(
            value = ""OCP_OVERLY_CONCRETE_PARAMETER"",
            justification = ""Using a specific type is required as different behaviour are expected"")
    protected void flattenQuery(DisjunctionMaxQuery query, float pathBoost, Object sourceOverride,
            IndexReader reader, Callback callback) {
        for (Query clause : query) {
            flatten(clause, pathBoost, sourceOverride, reader, callback);
        }
    }

    protected void flattenQuery(ConstantScoreQuery query, float pathBoost, Object sourceOverride,
            IndexReader reader, Callback callback) {
        if (query.getQuery() != null) {
            flatten(query.getQuery(), pathBoost, sourceOverride, reader,
                    callback);
        }
        // TODO maybe flatten filter like Elasticsearch does
    }

    protected void flattenQuery(MultiPhraseQuery query, float pathBoost, Object sourceOverride,
            IndexReader reader, Callback callback) {
        // Elasticsearch uses a more complicated method to preserve the phrase
        // queries.
        Term[][] termArrays = query.getTermArrays();

        if (phraseAsTerms) {
            for (Term[] terms : termArrays) {
                for (Term term : terms) {
                    callback.flattened(term.bytes(), pathBoost, sourceOverride);
                }
            }
        } else {
            callback.startPhrase(termArrays.length, pathBoost);
            String field = null;
            for (Term[] terms : termArrays) {
                callback.startPhrasePosition(terms.length);
                for (Term term : terms) {
                    callback.flattened(term.bytes(), 0, sourceOverride);
                    field = term.field();
                }
                callback.endPhrasePosition();
            }
            // field will be null if there are no terms in the phrase which
            // would be weird
            if (field != null) {
                callback.endPhrase(field, query.getSlop(), pathBoost);
            }
        }
    }

    protected void flattenQuery(SpanTermQuery query, float pathBoost, Object sourceOverride,
            IndexReader reader, Callback callback) {
        callback.flattened(query.getTerm().bytes(), pathBoost, sourceOverride);
    }

    protected void flattenQuery(SpanPositionCheckQuery query, float pathBoost,
            Object sourceOverride, IndexReader reader, Callback callback) {
        flattenSpan(query.getMatch(), pathBoost, sourceOverride, reader,
                callback);
    }

    protected void flattenQuery(SpanNearQuery query, float pathBoost, Object sourceOverride,
            IndexReader reader, Callback callback) {
        for (SpanQuery clause : query.getClauses()) {
            flattenSpan(clause, pathBoost, sourceOverride, reader, callback);
        }
    }

    protected void flattenQuery(SpanNotQuery query, float pathBoost, Object sourceOverride,
            IndexReader reader, Callback callback) {
        flattenSpan(query.getInclude(), pathBoost, sourceOverride, reader,
                callback);
    }

    protected void flattenQuery(SpanOrQuery query, float pathBoost, Object sourceOverride,
            IndexReader reader, Callback callback) {
        for (SpanQuery clause : query.getClauses()) {
            flattenSpan(clause, pathBoost, sourceOverride, reader, callback);
        }
    }

    protected void flattenQuery(RegexpQuery query, float pathBoost, Object sourceOverride,
            IndexReader reader, Callback callback) {
        // This isn't a great ""source"" because it contains the term's field but
        // its the best we can do here
        if (!sentAutomata.add(query)) {
            return;
        }
        int source = sourceOverride == null ? query.hashCode() : sourceOverride.hashCode();
        callback.flattened(query.getAutomaton(), pathBoost, source);
    }

    protected void flattenQuery(WildcardQuery query, float pathBoost, Object sourceOverride,
            IndexReader reader, Callback callback) {
        // Should be safe not to copy this because it is fixed...
        if (!sentAutomata.add(query.getTerm().bytes())) {
            return;
        }
        Object source = sourceOverride == null ? query.getTerm().bytes() : sourceOverride;
        callback.flattened(query.getAutomaton(), pathBoost, source.hashCode());
    }

    protected void flattenQuery(SynonymQuery query, float pathBoost, Object sourceOverride,
                                IndexReader reader, Callback callback) {
        for (Term t : query.getTerms()) {
            callback.flattened(t.bytes(), pathBoost, sourceOverride);
        }
    }

    protected void flattenQuery(PrefixQuery query, float pathBoost, Object sourceOverride,
            IndexReader reader, Callback callback) {
        flattenPrefixQuery(query.getPrefix().bytes(), pathBoost, sourceOverride,
                callback);
    }

    protected void flattenPrefixQuery(BytesRef bytes, float boost, Object sourceOverride,
            Callback callback) {
        // Should be safe not to copy this because it is fixed...
        if (!sentAutomata.add(bytes)) {
            return;
        }
        Object source = sourceOverride == null ? bytes : sourceOverride;
        Automaton automaton = Automata.makeString(bytes.utf8ToString());
        automaton = Operations.concatenate(automaton, Automata.makeAnyString());
        callback.flattened(automaton, boost, source.hashCode());
    }

    protected void flattenQuery(FuzzyQuery query, float pathBoost, Object sourceOverride,
            IndexReader reader, Callback callback) {
        float boost = pathBoost;
        if (query.getMaxEdits() == 0) {
            callback.flattened(query.getTerm().bytes(), boost, sourceOverride);
        }
        String term = query.getTerm().bytes().utf8ToString();
        if (query.getPrefixLength() >= term.length()) {
            callback.flattened(query.getTerm().bytes(), boost, sourceOverride);
            return;
        }

        FuzzyQueryInfo key = new FuzzyQueryInfo(term, query);
        if (!sentAutomata.add(key)) {
            return;
        }
        // Make an effort to resolve the fuzzy query to an automata
        Automaton automaton = getFuzzyAutomata(query, term);
        Object source = sourceOverride == null ? key : sourceOverride;
        callback.flattened(automaton, boost, source.hashCode());
    }

    private Automaton getFuzzyAutomata(FuzzyQuery query, String term) {
        int termLength = term.length();
        int[] termText = new int[term.codePointCount(0, termLength)];
        for (int cp, i = 0, j = 0; i < termLength; i += Character.charCount(cp)) {
            cp = term.codePointAt(i);
            termText[j++] = cp;
        }
        int prefixLen = query.getPrefixLength() > termText.length ? termText.length : query.getPrefixLength();
        int editDistance = query.getMaxEdits();
        if (editDistance > LevenshteinAutomata.MAXIMUM_SUPPORTED_DISTANCE) {
            editDistance = LevenshteinAutomata.MAXIMUM_SUPPORTED_DISTANCE;
        }
        LevenshteinAutomata automata = new LevenshteinAutomata(UnicodeUtil.newString(termText, prefixLen, termText.length - prefixLen),
                query.getTranspositions());
        Automaton automaton;
        if (prefixLen > 0) {
            automaton = automata.toAutomaton(editDistance, UnicodeUtil.newString(termText, 0, prefixLen));
        } else {
            automaton = automata.toAutomaton(editDistance);
        }
        return automaton;
    }

    @SuppressWarnings({""checkstyle:CyclomaticComplexity"", ""checkstyle:NPathComplexity""})
    protected void flattenQuery(CommonTermsQuery query, float pathBoost, Object sourceOverride,
            IndexReader reader, Callback callback) {
        Query rewritten = rewriteQuery(query, pathBoost, sourceOverride, reader);
        if (!removeHighFrequencyTermsFromCommonTerms) {
            flatten(rewritten, pathBoost, sourceOverride, reader, callback);
            return;
        }
        /*
         * Try to figure out if the query was rewritten into a list of low and
         * high frequency terms. If it was, remove the high frequency terms.
         *
         * Note that this only works if high frequency terms are set to
         * Occur.SHOULD and low frequency terms are set to Occur.MUST. That is
         * usually the way it is done though.
         */
        if (!(rewritten instanceof BooleanQuery)) {
            // Nope - its a term query or something more exotic
            flatten(rewritten, pathBoost, sourceOverride, reader, callback);
            return;
        }
        BooleanQuery bq = (BooleanQuery) rewritten;
        List<BooleanClause> clauses = bq.clauses();
        if (clauses.size() != 2) {
            // Nope - its just a list of terms.
            flattenQuery(bq, pathBoost, sourceOverride, reader, callback);
            return;
        }
        BooleanClause first = clauses.get(0);
        BooleanClause second = clauses.get(1);
        if ((first.getOccur() != Occur.SHOULD || second.getOccur() != Occur.MUST)
                && (first.getOccur() != Occur.MUST || second.getOccur() != Occur.SHOULD)) {
            // Nope - just a two term query
            flattenQuery(bq, pathBoost, sourceOverride, reader, callback);
            return;
        }

        Query firstQ = first.getQuery();
        Query secondQ = second.getQuery();

        // The query can be wrapped inside a BoostQuery
        if (firstQ instanceof BoostQuery && secondQ instanceof BoostQuery) {
            firstQ = ((BoostQuery)firstQ).getQuery();
            secondQ = ((BoostQuery)secondQ).getQuery();
        }

        if (!(firstQ instanceof BooleanQuery && secondQ instanceof BooleanQuery)) {
            // Nope - terms of the wrong type. not sure how that happened.
            flattenQuery(bq, pathBoost, sourceOverride, reader, callback);
            return;
        }

        final Query lowFrequency;
        if (first.getOccur() == Occur.MUST) {
            lowFrequency = first.getQuery();
        } else {
            lowFrequency = second.getQuery();
        }
        flatten(lowFrequency, pathBoost, sourceOverride, reader, callback);
    }

    protected Query rewriteQuery(MultiTermQuery query, float pathBoost, Object sourceOverride, IndexReader reader) {
        TopTermsScoringBooleanQueryRewrite method = new MultiTermQuery.TopTermsScoringBooleanQueryRewrite(
                maxMultiTermQueryTerms);
        try {
            return method.rewrite(reader, query);
        } catch (IOException ioe) {
            throw new WrappedExceptionFromLucene(ioe);
        }
    }

    protected Query rewriteQuery(Query query, float pathBoost, Object sourceOverride, IndexReader reader) {
        if (query instanceof MultiTermQuery) {
            return rewriteQuery((MultiTermQuery) query, pathBoost, sourceOverride, reader);
        }
        return rewritePreparedQuery(query, pathBoost, sourceOverride, reader);
    }

    /**
     * Rewrites a query that's already ready for rewriting.
     */
    protected Query rewritePreparedQuery(Query query, float pathBoost, Object sourceOverride, IndexReader reader) {
        try {
            return query.rewrite(reader);
        } catch (IOException e) {
            throw new WrappedExceptionFromLucene(e);
        }
    }

    private static class FuzzyQueryInfo {
        private final String term;
        private final int maxEdits;
        private final boolean transpositions;
        private final int prefixLength;

        FuzzyQueryInfo(String term, FuzzyQuery query) {
            this.term = term;
            this.maxEdits = query.getMaxEdits();
            this.transpositions = query.getTranspositions();
            this.prefixLength = query.getPrefixLength();
        }

        // Eclipse made these:
        @Override
        public int hashCode() {
            return Objects.hash(maxEdits, prefixLength, term, transpositions);
        }

        @Override
        public boolean equals(Object obj) {
            if (this == obj)
                return true;
            if (obj == null)
                return false;
            if (getClass() != obj.getClass())
                return false;
            FuzzyQueryInfo other = (FuzzyQueryInfo) obj;
            return Objects.equals(maxEdits, other.maxEdits)
                    && Objects.equals(prefixLength, other.prefixLength)
                    && Objects.equals(term, other.term)
                    && Objects.equals(transpositions, other.transpositions);
        }
    }
}
",['Unknown Test'],['Assertion Roulette'],1,1,0,15
37429_45.0_hbase_testtablelistxml,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/37429_test.java,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/37429_actual.java,"/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * ""License""); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hadoop.hbase.rest;

import static org.junit.Assert.assertEquals;
import static org.junit.Assert.assertTrue;

import java.io.ByteArrayInputStream;
import java.io.IOException;
import java.net.InetSocketAddress;
import java.util.ArrayList;
import java.util.Iterator;
import java.util.List;
import javax.xml.bind.JAXBContext;
import javax.xml.bind.JAXBException;
import org.apache.hadoop.hbase.CellUtil;
import org.apache.hadoop.hbase.HBaseClassTestRule;
import org.apache.hadoop.hbase.HBaseTestingUtil;
import org.apache.hadoop.hbase.HRegionLocation;
import org.apache.hadoop.hbase.ServerName;
import org.apache.hadoop.hbase.TableName;
import org.apache.hadoop.hbase.client.Connection;
import org.apache.hadoop.hbase.client.Durability;
import org.apache.hadoop.hbase.client.Put;
import org.apache.hadoop.hbase.client.RegionInfo;
import org.apache.hadoop.hbase.client.RegionLocator;
import org.apache.hadoop.hbase.client.Table;
import org.apache.hadoop.hbase.rest.client.Client;
import org.apache.hadoop.hbase.rest.client.Cluster;
import org.apache.hadoop.hbase.rest.client.Response;
import org.apache.hadoop.hbase.rest.model.TableInfoModel;
import org.apache.hadoop.hbase.rest.model.TableListModel;
import org.apache.hadoop.hbase.rest.model.TableModel;
import org.apache.hadoop.hbase.rest.model.TableRegionModel;
import org.apache.hadoop.hbase.testclassification.MediumTests;
import org.apache.hadoop.hbase.testclassification.RestTests;
import org.apache.hadoop.hbase.util.Bytes;
import org.junit.AfterClass;
import org.junit.BeforeClass;
import org.junit.ClassRule;
import org.junit.Test;
import org.junit.experimental.categories.Category;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

@Category({ RestTests.class, MediumTests.class })
public class TestTableResource {

  @ClassRule
  public static final HBaseClassTestRule CLASS_RULE =
    HBaseClassTestRule.forClass(TestTableResource.class);

  private static final Logger LOG = LoggerFactory.getLogger(TestTableResource.class);

  private static final TableName TABLE = TableName.valueOf(""TestTableResource"");
  private static final String COLUMN_FAMILY = ""test"";
  private static final String COLUMN = COLUMN_FAMILY + "":qualifier"";
  private static final int NUM_REGIONS = 4;
  private static List<HRegionLocation> regionMap;

  private static final HBaseTestingUtil TEST_UTIL = new HBaseTestingUtil();
  private static final HBaseRESTTestingUtility REST_TEST_UTIL = new HBaseRESTTestingUtility();
  private static Client client;
  private static JAXBContext context;

  @BeforeClass
  public static void setUpBeforeClass() throws Exception {
    TEST_UTIL.startMiniCluster(3);
    REST_TEST_UTIL.startServletContainer(TEST_UTIL.getConfiguration());
    client = new Client(new Cluster().add(""localhost"", REST_TEST_UTIL.getServletPort()));
    context = JAXBContext.newInstance(TableModel.class, TableInfoModel.class, TableListModel.class,
      TableRegionModel.class);
    TEST_UTIL.createMultiRegionTable(TABLE, Bytes.toBytes(COLUMN_FAMILY), NUM_REGIONS);
    byte[] k = new byte[3];
    byte[][] famAndQf = CellUtil.parseColumn(Bytes.toBytes(COLUMN));
    List<Put> puts = new ArrayList<>();
    for (byte b1 = 'a'; b1 < 'z'; b1++) {
      for (byte b2 = 'a'; b2 < 'z'; b2++) {
        for (byte b3 = 'a'; b3 < 'z'; b3++) {
          k[0] = b1;
          k[1] = b2;
          k[2] = b3;
          Put put = new Put(k);
          put.setDurability(Durability.SKIP_WAL);
          put.addColumn(famAndQf[0], famAndQf[1], k);
          puts.add(put);
        }
      }
    }

    Connection connection = TEST_UTIL.getConnection();

    Table table = connection.getTable(TABLE);
    table.put(puts);
    table.close();

    RegionLocator regionLocator = connection.getRegionLocator(TABLE);
    List<HRegionLocation> m = regionLocator.getAllRegionLocations();

    // should have four regions now
    assertEquals(NUM_REGIONS, m.size());
    regionMap = m;
    LOG.error(""regions: "" + regionMap);
    regionLocator.close();
  }

  @AfterClass
  public static void tearDownAfterClass() throws Exception {
    REST_TEST_UTIL.shutdownServletContainer();
    TEST_UTIL.shutdownMiniCluster();
  }

  private static void checkTableList(TableListModel model) {
    boolean found = false;
    Iterator<TableModel> tables = model.getTables().iterator();
    assertTrue(tables.hasNext());
    while (tables.hasNext()) {
      TableModel table = tables.next();
      if (table.getName().equals(TABLE.getNameAsString())) {
        found = true;
        break;
      }
    }
    assertTrue(found);
  }

  void checkTableInfo(TableInfoModel model) {
    assertEquals(model.getName(), TABLE.getNameAsString());
    Iterator<TableRegionModel> regions = model.getRegions().iterator();
    assertTrue(regions.hasNext());
    while (regions.hasNext()) {
      TableRegionModel region = regions.next();
      boolean found = false;
      LOG.debug(""looking for region "" + region.getName());
      for (HRegionLocation e : regionMap) {
        RegionInfo hri = e.getRegion();
        // getRegionNameAsString uses Bytes.toStringBinary which escapes some non-printable
        // characters
        String hriRegionName = Bytes.toString(hri.getRegionName());
        String regionName = region.getName();
        LOG.debug(""comparing to region "" + hriRegionName);
        if (hriRegionName.equals(regionName)) {
          found = true;
          byte[] startKey = hri.getStartKey();
          byte[] endKey = hri.getEndKey();
          ServerName serverName = e.getServerName();
          InetSocketAddress sa =
            new InetSocketAddress(serverName.getHostname(), serverName.getPort());
          String location = sa.getHostName() + "":"" + Integer.valueOf(sa.getPort());
          assertEquals(hri.getRegionId(), region.getId());
          assertTrue(Bytes.equals(startKey, region.getStartKey()));
          assertTrue(Bytes.equals(endKey, region.getEndKey()));
          assertEquals(location, region.getLocation());
          break;
        }
      }
      assertTrue(""Couldn't find region "" + region.getName(), found);
    }
  }

  @Test
  public void testTableListText() throws IOException {
    Response response = client.get(""/"", Constants.MIMETYPE_TEXT);
    assertEquals(200, response.getCode());
    assertEquals(Constants.MIMETYPE_TEXT, response.getHeader(""content-type""));
  }

  @Test
  public void testTableListXML() throws IOException, JAXBException {
    Response response = client.get(""/"", Constants.MIMETYPE_XML);
    assertEquals(200, response.getCode());
    assertEquals(Constants.MIMETYPE_XML, response.getHeader(""content-type""));
    TableListModel model = (TableListModel) context.createUnmarshaller()
      .unmarshal(new ByteArrayInputStream(response.getBody()));
    checkTableList(model);
  }

  @Test
  public void testTableListJSON() throws IOException {
    Response response = client.get(""/"", Constants.MIMETYPE_JSON);
    assertEquals(200, response.getCode());
    assertEquals(Constants.MIMETYPE_JSON, response.getHeader(""content-type""));
  }

  @Test
  public void testTableListPB() throws IOException, JAXBException {
    Response response = client.get(""/"", Constants.MIMETYPE_PROTOBUF);
    assertEquals(200, response.getCode());
    assertEquals(Constants.MIMETYPE_PROTOBUF, response.getHeader(""content-type""));
    TableListModel model = new TableListModel();
    model.getObjectFromMessage(response.getBody());
    checkTableList(model);
    response = client.get(""/"", Constants.MIMETYPE_PROTOBUF_IETF);
    assertEquals(200, response.getCode());
    assertEquals(Constants.MIMETYPE_PROTOBUF_IETF, response.getHeader(""content-type""));
    model = new TableListModel();
    model.getObjectFromMessage(response.getBody());
    checkTableList(model);
  }

  @Test
  public void testTableInfoText() throws IOException {
    Response response = client.get(""/"" + TABLE + ""/regions"", Constants.MIMETYPE_TEXT);
    assertEquals(200, response.getCode());
    assertEquals(Constants.MIMETYPE_TEXT, response.getHeader(""content-type""));
  }

  @Test
  public void testTableInfoXML() throws IOException, JAXBException {
    Response response = client.get(""/"" + TABLE + ""/regions"", Constants.MIMETYPE_XML);
    assertEquals(200, response.getCode());
    assertEquals(Constants.MIMETYPE_XML, response.getHeader(""content-type""));
    TableInfoModel model = (TableInfoModel) context.createUnmarshaller()
      .unmarshal(new ByteArrayInputStream(response.getBody()));
    checkTableInfo(model);
  }

  @Test
  public void testTableInfoJSON() throws IOException {
    Response response = client.get(""/"" + TABLE + ""/regions"", Constants.MIMETYPE_JSON);
    assertEquals(200, response.getCode());
    assertEquals(Constants.MIMETYPE_JSON, response.getHeader(""content-type""));
  }

  @Test
  public void testTableInfoPB() throws IOException, JAXBException {
    Response response = client.get(""/"" + TABLE + ""/regions"", Constants.MIMETYPE_PROTOBUF);
    assertEquals(200, response.getCode());
    assertEquals(Constants.MIMETYPE_PROTOBUF, response.getHeader(""content-type""));
    TableInfoModel model = new TableInfoModel();
    model.getObjectFromMessage(response.getBody());
    checkTableInfo(model);
    response = client.get(""/"" + TABLE + ""/regions"", Constants.MIMETYPE_PROTOBUF_IETF);
    assertEquals(200, response.getCode());
    assertEquals(Constants.MIMETYPE_PROTOBUF_IETF, response.getHeader(""content-type""));
    model = new TableInfoModel();
    model.getObjectFromMessage(response.getBody());
    checkTableInfo(model);
  }

  @Test
  public void testTableNotFound() throws IOException {
    String notExistTable = ""notexist"";
    Response response1 = client.get(""/"" + notExistTable + ""/schema"", Constants.MIMETYPE_JSON);
    assertEquals(404, response1.getCode());
    Response response2 = client.get(""/"" + notExistTable + ""/regions"", Constants.MIMETYPE_XML);
    assertEquals(404, response2.getCode());
  }

}
","/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * ""License""); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hadoop.hbase.rest;

import java.io.IOException;
import java.util.Base64.Decoder;
import java.util.List;
import org.apache.hadoop.hbase.CellUtil;
import org.apache.hadoop.hbase.TableName;
import org.apache.hadoop.hbase.client.Scan;
import org.apache.hadoop.hbase.client.Table;
import org.apache.hadoop.hbase.filter.Filter;
import org.apache.hadoop.hbase.filter.FilterList;
import org.apache.hadoop.hbase.filter.ParseFilter;
import org.apache.hadoop.hbase.filter.PrefixFilter;
import org.apache.hadoop.hbase.util.Bytes;
import org.apache.yetus.audience.InterfaceAudience;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import org.apache.hbase.thirdparty.javax.ws.rs.DefaultValue;
import org.apache.hbase.thirdparty.javax.ws.rs.Encoded;
import org.apache.hbase.thirdparty.javax.ws.rs.HeaderParam;
import org.apache.hbase.thirdparty.javax.ws.rs.Path;
import org.apache.hbase.thirdparty.javax.ws.rs.PathParam;
import org.apache.hbase.thirdparty.javax.ws.rs.QueryParam;

@InterfaceAudience.Private
public class TableResource extends ResourceBase {

  String table;
  private static final Logger LOG = LoggerFactory.getLogger(TableResource.class);

  private static final Decoder base64Urldecoder = java.util.Base64.getUrlDecoder();

  /**
   * Constructor
   */
  public TableResource(String table) throws IOException {
    super();
    this.table = table;
  }

  /** Returns the table name */
  String getName() {
    return table;
  }

  /** Returns true if the table exists n */
  boolean exists() throws IOException {
    return servlet.getAdmin().tableExists(TableName.valueOf(table));
  }

  @Path(""exists"")
  public ExistsResource getExistsResource() throws IOException {
    return new ExistsResource(this);
  }

  @Path(""regions"")
  public RegionsResource getRegionsResource() throws IOException {
    return new RegionsResource(this);
  }

  @Path(""scanner"")
  public ScannerResource getScannerResource() throws IOException {
    return new ScannerResource(this);
  }

  @Path(""schema"")
  public SchemaResource getSchemaResource() throws IOException {
    return new SchemaResource(this);
  }

  @Path(""{multiget: multiget.*}"")
  public MultiRowResource getMultipleRowResource(final @QueryParam(""v"") String versions,
    @PathParam(""multiget"") String path) throws IOException {
    return new MultiRowResource(this, versions, path.replace(""multiget"", """").replace(""/"", """"));
  }

  @Path(""{rowspec: [^*]+}"")
  public RowResource getRowResource(
    // We need the @Encoded decorator so Jersey won't urldecode before
    // the RowSpec constructor has a chance to parse
    final @PathParam(""rowspec"") @Encoded String rowspec, final @QueryParam(""v"") String versions,
    final @QueryParam(""check"") String check, final @QueryParam(""rr"") String returnResult,
    final @HeaderParam(""Encoding"") String keyEncodingHeader,
    final @QueryParam(Constants.KEY_ENCODING_QUERY_PARAM_NAME) String keyEncodingQuery)
    throws IOException {
    String keyEncoding = (keyEncodingHeader != null) ? keyEncodingHeader : keyEncodingQuery;
    return new RowResource(this, rowspec, versions, check, returnResult, keyEncoding);
  }

  // TODO document
  @Path(""{suffixglobbingspec: .*\\*/.+}"")
  public RowResource getRowResourceWithSuffixGlobbing(
    // We need the @Encoded decorator so Jersey won't urldecode before
    // the RowSpec constructor has a chance to parse
    final @PathParam(""suffixglobbingspec"") @Encoded String suffixglobbingspec,
    final @QueryParam(""v"") String versions, final @QueryParam(""check"") String check,
    final @QueryParam(""rr"") String returnResult,
    final @HeaderParam(""Encoding"") String keyEncodingHeader,
    final @QueryParam(Constants.KEY_ENCODING_QUERY_PARAM_NAME) String keyEncodingQuery)
    throws IOException {
    String keyEncoding = (keyEncodingHeader != null) ? keyEncodingHeader : keyEncodingQuery;
    return new RowResource(this, suffixglobbingspec, versions, check, returnResult, keyEncoding);
  }

  // TODO document
  // FIXME handle binary rowkeys (like put and delete does)
  @Path(""{scanspec: .*[*]$}"")
  public TableScanResource getScanResource(final @PathParam(""scanspec"") String scanSpec,
    @DefaultValue(Integer.MAX_VALUE + """") @QueryParam(Constants.SCAN_LIMIT) int userRequestedLimit,
    @DefaultValue("""") @QueryParam(Constants.SCAN_START_ROW) String startRow,
    @DefaultValue("""") @QueryParam(Constants.SCAN_END_ROW) String endRow,
    @QueryParam(Constants.SCAN_COLUMN) List<String> column,
    @DefaultValue(""1"") @QueryParam(Constants.SCAN_MAX_VERSIONS) int maxVersions,
    @DefaultValue(""-1"") @QueryParam(Constants.SCAN_BATCH_SIZE) int batchSize,
    @DefaultValue(""0"") @QueryParam(Constants.SCAN_START_TIME) long startTime,
    @DefaultValue(Long.MAX_VALUE + """") @QueryParam(Constants.SCAN_END_TIME) long endTime,
    @DefaultValue(""true"") @QueryParam(Constants.SCAN_CACHE_BLOCKS) boolean cacheBlocks,
    @DefaultValue(""false"") @QueryParam(Constants.SCAN_REVERSED) boolean reversed,
    @QueryParam(Constants.FILTER) String paramFilter,
    @QueryParam(Constants.FILTER_B64) @Encoded String paramFilterB64) {
    try {
      Filter prefixFilter = null;
      Scan tableScan = new Scan();
      if (scanSpec.indexOf('*') > 0) {
        String prefix = scanSpec.substring(0, scanSpec.indexOf('*'));
        byte[] prefixBytes = Bytes.toBytes(prefix);
        prefixFilter = new PrefixFilter(Bytes.toBytes(prefix));
        if (startRow.isEmpty()) {
          tableScan.withStartRow(prefixBytes);
        }
      }
      if (LOG.isTraceEnabled()) {
        LOG.trace(""Query parameters  : Table Name = > "" + this.table + "" Start Row => "" + startRow
          + "" End Row => "" + endRow + "" Columns => "" + column + "" Start Time => "" + startTime
          + "" End Time => "" + endTime + "" Cache Blocks => "" + cacheBlocks + "" Max Versions => ""
          + maxVersions + "" Batch Size => "" + batchSize);
      }
      Table hTable = RESTServlet.getInstance().getTable(this.table);
      tableScan.setBatch(batchSize);
      tableScan.readVersions(maxVersions);
      tableScan.setTimeRange(startTime, endTime);
      if (!startRow.isEmpty()) {
        tableScan.withStartRow(Bytes.toBytes(startRow));
      }
      tableScan.withStopRow(Bytes.toBytes(endRow));
      for (String col : column) {
        byte[][] parts = CellUtil.parseColumn(Bytes.toBytes(col.trim()));
        if (parts.length == 1) {
          if (LOG.isTraceEnabled()) {
            LOG.trace(""Scan family : "" + Bytes.toStringBinary(parts[0]));
          }
          tableScan.addFamily(parts[0]);
        } else if (parts.length == 2) {
          if (LOG.isTraceEnabled()) {
            LOG.trace(""Scan family and column : "" + Bytes.toStringBinary(parts[0]) + ""  ""
              + Bytes.toStringBinary(parts[1]));
          }
          tableScan.addColumn(parts[0], parts[1]);
        } else {
          throw new IllegalArgumentException(""Invalid column specifier."");
        }
      }
      FilterList filterList = new FilterList();
      byte[] filterBytes = null;
      if (paramFilterB64 != null) {
        filterBytes = base64Urldecoder.decode(paramFilterB64);
      } else if (paramFilter != null) {
        filterBytes = paramFilter.getBytes();
      }
      if (filterBytes != null) {
        // Note that this is a completely different representation of the filters
        // than the JSON one used in the /table/scanner endpoint
        ParseFilter pf = new ParseFilter();
        Filter parsedParamFilter = pf.parseFilterString(filterBytes);
        if (parsedParamFilter != null) {
          filterList.addFilter(parsedParamFilter);
        }
      }
      if (prefixFilter != null) {
        filterList.addFilter(prefixFilter);
      }
      if (filterList.size() > 0) {
        tableScan.setFilter(filterList);
      }

      int fetchSize = this.servlet.getConfiguration().getInt(Constants.SCAN_FETCH_SIZE, 10);
      tableScan.setCaching(fetchSize);
      tableScan.setReversed(reversed);
      tableScan.setCacheBlocks(cacheBlocks);
      return new TableScanResource(hTable.getScanner(tableScan), userRequestedLimit);
    } catch (IOException exp) {
      servlet.getMetrics().incrementFailedScanRequests(1);
      processException(exp);
      LOG.warn(exp.toString(), exp);
      return null;
    }
  }
}
",['Assertion Roulette'],"['Assertion Roulette', 'Eager Test', 'Magic Number Test', 'Mystery Guest', 'Redundant Print']",4,0,1,14
28095_4.0_hadoop_testlistlocatedstatus,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/28095_test.java,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/28095_actual.java,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * ""License""); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hadoop.mapred;

import java.io.FileNotFoundException;
import java.io.IOException;
import java.util.Arrays;
import java.util.Collection;
import java.util.List;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.BlockLocation;
import org.apache.hadoop.fs.FileStatus;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.LocatedFileStatus;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.fs.PathFilter;
import org.apache.hadoop.fs.RawLocalFileSystem;
import org.apache.hadoop.fs.RemoteIterator;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.util.Lists;
import org.junit.After;
import org.junit.Assert;
import org.junit.Before;
import org.junit.Test;
import org.junit.runner.RunWith;
import org.junit.runners.Parameterized;
import org.junit.runners.Parameterized.Parameters;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

@RunWith(value = Parameterized.class)
public class TestFileInputFormat {
  
  private static final Logger LOG =
      LoggerFactory.getLogger(TestFileInputFormat.class);
  
  private static String testTmpDir = System.getProperty(""test.build.data"", ""/tmp"");
  private static final Path TEST_ROOT_DIR = new Path(testTmpDir, ""TestFIF"");
  
  private static FileSystem localFs;
  
  private int numThreads;
  
  public TestFileInputFormat(int numThreads) {
    this.numThreads = numThreads;
    LOG.info(""Running with numThreads: "" + numThreads);
  }
  
  @Parameters
  public static Collection<Object[]> data() {
    Object[][] data = new Object[][] { { 1 }, { 5 }};
    return Arrays.asList(data);
  }
  
  @Before
  public void setup() throws IOException {
    LOG.info(""Using Test Dir: "" + TEST_ROOT_DIR);
    localFs = FileSystem.getLocal(new Configuration());
    localFs.delete(TEST_ROOT_DIR, true);
    localFs.mkdirs(TEST_ROOT_DIR);
  }
  
  @After
  public void cleanup() throws IOException {
    localFs.delete(TEST_ROOT_DIR, true);
  }
  
  @Test
  public void testListLocatedStatus() throws Exception {
    Configuration conf = getConfiguration();
    conf.setBoolean(""fs.test.impl.disable.cache"", false);
    conf.setInt(FileInputFormat.LIST_STATUS_NUM_THREADS, numThreads);
    conf.set(org.apache.hadoop.mapreduce.lib.input.FileInputFormat.INPUT_DIR,
        ""test:///a1/a2"");
    MockFileSystem mockFs =
        (MockFileSystem) new Path(""test:///"").getFileSystem(conf);
    Assert.assertEquals(""listLocatedStatus already called"",
        0, mockFs.numListLocatedStatusCalls);
    JobConf job = new JobConf(conf);
    TextInputFormat fileInputFormat = new TextInputFormat();
    fileInputFormat.configure(job);
    InputSplit[] splits = fileInputFormat.getSplits(job, 1);
    Assert.assertEquals(""Input splits are not correct"", 2, splits.length);
    Assert.assertEquals(""listLocatedStatuss calls"",
        1, mockFs.numListLocatedStatusCalls);
    FileSystem.closeAll();
  }

  @Test
  public void testIgnoreDirs() throws Exception {
    Configuration conf = getConfiguration();
    conf.setBoolean(FileInputFormat.INPUT_DIR_NONRECURSIVE_IGNORE_SUBDIRS, true);
    conf.setInt(FileInputFormat.LIST_STATUS_NUM_THREADS, numThreads);
    conf.set(org.apache.hadoop.mapreduce.lib.input.FileInputFormat.INPUT_DIR, ""test:///a1"");
    MockFileSystem mockFs = (MockFileSystem) new Path(""test:///"").getFileSystem(conf);
    JobConf job = new JobConf(conf);
    TextInputFormat fileInputFormat = new TextInputFormat();
    fileInputFormat.configure(job);
    InputSplit[] splits = fileInputFormat.getSplits(job, 1);
    Assert.assertEquals(""Input splits are not correct"", 1, splits.length);
    FileSystem.closeAll();
  }

  @Test
  public void testSplitLocationInfo() throws Exception {
    Configuration conf = getConfiguration();
    conf.set(org.apache.hadoop.mapreduce.lib.input.FileInputFormat.INPUT_DIR,
        ""test:///a1/a2"");
    JobConf job = new JobConf(conf);
    TextInputFormat fileInputFormat = new TextInputFormat();
    fileInputFormat.configure(job);
    FileSplit[] splits = (FileSplit[]) fileInputFormat.getSplits(job, 1);
    String[] locations = splits[0].getLocations();
    Assert.assertEquals(2, locations.length);
    SplitLocationInfo[] locationInfo = splits[0].getLocationInfo();
    Assert.assertEquals(2, locationInfo.length);
    SplitLocationInfo localhostInfo = locations[0].equals(""localhost"") ?
        locationInfo[0] : locationInfo[1];
    SplitLocationInfo otherhostInfo = locations[0].equals(""otherhost"") ?
        locationInfo[0] : locationInfo[1];
    Assert.assertTrue(localhostInfo.isOnDisk());
    Assert.assertTrue(localhostInfo.isInMemory());
    Assert.assertTrue(otherhostInfo.isOnDisk());
    Assert.assertFalse(otherhostInfo.isInMemory());
  }
  
  @Test
  public void testListStatusSimple() throws IOException {
    Configuration conf = new Configuration();
    conf.setInt(FileInputFormat.LIST_STATUS_NUM_THREADS, numThreads);

    List<Path> expectedPaths = org.apache.hadoop.mapreduce.lib.input.TestFileInputFormat
        .configureTestSimple(conf, localFs);

    JobConf jobConf = new JobConf(conf);
    TextInputFormat fif = new TextInputFormat();
    fif.configure(jobConf);
    FileStatus[] statuses = fif.listStatus(jobConf);

    org.apache.hadoop.mapreduce.lib.input.TestFileInputFormat
        .verifyFileStatuses(expectedPaths, Lists.newArrayList(statuses),
            localFs);
  }

  @Test
  public void testListStatusNestedRecursive() throws IOException {
    Configuration conf = new Configuration();
    conf.setInt(FileInputFormat.LIST_STATUS_NUM_THREADS, numThreads);

    List<Path> expectedPaths = org.apache.hadoop.mapreduce.lib.input.TestFileInputFormat
        .configureTestNestedRecursive(conf, localFs);
    JobConf jobConf = new JobConf(conf);
    TextInputFormat fif = new TextInputFormat();
    fif.configure(jobConf);
    FileStatus[] statuses = fif.listStatus(jobConf);

    org.apache.hadoop.mapreduce.lib.input.TestFileInputFormat
        .verifyFileStatuses(expectedPaths, Lists.newArrayList(statuses),
            localFs);
  }

  @Test
  public void testListStatusNestedNonRecursive() throws IOException {
    Configuration conf = new Configuration();
    conf.setInt(FileInputFormat.LIST_STATUS_NUM_THREADS, numThreads);

    List<Path> expectedPaths = org.apache.hadoop.mapreduce.lib.input.TestFileInputFormat
        .configureTestNestedNonRecursive(conf, localFs);
    JobConf jobConf = new JobConf(conf);
    TextInputFormat fif = new TextInputFormat();
    fif.configure(jobConf);
    FileStatus[] statuses = fif.listStatus(jobConf);

    org.apache.hadoop.mapreduce.lib.input.TestFileInputFormat
        .verifyFileStatuses(expectedPaths, Lists.newArrayList(statuses),
            localFs);
  }

  @Test
  public void testListStatusErrorOnNonExistantDir() throws IOException {
    Configuration conf = new Configuration();
    conf.setInt(FileInputFormat.LIST_STATUS_NUM_THREADS, numThreads);

    org.apache.hadoop.mapreduce.lib.input.TestFileInputFormat
        .configureTestErrorOnNonExistantDir(conf, localFs);
    JobConf jobConf = new JobConf(conf);
    TextInputFormat fif = new TextInputFormat();
    fif.configure(jobConf);
    try {
      fif.listStatus(jobConf);
      Assert.fail(""Expecting an IOException for a missing Input path"");
    } catch (IOException e) {
      Path expectedExceptionPath = new Path(TEST_ROOT_DIR, ""input2"");
      expectedExceptionPath = localFs.makeQualified(expectedExceptionPath);
      Assert.assertTrue(e instanceof InvalidInputException);
      Assert.assertEquals(
          ""Input path does not exist: "" + expectedExceptionPath.toString(),
          e.getMessage());
    }
  }

  private Configuration getConfiguration() {
    Configuration conf = new Configuration();
    conf.set(""fs.test.impl.disable.cache"", ""true"");
    conf.setClass(""fs.test.impl"", MockFileSystem.class, FileSystem.class);
    conf.set(org.apache.hadoop.mapreduce.lib.input.FileInputFormat.INPUT_DIR,
        ""test:///a1"");
    return conf;
  }

  static class MockFileSystem extends RawLocalFileSystem {
    int numListLocatedStatusCalls = 0;

    @Override
    public FileStatus[] listStatus(Path f) throws FileNotFoundException,
        IOException {
      if (f.toString().equals(""test:/a1"")) {
        return new FileStatus[] {
            new FileStatus(0, true, 1, 150, 150, new Path(""test:/a1/a2"")),
            new FileStatus(10, false, 1, 150, 150, new Path(""test:/a1/file1"")) };
      } else if (f.toString().equals(""test:/a1/a2"")) {
        return new FileStatus[] {
            new FileStatus(10, false, 1, 150, 150,
                new Path(""test:/a1/a2/file2"")),
            new FileStatus(10, false, 1, 151, 150,
                new Path(""test:/a1/a2/file3"")) };
      }
      return new FileStatus[0];
    }

    @Override
    public FileStatus[] globStatus(Path pathPattern, PathFilter filter)
        throws IOException {
      return new FileStatus[] { new FileStatus(10, true, 1, 150, 150,
          pathPattern) };
    }

    @Override
    public FileStatus[] listStatus(Path f, PathFilter filter)
        throws FileNotFoundException, IOException {
      return this.listStatus(f);
    }

    @Override
    public BlockLocation[] getFileBlockLocations(FileStatus file, long start, long len)
        throws IOException {
      return new BlockLocation[] {
          new BlockLocation(new String[] { ""localhost:9866"", ""otherhost:9866"" },
              new String[] { ""localhost"", ""otherhost"" }, new String[] { ""localhost"" },
              new String[0], 0, len, false) };
    }

    @Override
    protected RemoteIterator<LocatedFileStatus> listLocatedStatus(Path f,
        PathFilter filter) throws FileNotFoundException, IOException {
      ++numListLocatedStatusCalls;
      return super.listLocatedStatus(f, filter);
    }
  }
}
","/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * ""License""); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.hadoop.mapred;

import java.io.IOException;
import java.io.InterruptedIOException;
import java.util.ArrayList;
import java.util.Collections;
import java.util.Comparator;
import java.util.HashSet;
import java.util.IdentityHashMap;
import java.util.LinkedList;
import java.util.List;
import java.util.Map;
import java.util.Set;
import java.util.concurrent.TimeUnit;

import org.apache.hadoop.classification.InterfaceAudience;
import org.apache.hadoop.classification.InterfaceStability;
import org.apache.hadoop.fs.BlockLocation;
import org.apache.hadoop.fs.FileStatus;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.LocatedFileStatus;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.fs.PathFilter;
import org.apache.hadoop.fs.RemoteIterator;
import org.apache.hadoop.mapreduce.security.TokenCache;
import org.apache.hadoop.net.NetworkTopology;
import org.apache.hadoop.net.Node;
import org.apache.hadoop.net.NodeBase;
import org.apache.hadoop.util.ReflectionUtils;
import org.apache.hadoop.util.StopWatch;
import org.apache.hadoop.util.StringUtils;

import org.apache.hadoop.thirdparty.com.google.common.collect.Iterables;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/** 
 * A base class for file-based {@link InputFormat}.
 * 
 * <p><code>FileInputFormat</code> is the base class for all file-based 
 * <code>InputFormat</code>s. This provides a generic implementation of
 * {@link #getSplits(JobConf, int)}.
 *
 * Implementations of <code>FileInputFormat</code> can also override the
 * {@link #isSplitable(FileSystem, Path)} method to prevent input files
 * from being split-up in certain situations. Implementations that may
 * deal with non-splittable files <i>must</i> override this method, since
 * the default implementation assumes splitting is always possible.
 */
@InterfaceAudience.Public
@InterfaceStability.Stable
public abstract class FileInputFormat<K, V> implements InputFormat<K, V> {

  public static final Logger LOG =
      LoggerFactory.getLogger(FileInputFormat.class);
  
  @Deprecated
  public enum Counter {
    BYTES_READ
  }

  public static final String NUM_INPUT_FILES =
    org.apache.hadoop.mapreduce.lib.input.FileInputFormat.NUM_INPUT_FILES;

  public static final String INPUT_DIR_RECURSIVE = 
    org.apache.hadoop.mapreduce.lib.input.FileInputFormat.INPUT_DIR_RECURSIVE;

  public static final String INPUT_DIR_NONRECURSIVE_IGNORE_SUBDIRS =
    org.apache.hadoop.mapreduce.lib.input.FileInputFormat.INPUT_DIR_NONRECURSIVE_IGNORE_SUBDIRS;


  private static final double SPLIT_SLOP = 1.1;   // 10% slop

  private long minSplitSize = 1;
  private static final PathFilter hiddenFileFilter = new PathFilter(){
      public boolean accept(Path p){
        String name = p.getName(); 
        return !name.startsWith(""_"") && !name.startsWith("".""); 
      }
    }; 
  protected void setMinSplitSize(long minSplitSize) {
    this.minSplitSize = minSplitSize;
  }

  /**
   * Proxy PathFilter that accepts a path only if all filters given in the
   * constructor do. Used by the listPaths() to apply the built-in
   * hiddenFileFilter together with a user provided one (if any).
   */
  private static class MultiPathFilter implements PathFilter {
    private List<PathFilter> filters;

    public MultiPathFilter(List<PathFilter> filters) {
      this.filters = filters;
    }

    public boolean accept(Path path) {
      for (PathFilter filter : filters) {
        if (!filter.accept(path)) {
          return false;
        }
      }
      return true;
    }
  }

  /**
   * Is the given filename splittable? Usually, true, but if the file is
   * stream compressed, it will not be.
   *
   * The default implementation in <code>FileInputFormat</code> always returns
   * true. Implementations that may deal with non-splittable files <i>must</i>
   * override this method.
   *
   * <code>FileInputFormat</code> implementations can override this and return
   * <code>false</code> to ensure that individual input files are never split-up
   * so that {@link Mapper}s process entire files.
   * 
   * @param fs the file system that the file is on
   * @param filename the file name to check
   * @return is this file splitable?
   */
  protected boolean isSplitable(FileSystem fs, Path filename) {
    return true;
  }
  
  public abstract RecordReader<K, V> getRecordReader(InputSplit split,
                                               JobConf job,
                                               Reporter reporter)
    throws IOException;

  /**
   * Set a PathFilter to be applied to the input paths for the map-reduce job.
   *
   * @param filter the PathFilter class use for filtering the input paths.
   */
  public static void setInputPathFilter(JobConf conf,
                                        Class<? extends PathFilter> filter) {
    conf.setClass(org.apache.hadoop.mapreduce.lib.input.
      FileInputFormat.PATHFILTER_CLASS, filter, PathFilter.class);
  }

  /**
   * Get a PathFilter instance of the filter set for the input paths.
   *
   * @return the PathFilter instance set for the job, NULL if none has been set.
   */
  public static PathFilter getInputPathFilter(JobConf conf) {
    Class<? extends PathFilter> filterClass = conf.getClass(
	  org.apache.hadoop.mapreduce.lib.input.FileInputFormat.PATHFILTER_CLASS,
	  null, PathFilter.class);
    return (filterClass != null) ?
        ReflectionUtils.newInstance(filterClass, conf) : null;
  }

  /**
   * Add files in the input path recursively into the results.
   * @param result
   *          The List to store all files.
   * @param fs
   *          The FileSystem.
   * @param path
   *          The input path.
   * @param inputFilter
   *          The input filter that can be used to filter files/dirs. 
   * @throws IOException
   */
  protected void addInputPathRecursively(List<FileStatus> result,
      FileSystem fs, Path path, PathFilter inputFilter) 
      throws IOException {
    RemoteIterator<LocatedFileStatus> iter = fs.listLocatedStatus(path);
    while (iter.hasNext()) {
      LocatedFileStatus stat = iter.next();
      if (inputFilter.accept(stat.getPath())) {
        if (stat.isDirectory()) {
          addInputPathRecursively(result, fs, stat.getPath(), inputFilter);
        } else {
          result.add(org.apache.hadoop.mapreduce.lib.input.
              FileInputFormat.shrinkStatus(stat));
        }
      }
    }
  }
  
  /**
   * List input directories.
   * Subclasses may override to, e.g., select only files matching a regular
   * expression. 
   * 
   * If security is enabled, this method collects
   * delegation tokens from the input paths and adds them to the job's
   * credentials.
   * @param job the job to list input paths for and attach tokens to.
   * @return array of FileStatus objects
   * @throws IOException if zero items.
   */
  protected FileStatus[] listStatus(JobConf job) throws IOException {
    Path[] dirs = getInputPaths(job);
    if (dirs.length == 0) {
      throw new IOException(""No input paths specified in job"");
    }

    // get tokens for all the required FileSystems..
    TokenCache.obtainTokensForNamenodes(job.getCredentials(), dirs, job);
    
    // Whether we need to recursive look into the directory structure
    boolean recursive = job.getBoolean(INPUT_DIR_RECURSIVE, false);

    // creates a MultiPathFilter with the hiddenFileFilter and the
    // user provided one (if any).
    List<PathFilter> filters = new ArrayList<PathFilter>();
    filters.add(hiddenFileFilter);
    PathFilter jobFilter = getInputPathFilter(job);
    if (jobFilter != null) {
      filters.add(jobFilter);
    }
    PathFilter inputFilter = new MultiPathFilter(filters);

    FileStatus[] result;
    int numThreads = job
        .getInt(
            org.apache.hadoop.mapreduce.lib.input.FileInputFormat.LIST_STATUS_NUM_THREADS,
            org.apache.hadoop.mapreduce.lib.input.FileInputFormat.DEFAULT_LIST_STATUS_NUM_THREADS);
    
    StopWatch sw = new StopWatch().start();
    if (numThreads == 1) {
      List<FileStatus> locatedFiles = singleThreadedListStatus(job, dirs, inputFilter, recursive); 
      result = locatedFiles.toArray(new FileStatus[locatedFiles.size()]);
    } else {
      Iterable<FileStatus> locatedFiles = null;
      try {
        
        LocatedFileStatusFetcher locatedFileStatusFetcher = new LocatedFileStatusFetcher(
            job, dirs, recursive, inputFilter, false);
        locatedFiles = locatedFileStatusFetcher.getFileStatuses();
      } catch (InterruptedException e) {
        throw  (IOException)
            new InterruptedIOException(""Interrupted while getting file statuses"")
                .initCause(e);
      }
      result = Iterables.toArray(locatedFiles, FileStatus.class);
    }

    sw.stop();
    if (LOG.isDebugEnabled()) {
      LOG.debug(""Time taken to get FileStatuses: ""
          + sw.now(TimeUnit.MILLISECONDS));
    }
    LOG.info(""Total input files to process : "" + result.length);
    return result;
  }
  
  private List<FileStatus> singleThreadedListStatus(JobConf job, Path[] dirs,
      PathFilter inputFilter, boolean recursive) throws IOException {
    List<FileStatus> result = new ArrayList<FileStatus>();
    List<IOException> errors = new ArrayList<IOException>();
    for (Path p: dirs) {
      FileSystem fs = p.getFileSystem(job); 
      FileStatus[] matches = fs.globStatus(p, inputFilter);
      if (matches == null) {
        errors.add(new IOException(""Input path does not exist: "" + p));
      } else if (matches.length == 0) {
        errors.add(new IOException(""Input Pattern "" + p + "" matches 0 files""));
      } else {
        for (FileStatus globStat: matches) {
          if (globStat.isDirectory()) {
            RemoteIterator<LocatedFileStatus> iter =
                fs.listLocatedStatus(globStat.getPath());
            while (iter.hasNext()) {
              LocatedFileStatus stat = iter.next();
              if (inputFilter.accept(stat.getPath())) {
                if (recursive && stat.isDirectory()) {
                  addInputPathRecursively(result, fs, stat.getPath(),
                      inputFilter);
                } else {
                  result.add(org.apache.hadoop.mapreduce.lib.input.
                      FileInputFormat.shrinkStatus(stat));
                }
              }
            }
          } else {
            result.add(globStat);
          }
        }
      }
    }
    if (!errors.isEmpty()) {
      throw new InvalidInputException(errors);
    }
    return result;
  }

  /**
   * A factory that makes the split for this class. It can be overridden
   * by sub-classes to make sub-types
   */
  protected FileSplit makeSplit(Path file, long start, long length, 
                                String[] hosts) {
    return new FileSplit(file, start, length, hosts);
  }
  
  /**
   * A factory that makes the split for this class. It can be overridden
   * by sub-classes to make sub-types
   */
  protected FileSplit makeSplit(Path file, long start, long length, 
                                String[] hosts, String[] inMemoryHosts) {
    return new FileSplit(file, start, length, hosts, inMemoryHosts);
  }

  /** Splits files returned by {@link #listStatus(JobConf)} when
   * they're too big.*/ 
  public InputSplit[] getSplits(JobConf job, int numSplits)
    throws IOException {
    StopWatch sw = new StopWatch().start();
    FileStatus[] stats = listStatus(job);

    // Save the number of input files for metrics/loadgen
    job.setLong(NUM_INPUT_FILES, stats.length);
    long totalSize = 0;                           // compute total size
    boolean ignoreDirs = !job.getBoolean(INPUT_DIR_RECURSIVE, false)
      && job.getBoolean(INPUT_DIR_NONRECURSIVE_IGNORE_SUBDIRS, false);

    List<FileStatus> files = new ArrayList<>(stats.length);
    for (FileStatus file: stats) {                // check we have valid files
      if (file.isDirectory()) {
        if (!ignoreDirs) {
          throw new IOException(""Not a file: ""+ file.getPath());
        }
      } else {
        files.add(file);
        totalSize += file.getLen();
      }
    }

    long goalSize = totalSize / (numSplits == 0 ? 1 : numSplits);
    long minSize = Math.max(job.getLong(org.apache.hadoop.mapreduce.lib.input.
      FileInputFormat.SPLIT_MINSIZE, 1), minSplitSize);

    // generate splits
    ArrayList<FileSplit> splits = new ArrayList<FileSplit>(numSplits);
    NetworkTopology clusterMap = new NetworkTopology();
    for (FileStatus file: files) {
      Path path = file.getPath();
      long length = file.getLen();
      if (length != 0) {
        FileSystem fs = path.getFileSystem(job);
        BlockLocation[] blkLocations;
        if (file instanceof LocatedFileStatus) {
          blkLocations = ((LocatedFileStatus) file).getBlockLocations();
        } else {
          blkLocations = fs.getFileBlockLocations(file, 0, length);
        }
        if (isSplitable(fs, path)) {
          long blockSize = file.getBlockSize();
          long splitSize = computeSplitSize(goalSize, minSize, blockSize);

          long bytesRemaining = length;
          while (((double) bytesRemaining)/splitSize > SPLIT_SLOP) {
            String[][] splitHosts = getSplitHostsAndCachedHosts(blkLocations,
                length-bytesRemaining, splitSize, clusterMap);
            splits.add(makeSplit(path, length-bytesRemaining, splitSize,
                splitHosts[0], splitHosts[1]));
            bytesRemaining -= splitSize;
          }

          if (bytesRemaining != 0) {
            String[][] splitHosts = getSplitHostsAndCachedHosts(blkLocations, length
                - bytesRemaining, bytesRemaining, clusterMap);
            splits.add(makeSplit(path, length - bytesRemaining, bytesRemaining,
                splitHosts[0], splitHosts[1]));
          }
        } else {
          if (LOG.isDebugEnabled()) {
            // Log only if the file is big enough to be splitted
            if (length > Math.min(file.getBlockSize(), minSize)) {
              LOG.debug(""File is not splittable so no parallelization ""
                  + ""is possible: "" + file.getPath());
            }
          }
          String[][] splitHosts = getSplitHostsAndCachedHosts(blkLocations,0,length,clusterMap);
          splits.add(makeSplit(path, 0, length, splitHosts[0], splitHosts[1]));
        }
      } else { 
        //Create empty hosts array for zero length files
        splits.add(makeSplit(path, 0, length, new String[0]));
      }
    }
    sw.stop();
    if (LOG.isDebugEnabled()) {
      LOG.debug(""Total # of splits generated by getSplits: "" + splits.size()
          + "", TimeTaken: "" + sw.now(TimeUnit.MILLISECONDS));
    }
    return splits.toArray(new FileSplit[splits.size()]);
  }

  protected long computeSplitSize(long goalSize, long minSize,
                                       long blockSize) {
    return Math.max(minSize, Math.min(goalSize, blockSize));
  }

  protected int getBlockIndex(BlockLocation[] blkLocations, 
                              long offset) {
    for (int i = 0 ; i < blkLocations.length; i++) {
      // is the offset inside this block?
      if ((blkLocations[i].getOffset() <= offset) &&
          (offset < blkLocations[i].getOffset() + blkLocations[i].getLength())){
        return i;
      }
    }
    BlockLocation last = blkLocations[blkLocations.length -1];
    long fileLength = last.getOffset() + last.getLength() -1;
    throw new IllegalArgumentException(""Offset "" + offset + 
                                       "" is outside of file (0.."" +
                                       fileLength + "")"");
  }

  /**
   * Sets the given comma separated paths as the list of inputs 
   * for the map-reduce job.
   * 
   * @param conf Configuration of the job
   * @param commaSeparatedPaths Comma separated paths to be set as 
   *        the list of inputs for the map-reduce job.
   */
  public static void setInputPaths(JobConf conf, String commaSeparatedPaths) {
    setInputPaths(conf, StringUtils.stringToPath(
                        getPathStrings(commaSeparatedPaths)));
  }

  /**
   * Add the given comma separated paths to the list of inputs for
   *  the map-reduce job.
   * 
   * @param conf The configuration of the job 
   * @param commaSeparatedPaths Comma separated paths to be added to
   *        the list of inputs for the map-reduce job.
   */
  public static void addInputPaths(JobConf conf, String commaSeparatedPaths) {
    for (String str : getPathStrings(commaSeparatedPaths)) {
      addInputPath(conf, new Path(str));
    }
  }

  /**
   * Set the array of {@link Path}s as the list of inputs
   * for the map-reduce job.
   * 
   * @param conf Configuration of the job. 
   * @param inputPaths the {@link Path}s of the input directories/files 
   * for the map-reduce job.
   */ 
  public static void setInputPaths(JobConf conf, Path... inputPaths) {
    Path path = new Path(conf.getWorkingDirectory(), inputPaths[0]);
    StringBuilder str = new StringBuilder(StringUtils.escapeString(path.toString()));
    for(int i = 1; i < inputPaths.length;i++) {
      str.append(StringUtils.COMMA_STR);
      path = new Path(conf.getWorkingDirectory(), inputPaths[i]);
      str.append(StringUtils.escapeString(path.toString()));
    }
    conf.set(org.apache.hadoop.mapreduce.lib.input.
      FileInputFormat.INPUT_DIR, str.toString());
  }

  /**
   * Add a {@link Path} to the list of inputs for the map-reduce job.
   * 
   * @param conf The configuration of the job 
   * @param path {@link Path} to be added to the list of inputs for 
   *            the map-reduce job.
   */
  public static void addInputPath(JobConf conf, Path path ) {
    path = new Path(conf.getWorkingDirectory(), path);
    String dirStr = StringUtils.escapeString(path.toString());
    String dirs = conf.get(org.apache.hadoop.mapreduce.lib.input.
      FileInputFormat.INPUT_DIR);
    conf.set(org.apache.hadoop.mapreduce.lib.input.
      FileInputFormat.INPUT_DIR, dirs == null ? dirStr :
      dirs + StringUtils.COMMA_STR + dirStr);
  }
         
  // This method escapes commas in the glob pattern of the given paths.
  private static String[] getPathStrings(String commaSeparatedPaths) {
    int length = commaSeparatedPaths.length();
    int curlyOpen = 0;
    int pathStart = 0;
    boolean globPattern = false;
    List<String> pathStrings = new ArrayList<String>();
    
    for (int i=0; i<length; i++) {
      char ch = commaSeparatedPaths.charAt(i);
      switch(ch) {
        case '{' : {
          curlyOpen++;
          if (!globPattern) {
            globPattern = true;
          }
          break;
        }
        case '}' : {
          curlyOpen--;
          if (curlyOpen == 0 && globPattern) {
            globPattern = false;
          }
          break;
        }
        case ',' : {
          if (!globPattern) {
            pathStrings.add(commaSeparatedPaths.substring(pathStart, i));
            pathStart = i + 1 ;
          }
          break;
        }
        default:
          continue; // nothing special to do for this character
      }
    }
    pathStrings.add(commaSeparatedPaths.substring(pathStart, length));
    
    return pathStrings.toArray(new String[0]);
  }
  
  /**
   * Get the list of input {@link Path}s for the map-reduce job.
   * 
   * @param conf The configuration of the job 
   * @return the list of input {@link Path}s for the map-reduce job.
   */
  public static Path[] getInputPaths(JobConf conf) {
    String dirs = conf.get(org.apache.hadoop.mapreduce.lib.input.
      FileInputFormat.INPUT_DIR, """");
    String [] list = StringUtils.split(dirs);
    Path[] result = new Path[list.length];
    for (int i = 0; i < list.length; i++) {
      result[i] = new Path(StringUtils.unEscapeString(list[i]));
    }
    return result;
  }
  

  private void sortInDescendingOrder(List<NodeInfo> mylist) {
    Collections.sort(mylist, new Comparator<NodeInfo> () {
      public int compare(NodeInfo obj1, NodeInfo obj2) {

        if (obj1 == null || obj2 == null)
          return -1;

        if (obj1.getValue() == obj2.getValue()) {
          return 0;
        }
        else {
          return ((obj1.getValue() < obj2.getValue()) ? 1 : -1);
        }
      }
    }
    );
  }

  /** 
   * This function identifies and returns the hosts that contribute 
   * most for a given split. For calculating the contribution, rack
   * locality is treated on par with host locality, so hosts from racks
   * that contribute the most are preferred over hosts on racks that 
   * contribute less
   * @param blkLocations The list of block locations
   * @param offset 
   * @param splitSize 
   * @return an array of hosts that contribute most to this split
   * @throws IOException
   */
  protected String[] getSplitHosts(BlockLocation[] blkLocations, 
      long offset, long splitSize, NetworkTopology clusterMap) throws IOException {
    return getSplitHostsAndCachedHosts(blkLocations, offset, splitSize,
        clusterMap)[0];
  }
  
  /** 
   * This function identifies and returns the hosts that contribute 
   * most for a given split. For calculating the contribution, rack
   * locality is treated on par with host locality, so hosts from racks
   * that contribute the most are preferred over hosts on racks that 
   * contribute less
   * @param blkLocations The list of block locations
   * @param offset 
   * @param splitSize 
   * @return two arrays - one of hosts that contribute most to this split, and
   *    one of hosts that contribute most to this split that have the data
   *    cached on them
   * @throws IOException
   */
  private String[][] getSplitHostsAndCachedHosts(BlockLocation[] blkLocations, 
      long offset, long splitSize, NetworkTopology clusterMap)
  throws IOException {

    int startIndex = getBlockIndex(blkLocations, offset);

    long bytesInThisBlock = blkLocations[startIndex].getOffset() + 
                          blkLocations[startIndex].getLength() - offset;

    //If this is the only block, just return
    if (bytesInThisBlock >= splitSize) {
      return new String[][] { blkLocations[startIndex].getHosts(),
          blkLocations[startIndex].getCachedHosts() };
    }

    long bytesInFirstBlock = bytesInThisBlock;
    int index = startIndex + 1;
    splitSize -= bytesInThisBlock;

    while (splitSize > 0) {
      bytesInThisBlock =
        Math.min(splitSize, blkLocations[index++].getLength());
      splitSize -= bytesInThisBlock;
    }

    long bytesInLastBlock = bytesInThisBlock;
    int endIndex = index - 1;
    
    Map <Node,NodeInfo> hostsMap = new IdentityHashMap<Node,NodeInfo>();
    Map <Node,NodeInfo> racksMap = new IdentityHashMap<Node,NodeInfo>();
    String [] allTopos = new String[0];

    // Build the hierarchy and aggregate the contribution of 
    // bytes at each level. See TestGetSplitHosts.java 

    for (index = startIndex; index <= endIndex; index++) {

      // Establish the bytes in this block
      if (index == startIndex) {
        bytesInThisBlock = bytesInFirstBlock;
      }
      else if (index == endIndex) {
        bytesInThisBlock = bytesInLastBlock;
      }
      else {
        bytesInThisBlock = blkLocations[index].getLength();
      }
      
      allTopos = blkLocations[index].getTopologyPaths();

      // If no topology information is available, just
      // prefix a fakeRack
      if (allTopos.length == 0) {
        allTopos = fakeRacks(blkLocations, index);
      }

      // NOTE: This code currently works only for one level of
      // hierarchy (rack/host). However, it is relatively easy
      // to extend this to support aggregation at different
      // levels 
      
      for (String topo: allTopos) {

        Node node, parentNode;
        NodeInfo nodeInfo, parentNodeInfo;

        node = clusterMap.getNode(topo);

        if (node == null) {
          node = new NodeBase(topo);
          clusterMap.add(node);
        }
        
        nodeInfo = hostsMap.get(node);
        
        if (nodeInfo == null) {
          nodeInfo = new NodeInfo(node);
          hostsMap.put(node,nodeInfo);
          parentNode = node.getParent();
          parentNodeInfo = racksMap.get(parentNode);
          if (parentNodeInfo == null) {
            parentNodeInfo = new NodeInfo(parentNode);
            racksMap.put(parentNode,parentNodeInfo);
          }
          parentNodeInfo.addLeaf(nodeInfo);
        }
        else {
          nodeInfo = hostsMap.get(node);
          parentNode = node.getParent();
          parentNodeInfo = racksMap.get(parentNode);
        }

        nodeInfo.addValue(index, bytesInThisBlock);
        parentNodeInfo.addValue(index, bytesInThisBlock);

      } // for all topos
    
    } // for all indices

    // We don't yet support cached hosts when bytesInThisBlock > splitSize
    return new String[][] { identifyHosts(allTopos.length, racksMap),
        new String[0]};
  }
  
  private String[] identifyHosts(int replicationFactor, 
                                 Map<Node,NodeInfo> racksMap) {
    
    String [] retVal = new String[replicationFactor];
   
    List <NodeInfo> rackList = new LinkedList<NodeInfo>(); 

    rackList.addAll(racksMap.values());
    
    // Sort the racks based on their contribution to this split
    sortInDescendingOrder(rackList);
    
    boolean done = false;
    int index = 0;
    
    // Get the host list for all our aggregated items, sort
    // them and return the top entries
    for (NodeInfo ni: rackList) {

      Set<NodeInfo> hostSet = ni.getLeaves();

      List<NodeInfo>hostList = new LinkedList<NodeInfo>();
      hostList.addAll(hostSet);
    
      // Sort the hosts in this rack based on their contribution
      sortInDescendingOrder(hostList);

      for (NodeInfo host: hostList) {
        // Strip out the port number from the host name
        retVal[index++] = host.node.getName().split("":"")[0];
        if (index == replicationFactor) {
          done = true;
          break;
        }
      }
      
      if (done == true) {
        break;
      }
    }
    return retVal;
  }
  
  private String[] fakeRacks(BlockLocation[] blkLocations, int index) 
  throws IOException {
    String[] allHosts = blkLocations[index].getHosts();
    String[] allTopos = new String[allHosts.length];
    for (int i = 0; i < allHosts.length; i++) {
      allTopos[i] = NetworkTopology.DEFAULT_RACK + ""/"" + allHosts[i];
    }
    return allTopos;
  }


  private static class NodeInfo {
    final Node node;
    final Set<Integer> blockIds;
    final Set<NodeInfo> leaves;

    private long value;
    
    NodeInfo(Node node) {
      this.node = node;
      blockIds = new HashSet<Integer>();
      leaves = new HashSet<NodeInfo>();
    }

    long getValue() {return value;}

    void addValue(int blockIndex, long value) {
      if (blockIds.add(blockIndex) == true) {
        this.value += value;
      }
    }

    Set<NodeInfo> getLeaves() { return leaves;}

    void addLeaf(NodeInfo nodeInfo) {
      leaves.add(nodeInfo);
    }
  }
}
","['Assertion Roulette', 'Conditional Test Logic', 'Exception Catching Throwing', 'General Fixture', 'Sensitive Equality', 'Unknown Test']","['Assertion Roulette', 'Magic Number Test']",1,5,1,11
35272_30_joda-time_testserialization,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/35272_test.java,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/35272_actual.java,"/*
 *  Copyright 2001-2009 Stephen Colebourne
 *
 *  Licensed under the Apache License, Version 2.0 (the ""License"");
 *  you may not use this file except in compliance with the License.
 *  You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 *  Unless required by applicable law or agreed to in writing, software
 *  distributed under the License is distributed on an ""AS IS"" BASIS,
 *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 *  See the License for the specific language governing permissions and
 *  limitations under the License.
 */
package org.joda.time.field;

import java.io.ByteArrayInputStream;
import java.io.ByteArrayOutputStream;
import java.io.ObjectInputStream;
import java.io.ObjectOutputStream;

import junit.framework.TestCase;
import junit.framework.TestSuite;

import org.joda.time.DurationField;
import org.joda.time.DurationFieldType;
import org.joda.time.chrono.ISOChronology;

/**
 * This class is a Junit unit test for PreciseDurationField.
 *
 * @author Stephen Colebourne
 */
public class TestPreciseDurationField extends TestCase {
    
    private static final long LONG_INTEGER_MAX = Integer.MAX_VALUE;
    private static final int INTEGER_MAX = Integer.MAX_VALUE;
    private static final long LONG_MAX = Long.MAX_VALUE;
    
    private PreciseDurationField iField;

    public static void main(String[] args) {
        junit.textui.TestRunner.run(suite());
    }

    public static TestSuite suite() {
        return new TestSuite(TestPreciseDurationField.class);
    }

    public TestPreciseDurationField(String name) {
        super(name);
    }

    @Override
    protected void setUp() throws Exception {
        iField = new PreciseDurationField(DurationFieldType.seconds(), 1000);
    }

    @Override
    protected void tearDown() throws Exception {
        iField = null;
    }

    //-----------------------------------------------------------------------
    public void test_constructor() {
        try {
            new PreciseDurationField(null, 10);
            fail();
        } catch (IllegalArgumentException ex) {}
    }

    //-----------------------------------------------------------------------
    public void test_getType() {
        assertEquals(DurationFieldType.seconds(), iField.getType());
    }

    public void test_getName() {
        assertEquals(""seconds"", iField.getName());
    }
    
    public void test_isSupported() {
        assertEquals(true, iField.isSupported());
    }

    public void test_isPrecise() {
        assertEquals(true, iField.isPrecise());
    }

    public void test_getUnitMillis() {
        assertEquals(1000, iField.getUnitMillis());
    }

    public void test_toString() {
        assertEquals(""DurationField[seconds]"", iField.toString());
    }

    //-----------------------------------------------------------------------
    public void test_getValue_long() {
        assertEquals(0, iField.getValue(0L));
        assertEquals(12345, iField.getValue(12345678L));
        assertEquals(-1, iField.getValue(-1234L));
        assertEquals(INTEGER_MAX, iField.getValue(LONG_INTEGER_MAX * 1000L + 999L));
        try {
            iField.getValue(LONG_INTEGER_MAX * 1000L + 1000L);
            fail();
        } catch (ArithmeticException ex) {}
    }

    public void test_getValueAsLong_long() {
        assertEquals(0L, iField.getValueAsLong(0L));
        assertEquals(12345L, iField.getValueAsLong(12345678L));
        assertEquals(-1L, iField.getValueAsLong(-1234L));
        assertEquals(LONG_INTEGER_MAX + 1L, iField.getValueAsLong(LONG_INTEGER_MAX * 1000L + 1000L));
    }

    public void test_getValue_long_long() {
        assertEquals(0, iField.getValue(0L, 567L));
        assertEquals(12345, iField.getValue(12345678L, 567L));
        assertEquals(-1, iField.getValue(-1234L, 567L));
        assertEquals(INTEGER_MAX, iField.getValue(LONG_INTEGER_MAX * 1000L + 999L, 567L));
        try {
            iField.getValue(LONG_INTEGER_MAX * 1000L + 1000L, 567L);
            fail();
        } catch (ArithmeticException ex) {}
    }

    public void test_getValueAsLong_long_long() {
        assertEquals(0L, iField.getValueAsLong(0L, 567L));
        assertEquals(12345L, iField.getValueAsLong(12345678L, 567L));
        assertEquals(-1L, iField.getValueAsLong(-1234L, 567L));
        assertEquals(LONG_INTEGER_MAX + 1L, iField.getValueAsLong(LONG_INTEGER_MAX * 1000L + 1000L, 567L));
    }

    //-----------------------------------------------------------------------
    public void test_getMillis_int() {
        assertEquals(0, iField.getMillis(0));
        assertEquals(1234000L, iField.getMillis(1234));
        assertEquals(-1234000L, iField.getMillis(-1234));
        assertEquals(LONG_INTEGER_MAX * 1000L, iField.getMillis(INTEGER_MAX));
    }

    public void test_getMillis_long() {
        assertEquals(0L, iField.getMillis(0L));
        assertEquals(1234000L, iField.getMillis(1234L));
        assertEquals(-1234000L, iField.getMillis(-1234L));
        try {
            iField.getMillis(LONG_MAX);
            fail();
        } catch (ArithmeticException ex) {}
    }

    public void test_getMillis_int_long() {
        assertEquals(0L, iField.getMillis(0, 567L));
        assertEquals(1234000L, iField.getMillis(1234, 567L));
        assertEquals(-1234000L, iField.getMillis(-1234, 567L));
        assertEquals(LONG_INTEGER_MAX * 1000L, iField.getMillis(INTEGER_MAX, 567L));
    }

    public void test_getMillis_long_long() {
        assertEquals(0L, iField.getMillis(0L, 567L));
        assertEquals(1234000L, iField.getMillis(1234L, 567L));
        assertEquals(-1234000L, iField.getMillis(-1234L, 567L));
        try {
            iField.getMillis(LONG_MAX, 567L);
            fail();
        } catch (ArithmeticException ex) {}
    }

    //-----------------------------------------------------------------------
    public void test_add_long_int() {
        assertEquals(567L, iField.add(567L, 0));
        assertEquals(567L + 1234000L, iField.add(567L, 1234));
        assertEquals(567L - 1234000L, iField.add(567L, -1234));
        try {
            iField.add(LONG_MAX, 1);
            fail();
        } catch (ArithmeticException ex) {}
    }

    public void test_add_long_long() {
        assertEquals(567L, iField.add(567L, 0L));
        assertEquals(567L + 1234000L, iField.add(567L, 1234L));
        assertEquals(567L - 1234000L, iField.add(567L, -1234L));
        try {
            iField.add(LONG_MAX, 1L);
            fail();
        } catch (ArithmeticException ex) {}
        try {
            iField.add(1L, LONG_MAX);
            fail();
        } catch (ArithmeticException ex) {}
    }

    //-----------------------------------------------------------------------
    public void test_getDifference_long_int() {
        assertEquals(0, iField.getDifference(1L, 0L));
        assertEquals(567, iField.getDifference(567000L, 0L));
        assertEquals(567 - 1234, iField.getDifference(567000L, 1234000L));
        assertEquals(567 + 1234, iField.getDifference(567000L, -1234000L));
        try {
            iField.getDifference(LONG_MAX, -1L);
            fail();
        } catch (ArithmeticException ex) {}
    }

    public void test_getDifferenceAsLong_long_long() {
        assertEquals(0L, iField.getDifferenceAsLong(1L, 0L));
        assertEquals(567L, iField.getDifferenceAsLong(567000L, 0L));
        assertEquals(567L - 1234L, iField.getDifferenceAsLong(567000L, 1234000L));
        assertEquals(567L + 1234L, iField.getDifferenceAsLong(567000L, -1234000L));
        try {
            iField.getDifferenceAsLong(LONG_MAX, -1L);
            fail();
        } catch (ArithmeticException ex) {}
    }

    //-----------------------------------------------------------------------
    public void test_equals() {
        assertEquals(true, iField.equals(iField));
        assertEquals(false, iField.equals(ISOChronology.getInstance().minutes()));
        DurationField dummy = new PreciseDurationField(DurationFieldType.seconds(), 0);
        assertEquals(false, iField.equals(dummy));
        dummy = new PreciseDurationField(DurationFieldType.seconds(), 1000);
        assertEquals(true, iField.equals(dummy));
        dummy = new PreciseDurationField(DurationFieldType.millis(), 1000);
        assertEquals(false, iField.equals(dummy));
        assertEquals(false, iField.equals(""""));
        assertEquals(false, iField.equals(null));
    }

    public void test_hashCode() {
        assertEquals(true, iField.hashCode() == iField.hashCode());
        assertEquals(false, iField.hashCode() == ISOChronology.getInstance().minutes().hashCode());
        DurationField dummy = new PreciseDurationField(DurationFieldType.seconds(), 0);
        assertEquals(false, iField.hashCode() == dummy.hashCode());
        dummy = new PreciseDurationField(DurationFieldType.seconds(), 1000);
        assertEquals(true, iField.hashCode() == dummy.hashCode());
        dummy = new PreciseDurationField(DurationFieldType.millis(), 1000);
        assertEquals(false, iField.hashCode() == dummy.hashCode());
    }

    //-----------------------------------------------------------------------
    public void test_compareTo() {
        assertEquals(0, iField.compareTo(iField));
        assertEquals(-1, iField.compareTo(ISOChronology.getInstance().minutes()));
        DurationField dummy = new PreciseDurationField(DurationFieldType.seconds(), 0);
        assertEquals(1, iField.compareTo(dummy));
//        try {
//            iField.compareTo("""");
//            fail();
//        } catch (ClassCastException ex) {}
        try {
            iField.compareTo(null);
            fail();
        } catch (NullPointerException ex) {}
    }

    //-----------------------------------------------------------------------
    public void testSerialization() throws Exception {
        DurationField test = iField;
        
        ByteArrayOutputStream baos = new ByteArrayOutputStream();
        ObjectOutputStream oos = new ObjectOutputStream(baos);
        oos.writeObject(test);
        oos.close();
        byte[] bytes = baos.toByteArray();
        
        ByteArrayInputStream bais = new ByteArrayInputStream(bytes);
        ObjectInputStream ois = new ObjectInputStream(bais);
        DurationField result = (DurationField) ois.readObject();
        ois.close();
        
        assertEquals(test, result);
    }

}
","/*
 *  Copyright 2001-2005 Stephen Colebourne
 *
 *  Licensed under the Apache License, Version 2.0 (the ""License"");
 *  you may not use this file except in compliance with the License.
 *  You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 *  Unless required by applicable law or agreed to in writing, software
 *  distributed under the License is distributed on an ""AS IS"" BASIS,
 *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 *  See the License for the specific language governing permissions and
 *  limitations under the License.
 */
package org.joda.time.field;

import org.joda.time.DurationFieldType;

/**
 * Duration field class representing a field with a fixed unit length.
 * <p>
 * PreciseDurationField is thread-safe and immutable.
 * 
 * @author Stephen Colebourne
 * @author Brian S O'Neill
 * @since 1.0
 */
public class PreciseDurationField extends BaseDurationField {
    
    private static final long serialVersionUID = -8346152187724495365L;

    /** The size of the unit */
    private final long iUnitMillis;

    /**
     * Constructor.
     * 
     * @param type  the field type
     * @param unitMillis  the unit milliseconds
     */    
    public PreciseDurationField(DurationFieldType type, long unitMillis) {
        super(type);
        iUnitMillis = unitMillis;
    }
    
    //------------------------------------------------------------------------
    /**
     * This field is precise.
     * 
     * @return true always
     */
    @Override
    public final boolean isPrecise() {
        return true;
    }
    
    /**
     * Returns the amount of milliseconds per unit value of this field.
     *
     * @return the unit size of this field, in milliseconds
     */
    @Override
    public final long getUnitMillis() {
        return iUnitMillis;
    }

    //------------------------------------------------------------------------
    /**
     * Get the value of this field from the milliseconds.
     * 
     * @param duration  the milliseconds to query, which may be negative
     * @param instant  ignored
     * @return the value of the field, in the units of the field, which may be
     * negative
     */
    @Override
    public long getValueAsLong(long duration, long instant) {
        return duration / iUnitMillis;  // safe
    }

    /**
     * Get the millisecond duration of this field from its value.
     * 
     * @param value  the value of the field, which may be negative
     * @param instant  ignored
     * @return the milliseconds that the field represents, which may be
     * negative
     */
    @Override
    public long getMillis(int value, long instant) {
        return value * iUnitMillis;  // safe
    }

    /**
     * Get the millisecond duration of this field from its value.
     * 
     * @param value  the value of the field, which may be negative
     * @param instant  ignored
     * @return the milliseconds that the field represents, which may be
     * negative
     */
    @Override
    public long getMillis(long value, long instant) {
        return FieldUtils.safeMultiply(value, iUnitMillis);
    }

    @Override
    public long add(long instant, int value) {
        long addition = value * iUnitMillis;  // safe
        return FieldUtils.safeAdd(instant, addition);
    }

    @Override
    public long add(long instant, long value) {
        long addition = FieldUtils.safeMultiply(value, iUnitMillis);
        return FieldUtils.safeAdd(instant, addition);
    }

    @Override
    public long getDifferenceAsLong(long minuendInstant, long subtrahendInstant) {
        long difference = FieldUtils.safeSubtract(minuendInstant, subtrahendInstant);
        return difference / iUnitMillis;
    }

    //-----------------------------------------------------------------------
    /**
     * Compares this duration field to another.
     * Two fields are equal if of the same type and duration.
     * 
     * @param obj  the object to compare to
     * @return if equal
     */
    @Override
    public boolean equals(Object obj) {
        if (this == obj) {
            return true;
        } else if (obj instanceof PreciseDurationField) {
            PreciseDurationField other = (PreciseDurationField) obj;
            return (getType() == other.getType()) && (iUnitMillis == other.iUnitMillis);
        }
        return false;
    }

    /**
     * Gets a hash code for this instance.
     * 
     * @return a suitable hashcode
     */
    @Override
    public int hashCode() {
        long millis = iUnitMillis;
        int hash = (int) (millis ^ (millis >>> 32));
        hash += getType().hashCode();
        return hash;
    }

}
","['Assertion Roulette', 'Exception Catching Throwing', 'General Fixture', 'Sensitive Equality']",['Assertion Roulette'],0,3,1,13
1710_29.0_c2mon_testupdate,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/1710_test.java,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/1710_actual.java,"/******************************************************************************
 * Copyright (C) 2010-2016 CERN. All rights not expressly granted are reserved.
 * <p>
 * This file is part of the CERN Control and Monitoring Platform 'C2MON'.
 * C2MON is free software: you can redistribute it and/or modify it under the
 * terms of the GNU Lesser General Public License as published by the Free
 * Software Foundation, either version 3 of the license.
 * <p>
 * C2MON is distributed in the hope that it will be useful, but WITHOUT ANY
 * WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
 * FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for
 * more details.
 * <p>
 * You should have received a copy of the GNU Lesser General Public License
 * along with C2MON. If not, see <http://www.gnu.org/licenses/>.
 *****************************************************************************/
package cern.c2mon.server.cache.dbaccess;

import java.sql.Timestamp;
import java.util.LinkedList;
import java.util.List;

import org.junit.Before;
import org.junit.Test;
import org.springframework.beans.factory.annotation.Autowired;
import cern.c2mon.server.common.datatag.DataTagCacheObject;
import cern.c2mon.server.common.equipment.Equipment;
import cern.c2mon.server.common.equipment.EquipmentCacheObject;
import cern.c2mon.server.common.subequipment.SubEquipmentCacheObject;
import cern.c2mon.shared.common.command.CommandTag;
import cern.c2mon.shared.common.supervision.SupervisionConstants.SupervisionStatus;

import static org.junit.Assert.*;

public class EquipmentMapperTest extends AbstractMapperTest {

  /**
   * Class to test
   */
  @Autowired
  private EquipmentMapper equipmentMapper;

  /**
   * Used to create subequipments of the test equipment.
   */
  @Autowired
  private SubEquipmentMapper subEquipmentMapper;

  /**
   * Used to create test datatag.
   */
  @Autowired
  private DataTagMapper dataTagMapper;

  @Autowired
  private CommandTagMapper commandTagMapper;

  private EquipmentCacheObject equipmentCacheObject;

  private SubEquipmentCacheObject subEquipmentCacheObject;

//  private SubEquipmentCacheObject subEquipmentCacheObject2;

  private DataTagCacheObject dataTagCacheObject1;

  private DataTagCacheObject dataTagCacheObject2;

  private CommandTag commandTag;

  @Before
  public void insertTestData() {
    equipmentCacheObject = (EquipmentCacheObject) equipmentMapper.getItem(150L);
    subEquipmentCacheObject = (SubEquipmentCacheObject) subEquipmentMapper.getItem(250L);
//    subEquipmentCacheObject2 = CacheObjectCreation.createTestSubEquipment2();
    dataTagCacheObject1 = (DataTagCacheObject) dataTagMapper.getItem(200000L);
    dataTagCacheObject2 = (DataTagCacheObject) dataTagMapper.getItem(200001L);
    commandTag = commandTagMapper.getItem(11000L);
  }


//    @After
//    public void deleteTestProcess() {
//      equipmentMapper.deleteProcess(Long.valueOf(100000));
//    }

  /**
   * Test does not insert associated equipment yet and test equipment retrieval
   *  - add later TODO
   */
//    @Test
//    public void testInsertAndRetrieve() {
//      ProcessCacheObject originalProcess = createTestProcess1();
//      equipmentMapper.insertProcess(originalProcess);
//
//      ProcessCacheObject retrievedProcess = (ProcessCacheObject) equipmentMapper.getItem(originalProcess.getId());
//
//      assertNotNull(retrievedProcess);
//
//      assertEquals(originalProcess.getId(), retrievedProcess.getId());
//      assertEquals(originalProcess.getName(), retrievedProcess.getName());
//      assertEquals(originalProcess.getDescription(), retrievedProcess.getDescription());
//      assertEquals(originalProcess.getMaxMessageDelay(), retrievedProcess.getMaxMessageDelay());
//      assertEquals(originalProcess.getMaxMessageSize(), retrievedProcess.getMaxMessageSize());
//      assertEquals(originalProcess.getStateTagId(), retrievedProcess.getStateTagId());
//      assertEquals(originalProcess.getAliveInterval(), retrievedProcess.getAliveInterval());
//      assertEquals(originalProcess.getAliveTagId(), retrievedProcess.getAliveTagId());
//      assertEquals(originalProcess.getState(), retrievedProcess.getState());
//      assertEquals(originalProcess.getStartupTime(), retrievedProcess.getStartupTime());
//      assertEquals(originalProcess.getCurrentHost(), retrievedProcess.getCurrentHost());
//
//    }

  /**
   * Tests the result set is none empty.
   */
  @Test
  public void testGetAll() {
    List<Equipment> returnList = equipmentMapper.getAll();
    assertTrue(returnList.size() > 0);
  }


  @Test
  public void testInsertAndRetrieve() {
//      EquipmentCacheObject equipmentCacheObject = createTestEquipment();
//      equipmentMapper.insertEquipment(equipmentCacheObject);
//      SubEquipmentCacheObject subEquipmentCacheObject = createTestSubEquipment();
//      subEquipmentMapper.insertSubEquipment(subEquipmentCacheObject);
//      DataTagCacheObject dataTagCacheObject = DataTagMapperTest.createTestDataTag();
//      dataTagMapper.insertDataTag(dataTagCacheObject);
    EquipmentCacheObject retrievedObject = (EquipmentCacheObject) equipmentMapper.getItem(equipmentCacheObject.getId());
    assertEquals(equipmentCacheObject.getId(), retrievedObject.getId());
    assertEquals(equipmentCacheObject.getName(), retrievedObject.getName());
    assertEquals(equipmentCacheObject.getAddress(), retrievedObject.getAddress());
    assertEquals(equipmentCacheObject.getAliveInterval(), retrievedObject.getAliveInterval());
    assertEquals(equipmentCacheObject.getAliveTagId(), retrievedObject.getAliveTagId());
    assertEquals(equipmentCacheObject.getStatusDescription(), retrievedObject.getStatusDescription());
    assertEquals(equipmentCacheObject.getStatusTime(), retrievedObject.getStatusTime());
    assertEquals(equipmentCacheObject.getSupervisionStatus(), retrievedObject.getSupervisionStatus());
    assertEquals(equipmentCacheObject.getCommFaultTagId(), retrievedObject.getCommFaultTagId());
    assertEquals(equipmentCacheObject.getProcessId(), retrievedObject.getProcessId());
    assertEquals(equipmentCacheObject.getStateTagId(), retrievedObject.getStateTagId());
    assertEquals(equipmentCacheObject.getHandlerClassName(), retrievedObject.getHandlerClassName());
    assertEquals(equipmentCacheObject.getDescription(), retrievedObject.getDescription());

    List<Long> subEquipmentIds = new LinkedList<Long>();
    subEquipmentIds.add(subEquipmentCacheObject.getId());
//    subEquipmentIds.add(subEquipmentCacheObject2.getId());
    assertEquals(subEquipmentIds, retrievedObject.getSubEquipmentIds());
    assertTrue(retrievedObject.getCommandTagIds().contains(commandTag.getId())); //check it contains the commandtag

    //assertEquals(dataTagCacheObject.getId(), retrievedObject.getDataTagIds().iterator().next()); //TODO just one in array so far
  }

  @Test
  public void getByName() {
    Long retrievedId = equipmentMapper.getIdByName(""E_TESTHANDLER_TESTHANDLER03"");
    assertTrue(retrievedId == 150L);
  }

  @Test
  public void getByNameFailure() {
    Long retrievedId = equipmentMapper.getIdByName(""Test Equipment not there"");

    assertNull(retrievedId);

  }

  /**
   * May fail if changes done to test DB.
   */
  @Test
  public void testTagCollectionLoading() {
    EquipmentCacheObject equipment = (EquipmentCacheObject) equipmentMapper.getItem(150L); //using test DB data for this!! TODO use permanent
    // data instead
    assertEquals(2, equipment.getCommandTagIds().size());
  }

  @Test
  public void testUpdateConfig() {
    assertEquals(new Long(1224), equipmentCacheObject.getAliveTagId());
    equipmentCacheObject.setAliveTagId(1251L);
    equipmentCacheObject.setCommFaultTagId(1252L);
    equipmentCacheObject.setStateTagId(1250L);
    equipmentMapper.updateEquipmentConfig(equipmentCacheObject);
    Equipment updatedEquipment = equipmentMapper.getItem(equipmentCacheObject.getId());
    assertEquals(new Long(1251), updatedEquipment.getAliveTagId());
    assertEquals(new Long(1252), updatedEquipment.getCommFaultTagId());
    assertEquals(new Long(1250), updatedEquipment.getStateTagId());
  }

  @Test
  public void testGetNumberItems() {
    int nbItems = equipmentMapper.getNumberItems();
    assertEquals(3, nbItems);
  }

  /**
   * Tests the cache persistence method.
   */
  @Test
  public void testUpdate() {
    assertFalse(equipmentCacheObject.getSupervisionStatus().equals(SupervisionStatus.RUNNING));
    equipmentCacheObject.setSupervisionStatus(SupervisionStatus.RUNNING);
    Timestamp ts = new Timestamp(System.currentTimeMillis() + 1000);
    equipmentCacheObject.setStatusDescription(""New status description."");
    equipmentCacheObject.setStatusTime(ts);
    equipmentMapper.updateCacheable(equipmentCacheObject);

    EquipmentCacheObject retrievedEquipment = (EquipmentCacheObject) equipmentMapper.getItem(equipmentCacheObject.getId());
    assertEquals(SupervisionStatus.RUNNING, retrievedEquipment.getSupervisionStatus());
    assertEquals(ts, retrievedEquipment.getStatusTime());
    assertEquals(""New status description."", retrievedEquipment.getStatusDescription());
  }

  @Test
  public void testIsInDB() {
    assertTrue(equipmentMapper.isInDb(150L));
  }

  @Test
  public void testNotInDB() {
    assertFalse(equipmentMapper.isInDb(250L));
  }

}

//    /**
//     * Uses JECTEST01 alive id and state id (for FK constraints).
//     */
//    public static ProcessCacheObject createTestProcess1() {
//      ProcessCacheObject processCacheObject = new ProcessCacheObject();
//      processCacheObject.setId(new Long(100000));
//      processCacheObject.setName(""Test Process"");
//      processCacheObject.setDescription(""Test process description"");
//      processCacheObject.setMaxMessageDelay(100);
//      processCacheObject.setMaxMessageSize(100);
//      processCacheObject.setAliveInterval(60);
//      processCacheObject.setAliveTagId(new Long(100123)); //FK ref
//      processCacheObject.setStateTagId(new Long(100122)); //FK ref
//      processCacheObject.setState(ProcessState.PROCESS_DOWN);
//      processCacheObject.setStartupTime(new Timestamp(0));
//      processCacheObject.setCurrentHost(""test host"");
//      return processCacheObject;
//    }
//
//    public static ProcessCacheObject createTestProcess2() {
//      return null;
//    }

","/******************************************************************************
 * Copyright (C) 2010-2016 CERN. All rights not expressly granted are reserved.
 *
 * This file is part of the CERN Control and Monitoring Platform 'C2MON'.
 * C2MON is free software: you can redistribute it and/or modify it under the
 * terms of the GNU Lesser General Public License as published by the Free
 * Software Foundation, either version 3 of the license.
 *
 * C2MON is distributed in the hope that it will be useful, but WITHOUT ANY
 * WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
 * FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for
 * more details.
 *
 * You should have received a copy of the GNU Lesser General Public License
 * along with C2MON. If not, see <http://www.gnu.org/licenses/>.
 *****************************************************************************/
package cern.c2mon.server.cache.dbaccess;

import cern.c2mon.server.common.equipment.Equipment;
import cern.c2mon.server.common.equipment.EquipmentCacheObject;

public interface EquipmentMapper extends LoaderMapper<Equipment>, PersistenceMapper<Equipment> {
  void insertEquipment(EquipmentCacheObject equipmentCacheObject);
  void deleteEquipment(Long id);
  void updateEquipmentConfig(EquipmentCacheObject equipmentCacheObject);

  /**
   * Retrieve the id of the cache object with the given name.
   *
   * @param name the unique name of the cache object
   * @return the id of the cache object
   */
  Long getIdByName(String name);
}
",['Assertion Roulette'],"['Assertion Roulette', 'Eager Test', 'Magic Number Test', 'Redundant Print']",3,0,1,15
36192_76.0_achilles_should_validate_objectbytearray,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/36192_test.java,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/36192_actual.java,"/*
 * Copyright (C) 2012-2021 DuyHai DOAN
 *
 * Licensed under the Apache License, Version 2.0 (the ""License"");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package info.archinnov.achilles.internals.parser.validator;

import static org.assertj.core.api.Assertions.assertThat;
import static org.mockito.Mockito.*;

import java.util.Date;
import java.util.List;
import java.util.Map;
import java.util.Set;

import org.junit.Test;
import org.junit.runner.RunWith;
import org.mockito.ArgumentCaptor;
import org.mockito.Captor;
import org.mockito.Mock;
import org.mockito.runners.MockitoJUnitRunner;

import com.datastax.driver.core.ConsistencyLevel;
import com.datastax.driver.core.TupleValue;
import com.squareup.javapoet.TypeName;

import info.archinnov.achilles.internals.apt.AptUtils;
import info.archinnov.achilles.internals.parser.TypeUtils;
import info.archinnov.achilles.type.tuples.Tuple1;
import info.archinnov.achilles.type.tuples.Tuple2;
import info.archinnov.achilles.type.tuples.Tuple3;

@RunWith(MockitoJUnitRunner.class)
public class TypeValidatorTest {

    @Captor
    ArgumentCaptor<String> messageCaptor;
    @Captor
    ArgumentCaptor<Object> objectCaptor;
    @Mock
    private AptUtils aptUtils;

    private TypeValidator typeValidator = new TypeValidator() {
        @Override
        public List<TypeName> getAllowedTypes() {
            return TypeUtils.ALLOWED_TYPES_3_10;
        }
    };

    @Test
    public void should_validate_primitiveByte() throws Exception {
        final TypeName typeName = TypeName.get(TestTypes.class.getDeclaredField(""primitiveByte"").getGenericType());
        typeValidator.validateAllowedTypes(aptUtils, typeName, typeName);
    }

    @Test
    public void should_validate_objectByte() throws Exception {
        final TypeName typeName = TypeName.get(TestTypes.class.getDeclaredField(""objectByte"").getGenericType());
        typeValidator.validateAllowedTypes(aptUtils, typeName, typeName);
        verify(aptUtils).validateTrue(eq(true), anyString(), anyVararg());
    }

    @Test
    public void should_validate_primitiveByteArray() throws Exception {
        final TypeName typeName = TypeName.get(TestTypes.class.getDeclaredField(""primitiveByteArray"").getGenericType());
        typeValidator.validateAllowedTypes(aptUtils, typeName, typeName);
        verify(aptUtils).validateTrue(eq(true), anyString(), anyVararg());
    }

    @Test
    public void should_validate_objectByteArray() throws Exception {
        final TypeName typeName = TypeName.get(TestTypes.class.getDeclaredField(""objectByteArray"").getGenericType());
        typeValidator.validateAllowedTypes(aptUtils, typeName, typeName);
        verify(aptUtils).validateTrue(eq(true), anyString(), anyVararg());
    }

    @Test
    public void should_validate_listString() throws Exception {
        final TypeName typeName = TypeName.get(TestTypes.class.getDeclaredField(""listString"").getGenericType());
        typeValidator.validateAllowedTypes(aptUtils, typeName, typeName);
        verify(aptUtils, times(2)).validateTrue(eq(true), anyString(), anyVararg());
    }

    @Test
    public void should_validate_setByte() throws Exception {
        final TypeName typeName = TypeName.get(TestTypes.class.getDeclaredField(""setByte"").getGenericType());
        typeValidator.validateAllowedTypes(aptUtils, typeName, typeName);
        verify(aptUtils, times(2)).validateTrue(eq(true), anyString(), anyVararg());
    }

    @Test
    public void should_validate_mapByteArray() throws Exception {
        final TypeName typeName = TypeName.get(TestTypes.class.getDeclaredField(""mapByteArray"").getGenericType());
        typeValidator.validateAllowedTypes(aptUtils, typeName, typeName);
        verify(aptUtils, times(3)).validateTrue(eq(true), anyString(), anyVararg());
    }

    @Test
    public void should_validate_tuple1() throws Exception {
        final TypeName typeName = TypeName.get(TestTypes.class.getDeclaredField(""tuple1"").getGenericType());
        typeValidator.validateAllowedTypes(aptUtils, typeName, typeName);
        verify(aptUtils, times(2)).validateTrue(eq(true), anyString(), anyVararg());
    }

    @Test
    public void should_validate_tuple2() throws Exception {
        final TypeName typeName = TypeName.get(TestTypes.class.getDeclaredField(""tuple2"").getGenericType());
        typeValidator.validateAllowedTypes(aptUtils, typeName, typeName);
        verify(aptUtils, times(4)).validateTrue(eq(true), anyString(), anyVararg());
    }

    @Test
    public void should_validate_tuple3() throws Exception {
        final TypeName typeName = TypeName.get(TestTypes.class.getDeclaredField(""tuple3"").getGenericType());
        typeValidator.validateAllowedTypes(aptUtils, typeName, typeName);
        verify(aptUtils, times(4)).validateTrue(eq(true), anyString(), anyVararg());
    }

    @Test
    public void should_validate_tupleValue() throws Exception {
        final TypeName typeName = TypeName.get(TestTypes.class.getDeclaredField(""tupleValue"").getGenericType());
        typeValidator.validateAllowedTypes(aptUtils, typeName, typeName);
        verify(aptUtils, times(1)).validateTrue(eq(true), anyString(), anyVararg());
    }

    @Test
    public void should_fail_on_invalidType() throws Exception {
        final TypeName typeName = TypeName.get(TestTypes.class.getDeclaredField(""invalidType"").getGenericType());
        typeValidator.validateAllowedTypes(aptUtils, typeName, typeName);
        verify(aptUtils, times(1)).validateTrue(eq(false), messageCaptor.capture(), objectCaptor.capture(), objectCaptor.capture());

        assertThat(messageCaptor.getValue()).isEqualTo(""Type '%s' in '%s' is not a valid type for CQL"");
        assertThat(objectCaptor.getAllValues()).containsExactly(typeName.toString(), typeName.toString());
    }

    @Test
    public void should_fail_on_invalidTypeNestedType() throws Exception {
        String consistencyLevel = ConsistencyLevel.class.getCanonicalName();
        final TypeName typeName = TypeName.get(TestTypes.class.getDeclaredField(""invalidTypeNestedType"").getGenericType());
        typeValidator.validateAllowedTypes(aptUtils, typeName, typeName);
        verify(aptUtils, times(3)).validateTrue(eq(true), anyString(), anyVararg());
        verify(aptUtils, times(1)).validateTrue(eq(false), messageCaptor.capture(), objectCaptor.capture(), objectCaptor.capture());

        assertThat(messageCaptor.getValue()).isEqualTo(""Type '%s' in '%s' is not a valid type for CQL"");
        assertThat(objectCaptor.getAllValues()).containsExactly(consistencyLevel, typeName.toString());
    }

    @Test
    public void should_fail_on_invalidUpperBound() throws Exception {
        String consistencyLevel = ConsistencyLevel.class.getCanonicalName();
        final TypeName typeName = TypeName.get(TestTypes.class.getDeclaredField(""invalidUpperBound"").getGenericType());
        typeValidator.validateAllowedTypes(aptUtils, typeName, typeName);
        verify(aptUtils, times(1)).validateTrue(eq(true), anyString(), anyVararg());
        verify(aptUtils, times(1)).validateTrue(eq(false), messageCaptor.capture(), objectCaptor.capture(), objectCaptor.capture());

        assertThat(messageCaptor.getValue()).isEqualTo(""Type '%s' in '%s' is not a valid type for CQL"");
        assertThat(objectCaptor.getAllValues()).containsExactly(consistencyLevel, typeName.toString());
    }

    @Test
    public void should_fail_on_invalidLowerBound() throws Exception {
        String object = Object.class.getCanonicalName();
        final TypeName typeName = TypeName.get(TestTypes.class.getDeclaredField(""invalidLowerBound"").getGenericType());
        typeValidator.validateAllowedTypes(aptUtils, typeName, typeName);
        verify(aptUtils, times(1)).validateTrue(eq(true), anyString(), anyVararg());
        verify(aptUtils, times(1)).validateTrue(eq(false), messageCaptor.capture(), objectCaptor.capture(), objectCaptor.capture());

        assertThat(messageCaptor.getValue()).isEqualTo(""Type '%s' in '%s' is not a valid type for CQL"");
        assertThat(objectCaptor.getAllValues()).containsExactly(object, typeName.toString());
    }

    @Test
    public void should_fail_on_invalidWildCard() throws Exception {
        String object = Object.class.getCanonicalName();
        final TypeName typeName = TypeName.get(TestTypes.class.getDeclaredField(""invalidWildCard"").getGenericType());
        typeValidator.validateAllowedTypes(aptUtils, typeName, typeName);
        verify(aptUtils, times(1)).validateTrue(eq(true), anyString(), anyVararg());
        verify(aptUtils, times(1)).validateTrue(eq(false), messageCaptor.capture(), objectCaptor.capture(), objectCaptor.capture());

        assertThat(messageCaptor.getValue()).isEqualTo(""Type '%s' in '%s' is not a valid type for CQL"");
        assertThat(objectCaptor.getAllValues()).containsExactly(object, typeName.toString());
    }

    public static class TestTypes {

        private byte primitiveByte;
        private Byte objectByte;
        private byte[] primitiveByteArray;
        private Byte[] objectByteArray;
        private List<String> listString;
        private Set<Byte> setByte;
        private Map<Integer, Byte[]> mapByteArray;
        private Tuple1<String> tuple1;
        private Tuple2<List<Integer>, Byte[]> tuple2;
        private Tuple3<? extends Date, Integer, Byte> tuple3;
        private TupleValue tupleValue;

        private ConsistencyLevel invalidType;
        private List<Map<Integer, ConsistencyLevel>> invalidTypeNestedType;
        private List<? extends ConsistencyLevel> invalidUpperBound;
        private List<? super String> invalidLowerBound;
        private List<?> invalidWildCard;
    }

}","/*
 * Copyright (C) 2012-2021 DuyHai DOAN
 *
 * Licensed under the Apache License, Version 2.0 (the ""License"");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package info.archinnov.achilles.internals.parser.validator;

import java.util.List;

import com.squareup.javapoet.*;

import info.archinnov.achilles.internals.apt.AptUtils;

public abstract class TypeValidator {

    public abstract List<TypeName> getAllowedTypes();

    public void validateAllowedTypes(AptUtils aptUtils, TypeName parentType, TypeName type) {
        if (type.isPrimitive()) {
            return;
        } else if (type instanceof ParameterizedTypeName) {
            final ParameterizedTypeName parameterizedTypeName = (ParameterizedTypeName) type;
            validateAllowedTypes(aptUtils, parentType, parameterizedTypeName.rawType);

            for (TypeName x : parameterizedTypeName.typeArguments) {
                validateAllowedTypes(aptUtils, parentType, x);
            }
        } else if (type instanceof WildcardTypeName) {
            final WildcardTypeName wildcardTypeName = (WildcardTypeName) type;
            for (TypeName x : wildcardTypeName.upperBounds) {
                validateAllowedTypes(aptUtils, parentType, x);
            }
        } else if (type instanceof ClassName || type instanceof ArrayTypeName) {
            final boolean isValidType = getAllowedTypes().contains(type);
            aptUtils.validateTrue(isValidType, ""Type '%s' in '%s' is not a valid type for CQL"", type.toString(), parentType.toString());
        } else {
            aptUtils.printError(""Type '%s' in '%s' is not a valid type for CQL"", type.toString(), parentType.toString());
        }
    }

    public void validateAllowedTypesForFunction(AptUtils aptUtils, String className, String methodName, TypeName type) {
        if (type.isPrimitive()) {
            validateAllowedTypesForFunction(aptUtils, className, methodName, type.box());
        } else if (type instanceof ParameterizedTypeName) {
            final ParameterizedTypeName parameterizedTypeName = (ParameterizedTypeName) type;
            validateAllowedTypesForFunction(aptUtils, className, methodName, parameterizedTypeName.rawType);

            for (TypeName x : parameterizedTypeName.typeArguments) {
                validateAllowedTypesForFunction(aptUtils, className, methodName, x);
            }
        } else if (type instanceof WildcardTypeName) {
            final WildcardTypeName wildcardTypeName = (WildcardTypeName) type;
            for (TypeName x : wildcardTypeName.upperBounds) {
                validateAllowedTypesForFunction(aptUtils, className, methodName, x);
            }
        } else if (type instanceof ClassName || type instanceof ArrayTypeName) {
            final boolean isValidType = getAllowedTypes().contains(type);
            aptUtils.validateTrue(isValidType, ""Type '%s' in method '%s' return type/parameter on class '%s' is not a valid native Java type for Cassandra"",
                type.toString(), methodName, className);
        } else {
            aptUtils.printError(""Type '%s' in method '%s' return type/parameter on class '%s' is not a valid native Java type for Cassandra"",
                type.toString(), methodName, className);
        }
    }
}
","['Assertion Roulette', 'Unknown Test']",['Assertion Roulette'],0,1,1,15
11183_69.0_search-highlighter_shortstring,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/11183_test.java,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/11183_actual.java,"package org.wikimedia.search.highlighter.experimental.snippet;

import static org.junit.Assert.assertTrue;
import static org.hamcrest.MatcherAssert.assertThat;
import static org.hamcrest.Matchers.equalTo;
import static org.hamcrest.Matchers.greaterThanOrEqualTo;
import static org.hamcrest.Matchers.lessThan;
import static org.hamcrest.Matchers.lessThanOrEqualTo;
import static org.wikimedia.search.highlighter.experimental.Matchers.extracted;

import java.util.ArrayList;
import java.util.List;

import org.junit.Test;
import org.junit.runner.RunWith;
import org.wikimedia.search.highlighter.experimental.Segment;
import org.wikimedia.search.highlighter.experimental.Segmenter;
import org.wikimedia.search.highlighter.experimental.Segmenter.Memo;
import org.wikimedia.search.highlighter.experimental.SourceExtracter;
import org.wikimedia.search.highlighter.experimental.source.StringSourceExtracter;

import com.carrotsearch.randomizedtesting.RandomizedRunner;
import com.carrotsearch.randomizedtesting.RandomizedTest;
import com.carrotsearch.randomizedtesting.annotations.Repeat;

@RunWith(RandomizedRunner.class)
public class CharScanningSegmenterTest extends RandomizedTest {
    private String source;
    private Segmenter segmenter;
    private SourceExtracter<String> extracter;

    private void setup(String source) {
        setup(source, 200, 20);
    }

    private void setup(String source, int maxSnippetSize, int maxScan) {
        this.source = source;
        segmenter = new CharScanningSegmenter(source, maxSnippetSize, maxScan);
        extracter = new StringSourceExtracter(source);
    }

    @Test
    public void empty() {
        setup("""");
        assertTrue(segmenter.acceptable(0, 0));
        assertThat(segmenter.memo(0, 0).pickBounds(0, Integer.MAX_VALUE),
                extracted(extracter, equalTo("""")));
    }

    @Test
    public void singleChar() {
        setup(""a"");
        assertTrue(segmenter.acceptable(0, 1));
        assertThat(segmenter.memo(0, 1).pickBounds(0, Integer.MAX_VALUE),
                extracted(extracter, equalTo(""a"")));
    }

    @Test
    public void shortString() {
        setup(""short"");
        int end = source.length();
        for (int i = 0; i < end; i++) {
            assertTrue(segmenter.acceptable(0, i));
            assertThat(segmenter.memo(0, i).pickBounds(0, Integer.MAX_VALUE),
                    extracted(extracter, equalTo(""short"")));

            assertTrue(segmenter.acceptable(i, end));
            assertThat(segmenter.memo(i, end).pickBounds(0, Integer.MAX_VALUE),
                    extracted(extracter, equalTo(""short"")));
        }
    }

    @Test
    public void basicWordBreaks() {
        setup(""The quick brown fox jumped over the lazy dog."", 20, 10);

        // At the beginning with a small hit box so expand the segment forward
        assertThat(segmenter.memo(0, 8).pickBounds(0, Integer.MAX_VALUE),
                extracted(extracter, equalTo(""The quick brown fox jumped"")));

        // At the beginning with a big enough hit box not to expand the segment
        assertThat(segmenter.memo(0, 20).pickBounds(0, Integer.MAX_VALUE),
                extracted(extracter, equalTo(""The quick brown fox jumped"")));

        // Near the beginning with a small hit box so the segment expands a bit
        // in both directions
        assertThat(segmenter.memo(1, 8).pickBounds(0, Integer.MAX_VALUE),
                extracted(extracter, equalTo(""The quick brown fox"")));

        // Near the beginning with a big enough hit box not to expand the
        // segment
        assertThat(segmenter.memo(1, 21).pickBounds(0, Integer.MAX_VALUE),
                extracted(extracter, equalTo(""The quick brown fox jumped"")));

        // Near the end with a small hit box so expand the segment backwards
        assertThat(segmenter.memo(35, 43).pickBounds(0, Integer.MAX_VALUE),
                extracted(extracter, equalTo(""jumped over the lazy dog."")));

        // In the middle with small hit box so expand the segment box directions
        assertThat(segmenter.memo(20, 25).pickBounds(0, Integer.MAX_VALUE),
                extracted(extracter, equalTo(""brown fox jumped over the"")));

        // In the middle with a large hit box
        assertThat(segmenter.memo(0, 21).pickBounds(0, Integer.MAX_VALUE),
                extracted(extracter, equalTo(""The quick brown fox jumped"")));
    }

    @Test
    public void basicWordBreaksWithClamps() {
        setup(""The quick brown fox jumped over the lazy dog."", 20, 10);

        // Small hit box but clamped on start side so only expand towards the
        // end of the source
        assertThat(segmenter.memo(4, 8).pickBounds(4, Integer.MAX_VALUE),
                extracted(extracter, equalTo(""quick brown fox jumped"")));

        // Small hit box but clamped on the beginning and without enough room to
        // expand towards the end so just expand all the way to the end
        assertThat(segmenter.memo(35, 43).pickBounds(32, Integer.MAX_VALUE),
                extracted(extracter, equalTo(""the lazy dog."")));
    }

    @Test
    public void wordBreaksOnlyBetweenMinAndMax() {
        setup(""The quick brown fox jumped over the lazy dog."", 0, 10);

        // Find break while scanning from expanded start to beginning
        assertThat(segmenter.memo(4, 5).pickBounds(1, Integer.MAX_VALUE),
                extracted(extracter, equalTo(""quick"")));
        // Don't find break while scanning from expanded start to beginning
        assertThat(segmenter.memo(2, 5).pickBounds(1, Integer.MAX_VALUE),
                extracted(extracter, equalTo(""he quick"")));

        // Scanning backwards doesn't find the break but scanning forwards does
        setup(""Thequickbrown fox jumped over the lazy dog."", 10, 10);
        assertThat(segmenter.memo(15, 19).pickBounds(0, Integer.MAX_VALUE),
                extracted(extracter, equalTo(""fox jumped"")));

        // Scanning neither backwards nor forwards finds the break but we hit
        // the maxStart so use that
        setup(""Thequickbrown fox jumped over the lazy dog."", 0, 10);
        assertThat(segmenter.memo(12, 19).pickBounds(0, Integer.MAX_VALUE),
                extracted(extracter, equalTo(""n fox jumped"")));

        // Scanning neither backwards nor forwards finds the break and we
        // don't hit maxStart so just use the expanded start
        setup(""Thequickbrownfoxjumpedover the lazy dog."", 10, 2);
        assertThat(segmenter.memo(12, 16).pickBounds(0, Integer.MAX_VALUE),
                extracted(extracter, equalTo(""rownfoxjum"")));


        // Now repeat for the end offset...
        setup(""The quick brown fox jumped over the lazy dog."", 0, 10);

        // Find break while scanning from expanded end to end
        assertThat(segmenter.memo(4, 5).pickBounds(1, Integer.MAX_VALUE),
                extracted(extracter, equalTo(""quick"")));
        // Don't find break while scanning from expanded end to end
        assertThat(segmenter.memo(4, 5).pickBounds(1, 6),
                extracted(extracter, equalTo(""qu"")));

        // Scanning forwards doesn't find the break but scanning backwards does
        setup(""The quick brown foxjumpedoverthelazy dog."", 10, 10);
        assertThat(segmenter.memo(10, 14).pickBounds(0, Integer.MAX_VALUE),
                extracted(extracter, equalTo(""quick brown"")));

        // Scanning neither forwards nor backwards finds the break but we hit
        // the maxEnd so use that
        setup(""The quick brownfoxjumpedoverthelazy dog."", 0, 10);
        assertThat(segmenter.memo(10, 14).pickBounds(0, Integer.MAX_VALUE),
                extracted(extracter, equalTo(""brow"")));

        // Scanning neither backwards nor forwards finds the break and we
        // don't hit maxStart so just use the expanded end
        setup(""Thequickbrownfoxjumpedover the lazy dog."", 10, 2);
        assertThat(segmenter.memo(1, 2).pickBounds(0, Integer.MAX_VALUE),
                extracted(extracter, equalTo(""Thequickb"")));
    }

    @Test
    @Repeat(iterations = 1000)
    public void randomSegments() {
        int minStart = between(-100, 400);
        int maxStart = Math.max(0, minStart) + between(0, 400);
        int minEnd = maxStart + between(0, 400);
        int maxEnd = minEnd + between(0, 400);
        int length = minEnd + between(0, 800);
        StringBuilder b = new StringBuilder(length);
        for (int i = 0; i < length; i++) {
            b.append(rarely() ? ' ' : 'a');
        }
        setup(b.toString());
        Segment bounds = segmenter.memo(maxStart, minEnd).pickBounds(minStart, maxEnd);
        assertThat(bounds.startOffset(), lessThanOrEqualTo(maxStart));
        assertThat(bounds.endOffset(), greaterThanOrEqualTo(minEnd));
        if (segmenter.acceptable(maxStart, minEnd)) {
            // 240 = the max size + twice the scan
            assertThat(bounds.endOffset() - bounds.startOffset(), lessThanOrEqualTo(240));
        }
    }

    @Test(timeout = 100000L)
    public void quickAndDirtyPerformanceCheck() {
        int limit = 100000;
        StringBuilder b = new StringBuilder(limit);
        while (b.length() < limit) {
            if (between(0, 299) == 0) {
                b.append("".  "");
            }
            if (between(0, 9) == 0) {
                b.append(' ');
            } else {
                b.append('b');
            }
        }
        setup(b.toString());
        long start = System.currentTimeMillis();
        List<Memo> memos = new ArrayList<Memo>(b.length());
        for (int i = 0; i < limit; i++) {
            if (segmenter.acceptable(Math.max(0, i - 2), i)) {
                memos.add(segmenter.memo(Math.max(0, i - 2), i));
            }
        }
        long end = System.currentTimeMillis();
        assertThat(""CharScanningSegmenter#acceptable too slow"", end - start, lessThan(10000L));
        start = end;
        for (Memo memo : memos) {
            memo.pickBounds(0, Integer.MAX_VALUE);
        }
        end = System.currentTimeMillis();
        assertThat(""CharScanningSegmenter#pickBounds too slow"", end - start, lessThan(10000L));
    }
}
","package org.wikimedia.search.highlighter.experimental.snippet;

import java.util.Arrays;

import org.wikimedia.search.highlighter.experimental.Segment;
import org.wikimedia.search.highlighter.experimental.Segmenter;
import org.wikimedia.search.highlighter.experimental.SimpleSegment;

import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;

/**
 * Scans a char sequence looking for ""boundary characters"" to find that start
 * and end offset. Very similar to Lucene's SimpleBoundaryScanner.
 */
@SuppressFBWarnings(""EI_EXPOSE_REP2"")
public class CharScanningSegmenter implements Segmenter {
    private static final char[] DEFAULT_BOUNDARY_CHARACTERS = {'\t', '\n', ' ', '!', ',', '.', '?'};
    private final CharSequence source;
    private final char[] boundaryCharacters;
    private final int maxSnippetSize;
    private final int maxScan;

    /**
     * Build me with default boundary characters.
     */
    public CharScanningSegmenter(CharSequence source, int maxSnippetSize, int maxScan) {
        this(source, DEFAULT_BOUNDARY_CHARACTERS, maxSnippetSize, maxScan);
    }

    /**
     * Build me.
     *
     * @param boundaryCharacters must be in sorted order
     */
    public CharScanningSegmenter(CharSequence source, char[] boundaryCharacters,
            int maxSnippetSize, int maxScan) {
        this.boundaryCharacters = boundaryCharacters;
        this.source = source;
        this.maxSnippetSize = maxSnippetSize;
        this.maxScan = maxScan;
    }

    @Override
    public boolean acceptable(int maxStartOffset, int minEndOffset) {
        return minEndOffset - maxStartOffset < maxSnippetSize;
    }

    @Override
    public Memo memo(int maxStartOffset, int minEndOffset) {
        return new CharScanningSegmenterMemo(maxStartOffset, minEndOffset);
    }

    private class CharScanningSegmenterMemo implements Memo {
        private final int maxStartOffset;
        private final int minEndOffset;

        CharScanningSegmenterMemo(int maxStartOffset, int minEndOffset) {
            this.maxStartOffset = maxStartOffset;
            this.minEndOffset = minEndOffset;
        }

        @Override
        public Segment pickBounds(int minStartOffset, int maxEndOffset) {
            // Sanity
            minStartOffset = Math.max(0, minStartOffset);
            maxEndOffset = Math.min(source.length(), maxEndOffset);

            // Expand the minimum length segment (from maxStart to minEnd) to
            // maxSnippetSize
            int requestedSize = minEndOffset - maxStartOffset;
            int margin = Math.max(0, maxSnippetSize - requestedSize) / 2;
            int expandedStartOffset = maxStartOffset - margin;
            int expandedEndOffset = minEndOffset + margin;
            if (expandedStartOffset < minStartOffset) {
                expandedEndOffset += minStartOffset - expandedStartOffset;
                // No need to modify expandedStartOffset here, pickStartOffset
                // will clamp for us
            }
            if (maxEndOffset < expandedEndOffset) {
                expandedStartOffset -= expandedEndOffset - maxEndOffset;
            }

            // Now we have to pick a start and end offset, but there are
            // actually four cases that can happen given the above for start
            // offset and four for end. I'll show the start offset here
            // because the end is just the mirror image:
            //
            // Case 1:
            // --------+------[-------+-------]--+----------------------
            //        min          expand       max
            // Case 2:
            // ----[---+-------+--------+-------------------------------
            //        min   expand     max
            // Case 3:
            // --------+---[----+-------+---]---------------------------
            //        min    expand    max
            // Case 4:
            // --------+-------+---------+------------------------------
            //      expand    min       max
            //
            // Case 1 is ""normal"", there are no obstructions and we pick
            // the boundary by looking from expand to [, the max scan, and if
            // that doesn't find anything looking from expand to the ], and if
            // that doesn't find anything defaulting to expand.
            //
            // Case 2 is almost normal. We look from expand to min but if that
            // doesn't find anything we deem min a valid boundary. Min is
            // generally the beginning of the source or the end of the last
            // segment and therefore a valid boundary. The case where expand
            // is right on top of min is pretty much a variant of this case.
            //
            // Case 3 is like case 2 but in reverse. We look to [ as in case
            // 1 and if we don't find anything we look to max. If that doesn't
            // find anything then we declare max a valid boundary. Max is
            // generally the beginning of the first hit, so very likely a valid
            // boundary.
            //
            // Case 4 is different. We'd like to expand past min which isn't
            // allowed so instead we deem min the boundary and try to shift the
            // whole segment forward some to make up for it.
            return new SimpleSegment(pickStartOffset(expandedStartOffset, minStartOffset),
                    pickEndOffset(expandedEndOffset, maxEndOffset));
        }

        private int pickStartOffset(int expandedStartOffset, int minStartOffset) {
            if (expandedStartOffset <= minStartOffset) {
                // Case 4
                return minStartOffset;
            }
            int scanEnd = Math.max(minStartOffset, expandedStartOffset - maxScan);
            // On the off chance that expandedStartOffset == maxStartOffset and
            // is on a boundary, we really can't accept that boundary because
            // shifting forward past it (which we do below) would put us past
            // maxStartOffset. So we make sure we start before maxStartOffset-1.
            int scanStart = Math.min(maxStartOffset - 1, expandedStartOffset);
            int found = findBreakBefore(scanStart, scanEnd);
            if (found >= 0) {
                // +1 shifts us after the break
                return found + 1;
            }
            if (scanEnd == minStartOffset) {
                // Case 2
                return minStartOffset;
            }
            // maxStartOffset - 1 because we're going to add one to go one after the break
            scanEnd = Math.min(maxStartOffset, expandedStartOffset + maxScan);
            found = findBreakAfter(expandedStartOffset, scanEnd);
            if (found >= 0) {
                // +1 shifts us after the break
                return found + 1;
            }
            if (scanEnd == maxStartOffset) {
                // Case 3
                return maxStartOffset;
            }
            // Case 1
            return expandedStartOffset;
        }

        private int pickEndOffset(int expandedEndOffset, int maxEndOffset) {
            if (maxEndOffset <= expandedEndOffset) {
                // Case 4
                return maxEndOffset;
            }
            int scanEnd = Math.min(maxEndOffset, expandedEndOffset + maxScan);
            int found = findBreakAfter(expandedEndOffset, scanEnd);
            if (found >= 0) {
                return found;
            }
            if (scanEnd == maxEndOffset) {
                // Case 2
                return maxEndOffset;
            }
            scanEnd = Math.max(minEndOffset, expandedEndOffset - maxScan);
            found = findBreakBefore(expandedEndOffset, scanEnd);
            if (found >= 0) {
                return found;
            }
            if (scanEnd == minEndOffset) {
                // Case 3
                return minEndOffset;
            }
            // Case 1
            return expandedEndOffset;
        }

        private int findBreakBefore(int start, int scanEnd) {
            for (int scanPos = start; scanPos >= scanEnd; scanPos--) {
                if (Arrays.binarySearch(boundaryCharacters, source.charAt(scanPos)) >= 0) {
                    return scanPos;
                }
            }
            return -1;
        }

        private int findBreakAfter(int start, int max) {
            for (int scanPos = start; scanPos < max; scanPos++) {
                if (Arrays.binarySearch(boundaryCharacters, source.charAt(scanPos)) >= 0) {
                    return scanPos;
                }
            }
            return -1;
        }
    }
}
","['Assertion Roulette', 'Conditional Test Logic', 'General Fixture']",['Assertion Roulette'],0,2,1,14
32677_31_luwak_canclearthemonitor,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/32677_test.java,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/32677_actual.java,"package uk.co.flax.luwak;

import java.io.IOException;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.concurrent.atomic.AtomicInteger;

import org.apache.lucene.analysis.Analyzer;
import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
import org.apache.lucene.analysis.core.WhitespaceTokenizer;
import org.apache.lucene.search.Query;
import org.assertj.core.api.Assertions;
import org.junit.Before;
import org.junit.Test;
import uk.co.flax.luwak.matchers.SimpleMatcher;
import uk.co.flax.luwak.presearcher.MatchAllPresearcher;
import uk.co.flax.luwak.queryparsers.LuceneQueryParser;

import static uk.co.flax.luwak.assertions.MatchesAssert.assertThat;

/**
 * Copyright (c) 2013 Lemur Consulting Ltd.
 * <p/>
 * Licensed under the Apache License, Version 2.0 (the ""License"");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 * <p/>
 * http://www.apache.org/licenses/LICENSE-2.0
 * <p/>
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

public class TestMonitor {

    static final String TEXTFIELD = ""TEXTFIELD"";

    static final Analyzer ANALYZER = new WhitespaceAnalyzer();

    private Monitor monitor;

    @Before
    public void setUp() throws IOException {
        monitor = new Monitor(new LuceneQueryParser(TEXTFIELD, ANALYZER), new MatchAllPresearcher());
    }

    @Test
    public void singleTermQueryMatchesSingleDocument() throws IOException, UpdateException {

        String document = ""This is a test document"";

        DocumentBatch batch = DocumentBatch.of(InputDocument.builder(""doc1"")
                .addField(TEXTFIELD, document, ANALYZER)
                .build());

        monitor.update(new MonitorQuery(""query1"", ""test""));

        assertThat(monitor.match(batch, SimpleMatcher.FACTORY))
                .matchesDoc(""doc1"")
                .hasMatchCount(""doc1"", 1)
                .matchesQuery(""query1"", ""doc1"");

    }

    @Test
    public void matchStatisticsAreReported() throws IOException, UpdateException {
        String document = ""This is a test document"";
        DocumentBatch batch = DocumentBatch.of(InputDocument.builder(""doc1"")
                .addField(TEXTFIELD, document, ANALYZER)
                .build());

        monitor.update(new MonitorQuery(""query1"", ""test""));

        Matches<QueryMatch> matches = monitor.match(batch, SimpleMatcher.FACTORY);
        Assertions.assertThat(matches.getQueriesRun()).isEqualTo(1);
        Assertions.assertThat(matches.getQueryBuildTime()).isGreaterThan(-1);
        Assertions.assertThat(matches.getSearchTime()).isGreaterThan(-1);
    }

    @Test
    public void updatesOverwriteOldQueries() throws IOException, UpdateException {
        monitor.update(new MonitorQuery(""query1"", ""this""));

        monitor.update(new MonitorQuery(""query1"", ""that""));

        DocumentBatch batch = DocumentBatch.of(InputDocument.builder(""doc1"").addField(TEXTFIELD, ""that"", ANALYZER).build());
        assertThat(monitor.match(batch, SimpleMatcher.FACTORY))
                .hasQueriesRunCount(1)
                .matchesQuery(""query1"", ""doc1"");
    }

    @Test
    public void canDeleteById() throws IOException, UpdateException {

        monitor.update(new MonitorQuery(""query1"", ""this""));
        monitor.update(new MonitorQuery(""query2"", ""that""), new MonitorQuery(""query3"", ""other""));
        Assertions.assertThat(monitor.getQueryCount()).isEqualTo(3);

        monitor.deleteById(""query2"", ""query1"");
        Assertions.assertThat(monitor.getQueryCount()).isEqualTo(1);

        DocumentBatch batch = DocumentBatch.of(InputDocument.builder(""doc1"").addField(TEXTFIELD, ""other things"", ANALYZER).build());
        assertThat(monitor.match(batch, SimpleMatcher.FACTORY))
                .hasQueriesRunCount(1)
                .matchesQuery(""query3"", ""doc1"");

    }

    @Test
    public void canRetrieveQuery() throws IOException, UpdateException {

        monitor.update(new MonitorQuery(""query1"", ""this""), new MonitorQuery(""query2"", ""that""));
        Assertions.assertThat(monitor.getQueryCount()).isEqualTo(2);
        Assertions.assertThat(monitor.getQueryIds()).contains(""query1"", ""query2"");

        MonitorQuery mq = monitor.getQuery(""query2"");
        Assertions.assertThat(mq).isEqualTo(new MonitorQuery(""query2"", ""that""));

    }

    @Test
    public void canClearTheMonitor() throws IOException, UpdateException {
        monitor.update(new MonitorQuery(""query1"", ""a""), new MonitorQuery(""query2"", ""b""), new MonitorQuery(""query3"", ""c""));
        Assertions.assertThat(monitor.getQueryCount()).isEqualTo(3);

        monitor.clear();
        Assertions.assertThat(monitor.getQueryCount()).isEqualTo(0);
    }

    @Test
    public void testMatchesAgainstAnEmptyMonitor() throws IOException {

        monitor.clear();
        Assertions.assertThat(monitor.getQueryCount()).isEqualTo(0);

        InputDocument doc = InputDocument.builder(""doc1"").addField(TEXTFIELD, ""other things"", ANALYZER).build();
        Matches<QueryMatch> matches = monitor.match(doc, SimpleMatcher.FACTORY);

        Assertions.assertThat(matches.getQueriesRun()).isEqualTo(0);
    }

    @Test
    public void testUpdateReporting() throws IOException, UpdateException {

        List<MonitorQuery> queries = new ArrayList<>(10400);
        for (int i = 0; i < 10355; i++) {
            queries.add(new MonitorQuery(Integer.toString(i), ""test""));
        }

        final int[] expectedSizes = new int[]{ 5001, 5001, 353 };
        final AtomicInteger callCount = new AtomicInteger();
        final AtomicInteger updateCount = new AtomicInteger();

        QueryIndexUpdateListener listener = new QueryIndexUpdateListener() {

            @Override
            public void afterUpdate(List<Indexable> updates) {
                int calls = callCount.getAndIncrement();
                updateCount.addAndGet(updates.size());
                Assertions.assertThat(updates.size()).isEqualTo(expectedSizes[calls]);
            }
        };

        try (Monitor monitor = new Monitor(new LuceneQueryParser(TEXTFIELD, ANALYZER), new MatchAllPresearcher())) {
            monitor.addQueryIndexUpdateListener(listener);
            monitor.update(queries);
            Assertions.assertThat(updateCount.get()).isEqualTo(10355);
        }
    }

    @Test
    public void testMatcherMetadata() throws IOException, UpdateException {
        try (Monitor monitor = new Monitor(new LuceneQueryParser(""field""), new MatchAllPresearcher())) {
            HashMap<String, String> metadataMap = new HashMap<>();
            metadataMap.put(""key"", ""value"");

            monitor.update(new MonitorQuery(Integer.toString(1), ""+test "" + Integer.toString(1), metadataMap));

            InputDocument doc = InputDocument.builder(""1"").addField(""field"", ""test"", ANALYZER).build();

            MatcherFactory<QueryMatch> testMatcherFactory = new MatcherFactory<QueryMatch>() {
                @Override
                public CandidateMatcher<QueryMatch> createMatcher(DocumentBatch docs) {
                    return new CandidateMatcher<QueryMatch>(docs) {
                        @Override
                        protected void doMatchQuery(String queryId, Query matchQuery, Map<String, String> metadata) throws IOException {
                            Assertions.assertThat(metadata.get(""key"")).isEqualTo(""value"");
                        }

                        @Override
                        public QueryMatch resolve(QueryMatch match1, QueryMatch match2) {
                            return null;
                        }
                    };
                }
            };

            monitor.match(doc, testMatcherFactory);
        }
    }

    @Test
    public void testDocumentBatching() throws IOException, UpdateException {

        DocumentBatch batch = DocumentBatch.of(
            InputDocument.builder(""doc1"").addField(TEXTFIELD, ""this is a test"", ANALYZER).build(),
            InputDocument.builder(""doc2"").addField(TEXTFIELD, ""this is a kangaroo"", ANALYZER).build()
        );

        monitor.clear();
        monitor.update(new MonitorQuery(""1"", ""kangaroo""));

        Matches<QueryMatch> response = monitor.match(batch, SimpleMatcher.FACTORY);
        Assertions.assertThat(response.getBatchSize()).isEqualTo(2);
        Assertions.assertThat(response.iterator()).hasSize(2);

    }

    @Test
    public void testMutliValuedFieldWithNonDefaultGaps() throws IOException, UpdateException {

        Analyzer analyzer = new Analyzer() {
            @Override
            public int getPositionIncrementGap(String fieldName) {
                return 1000;
            }

            @Override
            public int getOffsetGap(String fieldName) {
                return 2000;
            }

            @Override
            protected TokenStreamComponents createComponents(String fieldName) {
                return new TokenStreamComponents(new WhitespaceTokenizer());
            }
        };

        MonitorQuery mq = new MonitorQuery(""query"", TEXTFIELD + "":\""hello world\""~5"");
        monitor.update(mq);

        InputDocument doc1 = InputDocument.builder(""doc1"")
                .addField(TEXTFIELD, ""hello world"", analyzer)
                .addField(TEXTFIELD, ""goodbye"", analyzer)
                .build();
        assertThat(monitor.match(doc1, SimpleMatcher.FACTORY))
                .matchesDoc(""doc1"")
                .hasMatchCount(""doc1"", 1)
                .matchesQuery(""query"", ""doc1"");

        InputDocument doc2 = InputDocument.builder(""doc2"")
                .addField(TEXTFIELD, ""hello"", analyzer)
                .addField(TEXTFIELD, ""world"", analyzer)
                .build();
        assertThat(monitor.match(doc2, SimpleMatcher.FACTORY))
                .hasMatchCount(""doc2"", 0);
    }

}
","package uk.co.flax.luwak;

import java.io.Closeable;
import java.io.IOException;
import java.util.*;
import java.util.concurrent.Executors;
import java.util.concurrent.ScheduledExecutorService;

import org.apache.lucene.analysis.core.KeywordAnalyzer;
import org.apache.lucene.document.*;
import org.apache.lucene.index.*;
import org.apache.lucene.search.*;
import org.apache.lucene.search.spans.SpanCollector;
import uk.co.flax.luwak.util.RewriteException;
import uk.co.flax.luwak.util.SpanExtractor;
import uk.co.flax.luwak.util.SpanRewriter;
import org.apache.lucene.store.Directory;
import org.apache.lucene.store.RAMDirectory;
import org.apache.lucene.util.BytesRef;
import org.apache.lucene.util.BytesRefBuilder;
import uk.co.flax.luwak.presearcher.PresearcherMatches;
import uk.co.flax.luwak.util.ForceNoBulkScoringQuery;

/*
 * Copyright (c) 2015 Lemur Consulting Ltd.
 * <p/>
 * Licensed under the Apache License, Version 2.0 (the ""License"");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 * <p/>
 * http://www.apache.org/licenses/LICENSE-2.0
 * <p/>
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/**
 * A Monitor contains a set of MonitorQuery objects, and runs them against
 * passed-in InputDocuments.
 */
public class Monitor implements Closeable {

    protected final MonitorQueryParser queryParser;
    protected final Presearcher presearcher;
    protected final QueryDecomposer decomposer;

    private final QueryIndex queryIndex;

    private final List<QueryIndexUpdateListener> listeners = new ArrayList<>();

    protected long slowLogLimit = 2000000;

    private final long commitBatchSize;
    private final boolean storeQueries;

    public static final class FIELDS {
        public static final String id = ""_id"";
        public static final String del = ""_del"";
        public static final String hash = ""_hash"";
        public static final String mq = ""_mq"";
    }

    private final ScheduledExecutorService purgeExecutor;

    private long lastPurged = -1;

    /**
     * Create a new Monitor instance, using a passed in IndexWriter for its queryindex
     *
     * Note that when the Monitor is closed, both the IndexWriter and its underlying
     * Directory will also be closed.
     *
     * @param queryParser the query parser to use
     * @param presearcher the presearcher to use
     * @param indexWriter an indexWriter for the query index
     * @param configuration the MonitorConfiguration
     * @throws IOException on IO errors
     */
    public Monitor(MonitorQueryParser queryParser, Presearcher presearcher,
                   IndexWriter indexWriter, QueryIndexConfiguration configuration) throws IOException {

        this.queryParser = queryParser;
        this.presearcher = presearcher;
        this.decomposer = configuration.getQueryDecomposer();
        
        this.queryIndex = new QueryIndex(indexWriter);

        this.storeQueries = configuration.storeQueries();
        prepareQueryCache(this.storeQueries);

        long purgeFrequency = configuration.getPurgeFrequency();
        this.purgeExecutor = Executors.newSingleThreadScheduledExecutor();
        this.purgeExecutor.scheduleAtFixedRate(() -> {
            try {
                purgeCache();
            }
            catch (Throwable e) {
                afterPurgeError(e);
            }
        }, purgeFrequency, purgeFrequency, configuration.getPurgeFrequencyUnits());

        this.commitBatchSize = configuration.getQueryUpdateBufferSize();
    }

    /**
     * Create a new Monitor instance, using a RAMDirectory and the default configuration
     * @param queryParser the query parser to use
     * @param presearcher the presearcher to use
     * @throws IOException on IO errors
     */
    public Monitor(MonitorQueryParser queryParser, Presearcher presearcher) throws IOException {
        this(queryParser, presearcher, defaultIndexWriter(new RAMDirectory()), new QueryIndexConfiguration());
    }

    /**
     * Create a new Monitor instance using a RAMDirectory
     * @param queryParser the query parser to use
     * @param presearcher the presearcher to use
     * @param config the monitor configuration
     * @throws IOException on IO errors
     */
    public Monitor(MonitorQueryParser queryParser, Presearcher presearcher, QueryIndexConfiguration config) throws IOException {
        this(queryParser, presearcher, defaultIndexWriter(new RAMDirectory()), config);
    }

    /**
     * Create a new Monitor instance, using the default QueryDecomposer and IndexWriter configuration
     * @param queryParser the query parser to use
     * @param presearcher the presearcher to use
     * @param directory the directory where the queryindex is stored
     * @throws IOException on IO errors
     */
    public Monitor(MonitorQueryParser queryParser, Presearcher presearcher, Directory directory) throws IOException {
        this(queryParser, presearcher, defaultIndexWriter(directory), new QueryIndexConfiguration());
    }

    /**
     * Create a new Monitor instance
     * @param queryParser the query parser to use
     * @param presearcher the presearcher to use
     * @param directory the directory where the queryindex is to be stored
     * @param config the monitor configuration
     * @throws IOException on IO errors
     */
    public Monitor(MonitorQueryParser queryParser, Presearcher presearcher, Directory directory, QueryIndexConfiguration config) throws IOException {
        this(queryParser, presearcher, defaultIndexWriter(directory), config);
    }

    /**
     * Create a new Monitor instance, using the default QueryDecomposer
     * @param queryParser the query parser to use
     * @param presearcher the presearcher to use
     * @param indexWriter a {@link IndexWriter} for the Monitor's query index
     * @throws IOException on IO errors
     */
    public Monitor(MonitorQueryParser queryParser, Presearcher presearcher, IndexWriter indexWriter) throws IOException {
        this(queryParser, presearcher, indexWriter, new QueryIndexConfiguration());
    }

    // package-private for testing
    static IndexWriter defaultIndexWriter(Directory directory) throws IOException {

        IndexWriterConfig iwc = new IndexWriterConfig(new KeywordAnalyzer());
        TieredMergePolicy mergePolicy = new TieredMergePolicy();
        mergePolicy.setSegmentsPerTier(4);
        iwc.setMergePolicy(mergePolicy);
        iwc.setOpenMode(IndexWriterConfig.OpenMode.CREATE_OR_APPEND);

        return new IndexWriter(directory, iwc);

    }

    /**
     * Register a {@link QueryIndexUpdateListener} that will be notified whenever changes
     * are made to the Monitor's queryindex
     *
     * @param listener listener to register
     */
    public void addQueryIndexUpdateListener(QueryIndexUpdateListener listener) {
        listeners.add(listener);
    }

    /**
     * @return Statistics for the internal query index and cache
     */
    public QueryCacheStats getQueryCacheStats() {
        return new QueryCacheStats(queryIndex.numDocs(), queryIndex.cacheSize(), lastPurged);
    }

    /**
     * Statistics for the query cache and query index
     */
    public static class QueryCacheStats {

        /** Total number of queries in the query index */
        public final int queries;

        /** Total number of queries int the query cache */
        public final int cachedQueries;

        /** Time the query cache was last purged */
        public final long lastPurged;

        public QueryCacheStats(int queries, int cachedQueries, long lastPurged) {
            this.queries = queries;
            this.cachedQueries = cachedQueries;
            this.lastPurged = lastPurged;
        }
    }

    private void prepareQueryCache(boolean storeQueries) throws IOException {

        if (storeQueries == false) {
            // we're not storing the queries, so ensure that the queryindex is empty
            // before we add any.
            clear();
            return;
        }

        // load any queries that have already been added to the queryindex
        final List<Exception> parseErrors = new LinkedList<>();
        final Set<BytesRef> seenHashes = new HashSet<>();
        final Set<String> seenIds = new HashSet<>();

        queryIndex.purgeCache(newCache -> queryIndex.scan((id, query, dataValues) -> {
            if (seenIds.contains(id)) {
                return;
            }
            seenIds.add(id);

            dataValues.advance(dataValues.doc);
            BytesRef serializedMQ = dataValues.mq.binaryValue();
            MonitorQuery mq = MonitorQuery.deserialize(serializedMQ);

            BytesRef hash = mq.hash();
            if (seenHashes.contains(hash)) {
                return;
            }
            seenHashes.add(hash);

            try {
                for (QueryCacheEntry ce : decomposeQuery(mq)) {
                    newCache.put(ce.hash, ce);
                }
            } catch (Exception e) {
                parseErrors.add(e);
            }
        }));
        if (parseErrors.size() != 0)
            throw new IOException(""Error populating cache - some queries couldn't be parsed:"" + parseErrors);
    }

    private void commit(List<Indexable> updates) throws IOException {
        beforeCommit(updates);
        queryIndex.commit(updates);
        afterCommit(updates);
    }

    private void afterPurge() {
        for (QueryIndexUpdateListener listener : listeners) {
            listener.onPurge();
        }
    }

    private void afterPurgeError(Throwable t) {
        for (QueryIndexUpdateListener listener : listeners) {
            listener.onPurgeError(t);
        }
    }

    private void beforeCommit(List<Indexable> updates) {
        if (updates == null) {
            for (QueryIndexUpdateListener listener : listeners) {
                listener.beforeDelete();
            }
        }
        else {
            for (QueryIndexUpdateListener listener : listeners) {
                listener.beforeUpdate(updates);
            }
        }
    }

    private void afterCommit(List<Indexable> updates) {
        if (updates == null) {
            for (QueryIndexUpdateListener listener : listeners) {
                listener.afterDelete();
            }
        }
        else {
            for (QueryIndexUpdateListener listener : listeners) {
                listener.afterUpdate(updates);
            }
        }
    }
    
    /**
     * Remove unused queries from the query cache.
     *
     * This is normally called from a background thread at a rate set by configurePurgeFrequency().
     *
     * @throws IOException on IO errors
     */
    public void purgeCache() throws IOException {
        queryIndex.purgeCache(newCache -> queryIndex.scan((id, query, dataValues) -> {
            if (query != null)
                newCache.put(BytesRef.deepCopyOf(query.hash), query);
        }));
        
        lastPurged = System.nanoTime();
        afterPurge();
    }

    /**
     * Set the slow log limit
     *
     * All queries that take longer than t nanoseconds to run will be recorded in
     * the slow log.  The default is 2,000,000 (2 milliseconds)
     *
     * @param limit the limit in nanoseconds
     *
     * @see Matches#getSlowLog()
     */
    public void setSlowLogLimit(long limit) {
        this.slowLogLimit = limit;
    }

    @Override
    public void close() throws IOException {
        purgeExecutor.shutdown();
        queryIndex.closeWhileHandlingException();
    }

    /**
     * Add new queries to the monitor
     * @param queries the MonitorQueries to add
     * @throws IOException on IO errors
     * @throws UpdateException if any of the queries could not be added
     */
    public void update(Iterable<MonitorQuery> queries) throws IOException, UpdateException {

        List<QueryError> errors = new ArrayList<>();
        List<Indexable> updates = new ArrayList<>();

        for (MonitorQuery query : queries) {
            try {
                for (QueryCacheEntry queryCacheEntry : decomposeQuery(query)) {
                    updates.add(new Indexable(query.getId(), queryCacheEntry, buildIndexableQuery(query.getId(), query, queryCacheEntry)));
                }
            } catch (Exception e) {
                errors.add(new QueryError(query, e));
            }
            if (updates.size() > commitBatchSize) {
                commit(updates);
                updates.clear();
            }
        }
        commit(updates);

        if (errors.isEmpty() == false)
            throw new UpdateException(errors);
    }

    private Iterable<QueryCacheEntry> decomposeQuery(MonitorQuery query) throws Exception {

        Query q = queryParser.parse(query.getQuery(), query.getMetadata());

        BytesRef rootHash = query.hash();

        int upto = 0;
        List<QueryCacheEntry> cacheEntries = new LinkedList<>();
        for (Query subquery : decomposer.decompose(q)) {
            BytesRefBuilder subHash = new BytesRefBuilder();
            subHash.append(rootHash);
            subHash.append(new BytesRef(""_"" + upto++));
            cacheEntries.add(new QueryCacheEntry(subHash.toBytesRef(), subquery, query.getMetadata()));
        }

        return cacheEntries;
    }

    /**
     * Add new queries to the monitor
     * @param queries the MonitorQueries to add
     * @throws IOException on IO errors
     * @throws UpdateException if any of the queries could not be added
     */
    public void update(MonitorQuery... queries) throws IOException, UpdateException {
        update(Arrays.asList(queries));
    }

    /**
     * Delete queries from the monitor
     * @param queries the queries to remove
     * @throws IOException on IO errors
     */
    public void delete(Iterable<MonitorQuery> queries) throws IOException {
        for (MonitorQuery mq : queries) {
            queryIndex.deleteDocuments(new Term(Monitor.FIELDS.del, mq.getId()));
        }
        commit(null);
    }

    /**
     * Delete queries from the monitor by ID
     * @param queryIds the IDs to delete
     * @throws IOException on IO errors
     */
    public void deleteById(Iterable<String> queryIds) throws IOException {
        for (String queryId : queryIds) {
            queryIndex.deleteDocuments(new Term(FIELDS.del, queryId));
        }
        commit(null);
    }

    /**
     * Delete queries from the monitor by ID
     * @param queryIds the IDs to delete
     * @throws IOException on IO errors
     */
    public void deleteById(String... queryIds) throws IOException {
        deleteById(Arrays.asList(queryIds));
    }

    /**
     * Delete all queries from the monitor
     * @throws IOException on IO errors
     */
    public void clear() throws IOException {
        queryIndex.deleteDocuments(new MatchAllDocsQuery());
        commit(null);
    }

    /**
     * Match a {@link DocumentBatch} against the queryindex, calling a {@link CandidateMatcher} produced by the
     * supplied {@link MatcherFactory} for each possible matching query.
     * @param docs the DocumentBatch to match
     * @param factory a {@link MatcherFactory} to use to create a {@link CandidateMatcher} for the match run
     * @param <T> the type of {@link QueryMatch} to return
     * @return a {@link Matches} object summarizing the match run.
     * @throws IOException on IO errors
     */
    public <T extends QueryMatch> Matches<T> match(DocumentBatch docs, MatcherFactory<T> factory) throws IOException {
        CandidateMatcher<T> matcher = factory.createMatcher(docs);
        matcher.setSlowLogLimit(slowLogLimit);
        match(matcher);
        return matcher.getMatches();
    }

    /**
     * Match a single {@link InputDocument} against the queryindex, calling a {@link CandidateMatcher} produced by the
     * supplied {@link MatcherFactory} for each possible matching query.
     * @param doc the InputDocument to match
     * @param factory a {@link MatcherFactory} to use to create a {@link CandidateMatcher} for the match run
     * @param <T> the type of {@link QueryMatch} to return
     * @return a {@link Matches} object summarizing the match run.
     * @throws IOException on IO errors
     */
    public <T extends QueryMatch> Matches<T> match(InputDocument doc, MatcherFactory<T> factory) throws IOException {
        return match(DocumentBatch.of(doc), factory);
    }

    private class PresearcherQueryBuilder implements QueryIndex.QueryBuilder {

        final LeafReader batchIndexReader;

        private PresearcherQueryBuilder(LeafReader batchIndexReader) {
            this.batchIndexReader = batchIndexReader;
        }

        @Override
        public Query buildQuery(QueryTermFilter termFilter) throws IOException {
            return presearcher.buildQuery(batchIndexReader, termFilter);
        }
    }

    private <T extends QueryMatch> void match(CandidateMatcher<T> matcher) throws IOException {
        StandardQueryCollector<T> collector = new StandardQueryCollector<>(matcher);
        long buildTime = queryIndex.search(new PresearcherQueryBuilder(matcher.getIndexReader()), collector);
        matcher.finish(buildTime, collector.queryCount);
    }

    /**
     * Get the MonitorQuery for a given query id
     * @param queryId the id of the query to get
     * @return the MonitorQuery stored for this id, or null if not found
     * @throws IOException on IO errors
     * @throws IllegalStateException if queries are not stored in the queryindex
     */
    public MonitorQuery getQuery(final String queryId) throws IOException {
        if (storeQueries == false)
            throw new IllegalStateException(""Cannot call getQuery() as queries are not stored"");
        final MonitorQuery[] queryHolder = new MonitorQuery[]{ null };
        queryIndex.search(new TermQuery(new Term(FIELDS.id, queryId)), (id, query, dataValues) -> {
            dataValues.advance(dataValues.doc);
            BytesRef serializedMQ = dataValues.mq.binaryValue();
            queryHolder[0] = MonitorQuery.deserialize(serializedMQ);
        });
        return queryHolder[0];
    }

    /**
     * @return the number of queries (after decomposition) stored in this Monitor
     */
    public int getDisjunctCount() {
        return queryIndex.numDocs();
    }

    /**
     * @return the number of queries stored in this Monitor
     * @throws IOException on IO errors
     */
    public int getQueryCount() throws IOException {
        return getQueryIds().size();
    }

    /**
     * @return the set of query ids of the queries stored in this Monitor
     * @throws IOException on IO errors
     */
    public Set<String> getQueryIds() throws IOException {
        final Set<String> ids = new HashSet<>();
        queryIndex.scan((id, query, dataValues) -> ids.add(id));
        return ids;
    }

    /**
     * Build a lucene {@link Document} to be stored in the queryindex from a query entry
     * @param id the query id
     * @param mq the MonitorQuery to be indexed
     * @param query the (possibly partial after decomposition) query to be indexed
     * @return a Document that will be indexed in the Monitor's queryindex
     */
    protected Document buildIndexableQuery(String id, MonitorQuery mq, QueryCacheEntry query) {
        Document doc = presearcher.indexQuery(query.matchQuery, mq.getMetadata());
        doc.add(new StringField(FIELDS.id, id, Field.Store.NO));
        doc.add(new StringField(FIELDS.del, id, Field.Store.NO));
        doc.add(new SortedDocValuesField(FIELDS.id, new BytesRef(id)));
        doc.add(new BinaryDocValuesField(FIELDS.hash, query.hash));
        if (storeQueries)
            doc.add(new BinaryDocValuesField(FIELDS.mq, MonitorQuery.serialize(mq)));
        return doc;
    }

    // For each query selected by the presearcher, pass on to a CandidateMatcher
    private static class StandardQueryCollector<T extends QueryMatch> implements QueryIndex.QueryCollector {

        final CandidateMatcher<T> matcher;
        int queryCount = 0;

        private StandardQueryCollector(CandidateMatcher<T> matcher) {
            this.matcher = matcher;
        }

        @Override
        public void matchQuery(String id, QueryCacheEntry query, QueryIndex.DataValues dataValues) throws IOException {
            if (query == null)
                return;
            try {
                queryCount++;
                matcher.matchQuery(id, query.matchQuery, query.metadata);
            }
            catch (Exception e) {
                matcher.reportError(new MatchError(id, e));
            }
        }

    }

    /**
     * Match a DocumentBatch against the queries stored in the Monitor, also returning information
     * about which queries were selected by the presearcher, and why.
     * @param docs a DocumentBatch to match against the index
     * @param factory a {@link MatcherFactory} to use to create a {@link CandidateMatcher} for the match run
     * @param <T> the type of QueryMatch produced by the CandidateMatcher
     * @return a {@link PresearcherMatches} object containing debug information
     * @throws IOException on IO errors
     */
    public <T extends QueryMatch> PresearcherMatches<T> debug(final DocumentBatch docs, MatcherFactory<T> factory)
            throws IOException {
        PresearcherQueryCollector<T> collector = new PresearcherQueryCollector<>(factory.createMatcher(docs));
        QueryIndex.QueryBuilder queryBuilder = new PresearcherQueryBuilder(docs.getIndexReader()){
            @Override
            public Query buildQuery(QueryTermFilter termFilter) throws IOException {
                try {
                    return new ForceNoBulkScoringQuery(SpanRewriter.INSTANCE.rewrite(super.buildQuery(termFilter), null));
                } catch (RewriteException e) {
                    throw new IOException(e);
                }
            }
        };
        queryIndex.search(queryBuilder, collector);
        return collector.getMatches();
    }

    /**
     * Match a single {@link InputDocument} against the queries stored in the Monitor, also returning information
     * about which queries were selected by the presearcher, and why.
     * @param doc an InputDocument to match against the index
     * @param factory a {@link MatcherFactory} to use to create a {@link CandidateMatcher} for the match run
     * @param <T> the type of QueryMatch produced by the CandidateMatcher
     * @return a {@link PresearcherMatches} object containing debug information
     * @throws IOException on IO errors
     */
    public <T extends QueryMatch> PresearcherMatches<T> debug(InputDocument doc, MatcherFactory<T> factory) throws IOException {
        return debug(DocumentBatch.of(doc), factory);
    }

    private class PresearcherQueryCollector<T extends QueryMatch> extends StandardQueryCollector<T> {

        public final Map<String, StringBuilder> matchingTerms = new HashMap<>();

        private PresearcherQueryCollector(CandidateMatcher<T> matcher) {
            super(matcher);
        }

        public PresearcherMatches<T> getMatches() {
            return new PresearcherMatches<>(matchingTerms, matcher.getMatches());
        }

        @Override
        public boolean needsScores() {
            return true;
        }

        @Override
        public void matchQuery(final String id, QueryCacheEntry query, QueryIndex.DataValues dataValues) throws IOException {

            SpanCollector collector = new SpanCollector() {
                @Override
                public void collectLeaf(PostingsEnum postingsEnum, int position, Term term) throws IOException {
                    matchingTerms.computeIfAbsent(id, i -> new StringBuilder())
                            .append("" "")
                            .append(term.field())
                            .append("":"")
                            .append(term.bytes().utf8ToString());
                }

                @Override
                public void reset() {

                }
            };

            SpanExtractor.collect(dataValues.scorer, collector, false);

            super.matchQuery(id, query, dataValues);
        }

    }

}
","['Assertion Roulette', 'Conditional Test Logic', 'General Fixture', 'Unknown Test']","['Assertion Roulette', 'Eager Test', 'Redundant Print', 'Redundant Assertion']",3,3,1,11
32685_31_luwak_testdocumentbatching,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/32685_test.java,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/32685_actual.java,"package uk.co.flax.luwak;

import java.io.IOException;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.concurrent.atomic.AtomicInteger;

import org.apache.lucene.analysis.Analyzer;
import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
import org.apache.lucene.analysis.core.WhitespaceTokenizer;
import org.apache.lucene.search.Query;
import org.assertj.core.api.Assertions;
import org.junit.Before;
import org.junit.Test;
import uk.co.flax.luwak.matchers.SimpleMatcher;
import uk.co.flax.luwak.presearcher.MatchAllPresearcher;
import uk.co.flax.luwak.queryparsers.LuceneQueryParser;

import static uk.co.flax.luwak.assertions.MatchesAssert.assertThat;

/**
 * Copyright (c) 2013 Lemur Consulting Ltd.
 * <p/>
 * Licensed under the Apache License, Version 2.0 (the ""License"");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 * <p/>
 * http://www.apache.org/licenses/LICENSE-2.0
 * <p/>
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

public class TestMonitor {

    static final String TEXTFIELD = ""TEXTFIELD"";

    static final Analyzer ANALYZER = new WhitespaceAnalyzer();

    private Monitor monitor;

    @Before
    public void setUp() throws IOException {
        monitor = new Monitor(new LuceneQueryParser(TEXTFIELD, ANALYZER), new MatchAllPresearcher());
    }

    @Test
    public void singleTermQueryMatchesSingleDocument() throws IOException, UpdateException {

        String document = ""This is a test document"";

        DocumentBatch batch = DocumentBatch.of(InputDocument.builder(""doc1"")
                .addField(TEXTFIELD, document, ANALYZER)
                .build());

        monitor.update(new MonitorQuery(""query1"", ""test""));

        assertThat(monitor.match(batch, SimpleMatcher.FACTORY))
                .matchesDoc(""doc1"")
                .hasMatchCount(""doc1"", 1)
                .matchesQuery(""query1"", ""doc1"");

    }

    @Test
    public void matchStatisticsAreReported() throws IOException, UpdateException {
        String document = ""This is a test document"";
        DocumentBatch batch = DocumentBatch.of(InputDocument.builder(""doc1"")
                .addField(TEXTFIELD, document, ANALYZER)
                .build());

        monitor.update(new MonitorQuery(""query1"", ""test""));

        Matches<QueryMatch> matches = monitor.match(batch, SimpleMatcher.FACTORY);
        Assertions.assertThat(matches.getQueriesRun()).isEqualTo(1);
        Assertions.assertThat(matches.getQueryBuildTime()).isGreaterThan(-1);
        Assertions.assertThat(matches.getSearchTime()).isGreaterThan(-1);
    }

    @Test
    public void updatesOverwriteOldQueries() throws IOException, UpdateException {
        monitor.update(new MonitorQuery(""query1"", ""this""));

        monitor.update(new MonitorQuery(""query1"", ""that""));

        DocumentBatch batch = DocumentBatch.of(InputDocument.builder(""doc1"").addField(TEXTFIELD, ""that"", ANALYZER).build());
        assertThat(monitor.match(batch, SimpleMatcher.FACTORY))
                .hasQueriesRunCount(1)
                .matchesQuery(""query1"", ""doc1"");
    }

    @Test
    public void canDeleteById() throws IOException, UpdateException {

        monitor.update(new MonitorQuery(""query1"", ""this""));
        monitor.update(new MonitorQuery(""query2"", ""that""), new MonitorQuery(""query3"", ""other""));
        Assertions.assertThat(monitor.getQueryCount()).isEqualTo(3);

        monitor.deleteById(""query2"", ""query1"");
        Assertions.assertThat(monitor.getQueryCount()).isEqualTo(1);

        DocumentBatch batch = DocumentBatch.of(InputDocument.builder(""doc1"").addField(TEXTFIELD, ""other things"", ANALYZER).build());
        assertThat(monitor.match(batch, SimpleMatcher.FACTORY))
                .hasQueriesRunCount(1)
                .matchesQuery(""query3"", ""doc1"");

    }

    @Test
    public void canRetrieveQuery() throws IOException, UpdateException {

        monitor.update(new MonitorQuery(""query1"", ""this""), new MonitorQuery(""query2"", ""that""));
        Assertions.assertThat(monitor.getQueryCount()).isEqualTo(2);
        Assertions.assertThat(monitor.getQueryIds()).contains(""query1"", ""query2"");

        MonitorQuery mq = monitor.getQuery(""query2"");
        Assertions.assertThat(mq).isEqualTo(new MonitorQuery(""query2"", ""that""));

    }

    @Test
    public void canClearTheMonitor() throws IOException, UpdateException {
        monitor.update(new MonitorQuery(""query1"", ""a""), new MonitorQuery(""query2"", ""b""), new MonitorQuery(""query3"", ""c""));
        Assertions.assertThat(monitor.getQueryCount()).isEqualTo(3);

        monitor.clear();
        Assertions.assertThat(monitor.getQueryCount()).isEqualTo(0);
    }

    @Test
    public void testMatchesAgainstAnEmptyMonitor() throws IOException {

        monitor.clear();
        Assertions.assertThat(monitor.getQueryCount()).isEqualTo(0);

        InputDocument doc = InputDocument.builder(""doc1"").addField(TEXTFIELD, ""other things"", ANALYZER).build();
        Matches<QueryMatch> matches = monitor.match(doc, SimpleMatcher.FACTORY);

        Assertions.assertThat(matches.getQueriesRun()).isEqualTo(0);
    }

    @Test
    public void testUpdateReporting() throws IOException, UpdateException {

        List<MonitorQuery> queries = new ArrayList<>(10400);
        for (int i = 0; i < 10355; i++) {
            queries.add(new MonitorQuery(Integer.toString(i), ""test""));
        }

        final int[] expectedSizes = new int[]{ 5001, 5001, 353 };
        final AtomicInteger callCount = new AtomicInteger();
        final AtomicInteger updateCount = new AtomicInteger();

        QueryIndexUpdateListener listener = new QueryIndexUpdateListener() {

            @Override
            public void afterUpdate(List<Indexable> updates) {
                int calls = callCount.getAndIncrement();
                updateCount.addAndGet(updates.size());
                Assertions.assertThat(updates.size()).isEqualTo(expectedSizes[calls]);
            }
        };

        try (Monitor monitor = new Monitor(new LuceneQueryParser(TEXTFIELD, ANALYZER), new MatchAllPresearcher())) {
            monitor.addQueryIndexUpdateListener(listener);
            monitor.update(queries);
            Assertions.assertThat(updateCount.get()).isEqualTo(10355);
        }
    }

    @Test
    public void testMatcherMetadata() throws IOException, UpdateException {
        try (Monitor monitor = new Monitor(new LuceneQueryParser(""field""), new MatchAllPresearcher())) {
            HashMap<String, String> metadataMap = new HashMap<>();
            metadataMap.put(""key"", ""value"");

            monitor.update(new MonitorQuery(Integer.toString(1), ""+test "" + Integer.toString(1), metadataMap));

            InputDocument doc = InputDocument.builder(""1"").addField(""field"", ""test"", ANALYZER).build();

            MatcherFactory<QueryMatch> testMatcherFactory = new MatcherFactory<QueryMatch>() {
                @Override
                public CandidateMatcher<QueryMatch> createMatcher(DocumentBatch docs) {
                    return new CandidateMatcher<QueryMatch>(docs) {
                        @Override
                        protected void doMatchQuery(String queryId, Query matchQuery, Map<String, String> metadata) throws IOException {
                            Assertions.assertThat(metadata.get(""key"")).isEqualTo(""value"");
                        }

                        @Override
                        public QueryMatch resolve(QueryMatch match1, QueryMatch match2) {
                            return null;
                        }
                    };
                }
            };

            monitor.match(doc, testMatcherFactory);
        }
    }

    @Test
    public void testDocumentBatching() throws IOException, UpdateException {

        DocumentBatch batch = DocumentBatch.of(
            InputDocument.builder(""doc1"").addField(TEXTFIELD, ""this is a test"", ANALYZER).build(),
            InputDocument.builder(""doc2"").addField(TEXTFIELD, ""this is a kangaroo"", ANALYZER).build()
        );

        monitor.clear();
        monitor.update(new MonitorQuery(""1"", ""kangaroo""));

        Matches<QueryMatch> response = monitor.match(batch, SimpleMatcher.FACTORY);
        Assertions.assertThat(response.getBatchSize()).isEqualTo(2);
        Assertions.assertThat(response.iterator()).hasSize(2);

    }

    @Test
    public void testMutliValuedFieldWithNonDefaultGaps() throws IOException, UpdateException {

        Analyzer analyzer = new Analyzer() {
            @Override
            public int getPositionIncrementGap(String fieldName) {
                return 1000;
            }

            @Override
            public int getOffsetGap(String fieldName) {
                return 2000;
            }

            @Override
            protected TokenStreamComponents createComponents(String fieldName) {
                return new TokenStreamComponents(new WhitespaceTokenizer());
            }
        };

        MonitorQuery mq = new MonitorQuery(""query"", TEXTFIELD + "":\""hello world\""~5"");
        monitor.update(mq);

        InputDocument doc1 = InputDocument.builder(""doc1"")
                .addField(TEXTFIELD, ""hello world"", analyzer)
                .addField(TEXTFIELD, ""goodbye"", analyzer)
                .build();
        assertThat(monitor.match(doc1, SimpleMatcher.FACTORY))
                .matchesDoc(""doc1"")
                .hasMatchCount(""doc1"", 1)
                .matchesQuery(""query"", ""doc1"");

        InputDocument doc2 = InputDocument.builder(""doc2"")
                .addField(TEXTFIELD, ""hello"", analyzer)
                .addField(TEXTFIELD, ""world"", analyzer)
                .build();
        assertThat(monitor.match(doc2, SimpleMatcher.FACTORY))
                .hasMatchCount(""doc2"", 0);
    }

}
","package uk.co.flax.luwak;

import java.io.Closeable;
import java.io.IOException;
import java.util.*;
import java.util.concurrent.Executors;
import java.util.concurrent.ScheduledExecutorService;

import org.apache.lucene.analysis.core.KeywordAnalyzer;
import org.apache.lucene.document.*;
import org.apache.lucene.index.*;
import org.apache.lucene.search.*;
import org.apache.lucene.search.spans.SpanCollector;
import uk.co.flax.luwak.util.RewriteException;
import uk.co.flax.luwak.util.SpanExtractor;
import uk.co.flax.luwak.util.SpanRewriter;
import org.apache.lucene.store.Directory;
import org.apache.lucene.store.RAMDirectory;
import org.apache.lucene.util.BytesRef;
import org.apache.lucene.util.BytesRefBuilder;
import uk.co.flax.luwak.presearcher.PresearcherMatches;
import uk.co.flax.luwak.util.ForceNoBulkScoringQuery;

/*
 * Copyright (c) 2015 Lemur Consulting Ltd.
 * <p/>
 * Licensed under the Apache License, Version 2.0 (the ""License"");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 * <p/>
 * http://www.apache.org/licenses/LICENSE-2.0
 * <p/>
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/**
 * A Monitor contains a set of MonitorQuery objects, and runs them against
 * passed-in InputDocuments.
 */
public class Monitor implements Closeable {

    protected final MonitorQueryParser queryParser;
    protected final Presearcher presearcher;
    protected final QueryDecomposer decomposer;

    private final QueryIndex queryIndex;

    private final List<QueryIndexUpdateListener> listeners = new ArrayList<>();

    protected long slowLogLimit = 2000000;

    private final long commitBatchSize;
    private final boolean storeQueries;

    public static final class FIELDS {
        public static final String id = ""_id"";
        public static final String del = ""_del"";
        public static final String hash = ""_hash"";
        public static final String mq = ""_mq"";
    }

    private final ScheduledExecutorService purgeExecutor;

    private long lastPurged = -1;

    /**
     * Create a new Monitor instance, using a passed in IndexWriter for its queryindex
     *
     * Note that when the Monitor is closed, both the IndexWriter and its underlying
     * Directory will also be closed.
     *
     * @param queryParser the query parser to use
     * @param presearcher the presearcher to use
     * @param indexWriter an indexWriter for the query index
     * @param configuration the MonitorConfiguration
     * @throws IOException on IO errors
     */
    public Monitor(MonitorQueryParser queryParser, Presearcher presearcher,
                   IndexWriter indexWriter, QueryIndexConfiguration configuration) throws IOException {

        this.queryParser = queryParser;
        this.presearcher = presearcher;
        this.decomposer = configuration.getQueryDecomposer();
        
        this.queryIndex = new QueryIndex(indexWriter);

        this.storeQueries = configuration.storeQueries();
        prepareQueryCache(this.storeQueries);

        long purgeFrequency = configuration.getPurgeFrequency();
        this.purgeExecutor = Executors.newSingleThreadScheduledExecutor();
        this.purgeExecutor.scheduleAtFixedRate(() -> {
            try {
                purgeCache();
            }
            catch (Throwable e) {
                afterPurgeError(e);
            }
        }, purgeFrequency, purgeFrequency, configuration.getPurgeFrequencyUnits());

        this.commitBatchSize = configuration.getQueryUpdateBufferSize();
    }

    /**
     * Create a new Monitor instance, using a RAMDirectory and the default configuration
     * @param queryParser the query parser to use
     * @param presearcher the presearcher to use
     * @throws IOException on IO errors
     */
    public Monitor(MonitorQueryParser queryParser, Presearcher presearcher) throws IOException {
        this(queryParser, presearcher, defaultIndexWriter(new RAMDirectory()), new QueryIndexConfiguration());
    }

    /**
     * Create a new Monitor instance using a RAMDirectory
     * @param queryParser the query parser to use
     * @param presearcher the presearcher to use
     * @param config the monitor configuration
     * @throws IOException on IO errors
     */
    public Monitor(MonitorQueryParser queryParser, Presearcher presearcher, QueryIndexConfiguration config) throws IOException {
        this(queryParser, presearcher, defaultIndexWriter(new RAMDirectory()), config);
    }

    /**
     * Create a new Monitor instance, using the default QueryDecomposer and IndexWriter configuration
     * @param queryParser the query parser to use
     * @param presearcher the presearcher to use
     * @param directory the directory where the queryindex is stored
     * @throws IOException on IO errors
     */
    public Monitor(MonitorQueryParser queryParser, Presearcher presearcher, Directory directory) throws IOException {
        this(queryParser, presearcher, defaultIndexWriter(directory), new QueryIndexConfiguration());
    }

    /**
     * Create a new Monitor instance
     * @param queryParser the query parser to use
     * @param presearcher the presearcher to use
     * @param directory the directory where the queryindex is to be stored
     * @param config the monitor configuration
     * @throws IOException on IO errors
     */
    public Monitor(MonitorQueryParser queryParser, Presearcher presearcher, Directory directory, QueryIndexConfiguration config) throws IOException {
        this(queryParser, presearcher, defaultIndexWriter(directory), config);
    }

    /**
     * Create a new Monitor instance, using the default QueryDecomposer
     * @param queryParser the query parser to use
     * @param presearcher the presearcher to use
     * @param indexWriter a {@link IndexWriter} for the Monitor's query index
     * @throws IOException on IO errors
     */
    public Monitor(MonitorQueryParser queryParser, Presearcher presearcher, IndexWriter indexWriter) throws IOException {
        this(queryParser, presearcher, indexWriter, new QueryIndexConfiguration());
    }

    // package-private for testing
    static IndexWriter defaultIndexWriter(Directory directory) throws IOException {

        IndexWriterConfig iwc = new IndexWriterConfig(new KeywordAnalyzer());
        TieredMergePolicy mergePolicy = new TieredMergePolicy();
        mergePolicy.setSegmentsPerTier(4);
        iwc.setMergePolicy(mergePolicy);
        iwc.setOpenMode(IndexWriterConfig.OpenMode.CREATE_OR_APPEND);

        return new IndexWriter(directory, iwc);

    }

    /**
     * Register a {@link QueryIndexUpdateListener} that will be notified whenever changes
     * are made to the Monitor's queryindex
     *
     * @param listener listener to register
     */
    public void addQueryIndexUpdateListener(QueryIndexUpdateListener listener) {
        listeners.add(listener);
    }

    /**
     * @return Statistics for the internal query index and cache
     */
    public QueryCacheStats getQueryCacheStats() {
        return new QueryCacheStats(queryIndex.numDocs(), queryIndex.cacheSize(), lastPurged);
    }

    /**
     * Statistics for the query cache and query index
     */
    public static class QueryCacheStats {

        /** Total number of queries in the query index */
        public final int queries;

        /** Total number of queries int the query cache */
        public final int cachedQueries;

        /** Time the query cache was last purged */
        public final long lastPurged;

        public QueryCacheStats(int queries, int cachedQueries, long lastPurged) {
            this.queries = queries;
            this.cachedQueries = cachedQueries;
            this.lastPurged = lastPurged;
        }
    }

    private void prepareQueryCache(boolean storeQueries) throws IOException {

        if (storeQueries == false) {
            // we're not storing the queries, so ensure that the queryindex is empty
            // before we add any.
            clear();
            return;
        }

        // load any queries that have already been added to the queryindex
        final List<Exception> parseErrors = new LinkedList<>();
        final Set<BytesRef> seenHashes = new HashSet<>();
        final Set<String> seenIds = new HashSet<>();

        queryIndex.purgeCache(newCache -> queryIndex.scan((id, query, dataValues) -> {
            if (seenIds.contains(id)) {
                return;
            }
            seenIds.add(id);

            dataValues.advance(dataValues.doc);
            BytesRef serializedMQ = dataValues.mq.binaryValue();
            MonitorQuery mq = MonitorQuery.deserialize(serializedMQ);

            BytesRef hash = mq.hash();
            if (seenHashes.contains(hash)) {
                return;
            }
            seenHashes.add(hash);

            try {
                for (QueryCacheEntry ce : decomposeQuery(mq)) {
                    newCache.put(ce.hash, ce);
                }
            } catch (Exception e) {
                parseErrors.add(e);
            }
        }));
        if (parseErrors.size() != 0)
            throw new IOException(""Error populating cache - some queries couldn't be parsed:"" + parseErrors);
    }

    private void commit(List<Indexable> updates) throws IOException {
        beforeCommit(updates);
        queryIndex.commit(updates);
        afterCommit(updates);
    }

    private void afterPurge() {
        for (QueryIndexUpdateListener listener : listeners) {
            listener.onPurge();
        }
    }

    private void afterPurgeError(Throwable t) {
        for (QueryIndexUpdateListener listener : listeners) {
            listener.onPurgeError(t);
        }
    }

    private void beforeCommit(List<Indexable> updates) {
        if (updates == null) {
            for (QueryIndexUpdateListener listener : listeners) {
                listener.beforeDelete();
            }
        }
        else {
            for (QueryIndexUpdateListener listener : listeners) {
                listener.beforeUpdate(updates);
            }
        }
    }

    private void afterCommit(List<Indexable> updates) {
        if (updates == null) {
            for (QueryIndexUpdateListener listener : listeners) {
                listener.afterDelete();
            }
        }
        else {
            for (QueryIndexUpdateListener listener : listeners) {
                listener.afterUpdate(updates);
            }
        }
    }
    
    /**
     * Remove unused queries from the query cache.
     *
     * This is normally called from a background thread at a rate set by configurePurgeFrequency().
     *
     * @throws IOException on IO errors
     */
    public void purgeCache() throws IOException {
        queryIndex.purgeCache(newCache -> queryIndex.scan((id, query, dataValues) -> {
            if (query != null)
                newCache.put(BytesRef.deepCopyOf(query.hash), query);
        }));
        
        lastPurged = System.nanoTime();
        afterPurge();
    }

    /**
     * Set the slow log limit
     *
     * All queries that take longer than t nanoseconds to run will be recorded in
     * the slow log.  The default is 2,000,000 (2 milliseconds)
     *
     * @param limit the limit in nanoseconds
     *
     * @see Matches#getSlowLog()
     */
    public void setSlowLogLimit(long limit) {
        this.slowLogLimit = limit;
    }

    @Override
    public void close() throws IOException {
        purgeExecutor.shutdown();
        queryIndex.closeWhileHandlingException();
    }

    /**
     * Add new queries to the monitor
     * @param queries the MonitorQueries to add
     * @throws IOException on IO errors
     * @throws UpdateException if any of the queries could not be added
     */
    public void update(Iterable<MonitorQuery> queries) throws IOException, UpdateException {

        List<QueryError> errors = new ArrayList<>();
        List<Indexable> updates = new ArrayList<>();

        for (MonitorQuery query : queries) {
            try {
                for (QueryCacheEntry queryCacheEntry : decomposeQuery(query)) {
                    updates.add(new Indexable(query.getId(), queryCacheEntry, buildIndexableQuery(query.getId(), query, queryCacheEntry)));
                }
            } catch (Exception e) {
                errors.add(new QueryError(query, e));
            }
            if (updates.size() > commitBatchSize) {
                commit(updates);
                updates.clear();
            }
        }
        commit(updates);

        if (errors.isEmpty() == false)
            throw new UpdateException(errors);
    }

    private Iterable<QueryCacheEntry> decomposeQuery(MonitorQuery query) throws Exception {

        Query q = queryParser.parse(query.getQuery(), query.getMetadata());

        BytesRef rootHash = query.hash();

        int upto = 0;
        List<QueryCacheEntry> cacheEntries = new LinkedList<>();
        for (Query subquery : decomposer.decompose(q)) {
            BytesRefBuilder subHash = new BytesRefBuilder();
            subHash.append(rootHash);
            subHash.append(new BytesRef(""_"" + upto++));
            cacheEntries.add(new QueryCacheEntry(subHash.toBytesRef(), subquery, query.getMetadata()));
        }

        return cacheEntries;
    }

    /**
     * Add new queries to the monitor
     * @param queries the MonitorQueries to add
     * @throws IOException on IO errors
     * @throws UpdateException if any of the queries could not be added
     */
    public void update(MonitorQuery... queries) throws IOException, UpdateException {
        update(Arrays.asList(queries));
    }

    /**
     * Delete queries from the monitor
     * @param queries the queries to remove
     * @throws IOException on IO errors
     */
    public void delete(Iterable<MonitorQuery> queries) throws IOException {
        for (MonitorQuery mq : queries) {
            queryIndex.deleteDocuments(new Term(Monitor.FIELDS.del, mq.getId()));
        }
        commit(null);
    }

    /**
     * Delete queries from the monitor by ID
     * @param queryIds the IDs to delete
     * @throws IOException on IO errors
     */
    public void deleteById(Iterable<String> queryIds) throws IOException {
        for (String queryId : queryIds) {
            queryIndex.deleteDocuments(new Term(FIELDS.del, queryId));
        }
        commit(null);
    }

    /**
     * Delete queries from the monitor by ID
     * @param queryIds the IDs to delete
     * @throws IOException on IO errors
     */
    public void deleteById(String... queryIds) throws IOException {
        deleteById(Arrays.asList(queryIds));
    }

    /**
     * Delete all queries from the monitor
     * @throws IOException on IO errors
     */
    public void clear() throws IOException {
        queryIndex.deleteDocuments(new MatchAllDocsQuery());
        commit(null);
    }

    /**
     * Match a {@link DocumentBatch} against the queryindex, calling a {@link CandidateMatcher} produced by the
     * supplied {@link MatcherFactory} for each possible matching query.
     * @param docs the DocumentBatch to match
     * @param factory a {@link MatcherFactory} to use to create a {@link CandidateMatcher} for the match run
     * @param <T> the type of {@link QueryMatch} to return
     * @return a {@link Matches} object summarizing the match run.
     * @throws IOException on IO errors
     */
    public <T extends QueryMatch> Matches<T> match(DocumentBatch docs, MatcherFactory<T> factory) throws IOException {
        CandidateMatcher<T> matcher = factory.createMatcher(docs);
        matcher.setSlowLogLimit(slowLogLimit);
        match(matcher);
        return matcher.getMatches();
    }

    /**
     * Match a single {@link InputDocument} against the queryindex, calling a {@link CandidateMatcher} produced by the
     * supplied {@link MatcherFactory} for each possible matching query.
     * @param doc the InputDocument to match
     * @param factory a {@link MatcherFactory} to use to create a {@link CandidateMatcher} for the match run
     * @param <T> the type of {@link QueryMatch} to return
     * @return a {@link Matches} object summarizing the match run.
     * @throws IOException on IO errors
     */
    public <T extends QueryMatch> Matches<T> match(InputDocument doc, MatcherFactory<T> factory) throws IOException {
        return match(DocumentBatch.of(doc), factory);
    }

    private class PresearcherQueryBuilder implements QueryIndex.QueryBuilder {

        final LeafReader batchIndexReader;

        private PresearcherQueryBuilder(LeafReader batchIndexReader) {
            this.batchIndexReader = batchIndexReader;
        }

        @Override
        public Query buildQuery(QueryTermFilter termFilter) throws IOException {
            return presearcher.buildQuery(batchIndexReader, termFilter);
        }
    }

    private <T extends QueryMatch> void match(CandidateMatcher<T> matcher) throws IOException {
        StandardQueryCollector<T> collector = new StandardQueryCollector<>(matcher);
        long buildTime = queryIndex.search(new PresearcherQueryBuilder(matcher.getIndexReader()), collector);
        matcher.finish(buildTime, collector.queryCount);
    }

    /**
     * Get the MonitorQuery for a given query id
     * @param queryId the id of the query to get
     * @return the MonitorQuery stored for this id, or null if not found
     * @throws IOException on IO errors
     * @throws IllegalStateException if queries are not stored in the queryindex
     */
    public MonitorQuery getQuery(final String queryId) throws IOException {
        if (storeQueries == false)
            throw new IllegalStateException(""Cannot call getQuery() as queries are not stored"");
        final MonitorQuery[] queryHolder = new MonitorQuery[]{ null };
        queryIndex.search(new TermQuery(new Term(FIELDS.id, queryId)), (id, query, dataValues) -> {
            dataValues.advance(dataValues.doc);
            BytesRef serializedMQ = dataValues.mq.binaryValue();
            queryHolder[0] = MonitorQuery.deserialize(serializedMQ);
        });
        return queryHolder[0];
    }

    /**
     * @return the number of queries (after decomposition) stored in this Monitor
     */
    public int getDisjunctCount() {
        return queryIndex.numDocs();
    }

    /**
     * @return the number of queries stored in this Monitor
     * @throws IOException on IO errors
     */
    public int getQueryCount() throws IOException {
        return getQueryIds().size();
    }

    /**
     * @return the set of query ids of the queries stored in this Monitor
     * @throws IOException on IO errors
     */
    public Set<String> getQueryIds() throws IOException {
        final Set<String> ids = new HashSet<>();
        queryIndex.scan((id, query, dataValues) -> ids.add(id));
        return ids;
    }

    /**
     * Build a lucene {@link Document} to be stored in the queryindex from a query entry
     * @param id the query id
     * @param mq the MonitorQuery to be indexed
     * @param query the (possibly partial after decomposition) query to be indexed
     * @return a Document that will be indexed in the Monitor's queryindex
     */
    protected Document buildIndexableQuery(String id, MonitorQuery mq, QueryCacheEntry query) {
        Document doc = presearcher.indexQuery(query.matchQuery, mq.getMetadata());
        doc.add(new StringField(FIELDS.id, id, Field.Store.NO));
        doc.add(new StringField(FIELDS.del, id, Field.Store.NO));
        doc.add(new SortedDocValuesField(FIELDS.id, new BytesRef(id)));
        doc.add(new BinaryDocValuesField(FIELDS.hash, query.hash));
        if (storeQueries)
            doc.add(new BinaryDocValuesField(FIELDS.mq, MonitorQuery.serialize(mq)));
        return doc;
    }

    // For each query selected by the presearcher, pass on to a CandidateMatcher
    private static class StandardQueryCollector<T extends QueryMatch> implements QueryIndex.QueryCollector {

        final CandidateMatcher<T> matcher;
        int queryCount = 0;

        private StandardQueryCollector(CandidateMatcher<T> matcher) {
            this.matcher = matcher;
        }

        @Override
        public void matchQuery(String id, QueryCacheEntry query, QueryIndex.DataValues dataValues) throws IOException {
            if (query == null)
                return;
            try {
                queryCount++;
                matcher.matchQuery(id, query.matchQuery, query.metadata);
            }
            catch (Exception e) {
                matcher.reportError(new MatchError(id, e));
            }
        }

    }

    /**
     * Match a DocumentBatch against the queries stored in the Monitor, also returning information
     * about which queries were selected by the presearcher, and why.
     * @param docs a DocumentBatch to match against the index
     * @param factory a {@link MatcherFactory} to use to create a {@link CandidateMatcher} for the match run
     * @param <T> the type of QueryMatch produced by the CandidateMatcher
     * @return a {@link PresearcherMatches} object containing debug information
     * @throws IOException on IO errors
     */
    public <T extends QueryMatch> PresearcherMatches<T> debug(final DocumentBatch docs, MatcherFactory<T> factory)
            throws IOException {
        PresearcherQueryCollector<T> collector = new PresearcherQueryCollector<>(factory.createMatcher(docs));
        QueryIndex.QueryBuilder queryBuilder = new PresearcherQueryBuilder(docs.getIndexReader()){
            @Override
            public Query buildQuery(QueryTermFilter termFilter) throws IOException {
                try {
                    return new ForceNoBulkScoringQuery(SpanRewriter.INSTANCE.rewrite(super.buildQuery(termFilter), null));
                } catch (RewriteException e) {
                    throw new IOException(e);
                }
            }
        };
        queryIndex.search(queryBuilder, collector);
        return collector.getMatches();
    }

    /**
     * Match a single {@link InputDocument} against the queries stored in the Monitor, also returning information
     * about which queries were selected by the presearcher, and why.
     * @param doc an InputDocument to match against the index
     * @param factory a {@link MatcherFactory} to use to create a {@link CandidateMatcher} for the match run
     * @param <T> the type of QueryMatch produced by the CandidateMatcher
     * @return a {@link PresearcherMatches} object containing debug information
     * @throws IOException on IO errors
     */
    public <T extends QueryMatch> PresearcherMatches<T> debug(InputDocument doc, MatcherFactory<T> factory) throws IOException {
        return debug(DocumentBatch.of(doc), factory);
    }

    private class PresearcherQueryCollector<T extends QueryMatch> extends StandardQueryCollector<T> {

        public final Map<String, StringBuilder> matchingTerms = new HashMap<>();

        private PresearcherQueryCollector(CandidateMatcher<T> matcher) {
            super(matcher);
        }

        public PresearcherMatches<T> getMatches() {
            return new PresearcherMatches<>(matchingTerms, matcher.getMatches());
        }

        @Override
        public boolean needsScores() {
            return true;
        }

        @Override
        public void matchQuery(final String id, QueryCacheEntry query, QueryIndex.DataValues dataValues) throws IOException {

            SpanCollector collector = new SpanCollector() {
                @Override
                public void collectLeaf(PostingsEnum postingsEnum, int position, Term term) throws IOException {
                    matchingTerms.computeIfAbsent(id, i -> new StringBuilder())
                            .append("" "")
                            .append(term.field())
                            .append("":"")
                            .append(term.bytes().utf8ToString());
                }

                @Override
                public void reset() {

                }
            };

            SpanExtractor.collect(dataValues.scorer, collector, false);

            super.matchQuery(id, query, dataValues);
        }

    }

}
","['Assertion Roulette', 'Conditional Test Logic', 'General Fixture', 'Unknown Test']","['Assertion Roulette', 'Eager Test', 'Lazy Test', 'Magic Number Test', 'Mystery Guest', 'Redundant Print']",5,3,1,10
31407_4.0_hadoop_testkeyaclconfigurationload,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/31407_test.java,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/31407_actual.java,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * ""License""); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hadoop.crypto.key.kms.server;

import static org.apache.hadoop.crypto.key.kms.server.KMSConfiguration.*;
import static org.apache.hadoop.crypto.key.kms.server.KeyAuthorizationKeyProvider.KEY_ACL;
import static org.apache.hadoop.crypto.key.kms.server.KeyAuthorizationKeyProvider.KeyOpType;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.security.UserGroupInformation;
import org.apache.hadoop.security.authorize.AccessControlList;
import org.junit.Assert;
import org.junit.Rule;
import org.junit.rules.Timeout;
import org.junit.Test;

import java.util.Collection;
import java.util.HashMap;
import java.util.HashSet;

public class TestKMSACLs {
  @Rule
  public final Timeout globalTimeout = new Timeout(180000);

  @Test
  public void testDefaults() {
    final KMSACLs acls = new KMSACLs(new Configuration(false));
    for (KMSACLs.Type type : KMSACLs.Type.values()) {
      Assert.assertTrue(acls.hasAccess(type,
          UserGroupInformation.createRemoteUser(""foo"")));
    }
  }

  @Test
  public void testCustom() {
    final Configuration conf = new Configuration(false);
    for (KMSACLs.Type type : KMSACLs.Type.values()) {
      conf.set(type.getAclConfigKey(), type.toString() + "" "");
    }
    final KMSACLs acls = new KMSACLs(conf);
    for (KMSACLs.Type type : KMSACLs.Type.values()) {
      Assert.assertTrue(acls.hasAccess(type,
          UserGroupInformation.createRemoteUser(type.toString())));
      Assert.assertFalse(acls.hasAccess(type,
          UserGroupInformation.createRemoteUser(""foo"")));
    }
  }

  @Test
  public void testKeyAclConfigurationLoad() {
    final Configuration conf = new Configuration(false);
    conf.set(KEY_ACL + ""test_key_1.MANAGEMENT"", ""CREATE"");
    conf.set(KEY_ACL + ""test_key_2.ALL"", ""CREATE"");
    conf.set(KEY_ACL + ""test_key_3.NONEXISTOPERATION"", ""CREATE"");
    conf.set(DEFAULT_KEY_ACL_PREFIX + ""MANAGEMENT"", ""ROLLOVER"");
    conf.set(WHITELIST_KEY_ACL_PREFIX + ""MANAGEMENT"", ""DECRYPT_EEK"");
    conf.set(DEFAULT_KEY_ACL_PREFIX + ""ALL"", ""invalid"");
    conf.set(WHITELIST_KEY_ACL_PREFIX + ""ALL"", ""invalid"");
    final KMSACLs acls = new KMSACLs(conf);
    Assert.assertTrue(""expected key ACL size is 2 but got ""
        + acls.keyAcls.size(), acls.keyAcls.size() == 2);
    Assert.assertTrue(""expected whitelist ACL size is 1 but got ""
        + acls.whitelistKeyAcls.size(), acls.whitelistKeyAcls.size() == 1);
    Assert.assertFalse(""ALL should not be allowed for whitelist ACLs."",
        acls.whitelistKeyAcls.containsKey(KeyOpType.ALL));
    Assert.assertTrue(""expected default ACL size is 1 but got ""
        + acls.defaultKeyAcls.size(), acls.defaultKeyAcls.size() == 1);
    Assert.assertTrue(""ALL should not be allowed for default ACLs."",
        acls.defaultKeyAcls.size() == 1);
  }

  @Test
  public void testKeyAclDuplicateEntries() {
    final Configuration conf = new Configuration(false);
    conf.set(KEY_ACL + ""test_key_1.DECRYPT_EEK"", ""decrypt1"");
    conf.set(KEY_ACL + ""test_key_2.ALL"", ""all2"");
    conf.set(KEY_ACL + ""test_key_1.DECRYPT_EEK"", ""decrypt2"");
    conf.set(KEY_ACL + ""test_key_2.ALL"", ""all1,all3"");
    conf.set(DEFAULT_KEY_ACL_PREFIX + ""MANAGEMENT"", ""default1"");
    conf.set(DEFAULT_KEY_ACL_PREFIX + ""MANAGEMENT"", """");
    conf.set(DEFAULT_KEY_ACL_PREFIX + ""DECRYPT_EEK"", ""*"");
    conf.set(DEFAULT_KEY_ACL_PREFIX + ""DECRYPT_EEK"", """");
    conf.set(WHITELIST_KEY_ACL_PREFIX + ""DECRYPT_EEK"", ""whitelist1"");
    conf.set(WHITELIST_KEY_ACL_PREFIX + ""DECRYPT_EEK"", ""*"");
    final KMSACLs acls = new KMSACLs(conf);
    Assert.assertTrue(""expected key ACL size is 2 but got ""
        + acls.keyAcls.size(), acls.keyAcls.size() == 2);
    assertKeyAcl(""test_key_1"", acls, KeyOpType.DECRYPT_EEK, ""decrypt2"");
    assertKeyAcl(""test_key_2"", acls, KeyOpType.ALL, ""all1"", ""all3"");
    assertDefaultKeyAcl(acls, KeyOpType.MANAGEMENT);
    assertDefaultKeyAcl(acls, KeyOpType.DECRYPT_EEK);
    AccessControlList acl = acls.whitelistKeyAcls.get(KeyOpType.DECRYPT_EEK);
    Assert.assertNotNull(acl);
    Assert.assertTrue(acl.isAllAllowed());
  }

  @Test
  public void testKeyAclReload() {
    Configuration conf = new Configuration(false);
    conf.set(DEFAULT_KEY_ACL_PREFIX + ""READ"", ""read1"");
    conf.set(DEFAULT_KEY_ACL_PREFIX + ""MANAGEMENT"", """");
    conf.set(DEFAULT_KEY_ACL_PREFIX + ""GENERATE_EEK"", ""*"");
    conf.set(DEFAULT_KEY_ACL_PREFIX + ""DECRYPT_EEK"", ""decrypt1"");
    conf.set(KEY_ACL + ""testuser1.ALL"", ""testkey1"");
    conf.set(WHITELIST_KEY_ACL_PREFIX + ""READ"", ""admin_read1"");
    conf.set(WHITELIST_KEY_ACL_PREFIX + ""MANAGEMENT"", """");
    conf.set(WHITELIST_KEY_ACL_PREFIX + ""GENERATE_EEK"", ""*"");
    conf.set(WHITELIST_KEY_ACL_PREFIX + ""DECRYPT_EEK"", ""admin_decrypt1"");
    final KMSACLs acls = new KMSACLs(conf);

    // update config and hot-reload.
    conf.set(DEFAULT_KEY_ACL_PREFIX + ""READ"", ""read2"");
    conf.set(DEFAULT_KEY_ACL_PREFIX + ""MANAGEMENT"", ""mgmt1,mgmt2"");
    conf.set(DEFAULT_KEY_ACL_PREFIX + ""GENERATE_EEK"", """");
    conf.set(DEFAULT_KEY_ACL_PREFIX + ""DECRYPT_EEK"", ""decrypt2"");
    conf.set(KEY_ACL + ""testkey1.ALL"", ""testkey1,testkey2"");
    conf.set(WHITELIST_KEY_ACL_PREFIX + ""READ"", ""admin_read2"");
    conf.set(WHITELIST_KEY_ACL_PREFIX + ""MANAGEMENT"", ""admin_mgmt,admin_mgmt1"");
    conf.set(WHITELIST_KEY_ACL_PREFIX + ""GENERATE_EEK"", """");
    conf.set(WHITELIST_KEY_ACL_PREFIX + ""DECRYPT_EEK"", ""admin_decrypt2"");
    acls.setKeyACLs(conf);

    assertDefaultKeyAcl(acls, KeyOpType.READ, ""read2"");
    assertDefaultKeyAcl(acls, KeyOpType.MANAGEMENT, ""mgmt1"", ""mgmt2"");
    assertDefaultKeyAcl(acls, KeyOpType.GENERATE_EEK);
    assertDefaultKeyAcl(acls, KeyOpType.DECRYPT_EEK, ""decrypt2"");
    assertKeyAcl(""testuser1"", acls, KeyOpType.ALL, ""testkey1"");
    assertWhitelistKeyAcl(acls, KeyOpType.READ, ""admin_read2"");
    assertWhitelistKeyAcl(acls, KeyOpType.MANAGEMENT,
        ""admin_mgmt"", ""admin_mgmt1"");
    assertWhitelistKeyAcl(acls, KeyOpType.GENERATE_EEK);
    assertWhitelistKeyAcl(acls, KeyOpType.DECRYPT_EEK, ""admin_decrypt2"");

    // reloading same config, nothing should change.
    acls.setKeyACLs(conf);
    assertDefaultKeyAcl(acls, KeyOpType.READ, ""read2"");
    assertDefaultKeyAcl(acls, KeyOpType.MANAGEMENT, ""mgmt1"", ""mgmt2"");
    assertDefaultKeyAcl(acls, KeyOpType.GENERATE_EEK);
    assertDefaultKeyAcl(acls, KeyOpType.DECRYPT_EEK, ""decrypt2"");
    assertKeyAcl(""testuser1"", acls, KeyOpType.ALL, ""testkey1"");
    assertWhitelistKeyAcl(acls, KeyOpType.READ, ""admin_read2"");
    assertWhitelistKeyAcl(acls, KeyOpType.MANAGEMENT,
        ""admin_mgmt"", ""admin_mgmt1"");
    assertWhitelistKeyAcl(acls, KeyOpType.GENERATE_EEK);
    assertWhitelistKeyAcl(acls, KeyOpType.DECRYPT_EEK, ""admin_decrypt2"");

    // test wildcard.
    conf.set(DEFAULT_KEY_ACL_PREFIX + ""DECRYPT_EEK"", ""*"");
    acls.setKeyACLs(conf);
    AccessControlList acl = acls.defaultKeyAcls.get(KeyOpType.DECRYPT_EEK);
    Assert.assertTrue(acl.isAllAllowed());
    Assert.assertTrue(acl.getUsers().isEmpty());
    // everything else should still be the same.
    assertDefaultKeyAcl(acls, KeyOpType.READ, ""read2"");
    assertDefaultKeyAcl(acls, KeyOpType.MANAGEMENT, ""mgmt1"", ""mgmt2"");
    assertDefaultKeyAcl(acls, KeyOpType.GENERATE_EEK);
    assertKeyAcl(""testuser1"", acls, KeyOpType.ALL, ""testkey1"");
    assertWhitelistKeyAcl(acls, KeyOpType.READ, ""admin_read2"");
    assertWhitelistKeyAcl(acls, KeyOpType.MANAGEMENT,
        ""admin_mgmt"", ""admin_mgmt1"");
    assertWhitelistKeyAcl(acls, KeyOpType.GENERATE_EEK);
    assertWhitelistKeyAcl(acls, KeyOpType.DECRYPT_EEK, ""admin_decrypt2"");

    // test new configuration should clear other items
    conf = new Configuration();
    conf.set(DEFAULT_KEY_ACL_PREFIX + ""DECRYPT_EEK"", ""new"");
    acls.setKeyACLs(conf);
    assertDefaultKeyAcl(acls, KeyOpType.DECRYPT_EEK, ""new"");
    Assert.assertTrue(acls.keyAcls.isEmpty());
    Assert.assertTrue(acls.whitelistKeyAcls.isEmpty());
    Assert.assertEquals(""Got unexpected sized acls:""
        + acls.defaultKeyAcls, 1, acls.defaultKeyAcls.size());
  }

  private void assertDefaultKeyAcl(final KMSACLs acls, final KeyOpType op,
      final String... names) {
    final AccessControlList acl = acls.defaultKeyAcls.get(op);
    assertAcl(acl, op, names);
  }

  private void assertWhitelistKeyAcl(final KMSACLs acls, final KeyOpType op,
      final String... names) {
    final AccessControlList acl = acls.whitelistKeyAcls.get(op);
    assertAcl(acl, op, names);
  }

  private void assertKeyAcl(final String keyName, final KMSACLs acls,
      final KeyOpType op, final String... names) {
    Assert.assertTrue(acls.keyAcls.containsKey(keyName));
    final HashMap<KeyOpType, AccessControlList> keyacl =
        acls.keyAcls.get(keyName);
    Assert.assertNotNull(keyacl.get(op));
    assertAcl(keyacl.get(op), op, names);
  }

  private void assertAcl(final AccessControlList acl,
      final KeyOpType op, final String... names) {
    Assert.assertNotNull(acl);
    Assert.assertFalse(acl.isAllAllowed());
    final Collection<String> actual = acl.getUsers();
    final HashSet<String> expected = new HashSet<>();
    for (String name : names) {
      expected.add(name);
    }
    Assert.assertEquals(""defaultKeyAcls don't match for op:"" + op,
        expected, actual);
  }
}
","/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * ""License""); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hadoop.crypto.key.kms.server;

import org.apache.hadoop.classification.InterfaceAudience;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.crypto.key.kms.server.KMS.KMSOp;
import org.apache.hadoop.crypto.key.kms.server.KeyAuthorizationKeyProvider.KeyACLs;
import org.apache.hadoop.crypto.key.kms.server.KeyAuthorizationKeyProvider.KeyOpType;
import org.apache.hadoop.security.AccessControlException;
import org.apache.hadoop.security.UserGroupInformation;
import org.apache.hadoop.security.authorize.AccessControlList;
import org.apache.hadoop.security.authorize.AuthorizationException;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.util.EnumSet;
import java.util.HashMap;
import java.util.Map;
import java.util.concurrent.Executors;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.TimeUnit;

import org.apache.hadoop.classification.VisibleForTesting;

/**
 * Provides access to the <code>AccessControlList</code>s used by KMS,
 * hot-reloading them if the <code>kms-acls.xml</code> file where the ACLs
 * are defined has been updated.
 */
@InterfaceAudience.Private
public class KMSACLs implements Runnable, KeyACLs {
  private static final Logger LOG = LoggerFactory.getLogger(KMSACLs.class);

  private static final String UNAUTHORIZED_MSG_WITH_KEY =
      ""User:%s not allowed to do '%s' on '%s'"";

  private static final String UNAUTHORIZED_MSG_WITHOUT_KEY =
      ""User:%s not allowed to do '%s'"";

  public enum Type {
    CREATE, DELETE, ROLLOVER, GET, GET_KEYS, GET_METADATA,
    SET_KEY_MATERIAL, GENERATE_EEK, DECRYPT_EEK;

    public String getAclConfigKey() {
      return KMSConfiguration.CONFIG_PREFIX + ""acl."" + this.toString();
    }

    public String getBlacklistConfigKey() {
      return KMSConfiguration.CONFIG_PREFIX + ""blacklist."" + this.toString();
    }
  }

  public static final String ACL_DEFAULT = AccessControlList.WILDCARD_ACL_VALUE;

  public static final int RELOADER_SLEEP_MILLIS = 1000;

  // Allow both ROLLOVER and DELETE to invalidate cache.
  public static final EnumSet<KMSACLs.Type> INVALIDATE_CACHE_TYPES =
      EnumSet.of(KMSACLs.Type.ROLLOVER, KMSACLs.Type.DELETE);

  private volatile Map<Type, AccessControlList> acls;
  private volatile Map<Type, AccessControlList> blacklistedAcls;
  @VisibleForTesting
  volatile Map<String, HashMap<KeyOpType, AccessControlList>> keyAcls;
  @VisibleForTesting
  volatile Map<KeyOpType, AccessControlList> defaultKeyAcls = new HashMap<>();
  @VisibleForTesting
  volatile Map<KeyOpType, AccessControlList> whitelistKeyAcls = new HashMap<>();
  private ScheduledExecutorService executorService;
  private long lastReload;

  KMSACLs(Configuration conf) {
    if (conf == null) {
      conf = loadACLs();
    }
    setKMSACLs(conf);
    setKeyACLs(conf);
  }

  public KMSACLs() {
    this(null);
  }

  private void setKMSACLs(Configuration conf) {
    Map<Type, AccessControlList> tempAcls = new HashMap<Type, AccessControlList>();
    Map<Type, AccessControlList> tempBlacklist = new HashMap<Type, AccessControlList>();
    for (Type aclType : Type.values()) {
      String aclStr = conf.get(aclType.getAclConfigKey(), ACL_DEFAULT);
      tempAcls.put(aclType, new AccessControlList(aclStr));
      String blacklistStr = conf.get(aclType.getBlacklistConfigKey());
      if (blacklistStr != null) {
        // Only add if blacklist is present
        tempBlacklist.put(aclType, new AccessControlList(blacklistStr));
        LOG.info(""'{}' Blacklist '{}'"", aclType, blacklistStr);
      }
      LOG.info(""'{}' ACL '{}'"", aclType, aclStr);
    }
    acls = tempAcls;
    blacklistedAcls = tempBlacklist;
  }

  @VisibleForTesting
  void setKeyACLs(Configuration conf) {
    Map<String, HashMap<KeyOpType, AccessControlList>> tempKeyAcls =
        new HashMap<String, HashMap<KeyOpType,AccessControlList>>();
    Map<String, String> allKeyACLS =
        conf.getValByRegex(KMSConfiguration.KEY_ACL_PREFIX_REGEX);
    for (Map.Entry<String, String> keyAcl : allKeyACLS.entrySet()) {
      String k = keyAcl.getKey();
      // this should be of type ""key.acl.<KEY_NAME>.<OP_TYPE>""
      int keyNameStarts = KMSConfiguration.KEY_ACL_PREFIX.length();
      int keyNameEnds = k.lastIndexOf(""."");
      if (keyNameStarts >= keyNameEnds) {
        LOG.warn(""Invalid key name '{}'"", k);
      } else {
        String aclStr = keyAcl.getValue();
        String keyName = k.substring(keyNameStarts, keyNameEnds);
        String keyOp = k.substring(keyNameEnds + 1);
        KeyOpType aclType = null;
        try {
          aclType = KeyOpType.valueOf(keyOp);
        } catch (IllegalArgumentException e) {
          LOG.warn(""Invalid key Operation '{}'"", keyOp);
        }
        if (aclType != null) {
          // On the assumption this will be single threaded.. else we need to
          // ConcurrentHashMap
          HashMap<KeyOpType,AccessControlList> aclMap =
              tempKeyAcls.get(keyName);
          if (aclMap == null) {
            aclMap = new HashMap<KeyOpType, AccessControlList>();
            tempKeyAcls.put(keyName, aclMap);
          }
          aclMap.put(aclType, new AccessControlList(aclStr));
          LOG.info(""KEY_NAME '{}' KEY_OP '{}' ACL '{}'"",
              keyName, aclType, aclStr);
        }
      }
    }
    keyAcls = tempKeyAcls;

    final Map<KeyOpType, AccessControlList> tempDefaults = new HashMap<>();
    final Map<KeyOpType, AccessControlList> tempWhitelists = new HashMap<>();
    for (KeyOpType keyOp : KeyOpType.values()) {
      parseAclsWithPrefix(conf, KMSConfiguration.DEFAULT_KEY_ACL_PREFIX,
          keyOp, tempDefaults);
      parseAclsWithPrefix(conf, KMSConfiguration.WHITELIST_KEY_ACL_PREFIX,
          keyOp, tempWhitelists);
    }
    defaultKeyAcls = tempDefaults;
    whitelistKeyAcls = tempWhitelists;
  }

  /**
   * Parse the acls from configuration with the specified prefix. Currently
   * only 2 possible prefixes: whitelist and default.
   *
   * @param conf The configuration.
   * @param prefix The prefix.
   * @param keyOp The key operation.
   * @param results The collection of results to add to.
   */
  private void parseAclsWithPrefix(final Configuration conf,
      final String prefix, final KeyOpType keyOp,
      Map<KeyOpType, AccessControlList> results) {
    String confKey = prefix + keyOp;
    String aclStr = conf.get(confKey);
    if (aclStr != null) {
      if (keyOp == KeyOpType.ALL) {
        // Ignore All operation for default key and whitelist key acls
        LOG.warn(""Invalid KEY_OP '{}' for {}, ignoring"", keyOp, prefix);
      } else {
        if (aclStr.equals(""*"")) {
          LOG.info(""{} for KEY_OP '{}' is set to '*'"", prefix, keyOp);
        }
        results.put(keyOp, new AccessControlList(aclStr));
      }
    }
  }

  @Override
  public void run() {
    try {
      if (KMSConfiguration.isACLsFileNewer(lastReload)) {
        setKMSACLs(loadACLs());
        setKeyACLs(loadACLs());
      }
    } catch (Exception ex) {
      LOG.warn(
          String.format(""Could not reload ACLs file: '%s'"", ex.toString()), ex);
    }
  }

  public synchronized void startReloader() {
    if (executorService == null) {
      executorService = Executors.newScheduledThreadPool(1);
      executorService.scheduleAtFixedRate(this, RELOADER_SLEEP_MILLIS,
          RELOADER_SLEEP_MILLIS, TimeUnit.MILLISECONDS);
    }
  }

  public synchronized void stopReloader() {
    if (executorService != null) {
      executorService.shutdownNow();
      executorService = null;
    }
  }

  private Configuration loadACLs() {
    LOG.debug(""Loading ACLs file"");
    lastReload = System.currentTimeMillis();
    Configuration conf = KMSConfiguration.getACLsConf();
    // triggering the resource loading.
    conf.get(Type.CREATE.getAclConfigKey());
    return conf;
  }

  /**
   * First Check if user is in ACL for the KMS operation, if yes, then
   * return true if user is not present in any configured blacklist for
   * the operation
   * @param type KMS Operation
   * @param ugi UserGroupInformation of user
   * @return true is user has access
   */
  public boolean hasAccess(Type type, UserGroupInformation ugi) {
    boolean access = acls.get(type).isUserAllowed(ugi);
    if (LOG.isDebugEnabled()) {
      LOG.debug(""Checking user [{}] for: {} {} "", ugi.getShortUserName(),
          type.toString(), acls.get(type).getAclString());
    }
    if (access) {
      AccessControlList blacklist = blacklistedAcls.get(type);
      access = (blacklist == null) || !blacklist.isUserInList(ugi);
      if (LOG.isDebugEnabled()) {
        if (blacklist == null) {
          LOG.debug(""No blacklist for {}"", type.toString());
        } else if (access) {
          LOG.debug(""user is not in {}"" , blacklist.getAclString());
        } else {
          LOG.debug(""user is in {}"" , blacklist.getAclString());
        }
      }
    }
    if (LOG.isDebugEnabled()) {
      LOG.debug(""User: [{}], Type: {} Result: {}"", ugi.getShortUserName(),
          type.toString(), access);
    }
    return access;
  }

  public void assertAccess(KMSACLs.Type aclType,
      UserGroupInformation ugi, KMSOp operation, String key)
      throws AccessControlException {
    if (!KMSWebApp.getACLs().hasAccess(aclType, ugi)) {
      KMSWebApp.getUnauthorizedCallsMeter().mark();
      KMSWebApp.getKMSAudit().unauthorized(ugi, operation, key);
      throw new AuthorizationException(String.format(
          (key != null) ? UNAUTHORIZED_MSG_WITH_KEY
                        : UNAUTHORIZED_MSG_WITHOUT_KEY,
          ugi.getShortUserName(), operation, key));
    }
  }

  public void assertAccess(EnumSet<Type> aclTypes,
      UserGroupInformation ugi, KMSOp operation, String key)
      throws AccessControlException {
    boolean accessAllowed = false;
    for (KMSACLs.Type type : aclTypes) {
      if (KMSWebApp.getACLs().hasAccess(type, ugi)){
        accessAllowed = true;
        break;
      }
    }

    if (!accessAllowed) {
      KMSWebApp.getUnauthorizedCallsMeter().mark();
      KMSWebApp.getKMSAudit().unauthorized(ugi, operation, key);
      throw new AuthorizationException(String.format(
          (key != null) ? UNAUTHORIZED_MSG_WITH_KEY
              : UNAUTHORIZED_MSG_WITHOUT_KEY,
          ugi.getShortUserName(), operation, key));
    }
  }

  @Override
  public boolean hasAccessToKey(String keyName, UserGroupInformation ugi,
      KeyOpType opType) {
    boolean access = checkKeyAccess(keyName, ugi, opType)
        || checkKeyAccess(whitelistKeyAcls, ugi, opType);
    if (!access) {
      KMSWebApp.getKMSAudit().unauthorized(ugi, opType, keyName);
    }
    return access;
  }

  private boolean checkKeyAccess(String keyName, UserGroupInformation ugi,
      KeyOpType opType) {
    Map<KeyOpType, AccessControlList> keyAcl = keyAcls.get(keyName);
    if (keyAcl == null) {
      // If No key acl defined for this key, check to see if
      // there are key defaults configured for this operation
      LOG.debug(""Key: {} has no ACLs defined, using defaults."", keyName);
      keyAcl = defaultKeyAcls;
    }
    boolean access = checkKeyAccess(keyAcl, ugi, opType);
    if (LOG.isDebugEnabled()) {
      LOG.debug(""User: [{}], OpType: {}, KeyName: {} Result: {}"",
          ugi.getShortUserName(), opType.toString(), keyName, access);
    }
    return access;
  }

  private boolean checkKeyAccess(Map<KeyOpType, AccessControlList> keyAcl,
      UserGroupInformation ugi, KeyOpType opType) {
    AccessControlList acl = keyAcl.get(opType);
    if (acl == null) {
      // If no acl is specified for this operation,
      // deny access
      LOG.debug(""No ACL available for key, denying access for {}"", opType);
      return false;
    } else {
      if (LOG.isDebugEnabled()) {
        LOG.debug(""Checking user [{}] for: {}: {}"", ugi.getShortUserName(),
            opType.toString(), acl.getAclString());
      }
      return acl.isUserAllowed(ugi);
    }
  }


  @Override
  public boolean isACLPresent(String keyName, KeyOpType opType) {
    return (keyAcls.containsKey(keyName)
        || defaultKeyAcls.containsKey(opType)
        || whitelistKeyAcls.containsKey(opType));
  }

  @VisibleForTesting
  void forceNextReloadForTesting() {
    lastReload = 0;
  }
}
","['Assertion Roulette', 'Conditional Test Logic', 'Sensitive Equality', 'Lazy Assert']",['Assertion Roulette'],0,3,1,13
15658_7.0_dnsjava_test_setactive_boundary1,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/15658_test.java,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/15658_actual.java,"// SPDX-License-Identifier: BSD-3-Clause
// -*- Java -*-
//
// Copyright (c) 2005, Matthew J. Rutherford <rutherfo@cs.colorado.edu>
// Copyright (c) 2005, University of Colorado at Boulder
// All rights reserved.
//
// Redistribution and use in source and binary forms, with or without
// modification, are permitted provided that the following conditions are
// met:
//
// * Redistributions of source code must retain the above copyright
//   notice, this list of conditions and the following disclaimer.
//
// * Redistributions in binary form must reproduce the above copyright
//   notice, this list of conditions and the following disclaimer in the
//   documentation and/or other materials provided with the distribution.
//
// * Neither the name of the University of Colorado at Boulder nor the
//   names of its contributors may be used to endorse or promote
//   products derived from this software without specific prior written
//   permission.
//
// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
// ""AS IS"" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
// LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
// A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
// OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
// LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
// DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
// THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
//
package org.xbill.DNS;

import static org.junit.jupiter.api.Assertions.assertArrayEquals;
import static org.junit.jupiter.api.Assertions.assertEquals;
import static org.junit.jupiter.api.Assertions.assertThrows;

import java.io.ByteArrayOutputStream;
import java.io.IOException;
import java.nio.ByteBuffer;
import org.junit.jupiter.api.BeforeEach;
import org.junit.jupiter.api.Test;
import org.junit.jupiter.params.ParameterizedTest;
import org.junit.jupiter.params.provider.ValueSource;

abstract class DNSInputTest {
  protected byte[] m_raw;
  protected DNSInput m_di;

  static class DNSInputArrayTest extends DNSInputTest {
    @BeforeEach
    void setUp() {
      m_raw = new byte[] {0, 1, 2, 3, 4, 5, (byte) 255, (byte) 255, (byte) 255, (byte) 255};
      m_di = new DNSInput(m_raw);
    }
  }

  static class DNSInputByteBufferTest extends DNSInputTest {
    @BeforeEach
    void setUp() {
      m_raw = new byte[] {0, 1, 2, 3, 4, 5, (byte) 255, (byte) 255, (byte) 255, (byte) 255};
      ByteBuffer buffer = ByteBuffer.allocate(m_raw.length + 2);
      buffer.putShort((short) 0xFF);
      buffer.put(m_raw);
      buffer.flip();
      buffer.getShort();
      m_di = new DNSInput(buffer);
    }
  }

  static class DNSInputByteBufferLimitTest extends DNSInputTest {
    @BeforeEach
    void setUp() {
      m_raw = new byte[] {0, 1, 2, 3, 4, 5, (byte) 255, (byte) 255, (byte) 255, (byte) 255};
      ByteBuffer buffer = ByteBuffer.allocate(m_raw.length + 10);
      buffer.putShort((short) 0xFF);
      buffer.put(m_raw);
      buffer.flip();
      buffer.getShort();
      buffer.limit(m_raw.length + 2);
      m_di = new DNSInput(buffer);
    }
  }

  static class DNSInputByteBufferLimitOffsetTest extends DNSInputTest {
    @BeforeEach
    void setUp() throws IOException {
      m_raw = new byte[] {0, 1, 2, 3, 4, 5, (byte) 255, (byte) 255, (byte) 255, (byte) 255};
      // create a new byte array with a prefix and a suffix to be ignored
      ByteArrayOutputStream out = new ByteArrayOutputStream();
      out.write(42);
      out.write(m_raw);
      out.write(47);
      m_di = new DNSInput(ByteBuffer.wrap(out.toByteArray(), 1, 10));
    }
  }

  @Test
  void initial_state() {
    assertEquals(0, m_di.current());
    assertEquals(10, m_di.remaining());
  }

  @Test
  void jump1() {
    m_di.jump(1);
    assertEquals(1, m_di.current());
    assertEquals(9, m_di.remaining());
  }

  @Test
  void jump2() {
    m_di.jump(9);
    assertEquals(9, m_di.current());
    assertEquals(1, m_di.remaining());
  }

  @Test
  void jump_invalid() {
    assertThrows(IllegalArgumentException.class, () -> m_di.jump(10));
  }

  @ParameterizedTest
  @ValueSource(ints = {5, 10, 0})
  void setActive(int active) {
    m_di.setActive(active);
    assertEquals(0, m_di.current());
    assertEquals(active, m_di.remaining());
  }

  @Test
  void setActive_invalid() {
    assertThrows(IllegalArgumentException.class, () -> m_di.setActive(11));
  }

  @Test
  void clearActive() {
    // first without setting active:
    m_di.clearActive();
    assertEquals(0, m_di.current());
    assertEquals(10, m_di.remaining());

    m_di.setActive(5);
    m_di.clearActive();
    assertEquals(0, m_di.current());
    assertEquals(10, m_di.remaining());
  }

  @Test
  void restore_invalid() {
    assertThrows(IllegalStateException.class, () -> m_di.restore());
  }

  @Test
  void save_restore() {
    m_di.jump(4);
    assertEquals(4, m_di.current());
    assertEquals(6, m_di.remaining());

    m_di.save();
    m_di.jump(0);
    assertEquals(0, m_di.current());
    assertEquals(10, m_di.remaining());

    m_di.restore();
    assertEquals(4, m_di.current());
    assertEquals(6, m_di.remaining());
  }

  @Test
  void save_set_restore() {
    m_di.jump(4);
    assertEquals(4, m_di.current());
    assertEquals(6, m_di.remaining());

    int save = m_di.saveActive();
    assertEquals(10, save);
    assertEquals(6, m_di.remaining());

    m_di.setActive(4);
    assertEquals(4, m_di.remaining());

    m_di.restoreActive(save);
    assertEquals(6, m_di.remaining());
  }

  @Test
  void save_set_restore_boundary() {
    m_di.setActive(4);
    assertEquals(4, m_di.remaining());

    m_di.restoreActive(10);
    assertEquals(10, m_di.remaining());

    assertThrows(IllegalArgumentException.class, () -> m_di.restoreActive(12));
  }

  @Test
  void readU8_basic() throws WireParseException {
    int v1 = m_di.readU8();
    assertEquals(1, m_di.current());
    assertEquals(9, m_di.remaining());
    assertEquals(0, v1);
  }

  @Test
  void readU8_maxval() throws WireParseException {
    m_di.jump(9);
    final int[] v1 = {m_di.readU8()};
    assertEquals(10, m_di.current());
    assertEquals(0, m_di.remaining());
    assertEquals(255, v1[0]);

    assertThrows(WireParseException.class, () -> v1[0] = m_di.readU8());
  }

  @Test
  void readU16_basic() throws WireParseException {
    int v1 = m_di.readU16();
    assertEquals(2, m_di.current());
    assertEquals(8, m_di.remaining());
    assertEquals(1, v1);

    m_di.jump(1);
    v1 = m_di.readU16();
    assertEquals(258, v1);
  }

  @Test
  void readU16_maxval() throws WireParseException {
    m_di.jump(8);
    int v = m_di.readU16();
    assertEquals(10, m_di.current());
    assertEquals(0, m_di.remaining());
    assertEquals(0xFFFF, v);

    assertThrows(
        WireParseException.class,
        () -> {
          m_di.jump(9);
          m_di.readU16();
        });
  }

  @Test
  void readU32_basic() throws WireParseException {
    long v1 = m_di.readU32();
    assertEquals(4, m_di.current());
    assertEquals(6, m_di.remaining());
    assertEquals(66051, v1);
  }

  @Test
  void readU32_maxval() throws WireParseException {
    m_di.jump(6);
    long v = m_di.readU32();
    assertEquals(10, m_di.current());
    assertEquals(0, m_di.remaining());
    assertEquals(0xFFFFFFFFL, v);

    assertThrows(
        WireParseException.class,
        () -> {
          m_di.jump(7);
          m_di.readU32();
        });
  }

  @Test
  void readByteArray_0arg() {
    m_di.jump(1);
    byte[] out = m_di.readByteArray();
    assertEquals(10, m_di.current());
    assertEquals(0, m_di.remaining());
    assertEquals(9, out.length);
    for (int i = 0; i < 9; ++i) {
      assertEquals(m_raw[i + 1], out[i]);
    }
  }

  @Test
  void readByteArray_0arg_boundary() throws WireParseException {
    m_di.jump(9);
    m_di.readU8();
    byte[] out = m_di.readByteArray();
    assertEquals(0, out.length);
  }

  @Test
  void readByteArray_1arg() throws WireParseException {
    byte[] out = m_di.readByteArray(2);
    assertEquals(2, m_di.current());
    assertEquals(8, m_di.remaining());
    assertEquals(2, out.length);
    assertEquals(0, out[0]);
    assertEquals(1, out[1]);
  }

  @Test
  void readByteArray_1arg_boundary() throws WireParseException {
    byte[] out = m_di.readByteArray(10);
    assertEquals(10, m_di.current());
    assertEquals(0, m_di.remaining());
    assertArrayEquals(m_raw, out);
  }

  @Test
  void readByteArray_1arg_invalid() {
    assertThrows(WireParseException.class, () -> m_di.readByteArray(11));
  }

  @Test
  void readByteArray_3arg() throws WireParseException {
    byte[] data = new byte[5];
    m_di.jump(4);

    m_di.readByteArray(data, 1, 4);
    assertEquals(8, m_di.current());
    assertEquals(0, data[0]);
    for (int i = 0; i < 4; ++i) {
      assertEquals(m_raw[i + 4], data[i + 1]);
    }
  }

  @Test
  void readCountedSting() throws WireParseException {
    m_di.jump(1);
    byte[] out = m_di.readCountedString();
    assertEquals(1, out.length);
    assertEquals(3, m_di.current());
    assertEquals(2, out[0]);
  }

  @Test
  void setActive_recursive() throws WireParseException {
    int outer = m_di.saveActive();
    m_di.setActive(3);

    assertEquals(0x00, m_di.readU8());
    assertEquals(2, m_di.remaining());

    int inner = m_di.saveActive();

    m_di.setActive(1);
    assertArrayEquals(new byte[] {0x01}, m_di.readByteArray());

    m_di.restoreActive(inner);

    assertArrayEquals(new byte[] {0x02}, m_di.readByteArray());
    assertEquals(0, m_di.remaining());

    m_di.restoreActive(outer);

    assertEquals(0x03, m_di.readU8());
    assertEquals(6, m_di.remaining());
  }
}
","// SPDX-License-Identifier: BSD-3-Clause
// Copyright (c) 2004 Brian Wellington (bwelling@xbill.org)

package org.xbill.DNS;

import java.nio.ByteBuffer;

/**
 * A class for parsing DNS messages.
 *
 * @author Brian Wellington
 */
public class DNSInput {
  private final ByteBuffer byteBuffer;
  private final int offset;
  private final int limit;
  private int savedPos;
  private int savedEnd;

  /**
   * Creates a new DNSInput
   *
   * @param input The byte array to read from
   */
  public DNSInput(byte[] input) {
    this(ByteBuffer.wrap(input));
  }

  /**
   * Creates a new DNSInput from the given {@link ByteBuffer}
   *
   * @param byteBuffer The ByteBuffer
   */
  public DNSInput(ByteBuffer byteBuffer) {
    this.byteBuffer = byteBuffer;
    offset = byteBuffer.position();
    limit = byteBuffer.limit();
    savedPos = -1;
    savedEnd = -1;
  }

  /** Returns the current position. */
  public int current() {
    return byteBuffer.position() - offset;
  }

  /** Returns the number of bytes that can be read from this stream before reaching the end. */
  public int remaining() {
    return byteBuffer.remaining();
  }

  private void require(int n) throws WireParseException {
    if (n > remaining()) {
      throw new WireParseException(""end of input"");
    }
  }

  /**
   * Marks the following bytes in the stream as active.
   *
   * @param len The number of bytes in the active region.
   * @throws IllegalArgumentException The number of bytes in the active region is longer than the
   *     remainder of the input.
   */
  public void setActive(int len) {
    if (len > limit - byteBuffer.position()) {
      throw new IllegalArgumentException(""cannot set active region past end of input"");
    }
    byteBuffer.limit(byteBuffer.position() + len);
  }

  /**
   * Clears the active region of the string. Further operations are not restricted to part of the
   * input.
   */
  public void clearActive() {
    byteBuffer.limit(limit);
  }

  /** Returns the position of the end of the current active region. */
  public int saveActive() {
    return byteBuffer.limit() - offset;
  }

  /**
   * Restores the previously set active region. This differs from setActive() in that
   * restoreActive() takes an absolute position, and setActive takes an offset from the current
   * location.
   *
   * @param pos The end of the active region.
   */
  public void restoreActive(int pos) {
    if (pos + offset > limit) {
      throw new IllegalArgumentException(""cannot set active region past end of input"");
    }
    byteBuffer.limit(pos + offset);
  }

  /**
   * Resets the current position of the input stream to the specified index, and clears the active
   * region.
   *
   * @param index The position to continue parsing at.
   * @throws IllegalArgumentException The index is not within the input.
   */
  public void jump(int index) {
    if (index + offset >= limit) {
      throw new IllegalArgumentException(""cannot jump past end of input"");
    }
    byteBuffer.position(offset + index);
    byteBuffer.limit(limit);
  }

  /**
   * Saves the current state of the input stream. Both the current position and the end of the
   * active region are saved.
   *
   * @throws IllegalArgumentException The index is not within the input.
   */
  public void save() {
    savedPos = byteBuffer.position();
    savedEnd = byteBuffer.limit();
  }

  /** Restores the input stream to its state before the call to {@link #save}. */
  public void restore() {
    if (savedPos < 0) {
      throw new IllegalStateException(""no previous state"");
    }
    byteBuffer.position(savedPos);
    byteBuffer.limit(savedEnd);
    savedPos = -1;
    savedEnd = -1;
  }

  /**
   * Reads an unsigned 8 bit value from the stream, as an int.
   *
   * @return An unsigned 8 bit value.
   * @throws WireParseException The end of the stream was reached.
   */
  public int readU8() throws WireParseException {
    require(1);
    return byteBuffer.get() & 0xFF;
  }

  /**
   * Reads an unsigned 16 bit value from the stream, as an int.
   *
   * @return An unsigned 16 bit value.
   * @throws WireParseException The end of the stream was reached.
   */
  public int readU16() throws WireParseException {
    require(2);
    return byteBuffer.getShort() & 0xFFFF;
  }

  /**
   * Reads an unsigned 32 bit value from the stream, as a long.
   *
   * @return An unsigned 32 bit value.
   * @throws WireParseException The end of the stream was reached.
   */
  public long readU32() throws WireParseException {
    require(4);
    return byteBuffer.getInt() & 0xFFFFFFFFL;
  }

  /**
   * Reads a byte array of a specified length from the stream into an existing array.
   *
   * @param b The array to read into.
   * @param off The offset of the array to start copying data into.
   * @param len The number of bytes to copy.
   * @throws WireParseException The end of the stream was reached.
   */
  public void readByteArray(byte[] b, int off, int len) throws WireParseException {
    require(len);
    byteBuffer.get(b, off, len);
  }

  /**
   * Reads a byte array of a specified length from the stream.
   *
   * @return The byte array.
   * @throws WireParseException The end of the stream was reached.
   */
  public byte[] readByteArray(int len) throws WireParseException {
    require(len);
    byte[] out = new byte[len];
    byteBuffer.get(out, 0, len);
    return out;
  }

  /**
   * Reads a byte array consisting of the remainder of the stream (or the active region, if one is
   * set.
   *
   * @return The byte array.
   */
  public byte[] readByteArray() {
    int len = remaining();
    byte[] out = new byte[len];
    byteBuffer.get(out, 0, len);
    return out;
  }

  /**
   * Reads a counted string from the stream. A counted string is a one byte value indicating string
   * length, followed by bytes of data.
   *
   * @return A byte array containing the string.
   * @throws WireParseException The end of the stream was reached.
   */
  public byte[] readCountedString() throws WireParseException {
    int len = readU8();
    return readByteArray(len);
  }
}
","['Assertion Roulette', 'Conditional Test Logic']","['Assertion Roulette', 'Magic Number Test', 'Lazy Test', 'Eager Test', 'Exception Handling', 'Sensitive Equality', 'Sleepy Test', 'Redundant Print', 'Resource Optimism']",8,1,1,12
24691_4.0_hadoop_testsuccess,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/24691_test.java,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/24691_actual.java,"/**
* Licensed to the Apache Software Foundation (ASF) under one
* or more contributor license agreements.  See the NOTICE file
* distributed with this work for additional information
* regarding copyright ownership.  The ASF licenses this file
* to you under the Apache License, Version 2.0 (the
* ""License""); you may not use this file except in compliance
* with the License.  You may obtain a copy of the License at
*
*     http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an ""AS IS"" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
*/

package org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache;

import static org.junit.Assert.assertEquals;
import static org.junit.Assert.assertFalse;
import static org.junit.Assert.assertTrue;
import static org.mockito.ArgumentMatchers.anyBoolean;
import static org.mockito.ArgumentMatchers.isA;
import static org.mockito.Mockito.doReturn;
import static org.mockito.Mockito.mock;
import static org.mockito.Mockito.spy;
import static org.mockito.Mockito.verify;
import static org.mockito.Mockito.when;

import java.io.IOException;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileStatus;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.yarn.api.records.LocalResource;
import org.apache.hadoop.yarn.api.records.LocalResourceVisibility;
import org.apache.hadoop.yarn.conf.YarnConfiguration;
import org.apache.hadoop.yarn.server.api.SCMUploaderProtocol;
import org.apache.hadoop.yarn.server.api.protocolrecords.SCMUploaderNotifyRequest;
import org.apache.hadoop.yarn.server.api.protocolrecords.SCMUploaderNotifyResponse;
import org.junit.Test;

public class TestSharedCacheUploader {

  /**
   * If verifyAccess fails, the upload should fail
   */
  @Test
  public void testFailVerifyAccess() throws Exception {
    SharedCacheUploader spied = createSpiedUploader();
    doReturn(false).when(spied).verifyAccess();

    assertFalse(spied.call());
  }

  /**
   * If rename fails, the upload should fail
   */
  @Test
  public void testRenameFail() throws Exception {
    Configuration conf = new Configuration();
    conf.setBoolean(YarnConfiguration.SHARED_CACHE_ENABLED, true);
    LocalResource resource = mock(LocalResource.class);
    Path localPath = mock(Path.class);
    when(localPath.getName()).thenReturn(""foo.jar"");
    String user = ""joe"";
    SCMUploaderProtocol scmClient = mock(SCMUploaderProtocol.class);
    SCMUploaderNotifyResponse response = mock(SCMUploaderNotifyResponse.class);
    when(response.getAccepted()).thenReturn(true);
    when(scmClient.notify(isA(SCMUploaderNotifyRequest.class))).
        thenReturn(response);
    FileSystem fs = mock(FileSystem.class);
    // return false when rename is called
    when(fs.rename(isA(Path.class), isA(Path.class))).thenReturn(false);
    FileSystem localFs = FileSystem.getLocal(conf);
    SharedCacheUploader spied =
        createSpiedUploader(resource, localPath, user, conf, scmClient, fs,
            localFs);
    // stub verifyAccess() to return true
    doReturn(true).when(spied).verifyAccess();
    // stub getActualPath()
    doReturn(localPath).when(spied).getActualPath();
    // stub computeChecksum()
    doReturn(""abcdef0123456789"").when(spied).computeChecksum(isA(Path.class));
    // stub uploadFile() to return true
    doReturn(true).when(spied).uploadFile(isA(Path.class), isA(Path.class));

    assertFalse(spied.call());
  }

  /**
   * If verifyAccess, uploadFile, rename, and notification succeed, the upload
   * should succeed
   */
  @Test
  public void testSuccess() throws Exception {
    Configuration conf = new Configuration();
    conf.setBoolean(YarnConfiguration.SHARED_CACHE_ENABLED, true);
    LocalResource resource = mock(LocalResource.class);
    Path localPath = mock(Path.class);
    when(localPath.getName()).thenReturn(""foo.jar"");
    String user = ""joe"";
    SCMUploaderProtocol scmClient = mock(SCMUploaderProtocol.class);
    SCMUploaderNotifyResponse response = mock(SCMUploaderNotifyResponse.class);
    when(response.getAccepted()).thenReturn(true);
    when(scmClient.notify(isA(SCMUploaderNotifyRequest.class))).
        thenReturn(response);
    FileSystem fs = mock(FileSystem.class);
    // return false when rename is called
    when(fs.rename(isA(Path.class), isA(Path.class))).thenReturn(true);
    FileSystem localFs = FileSystem.getLocal(conf);
    SharedCacheUploader spied =
        createSpiedUploader(resource, localPath, user, conf, scmClient, fs,
            localFs);
    // stub verifyAccess() to return true
    doReturn(true).when(spied).verifyAccess();
    // stub getActualPath()
    doReturn(localPath).when(spied).getActualPath();
    // stub computeChecksum()
    doReturn(""abcdef0123456789"").when(spied).computeChecksum(isA(Path.class));
    // stub uploadFile() to return true
    doReturn(true).when(spied).uploadFile(isA(Path.class), isA(Path.class));
    // stub notifySharedCacheManager to return true
    doReturn(true).when(spied).notifySharedCacheManager(isA(String.class),
        isA(String.class));

    assertTrue(spied.call());
  }

  /**
   * If verifyAccess, uploadFile, and rename succed, but it receives a nay from
   * SCM, the file should be deleted
   */
  @Test
  public void testNotifySCMFail() throws Exception {
    Configuration conf = new Configuration();
    conf.setBoolean(YarnConfiguration.SHARED_CACHE_ENABLED, true);
    LocalResource resource = mock(LocalResource.class);
    Path localPath = mock(Path.class);
    when(localPath.getName()).thenReturn(""foo.jar"");
    String user = ""joe"";
    FileSystem fs = mock(FileSystem.class);
    // return false when rename is called
    when(fs.rename(isA(Path.class), isA(Path.class))).thenReturn(true);
    FileSystem localFs = FileSystem.getLocal(conf);
    SharedCacheUploader spied =
        createSpiedUploader(resource, localPath, user, conf, null, fs,
            localFs);
    // stub verifyAccess() to return true
    doReturn(true).when(spied).verifyAccess();
    // stub getActualPath()
    doReturn(localPath).when(spied).getActualPath();
    // stub computeChecksum()
    doReturn(""abcdef0123456789"").when(spied).computeChecksum(isA(Path.class));
    // stub uploadFile() to return true
    doReturn(true).when(spied).uploadFile(isA(Path.class), isA(Path.class));
    // stub notifySharedCacheManager to return true
    doReturn(false).when(spied).notifySharedCacheManager(isA(String.class),
        isA(String.class));

    assertFalse(spied.call());
    verify(fs).delete(isA(Path.class), anyBoolean());
  }

  /**
   * If resource is public, verifyAccess should succeed
   */
  @Test
  public void testVerifyAccessPublicResource() throws Exception {
    Configuration conf = new Configuration();
    conf.setBoolean(YarnConfiguration.SHARED_CACHE_ENABLED, true);
    LocalResource resource = mock(LocalResource.class);
    // give public visibility
    when(resource.getVisibility()).thenReturn(LocalResourceVisibility.PUBLIC);
    Path localPath = mock(Path.class);
    when(localPath.getName()).thenReturn(""foo.jar"");
    String user = ""joe"";
    SCMUploaderProtocol scmClient = mock(SCMUploaderProtocol.class);
    FileSystem fs = mock(FileSystem.class);
    FileSystem localFs = FileSystem.getLocal(conf);
    SharedCacheUploader spied =
        createSpiedUploader(resource, localPath, user, conf, scmClient, fs,
            localFs);

    assertTrue(spied.verifyAccess());
  }

  /**
   * If the localPath does not exists, getActualPath should get to one level
   * down
   */
  @Test
  public void testGetActualPath() throws Exception {
    Configuration conf = new Configuration();
    conf.setBoolean(YarnConfiguration.SHARED_CACHE_ENABLED, true);
    LocalResource resource = mock(LocalResource.class);
    // give public visibility
    when(resource.getVisibility()).thenReturn(LocalResourceVisibility.PUBLIC);
    Path localPath = new Path(""foo.jar"");
    String user = ""joe"";
    SCMUploaderProtocol scmClient = mock(SCMUploaderProtocol.class);
    FileSystem fs = mock(FileSystem.class);
    FileSystem localFs = mock(FileSystem.class);
    // stub it to return a status that indicates a directory
    FileStatus status = mock(FileStatus.class);
    when(status.isDirectory()).thenReturn(true);
    when(localFs.getFileStatus(localPath)).thenReturn(status);
    SharedCacheUploader spied =
        createSpiedUploader(resource, localPath, user, conf, scmClient, fs,
            localFs);

    Path actualPath = spied.getActualPath();
    assertEquals(actualPath.getName(), localPath.getName());
    assertEquals(actualPath.getParent().getName(), localPath.getName());
  }

  private SharedCacheUploader createSpiedUploader() throws IOException {
    Configuration conf = new Configuration();
    conf.setBoolean(YarnConfiguration.SHARED_CACHE_ENABLED, true);
    LocalResource resource = mock(LocalResource.class);
    Path localPath = mock(Path.class);
    String user = ""foo"";
    SCMUploaderProtocol scmClient = mock(SCMUploaderProtocol.class);
    FileSystem fs = FileSystem.get(conf);
    FileSystem localFs = FileSystem.getLocal(conf);
    return createSpiedUploader(resource, localPath, user, conf, scmClient, fs,
        localFs);
  }

  private SharedCacheUploader createSpiedUploader(LocalResource resource, Path localPath,
      String user, Configuration conf, SCMUploaderProtocol scmClient,
      FileSystem fs, FileSystem localFs)
          throws IOException {
    SharedCacheUploader uploader = new SharedCacheUploader(resource, localPath, user, conf, scmClient,
        fs, localFs);
    return spy(uploader);
  }
}
","/**
* Licensed to the Apache Software Foundation (ASF) under one
* or more contributor license agreements.  See the NOTICE file
* distributed with this work for additional information
* regarding copyright ownership.  The ASF licenses this file
* to you under the Apache License, Version 2.0 (the
* ""License""); you may not use this file except in compliance
* with the License.  You may obtain a copy of the License at
*
*     http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an ""AS IS"" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
*/

package org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache;

import java.io.IOException;
import java.io.InputStream;
import java.lang.reflect.UndeclaredThrowableException;
import java.net.URISyntaxException;
import java.util.concurrent.Callable;
import java.util.concurrent.ThreadLocalRandom;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileStatus;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.FileUtil;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.fs.permission.FsPermission;
import org.apache.hadoop.yarn.api.records.LocalResource;
import org.apache.hadoop.yarn.api.records.LocalResourceVisibility;
import org.apache.hadoop.yarn.conf.YarnConfiguration;
import org.apache.hadoop.yarn.exceptions.YarnException;
import org.apache.hadoop.yarn.factories.RecordFactory;
import org.apache.hadoop.yarn.factory.providers.RecordFactoryProvider;
import org.apache.hadoop.yarn.server.api.SCMUploaderProtocol;
import org.apache.hadoop.yarn.server.api.protocolrecords.SCMUploaderNotifyRequest;
import org.apache.hadoop.yarn.server.sharedcache.SharedCacheUtil;
import org.apache.hadoop.yarn.sharedcache.SharedCacheChecksum;
import org.apache.hadoop.yarn.sharedcache.SharedCacheChecksumFactory;
import org.apache.hadoop.yarn.util.FSDownload;

import org.apache.hadoop.classification.VisibleForTesting;

/**
 * The callable class that handles the actual upload to the shared cache.
 */
class SharedCacheUploader implements Callable<Boolean> {
  // rwxr-xr-x
  static final FsPermission DIRECTORY_PERMISSION =
      new FsPermission((short)00755);
  // r-xr-xr-x
  static final FsPermission FILE_PERMISSION =
      new FsPermission((short)00555);

  private static final Logger LOG =
       LoggerFactory.getLogger(SharedCacheUploader.class);

  private final LocalResource resource;
  private final Path localPath;
  private final String user;
  private final Configuration conf;
  private final SCMUploaderProtocol scmClient;
  private final FileSystem fs;
  private final FileSystem localFs;
  private final String sharedCacheRootDir;
  private final int nestedLevel;
  private final SharedCacheChecksum checksum;
  private final RecordFactory recordFactory;

  public SharedCacheUploader(LocalResource resource, Path localPath,
      String user, Configuration conf, SCMUploaderProtocol scmClient)
          throws IOException {
    this(resource, localPath, user, conf, scmClient,
        FileSystem.get(conf), localPath.getFileSystem(conf));
  }

  /**
   * @param resource the local resource that contains the original remote path
   * @param localPath the path in the local filesystem where the resource is
   * localized
   * @param fs the filesystem of the shared cache
   * @param localFs the local filesystem
   */
  public SharedCacheUploader(LocalResource resource, Path localPath,
      String user, Configuration conf, SCMUploaderProtocol scmClient,
      FileSystem fs, FileSystem localFs) {
    this.resource = resource;
    this.localPath = localPath;
    this.user = user;
    this.conf = conf;
    this.scmClient = scmClient;
    this.fs = fs;
    this.sharedCacheRootDir =
        conf.get(YarnConfiguration.SHARED_CACHE_ROOT,
            YarnConfiguration.DEFAULT_SHARED_CACHE_ROOT);
    this.nestedLevel = SharedCacheUtil.getCacheDepth(conf);
    this.checksum = SharedCacheChecksumFactory.getChecksum(conf);
    this.localFs = localFs;
    this.recordFactory = RecordFactoryProvider.getRecordFactory(null);
  }

  /**
   * Uploads the file under the shared cache, and notifies the shared cache
   * manager. If it is unable to upload the file because it already exists, it
   * returns false.
   */
  @Override
  public Boolean call() throws Exception {
    Path tempPath = null;
    try {
      if (!verifyAccess()) {
        LOG.warn(""User "" + user + "" is not authorized to upload file "" +
            localPath.getName());
        return false;
      }

      // first determine the actual local path that will be used for upload
      Path actualPath = getActualPath();
      // compute the checksum
      String checksumVal = computeChecksum(actualPath);
      // create the directory (if it doesn't exist)
      Path directoryPath =
          new Path(SharedCacheUtil.getCacheEntryPath(nestedLevel,
              sharedCacheRootDir, checksumVal));
      // let's not check if the directory already exists: in the vast majority
      // of the cases, the directory does not exist; as long as mkdirs does not
      // error out if it exists, we should be fine
      fs.mkdirs(directoryPath, DIRECTORY_PERMISSION);
      // create the temporary file
      tempPath = new Path(directoryPath, getTemporaryFileName(actualPath));
      if (!uploadFile(actualPath, tempPath)) {
        LOG.warn(""Could not copy the file to the shared cache at "" + tempPath);
        return false;
      }

      // set the permission so that it is readable but not writable
      fs.setPermission(tempPath, FILE_PERMISSION);
      // rename it to the final filename
      Path finalPath = new Path(directoryPath, actualPath.getName());
      if (!fs.rename(tempPath, finalPath)) {
        LOG.warn(""The file already exists under "" + finalPath +
            "". Ignoring this attempt."");
        deleteTempFile(tempPath);
        return false;
      }

      // notify the SCM
      if (!notifySharedCacheManager(checksumVal, actualPath.getName())) {
        // the shared cache manager rejected the upload (as it is likely
        // uploaded under a different name
        // clean up this file and exit
        fs.delete(finalPath, false);
        return false;
      }

      // set the replication factor
      short replication =
          (short)conf.getInt(YarnConfiguration.SHARED_CACHE_NM_UPLOADER_REPLICATION_FACTOR,
              YarnConfiguration.DEFAULT_SHARED_CACHE_NM_UPLOADER_REPLICATION_FACTOR);
      fs.setReplication(finalPath, replication);
      LOG.info(""File "" + actualPath.getName() +
          "" was uploaded to the shared cache at "" + finalPath);
      return true;
    } catch (IOException e) {
      LOG.warn(""Exception while uploading the file "" + localPath.getName(), e);
      // in case an exception is thrown, delete the temp file
      deleteTempFile(tempPath);
      throw e;
    }
  }

  @VisibleForTesting
  Path getActualPath() throws IOException {
    Path path = localPath;
    FileStatus status = localFs.getFileStatus(path);
    if (status != null && status.isDirectory()) {
      // for certain types of resources that get unpacked, the original file may
      // be found under the directory with the same name (see
      // FSDownload.unpack); check if the path is a directory and if so look
      // under it
      path = new Path(path, path.getName());
    }
    return path;
  }

  private void deleteTempFile(Path tempPath) {
    try {
      if (tempPath != null) {
        fs.delete(tempPath, false);
      }
    } catch (IOException ioe) {
      LOG.debug(""Exception received while deleting temp files"", ioe);
    }
  }

  /**
   * Checks that the (original) remote file is either owned by the user who
   * started the app or public.
   */
  @VisibleForTesting
  boolean verifyAccess() throws IOException {
    // if it is in the public cache, it's trivially OK
    if (resource.getVisibility() == LocalResourceVisibility.PUBLIC) {
      return true;
    }

    final Path remotePath;
    try {
      remotePath = resource.getResource().toPath();
    } catch (URISyntaxException e) {
      throw new IOException(""Invalid resource"", e);
    }

    // get the file status of the HDFS file
    FileSystem remoteFs = remotePath.getFileSystem(conf);
    FileStatus status = remoteFs.getFileStatus(remotePath);
    // check to see if the file has been modified in any way
    if (status.getModificationTime() != resource.getTimestamp()) {
      LOG.warn(""The remote file "" + remotePath +
          "" has changed since it's localized; will not consider it for upload"");
      return false;
    }

    // check for the user ownership
    if (status.getOwner().equals(user)) {
      return true; // the user owns the file
    }
    // check if the file is publicly readable otherwise
    return fileIsPublic(remotePath, remoteFs, status);
  }

  @VisibleForTesting
  boolean fileIsPublic(final Path remotePath, FileSystem remoteFs,
      FileStatus status) throws IOException {
    return FSDownload.isPublic(remoteFs, remotePath, status, null);
  }

  /**
   * Uploads the file to the shared cache under a temporary name, and returns
   * the result.
   */
  @VisibleForTesting
  boolean uploadFile(Path sourcePath, Path tempPath) throws IOException {
    return FileUtil.copy(localFs, sourcePath, fs, tempPath, false, conf);
  }

  @VisibleForTesting
  String computeChecksum(Path path) throws IOException {
    InputStream is = localFs.open(path);
    try {
      return checksum.computeChecksum(is);
    } finally {
      try { is.close(); } catch (IOException ignore) {}
    }
  }

  private String getTemporaryFileName(Path path) {
    return path.getName() + ""-"" + ThreadLocalRandom.current().nextLong();
  }

  @VisibleForTesting
  boolean notifySharedCacheManager(String checksumVal, String fileName)
      throws IOException {
    try {
      SCMUploaderNotifyRequest request =
          recordFactory.newRecordInstance(SCMUploaderNotifyRequest.class);
      request.setResourceKey(checksumVal);
      request.setFilename(fileName);
      return scmClient.notify(request).getAccepted();
    } catch (YarnException e) {
      throw new IOException(e);
    } catch (UndeclaredThrowableException e) {
      // retrieve the cause of the exception and throw it as an IOException
      throw new IOException(e.getCause() == null ? e : e.getCause());
    }
  }
}
","['Assertion Roulette', 'Lazy Test']","['Assertion Roulette', 'Lazy Test', 'Eager Test', 'Magic Number Test']",2,0,2,14
50129_5.0_jfreechart_testremovevalue,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/50129_test.java,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/50129_actual.java,"/* ===========================================================
 * JFreeChart : a free chart library for the Java(tm) platform
 * ===========================================================
 *
 * (C) Copyright 2000-2022, by David Gilbert and Contributors.
 *
 * Project Info:  http://www.jfree.org/jfreechart/index.html
 *
 * This library is free software; you can redistribute it and/or modify it
 * under the terms of the GNU Lesser General Public License as published by
 * the Free Software Foundation; either version 2.1 of the License, or
 * (at your option) any later version.
 *
 * This library is distributed in the hope that it will be useful, but
 * WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
 * or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public
 * License for more details.
 *
 * You should have received a copy of the GNU Lesser General Public
 * License along with this library; if not, write to the Free Software
 * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301,
 * USA.
 *
 * [Oracle and Java are registered trademarks of Oracle and/or its affiliates. 
 * Other names may be trademarks of their respective owners.]
 *
 * -----------------------------
 * DefaultKeyedValues2DTest.java
 * -----------------------------
 * (C) Copyright 2003-2022, by David Gilbert and Contributors.
 *
 * Original Author:  David Gilbert;
 * Contributor(s):   -;
 *
 */

package org.jfree.data;

import org.jfree.chart.TestUtils;
import org.jfree.chart.internal.CloneUtils;
import org.junit.jupiter.api.Test;

import static org.junit.jupiter.api.Assertions.*;

/**
 * Tests for the {@link DefaultKeyedValues2D} class.
 */
public class DefaultKeyedValues2DTest {

    /**
     * Some checks for the getValue() method.
     */
    @Test
    public void testGetValue() {
        DefaultKeyedValues2D<String, String> d = new DefaultKeyedValues2D<>();
        d.addValue(1.0, ""R1"", ""C1"");
        assertEquals(1.0, d.getValue(""R1"", ""C1""));
        boolean pass = false;
        try {
            d.getValue(""XX"", ""C1"");
        }
        catch (UnknownKeyException e) {
            pass = true;
        }
        assertTrue(pass);

        pass = false;
        try {
            d.getValue(""R1"", ""XX"");
        }
        catch (UnknownKeyException e) {
            pass = true;
        }
        assertTrue(pass);
    }

    /**
     * Some checks for the clone() method.
     * @throws java.lang.CloneNotSupportedException
     */
    @Test
    public void testCloning() throws CloneNotSupportedException {
        DefaultKeyedValues2D<String, String> v1 = new DefaultKeyedValues2D<>();
        v1.setValue(1, ""V1"", ""C1"");
        v1.setValue(null, ""V2"", ""C1"");
        v1.setValue(3, ""V3"", ""C2"");
        DefaultKeyedValues2D<String, String> v2 = CloneUtils.clone(v1);
        assertNotSame(v1, v2);
        assertSame(v1.getClass(), v2.getClass());
        assertEquals(v1, v2);

        // check that clone is independent of the original
        v2.setValue(2, ""V2"", ""C1"");
        assertNotEquals(v1, v2);
    }

    /**
     * Serialize an instance, restore it, and check for equality.
     */
    @Test
    public void testSerialization() {
        DefaultKeyedValues2D<String, String> kv2D1 = new DefaultKeyedValues2D<>();
        kv2D1.addValue(234.2, ""Row1"", ""Col1"");
        kv2D1.addValue(null, ""Row1"", ""Col2"");
        kv2D1.addValue(345.9, ""Row2"", ""Col1"");
        kv2D1.addValue(452.7, ""Row2"", ""Col2"");

        DefaultKeyedValues2D<String, String> kv2D2 = TestUtils.serialised(kv2D1);
        assertEquals(kv2D1, kv2D2);
    }

    /**
     * Some checks for the equals() method.
     */
    @Test
    public void testEquals() {
        DefaultKeyedValues2D<String, String> d1 = new DefaultKeyedValues2D<>();
        DefaultKeyedValues2D<String, String> d2 = new DefaultKeyedValues2D<>();
        assertEquals(d1, d2);
        assertEquals(d2, d1);

        d1.addValue(1.0, ""R1"", ""C1"");
        assertNotEquals(d1, d2);
        d2.addValue(1.0, ""R1"", ""C1"");
        assertEquals(d1, d2);
    }

    /**
     * Populates a data structure with sparse entries, then checks that
     * the unspecified entries return null.
     */
    @Test
    public void testSparsePopulation() {
        DefaultKeyedValues2D<String, String> d = new DefaultKeyedValues2D<>();
        d.addValue(11, ""R1"", ""C1"");
        d.addValue(22, ""R2"", ""C2"");

        assertEquals(11, d.getValue(""R1"", ""C1""));
        assertNull(d.getValue(""R1"", ""C2""));
        assertEquals(22, d.getValue(""R2"", ""C2""));
        assertNull(d.getValue(""R2"", ""C1""));
    }

    /**
     * Some basic checks for the getRowCount() method.
     */
    @Test
    public void testRowCount() {
        DefaultKeyedValues2D<String, String> d = new DefaultKeyedValues2D<>();
        assertEquals(0, d.getRowCount());
        d.addValue(1.0, ""R1"", ""C1"");
        assertEquals(1, d.getRowCount());
        d.addValue(2.0, ""R2"", ""C1"");
        assertEquals(2, d.getRowCount());
    }

    /**
     * Some basic checks for the getColumnCount() method.
     */
    @Test
    public void testColumnCount() {
        DefaultKeyedValues2D<String, String> d = new DefaultKeyedValues2D<>();
        assertEquals(0, d.getColumnCount());
        d.addValue(1.0, ""R1"", ""C1"");
        assertEquals(1, d.getColumnCount());
        d.addValue(2.0, ""R1"", ""C2"");
        assertEquals(2, d.getColumnCount());
    }

    private static final double EPSILON = 0.0000000001;

    /**
     * Some basic checks for the getValue(int, int) method.
     */
    @Test
    public void testGetValue2() {
        DefaultKeyedValues2D<String, String> d = new DefaultKeyedValues2D<>();
        boolean pass = false;
        try {
            d.getValue(0, 0);
        }
        catch (IndexOutOfBoundsException e) {
            pass = true;
        }
        assertTrue(pass);
        d.addValue(1.0, ""R1"", ""C1"");
        assertEquals(1.0, d.getValue(0, 0).doubleValue(), EPSILON);
        d.addValue(2.0, ""R2"", ""C2"");
        assertEquals(2.0, d.getValue(1, 1).doubleValue(), EPSILON);
        assertNull(d.getValue(1, 0));
        assertNull(d.getValue(0, 1));

        pass = false;
        try {
            d.getValue(2, 0);
        }
        catch (IndexOutOfBoundsException e) {
            pass = true;
        }
        assertTrue(pass);
    }

    /**
     * Some basic checks for the getRowKey() method.
     */
    @Test
    public void testGetRowKey() {
        DefaultKeyedValues2D<String, String> d = new DefaultKeyedValues2D<>();
        boolean pass = false;
        try {
            d.getRowKey(0);
        }
        catch (IndexOutOfBoundsException e) {
            pass = true;
        }
        assertTrue(pass);
        d.addValue(1.0, ""R1"", ""C1"");
        d.addValue(1.0, ""R2"", ""C1"");
        assertEquals(""R1"", d.getRowKey(0));
        assertEquals(""R2"", d.getRowKey(1));

        // check sorted rows
        d = new DefaultKeyedValues2D<>(true);
        d.addValue(1.0, ""R1"", ""C1"");
        assertEquals(""R1"", d.getRowKey(0));
        d.addValue(0.0, ""R0"", ""C1"");
        assertEquals(""R0"", d.getRowKey(0));
        assertEquals(""R1"", d.getRowKey(1));
    }

    /**
     * Some basic checks for the getColumnKey() method.
     */
    @Test
    public void testGetColumnKey() {
        DefaultKeyedValues2D<String, String> d = new DefaultKeyedValues2D<>();
        boolean pass = false;
        try {
            d.getColumnKey(0);
        }
        catch (IndexOutOfBoundsException e) {
            pass = true;
        }
        assertTrue(pass);
        d.addValue(1.0, ""R1"", ""C1"");
        d.addValue(1.0, ""R1"", ""C2"");
        assertEquals(""C1"", d.getColumnKey(0));
        assertEquals(""C2"", d.getColumnKey(1));
    }

    /**
     * Some basic checks for the removeValue() method.
     */
    @Test
    public void testRemoveValue() {
        DefaultKeyedValues2D<String, String> d = new DefaultKeyedValues2D<>();
        d.removeValue(""R1"", ""C1"");
        d.addValue(1.0, ""R1"", ""C1"");
        d.removeValue(""R1"", ""C1"");
        assertEquals(0, d.getRowCount());
        assertEquals(0, d.getColumnCount());

        d.addValue(1.0, ""R1"", ""C1"");
        d.addValue(2.0, ""R2"", ""C1"");
        d.removeValue(""R1"", ""C1"");
        assertEquals(2.0, d.getValue(0, 0));
    }

    /**
     * A test for bug 1690654.
     */
    @Test
    public void testRemoveValueBug1690654() {
        DefaultKeyedValues2D<String, String> d = new DefaultKeyedValues2D<>();
        d.addValue(1.0, ""R1"", ""C1"");
        d.addValue(2.0, ""R2"", ""C2"");
        assertEquals(2, d.getColumnCount());
        assertEquals(2, d.getRowCount());
        d.removeValue(""R2"", ""C2"");
        assertEquals(1, d.getColumnCount());
        assertEquals(1, d.getRowCount());
        assertEquals(1.0, d.getValue(0, 0));
    }

    /**
     * Some basic checks for the removeRow() method.
     */
    @Test
    public void testRemoveRow() {
        DefaultKeyedValues2D<String, String> d = new DefaultKeyedValues2D<>();
        boolean pass = false;
        try {
            d.removeRow(0);
        }
        catch (IndexOutOfBoundsException e) {
            pass = true;
        }
        assertTrue(pass);
    }

    /**
     * Some basic checks for the removeColumn(Comparable) method.
     */
    @Test
    public void testRemoveColumnByKey() {
        DefaultKeyedValues2D<String, String> d = new DefaultKeyedValues2D<>();
        d.addValue(1.0, ""R1"", ""C1"");
        d.addValue(2.0, ""R2"", ""C2"");
        d.removeColumn(""C2"");
        d.addValue(3.0, ""R2"", ""C2"");
        assertEquals(3.0, d.getValue(""R2"", ""C2"").doubleValue(), EPSILON);

        // check for unknown column
        boolean pass = false;
        try {
            d.removeColumn(""XXX"");
        }
        catch (UnknownKeyException e) {
            pass = true;
        }
        assertTrue(pass);
    }

}
","/* ===========================================================
 * JFreeChart : a free chart library for the Java(tm) platform
 * ===========================================================
 *
 * (C) Copyright 2000-2022, by David Gilbert and Contributors.
 *
 * Project Info:  http://www.jfree.org/jfreechart/index.html
 *
 * This library is free software; you can redistribute it and/or modify it
 * under the terms of the GNU Lesser General Public License as published by
 * the Free Software Foundation; either version 2.1 of the License, or
 * (at your option) any later version.
 *
 * This library is distributed in the hope that it will be useful, but
 * WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
 * or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public
 * License for more details.
 *
 * You should have received a copy of the GNU Lesser General Public
 * License along with this library; if not, write to the Free Software
 * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301,
 * USA.
 *
 * [Oracle and Java are registered trademarks of Oracle and/or its affiliates. 
 * Other names may be trademarks of their respective owners.]
 *
 * -------------------------
 * DefaultKeyedValues2D.java
 * -------------------------
 * (C) Copyright 2002-2022, by David Gilbert.
 *
 * Original Author:  David Gilbert;
 * Contributor(s):   Andreas Schroeder;
 * 
 */

package org.jfree.data;

import java.io.Serializable;
import java.util.ArrayList;
import java.util.Collections;
import java.util.List;

import org.jfree.chart.internal.CloneUtils;
import org.jfree.chart.internal.Args;
import org.jfree.chart.api.PublicCloneable;

/**
 * A data structure that stores zero, one or many values, where each value
 * is associated with two keys (a 'row' key and a 'column' key).  The keys
 * should be (a) instances of {@link Comparable} and (b) immutable.
 */
public class DefaultKeyedValues2D<R extends Comparable<R>, C extends Comparable<C>> 
        implements KeyedValues2D<R, C>, PublicCloneable, Cloneable, Serializable {

    /** For serialization. */
    private static final long serialVersionUID = -5514169970951994748L;

    /** The row keys. */
    private List<R> rowKeys;

    /** The column keys. */
    private List<C> columnKeys;

    /** The row data. */
    private List<DefaultKeyedValues<C>> rows;

    /** If the row keys should be sorted by their comparable order. */
    private final boolean sortRowKeys;

    /**
     * Creates a new instance (initially empty).
     */
    public DefaultKeyedValues2D() {
        this(false);
    }

    /**
     * Creates a new instance (initially empty).
     *
     * @param sortRowKeys  if the row keys should be sorted.
     */
    public DefaultKeyedValues2D(boolean sortRowKeys) {
        this.rowKeys = new ArrayList<>();
        this.columnKeys = new ArrayList<>();
        this.rows = new ArrayList<>();
        this.sortRowKeys = sortRowKeys;
    }

    /**
     * Returns the row count.
     *
     * @return The row count.
     *
     * @see #getColumnCount()
     */
    @Override
    public int getRowCount() {
        return this.rowKeys.size();
    }

    /**
     * Returns the column count.
     *
     * @return The column count.
     *
     * @see #getRowCount()
     */
    @Override
    public int getColumnCount() {
        return this.columnKeys.size();
    }

    /**
     * Returns the value for a given row and column.
     *
     * @param row  the row index.
     * @param column  the column index.
     *
     * @return The value.
     *
     * @see #getValue(Comparable, Comparable)
     */
    @Override
    public Number getValue(int row, int column) {
        Number result = null;
        DefaultKeyedValues<C> rowData = this.rows.get(row);
        if (rowData != null) {
            C columnKey = this.columnKeys.get(column);
            // the row may not have an entry for this key, in which case the
            // return value is null
            int index = rowData.getIndex(columnKey);
            if (index >= 0) {
                result = rowData.getValue(index);
            }
        }
        return result;
    }

    /**
     * Returns the key for a given row.
     *
     * @param row  the row index (in the range 0 to {@link #getRowCount()} - 1).
     *
     * @return The row key.
     *
     * @see #getRowIndex(Comparable)
     * @see #getColumnKey(int)
     */
    @Override
    public R getRowKey(int row) {
        return this.rowKeys.get(row);
    }

    /**
     * Returns the row index for a given key.
     *
     * @param key  the key ({@code null} not permitted).
     *
     * @return The row index.
     *
     * @see #getRowKey(int)
     * @see #getColumnIndex(Comparable)
     */
    @Override
    public int getRowIndex(R key) {
        Args.nullNotPermitted(key, ""key"");
        if (this.sortRowKeys) {
            return Collections.<R>binarySearch(this.rowKeys, key);
        } else {
            return this.rowKeys.indexOf(key);
        }
    }

    /**
     * Returns the row keys in an unmodifiable list.
     *
     * @return The row keys.
     *
     * @see #getColumnKeys()
     */
    @Override
    public List<R> getRowKeys() {
        return Collections.unmodifiableList(this.rowKeys);
    }

    /**
     * Returns the key for a given column.
     *
     * @param column  the column (in the range 0 to {@link #getColumnCount()}
     *     - 1).
     *
     * @return The key.
     *
     * @see #getColumnIndex(Comparable)
     * @see #getRowKey(int)
     */
    @Override
    public C getColumnKey(int column) {
        return this.columnKeys.get(column);
    }

    /**
     * Returns the column index for a given key.
     *
     * @param key  the key ({@code null} not permitted).
     *
     * @return The column index.
     *
     * @see #getColumnKey(int)
     * @see #getRowIndex(Comparable)
     */
    @Override
    public int getColumnIndex(C key) {
        Args.nullNotPermitted(key, ""key"");
        return this.columnKeys.indexOf(key);
    }

    /**
     * Returns the column keys in an unmodifiable list.
     *
     * @return The column keys.
     *
     * @see #getRowKeys()
     */
    @Override
    public List<C> getColumnKeys() {
        return Collections.unmodifiableList(this.columnKeys);
    }

    /**
     * Returns the value for the given row and column keys.  This method will
     * throw an {@link UnknownKeyException} if either key is not defined in the
     * data structure.
     *
     * @param rowKey  the row key ({@code null} not permitted).
     * @param columnKey  the column key ({@code null} not permitted).
     *
     * @return The value (possibly {@code null}).
     *
     * @see #addValue(Number, Comparable, Comparable)
     * @see #removeValue(Comparable, Comparable)
     */
    @Override
    public Number getValue(R rowKey, C columnKey) {
        Args.nullNotPermitted(rowKey, ""rowKey"");
        Args.nullNotPermitted(columnKey, ""columnKey"");

        // check that the column key is defined in the 2D structure
        if (!(this.columnKeys.contains(columnKey))) {
            throw new UnknownKeyException(""Unrecognised columnKey: ""
                    + columnKey);
        }

        // now fetch the row data - need to bear in mind that the row
        // structure may not have an entry for the column key, but that we
        // have already checked that the key is valid for the 2D structure
        int row = getRowIndex(rowKey);
        if (row >= 0) {
            DefaultKeyedValues rowData = this.rows.get(row);
            int col = rowData.getIndex(columnKey);
            return (col >= 0 ? rowData.getValue(col) : null);
        }
        else {
            throw new UnknownKeyException(""Unrecognised rowKey: "" + rowKey);
        }
    }

    /**
     * Adds a value to the table.  Performs the same function as
     * #setValue(Number, Comparable, Comparable).
     *
     * @param value  the value ({@code null} permitted).
     * @param rowKey  the row key ({@code null} not permitted).
     * @param columnKey  the column key ({@code null} not permitted).
     *
     * @see #setValue(Number, Comparable, Comparable)
     * @see #removeValue(Comparable, Comparable)
     */
    public void addValue(Number value, R rowKey, C columnKey) {
        // defer argument checking
        setValue(value, rowKey, columnKey);
    }

    /**
     * Adds or updates a value.
     *
     * @param value  the value ({@code null} permitted).
     * @param rowKey  the row key ({@code null} not permitted).
     * @param columnKey  the column key ({@code null} not permitted).
     *
     * @see #addValue(Number, Comparable, Comparable)
     * @see #removeValue(Comparable, Comparable)
     */
    public void setValue(Number value, R rowKey, C columnKey) {

        DefaultKeyedValues row;
        int rowIndex = getRowIndex(rowKey);

        if (rowIndex >= 0) {
            row = this.rows.get(rowIndex);
        }
        else {
            row = new DefaultKeyedValues();
            if (this.sortRowKeys) {
                rowIndex = -rowIndex - 1;
                this.rowKeys.add(rowIndex, rowKey);
                this.rows.add(rowIndex, row);
            }
            else {
                this.rowKeys.add(rowKey);
                this.rows.add(row);
            }
        }
        row.setValue(columnKey, value);

        int columnIndex = this.columnKeys.indexOf(columnKey);
        if (columnIndex < 0) {
            this.columnKeys.add(columnKey);
        }
    }

    /**
     * Removes a value from the table by setting it to {@code null}.  If
     * all the values in the specified row and/or column are now
     * {@code null}, the row and/or column is removed from the table.
     *
     * @param rowKey  the row key ({@code null} not permitted).
     * @param columnKey  the column key ({@code null} not permitted).
     *
     * @see #addValue(Number, Comparable, Comparable)
     */
    public void removeValue(R rowKey, C columnKey) {
        setValue(null, rowKey, columnKey);

        // 1. check whether the row is now empty.
        boolean allNull = true;
        int rowIndex = getRowIndex(rowKey);
        DefaultKeyedValues row = this.rows.get(rowIndex);

        for (int item = 0, itemCount = row.getItemCount(); item < itemCount;
             item++) {
            if (row.getValue(item) != null) {
                allNull = false;
                break;
            }
        }

        if (allNull) {
            this.rowKeys.remove(rowIndex);
            this.rows.remove(rowIndex);
        }

        // 2. check whether the column is now empty.
        allNull = true;
        //int columnIndex = getColumnIndex(columnKey);

        for (int item = 0, itemCount = this.rows.size(); item < itemCount;
             item++) {
            row = this.rows.get(item);
            int columnIndex = row.getIndex(columnKey);
            if (columnIndex >= 0 && row.getValue(columnIndex) != null) {
                allNull = false;
                break;
            }
        }

        if (allNull) {
            for (int item = 0, itemCount = this.rows.size(); item < itemCount;
                 item++) {
                row = this.rows.get(item);
                int columnIndex = row.getIndex(columnKey);
                if (columnIndex >= 0) {
                    row.removeValue(columnIndex);
                }
            }
            this.columnKeys.remove(columnKey);
        }
    }

    /**
     * Removes a row.
     *
     * @param rowIndex  the row index.
     *
     * @see #removeRow(Comparable)
     * @see #removeColumn(int)
     */
    public void removeRow(int rowIndex) {
        this.rowKeys.remove(rowIndex);
        this.rows.remove(rowIndex);
    }

    /**
     * Removes a row from the table.
     *
     * @param rowKey  the row key ({@code null} not permitted).
     *
     * @see #removeRow(int)
     * @see #removeColumn(Comparable)
     *
     * @throws UnknownKeyException if {@code rowKey} is not defined in the
     *         table.
     */
    public void removeRow(R rowKey) {
        Args.nullNotPermitted(rowKey, ""rowKey"");
        int index = getRowIndex(rowKey);
        if (index >= 0) {
            removeRow(index);
        }
        else {
            throw new UnknownKeyException(""Unknown key: "" + rowKey);
        }
    }

    /**
     * Removes a column.
     *
     * @param columnIndex  the column index.
     *
     * @see #removeColumn(Comparable)
     * @see #removeRow(int)
     */
    public void removeColumn(int columnIndex) {
        C columnKey = getColumnKey(columnIndex);
        removeColumn(columnKey);
    }

    /**
     * Removes a column from the table.
     *
     * @param columnKey  the column key ({@code null} not permitted).
     *
     * @throws UnknownKeyException if the table does not contain a column with
     *     the specified key.
     * @throws IllegalArgumentException if {@code columnKey} is
     *     {@code null}.
     *
     * @see #removeColumn(int)
     * @see #removeRow(Comparable)
     */
    public void removeColumn(C columnKey) {
        Args.nullNotPermitted(columnKey, ""columnKey"");
        if (!this.columnKeys.contains(columnKey)) {
            throw new UnknownKeyException(""Unknown key: "" + columnKey);
        }
        for (DefaultKeyedValues rowData : this.rows) {
            int index = rowData.getIndex(columnKey);
            if (index >= 0) {
                rowData.removeValue(columnKey);
            }
        }
        this.columnKeys.remove(columnKey);
    }

    /**
     * Clears all the data and associated keys.
     */
    public void clear() {
        this.rowKeys.clear();
        this.columnKeys.clear();
        this.rows.clear();
    }

    /**
     * Tests if this object is equal to another.
     *
     * @param o  the other object ({@code null} permitted).
     *
     * @return A boolean.
     */
    @Override
    public boolean equals(Object o) {

        if (o == null) {
            return false;
        }
        if (o == this) {
            return true;
        }

        if (!(o instanceof KeyedValues2D)) {
            return false;
        }
        KeyedValues2D kv2D = (KeyedValues2D) o;
        if (!getRowKeys().equals(kv2D.getRowKeys())) {
            return false;
        }
        if (!getColumnKeys().equals(kv2D.getColumnKeys())) {
            return false;
        }
        int rowCount = getRowCount();
        if (rowCount != kv2D.getRowCount()) {
            return false;
        }

        int colCount = getColumnCount();
        if (colCount != kv2D.getColumnCount()) {
            return false;
        }

        for (int r = 0; r < rowCount; r++) {
            for (int c = 0; c < colCount; c++) {
                Number v1 = getValue(r, c);
                Number v2 = kv2D.getValue(r, c);
                if (v1 == null) {
                    if (v2 != null) {
                        return false;
                    }
                }
                else {
                    if (!v1.equals(v2)) {
                        return false;
                    }
                }
            }
        }
        return true;
    }

    /**
     * Returns a hash code.
     *
     * @return A hash code.
     */
    @Override
    public int hashCode() {
        int result;
        result = this.rowKeys.hashCode();
        result = 29 * result + this.columnKeys.hashCode();
        result = 29 * result + this.rows.hashCode();
        return result;
    }

    /**
     * Returns a clone.
     *
     * @return A clone.
     *
     * @throws CloneNotSupportedException  this class will not throw this
     *         exception, but subclasses (if any) might.
     */
    @Override
    public Object clone() throws CloneNotSupportedException {
        DefaultKeyedValues2D clone = (DefaultKeyedValues2D) super.clone();
        // for the keys, a shallow copy should be fine because keys
        // should be immutable...
        clone.columnKeys = new java.util.ArrayList(this.columnKeys);
        clone.rowKeys = new java.util.ArrayList(this.rowKeys);

        // but the row data requires a deep copy
        clone.rows = CloneUtils.cloneList(this.rows);
        return clone;
    }

}
","['Assertion Roulette', 'Exception Catching Throwing']","['Assertion Roulette', 'Eager Test', 'Magic Number Test', 'Sensitive Equality']",3,1,1,13
24427_4.0_hadoop_testlifecycle,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/24427_test.java,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/24427_actual.java,"/*
 * *
 *  Licensed to the Apache Software Foundation (ASF) under one
 *  or more contributor license agreements. See the NOTICE file
 *  distributed with this work for additional information
 *  regarding copyright ownership. The ASF licenses this file
 *  to you under the Apache License, Version 2.0 (the
 *  ""License""); you may not use this file except in compliance
 *  with the License. You may obtain a copy of the License at
 *
 *  http://www.apache.org/licenses/LICENSE-2.0
 *
 *  Unless required by applicable law or agreed to in writing, software
 *  distributed under the License is distributed on an ""AS IS"" BASIS,
 *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 *  See the License for the specific language governing permissions and
 *  limitations under the License.
 * /
 */

package org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileUtil;
import org.apache.hadoop.yarn.api.records.ContainerId;
import org.apache.hadoop.yarn.conf.YarnConfiguration;
import org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container;
import org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation;
import org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationException;
import org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor;
import org.junit.After;
import org.junit.Assert;
import org.junit.Before;
import org.junit.Test;
import org.mockito.ArgumentCaptor;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.io.File;
import java.util.List;

import static org.mockito.ArgumentMatchers.eq;
import static org.mockito.Mockito.doReturn;
import static org.mockito.Mockito.mock;
import static org.mockito.Mockito.reset;
import static org.mockito.Mockito.spy;
import static org.mockito.Mockito.verify;
import static org.mockito.Mockito.verifyNoMoreInteractions;
import static org.mockito.Mockito.when;

public class TestTrafficControlBandwidthHandlerImpl {
  private static final Logger LOG =
      LoggerFactory.getLogger(TestTrafficControlBandwidthHandlerImpl.class);
  private static final int ROOT_BANDWIDTH_MBIT = 100;
  private static final int YARN_BANDWIDTH_MBIT = 70;
  private static final int TEST_CLASSID = 100;
  private static final String TEST_CLASSID_STR = ""42:100"";
  private static final String TEST_CONTAINER_ID_STR = ""container_01"";
  private static final String TEST_TASKS_FILE = ""testTasksFile"";

  private PrivilegedOperationExecutor privilegedOperationExecutorMock;
  private CGroupsHandler cGroupsHandlerMock;
  private TrafficController trafficControllerMock;
  private Configuration conf;
  private String tmpPath;
  private String device;
  ContainerId containerIdMock;
  Container containerMock;

  @Before
  public void setup() {
    privilegedOperationExecutorMock = mock(PrivilegedOperationExecutor.class);
    cGroupsHandlerMock = mock(CGroupsHandler.class);
    trafficControllerMock = mock(TrafficController.class);
    conf = new YarnConfiguration();
    tmpPath = new StringBuilder(System.getProperty(""test.build.data"")).append
        ('/').append(""hadoop.tmp.dir"").toString();
    device = YarnConfiguration.DEFAULT_NM_NETWORK_RESOURCE_INTERFACE;
    containerIdMock = mock(ContainerId.class);
    containerMock = mock(Container.class);
    when(containerIdMock.toString()).thenReturn(TEST_CONTAINER_ID_STR);
    //mock returning a mock - an angel died somewhere.
    when(containerMock.getContainerId()).thenReturn(containerIdMock);

    conf.setInt(YarnConfiguration
        .NM_NETWORK_RESOURCE_OUTBOUND_BANDWIDTH_MBIT, ROOT_BANDWIDTH_MBIT);
    conf.setInt(YarnConfiguration
        .NM_NETWORK_RESOURCE_OUTBOUND_BANDWIDTH_YARN_MBIT, YARN_BANDWIDTH_MBIT);
    conf.set(""hadoop.tmp.dir"", tmpPath);
    //In these tests, we'll only use TrafficController with recovery disabled
    conf.setBoolean(YarnConfiguration.NM_RECOVERY_ENABLED, false);
  }

  @Test
  public void testBootstrap() {
    TrafficControlBandwidthHandlerImpl handlerImpl = new
        TrafficControlBandwidthHandlerImpl(privilegedOperationExecutorMock,
        cGroupsHandlerMock, trafficControllerMock);

    try {
      handlerImpl.bootstrap(conf);
      verify(cGroupsHandlerMock).initializeCGroupController(
          eq(CGroupsHandler.CGroupController.NET_CLS));
      verifyNoMoreInteractions(cGroupsHandlerMock);
      verify(trafficControllerMock).bootstrap(eq(device),
          eq(ROOT_BANDWIDTH_MBIT),
          eq(YARN_BANDWIDTH_MBIT));
      verifyNoMoreInteractions(trafficControllerMock);
    } catch (ResourceHandlerException e) {
      LOG.error(""Unexpected exception: "" + e);
      Assert.fail(""Caught unexpected ResourceHandlerException!"");
    }
  }

  @Test
  public void testLifeCycle() {
    TrafficController trafficControllerSpy = spy(new TrafficController(conf,
        privilegedOperationExecutorMock));
    TrafficControlBandwidthHandlerImpl handlerImpl = new
        TrafficControlBandwidthHandlerImpl(privilegedOperationExecutorMock,
        cGroupsHandlerMock, trafficControllerSpy);

    try {
      handlerImpl.bootstrap(conf);
      testPreStart(trafficControllerSpy, handlerImpl);
      testPostComplete(trafficControllerSpy, handlerImpl);
    } catch (ResourceHandlerException e) {
      LOG.error(""Unexpected exception: "" + e);
      Assert.fail(""Caught unexpected ResourceHandlerException!"");
    }
  }

  private void testPreStart(TrafficController trafficControllerSpy,
      TrafficControlBandwidthHandlerImpl handlerImpl) throws
      ResourceHandlerException {
    //This is not the cleanest of solutions - but since we are testing the
    //preStart/postComplete lifecycle, we don't have a different way of
    //handling this - we don't keep track of the number of invocations by
    //a class we are not testing here (TrafficController)
    //So, we'll reset this mock. This is not a problem with other mocks.
    reset(privilegedOperationExecutorMock);

    doReturn(TEST_CLASSID).when(trafficControllerSpy).getNextClassId();
    doReturn(TEST_CLASSID_STR).when(trafficControllerSpy)
        .getStringForNetClsClassId(TEST_CLASSID);
    when(cGroupsHandlerMock.getPathForCGroupTasks(CGroupsHandler
        .CGroupController.NET_CLS, TEST_CONTAINER_ID_STR)).thenReturn(
        TEST_TASKS_FILE);

    List<PrivilegedOperation> ops = handlerImpl.preStart(containerMock);

    //Ensure that cgroups is created and updated as expected
    verify(cGroupsHandlerMock).createCGroup(
        eq(CGroupsHandler.CGroupController.NET_CLS), eq(TEST_CONTAINER_ID_STR));
    verify(cGroupsHandlerMock).updateCGroupParam(
        eq(CGroupsHandler.CGroupController.NET_CLS), eq(TEST_CONTAINER_ID_STR),
        eq(CGroupsHandler.CGROUP_PARAM_CLASSID), eq(TEST_CLASSID_STR));

    //Now check the privileged operations being returned
    //We expect two operations - one for adding pid to tasks file and another
    //for a tc modify operation
    Assert.assertEquals(2, ops.size());

    //Verify that the add pid op is correct
    PrivilegedOperation addPidOp = ops.get(0);
    String expectedAddPidOpArg = PrivilegedOperation.CGROUP_ARG_PREFIX +
        TEST_TASKS_FILE;
    List<String> addPidOpArgs = addPidOp.getArguments();

    Assert.assertEquals(PrivilegedOperation.OperationType.ADD_PID_TO_CGROUP,
        addPidOp.getOperationType());
    Assert.assertEquals(1, addPidOpArgs.size());
    Assert.assertEquals(expectedAddPidOpArg, addPidOpArgs.get(0));

    //Verify that that tc modify op is correct
    PrivilegedOperation tcModifyOp = ops.get(1);
    List<String> tcModifyOpArgs = tcModifyOp.getArguments();

    Assert.assertEquals(PrivilegedOperation.OperationType.TC_MODIFY_STATE,
        tcModifyOp.getOperationType());
    Assert.assertEquals(1, tcModifyOpArgs.size());
    //verify that the tc command file exists
    Assert.assertTrue(new File(tcModifyOpArgs.get(0)).exists());
  }

  private void testPostComplete(TrafficController trafficControllerSpy,
      TrafficControlBandwidthHandlerImpl handlerImpl) throws
      ResourceHandlerException {
    //This is not the cleanest of solutions - but since we are testing the
    //preStart/postComplete lifecycle, we don't have a different way of
    //handling this - we don't keep track of the number of invocations by
    //a class we are not testing here (TrafficController)
    //So, we'll reset this mock. This is not a problem with other mocks.
    reset(privilegedOperationExecutorMock);

    List<PrivilegedOperation> ops = handlerImpl.postComplete(containerIdMock);

    verify(cGroupsHandlerMock).deleteCGroup(
        eq(CGroupsHandler.CGroupController.NET_CLS), eq(TEST_CONTAINER_ID_STR));

    try {
      //capture privileged op argument and ensure it is correct
      ArgumentCaptor<PrivilegedOperation> opCaptor = ArgumentCaptor.forClass
          (PrivilegedOperation.class);

      verify(privilegedOperationExecutorMock)
          .executePrivilegedOperation(opCaptor.capture(), eq(false));

      List<String> args = opCaptor.getValue().getArguments();

      Assert.assertEquals(PrivilegedOperation.OperationType.TC_MODIFY_STATE,
          opCaptor.getValue().getOperationType());
      Assert.assertEquals(1, args.size());
      //ensure that tc command file exists
      Assert.assertTrue(new File(args.get(0)).exists());

      verify(trafficControllerSpy).releaseClassId(TEST_CLASSID);
    } catch (PrivilegedOperationException e) {
      LOG.error(""Caught exception: "" + e);
      Assert.fail(""Unexpected PrivilegedOperationException from mock!"");
    }

    //We don't expect any operations to be returned here
    Assert.assertNull(ops);
  }

  @After
  public void teardown() {
    FileUtil.fullyDelete(new File(tmpPath));
  }
}
","/*
 * *
 *  Licensed to the Apache Software Foundation (ASF) under one
 *  or more contributor license agreements. See the NOTICE file
 *  distributed with this work for additional information
 *  regarding copyright ownership. The ASF licenses this file
 *  to you under the Apache License, Version 2.0 (the
 *  ""License""); you may not use this file except in compliance
 *  with the License. You may obtain a copy of the License at
 *
 *  http://www.apache.org/licenses/LICENSE-2.0
 *
 *  Unless required by applicable law or agreed to in writing, software
 *  distributed under the License is distributed on an ""AS IS"" BASIS,
 *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 *  See the License for the specific language governing permissions and
 *  limitations under the License.
 * /
 */

package org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources;

import org.apache.hadoop.classification.InterfaceAudience;
import org.apache.hadoop.classification.InterfaceStability;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.yarn.api.records.ContainerId;
import org.apache.hadoop.yarn.conf.YarnConfiguration;
import org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container;
import org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation;
import org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationException;
import org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.concurrent.ConcurrentHashMap;

@InterfaceAudience.Private
@InterfaceStability.Unstable
public class TrafficControlBandwidthHandlerImpl
    implements OutboundBandwidthResourceHandler {

  private static final Logger LOG =
       LoggerFactory.getLogger(TrafficControlBandwidthHandlerImpl.class);
  //In the absence of 'scheduling' support, we'll 'infer' the guaranteed
  //outbound bandwidth for each container based on this number. This will
  //likely go away once we add support on the RM for this resource type.
  private static final int MAX_CONTAINER_COUNT = 50;

  private final PrivilegedOperationExecutor privilegedOperationExecutor;
  private final CGroupsHandler cGroupsHandler;
  private final TrafficController trafficController;
  private final ConcurrentHashMap<ContainerId, Integer> containerIdClassIdMap;

  private Configuration conf;
  private String device;
  private boolean strictMode;
  private int containerBandwidthMbit;
  private int rootBandwidthMbit;
  private int yarnBandwidthMbit;

  public TrafficControlBandwidthHandlerImpl(PrivilegedOperationExecutor
      privilegedOperationExecutor, CGroupsHandler cGroupsHandler,
      TrafficController trafficController) {
    this.privilegedOperationExecutor = privilegedOperationExecutor;
    this.cGroupsHandler = cGroupsHandler;
    this.trafficController = trafficController;
    this.containerIdClassIdMap = new ConcurrentHashMap<>();
  }

  /**
   * Bootstrapping 'outbound-bandwidth' resource handler - mounts net_cls
   * controller and bootstraps a traffic control bandwidth shaping hierarchy
   * @param configuration yarn configuration in use
   * @return (potentially empty) list of privileged operations to execute.
   * @throws ResourceHandlerException
   */

  @Override
  public List<PrivilegedOperation> bootstrap(Configuration configuration)
      throws ResourceHandlerException {
    conf = configuration;
    //We'll do this inline for the time being - since this is a one time
    //operation. At some point, LCE code can be refactored to batch mount
    //operations across multiple controllers - cpu, net_cls, blkio etc
    cGroupsHandler
        .initializeCGroupController(CGroupsHandler.CGroupController.NET_CLS);
    device = conf.get(YarnConfiguration.NM_NETWORK_RESOURCE_INTERFACE,
        YarnConfiguration.DEFAULT_NM_NETWORK_RESOURCE_INTERFACE);
    strictMode = configuration.getBoolean(YarnConfiguration
        .NM_LINUX_CONTAINER_CGROUPS_STRICT_RESOURCE_USAGE, YarnConfiguration
        .DEFAULT_NM_LINUX_CONTAINER_CGROUPS_STRICT_RESOURCE_USAGE);
    rootBandwidthMbit = conf.getInt(YarnConfiguration
        .NM_NETWORK_RESOURCE_OUTBOUND_BANDWIDTH_MBIT, YarnConfiguration
        .DEFAULT_NM_NETWORK_RESOURCE_OUTBOUND_BANDWIDTH_MBIT);
    yarnBandwidthMbit = conf.getInt(YarnConfiguration
        .NM_NETWORK_RESOURCE_OUTBOUND_BANDWIDTH_YARN_MBIT, rootBandwidthMbit);
    containerBandwidthMbit = (int) Math.ceil((double) yarnBandwidthMbit /
        MAX_CONTAINER_COUNT);

    StringBuilder logLine = new StringBuilder(""strict mode is set to :"")
        .append(strictMode).append(System.lineSeparator());

    if (strictMode) {
      logLine.append(""container bandwidth will be capped to soft limit."")
          .append(System.lineSeparator());
    } else {
      logLine.append(
          ""containers will be allowed to use spare YARN bandwidth."")
          .append(System.lineSeparator());
    }

    logLine
        .append(""containerBandwidthMbit soft limit (in mbit/sec) is set to : "")
        .append(containerBandwidthMbit);

    LOG.info(logLine.toString());
    trafficController.bootstrap(device, rootBandwidthMbit, yarnBandwidthMbit);

    return null;
  }

  /**
   * Pre-start hook for 'outbound-bandwidth' resource. A cgroup is created
   * and a net_cls classid is generated and written to a cgroup file. A
   * traffic control shaping rule is created in order to limit outbound
   * bandwidth utilization.
   * @param container Container being launched
   * @return privileged operations for some cgroups/tc operations.
   * @throws ResourceHandlerException
   */
  @Override
  public List<PrivilegedOperation> preStart(Container container)
      throws ResourceHandlerException {
    String containerIdStr = container.getContainerId().toString();
    int classId = trafficController.getNextClassId();
    String classIdStr = trafficController.getStringForNetClsClassId(classId);

    cGroupsHandler.createCGroup(CGroupsHandler.CGroupController
            .NET_CLS,
        containerIdStr);
    cGroupsHandler.updateCGroupParam(CGroupsHandler.CGroupController.NET_CLS,
        containerIdStr, CGroupsHandler.CGROUP_PARAM_CLASSID, classIdStr);
    containerIdClassIdMap.put(container.getContainerId(), classId);

    //Now create a privileged operation in order to update the tasks file with
    //the pid of the running container process (root of process tree). This can
    //only be done at the time of launching the container, in a privileged
    //executable.
    String tasksFile = cGroupsHandler.getPathForCGroupTasks(
        CGroupsHandler.CGroupController.NET_CLS, containerIdStr);
    String opArg = new StringBuilder(PrivilegedOperation.CGROUP_ARG_PREFIX)
        .append(tasksFile).toString();
    List<PrivilegedOperation> ops = new ArrayList<>();

    ops.add(new PrivilegedOperation(
        PrivilegedOperation.OperationType.ADD_PID_TO_CGROUP, opArg));

    //Create a privileged operation to create a tc rule for this container
    //We'll return this to the calling (Linux) Container Executor
    //implementation for batching optimizations so that we don't fork/exec
    //additional times during container launch.
    TrafficController.BatchBuilder builder = trafficController.new
        BatchBuilder(PrivilegedOperation.OperationType.TC_MODIFY_STATE);

    builder.addContainerClass(classId, containerBandwidthMbit, strictMode);
    ops.add(builder.commitBatchToTempFile());

    return ops;
  }

  /**
   * Reacquires state for a container - reads the classid from the cgroup
   * being used for the container being reacquired
   * @param containerId if of the container being reacquired.
   * @return (potentially empty) list of privileged operations
   * @throws ResourceHandlerException
   */

  @Override
  public List<PrivilegedOperation> reacquireContainer(ContainerId containerId)
      throws ResourceHandlerException {
    String containerIdStr = containerId.toString();

    LOG.debug(""Attempting to reacquire classId for container: {}"",
        containerIdStr);

    String classIdStrFromFile = cGroupsHandler.getCGroupParam(
        CGroupsHandler.CGroupController.NET_CLS, containerIdStr,
        CGroupsHandler.CGROUP_PARAM_CLASSID);
    int classId = trafficController
        .getClassIdFromFileContents(classIdStrFromFile);

    LOG.info(""Reacquired containerId -> classId mapping: "" + containerIdStr
        + "" -> "" + classId);
    containerIdClassIdMap.put(containerId, classId);

    return null;
  }

  @Override
  public List<PrivilegedOperation> updateContainer(Container container)
      throws ResourceHandlerException {
    return null;
  }

  /**
   * Returns total bytes sent per container to be used for metrics tracking
   * purposes.
   * @return a map of containerId to bytes sent
   * @throws ResourceHandlerException
   */
  public Map<ContainerId, Integer> getBytesSentPerContainer()
      throws ResourceHandlerException {
    Map<Integer, Integer> classIdStats = trafficController.readStats();
    Map<ContainerId, Integer> containerIdStats = new HashMap<>();

    for (Map.Entry<ContainerId, Integer> entry : containerIdClassIdMap
        .entrySet()) {
      ContainerId containerId = entry.getKey();
      Integer classId = entry.getValue();
      Integer bytesSent = classIdStats.get(classId);

      if (bytesSent == null) {
        LOG.warn(""No bytes sent metric found for container: "" + containerId +
            "" with classId: "" + classId);
        continue;
      }
      containerIdStats.put(containerId, bytesSent);
    }

    return containerIdStats;
  }

  /**
   * Cleanup operations once container is completed - deletes cgroup and
   * removes traffic shaping rule(s).
   * @param containerId of the container that was completed.
   * @return null
   * @throws ResourceHandlerException
   */
  @Override
  public List<PrivilegedOperation> postComplete(ContainerId containerId)
      throws ResourceHandlerException {
    LOG.info(""postComplete for container: "" + containerId.toString());
    cGroupsHandler.deleteCGroup(CGroupsHandler.CGroupController.NET_CLS,
        containerId.toString());

    Integer classId = containerIdClassIdMap.get(containerId);

    if (classId != null) {
      PrivilegedOperation op = trafficController.new
          BatchBuilder(PrivilegedOperation.OperationType.TC_MODIFY_STATE)
          .deleteContainerClass(classId).commitBatchToTempFile();

      try {
        privilegedOperationExecutor.executePrivilegedOperation(op, false);
        trafficController.releaseClassId(classId);
      } catch (PrivilegedOperationException e) {
        LOG.warn(""Failed to delete tc rule for classId: "" + classId);
        throw new ResourceHandlerException(
            ""Failed to delete tc rule for classId:"" + classId);
      }
    } else {
      LOG.warn(""Not cleaning up tc rules. classId unknown for container: "" +
          containerId.toString());
    }

    return null;
  }

  @Override
  public List<PrivilegedOperation> teardown()
      throws ResourceHandlerException {
    LOG.debug(""teardown(): Nothing to do"");

    return null;
  }

  @Override
  public String toString() {
    return TrafficControlBandwidthHandlerImpl.class.getName();
  }
}
","['Assertion Roulette', 'Exception Catching Throwing', 'General Fixture', 'Lazy Test']","['Assertion Roulette', 'Magic Number Test', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette', 'Assertion Roulette']",1,3,1,13
42862_54.0_activiti_daylightsavingfallobservedsecondhour,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/42862_test.java,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/42862_actual.java,"/*
 * Copyright 2010-2020 Alfresco Software, Ltd.
 *
 * Licensed under the Apache License, Version 2.0 (the ""License"");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */


package org.activiti.standalone.calendar;

import static org.assertj.core.api.Assertions.assertThat;

import java.text.SimpleDateFormat;
import java.util.Calendar;
import java.util.Date;
import java.util.GregorianCalendar;
import java.util.TimeZone;

import org.activiti.engine.impl.calendar.DurationHelper;
import org.activiti.engine.impl.util.DefaultClockImpl;
import org.activiti.engine.runtime.Clock;
import org.apache.commons.lang3.time.DateUtils;
import org.junit.Test;

public class DurationHelperTest {

    @Test
    public void shouldNotExceedNumber() throws Exception {
        Clock testingClock = new DefaultClockImpl();
        testingClock.setCurrentTime(new Date(0));
        DurationHelper dh = new DurationHelper(""R2/PT10S"", testingClock);

        testingClock.setCurrentTime(new Date(15000));
        assertThat(dh.getDateAfter().getTime()).isEqualTo(20000);

        testingClock.setCurrentTime(new Date(30000));
        assertThat(dh.getDateAfter().getTime()).isEqualTo(30000);
    }

    @Test
    public void shouldNotExceedNumberPeriods() throws Exception {
        Clock testingClock = new DefaultClockImpl();
        testingClock.setCurrentTime(parse(""19700101-00:00:00""));
        DurationHelper dh = new DurationHelper(""R2/1970-01-01T00:00:00/1970-01-01T00:00:10"", testingClock);

        testingClock.setCurrentTime(parse(""19700101-00:00:15""));
        assertThat(dh.getDateAfter()).isEqualTo(parse(""19700101-00:00:20""));

        testingClock.setCurrentTime(parse(""19700101-00:00:30""));
        assertThat(dh.getDateAfter()).isEqualTo(parse(""19700101-00:00:30""));
    }

    @Test
    public void shouldNotExceedNumberNegative() throws Exception {
        Clock testingClock = new DefaultClockImpl();
        testingClock.setCurrentTime(parse(""19700101-00:00:00""));
        DurationHelper dh = new DurationHelper(""R2/PT10S/1970-01-01T00:00:50"", testingClock);

        testingClock.setCurrentTime(parse(""19700101-00:00:20""));
        assertThat(dh.getDateAfter()).isEqualTo(parse(""19700101-00:00:30""));

        testingClock.setCurrentTime(parse(""19700101-00:00:35""));

        assertThat(dh.getDateAfter()).isEqualTo(parse(""19700101-00:00:35""));
    }

    @Test
    public void daylightSavingFall() throws Exception {
        Clock testingClock = new DefaultClockImpl();
        testingClock.setCurrentCalendar(parseCalendar(""20131103-04:45:00"", TimeZone.getTimeZone(""UTC"")));

        DurationHelper dh = new DurationHelper(""R2/2013-11-03T00:45:00-04:00/PT1H"", testingClock);

        assertThat(dh.getCalendarAfter(testingClock.getCurrentCalendar(TimeZone.getTimeZone(""US/Eastern"")))).isEqualTo(parseCalendar(""20131103-05:45:00"", TimeZone.getTimeZone(""UTC"")));

        testingClock.setCurrentCalendar(parseCalendar(""20131103-05:45:00"", TimeZone.getTimeZone(""UTC"")));

        assertThat(dh.getCalendarAfter(testingClock.getCurrentCalendar(TimeZone.getTimeZone(""US/Eastern"")))).isEqualTo(parseCalendar(""20131103-06:45:00"", TimeZone.getTimeZone(""UTC"")));
    }

    @Test
    public void daylightSavingFallFirstHour() throws Exception {
        Clock testingClock = new DefaultClockImpl();
        testingClock.setCurrentCalendar(parseCalendar(""20131103-05:45:00"", TimeZone.getTimeZone(""UTC"")));
        Calendar easternTime = testingClock.getCurrentCalendar(TimeZone.getTimeZone(""US/Eastern""));

        DurationHelper dh = new DurationHelper(""R2/2013-11-03T01:45:00-04:00/PT1H"", testingClock);

        assertThat(dh.getCalendarAfter(easternTime)).isEqualTo(parseCalendar(""20131103-06:45:00"", TimeZone.getTimeZone(""UTC"")));
    }

    @Test
    public void daylightSavingFallSecondHour() throws Exception {
        Clock testingClock = new DefaultClockImpl();
        testingClock.setCurrentCalendar(parseCalendar(""20131103-06:45:00"", TimeZone.getTimeZone(""UTC"")));
        Calendar easternTime = testingClock.getCurrentCalendar(TimeZone.getTimeZone(""US/Eastern""));

        DurationHelper dh = new DurationHelper(""R2/2013-11-03T01:45:00-05:00/PT1H"", testingClock);

        assertThat(dh.getCalendarAfter(easternTime)).isEqualTo(parseCalendar(""20131103-07:45:00"", TimeZone.getTimeZone(""UTC"")));
    }

    @Test
    public void daylightSavingFallObservedFirstHour() throws Exception {
        Clock testingClock = new DefaultClockImpl();
        testingClock.setCurrentCalendar(parseCalendar(""20131103-00:45:00"", TimeZone.getTimeZone(""US/Eastern"")));

        DurationHelper dh = new DurationHelper(""R2/2013-11-03T00:45:00-04:00/PT1H"", testingClock);
        Calendar expected = parseCalendarWithOffset(""20131103-01:45:00 -04:00"");

        assertThat(expected.compareTo(dh.getCalendarAfter()) == 0).isTrue();
    }

    @Test
    public void daylightSavingFallObservedSecondHour() throws Exception {
        Clock testingClock = new DefaultClockImpl();
        testingClock.setCurrentCalendar(parseCalendar(""20131103-00:45:00"", TimeZone.getTimeZone(""US/Eastern"")));

        DurationHelper dh = new DurationHelper(""R2/2013-11-03T00:45:00-04:00/PT2H"", testingClock);
        Calendar expected = parseCalendarWithOffset(""20131103-01:45:00 -05:00"");

        assertThat(expected.compareTo(dh.getCalendarAfter()) == 0).isTrue();
    }

    @Test
    public void daylightSavingSpring() throws Exception {
        Clock testingClock = new DefaultClockImpl();
        testingClock.setCurrentCalendar(parseCalendar(""20140309-05:45:00"", TimeZone.getTimeZone(""UTC"")));

        DurationHelper dh = new DurationHelper(""R2/2014-03-09T00:45:00-05:00/PT1H"", testingClock);

        assertThat(dh.getCalendarAfter(testingClock.getCurrentCalendar(TimeZone.getTimeZone(""US/Eastern"")))).isEqualTo(parseCalendar(""20140309-06:45:00"", TimeZone.getTimeZone(""UTC"")));
    }

    @Test
    public void daylightSavingSpringObserved() throws Exception {
        Clock testingClock = new DefaultClockImpl();
        testingClock.setCurrentCalendar(parseCalendar(""20140309-01:45:00"", TimeZone.getTimeZone(""US/Eastern"")));

        DurationHelper dh = new DurationHelper(""R2/2014-03-09T01:45:00/PT1H"", testingClock);
        Calendar expected = parseCalendar(""20140309-03:45:00"", TimeZone.getTimeZone(""US/Eastern""));

        assertThat(dh.getCalendarAfter()).isEqualTo(expected);
    }

    @Test
    public void daylightSaving25HourDay() throws Exception {
        Clock testingClock = new DefaultClockImpl();
        testingClock.setCurrentCalendar(parseCalendar(""20131103-00:00:00"", TimeZone.getTimeZone(""US/Eastern"")));

        DurationHelper dh = new DurationHelper(""R2/2013-11-03T00:00:00/P1D"", testingClock);

        assertThat(dh.getCalendarAfter(testingClock.getCurrentCalendar())).isEqualTo(parseCalendar(""20131104-00:00:00"", TimeZone.getTimeZone(""US/Eastern"")));
    }

    @Test
    public void daylightSaving23HourDay() throws Exception {
        Clock testingClock = new DefaultClockImpl();
        testingClock.setCurrentCalendar(parseCalendar(""20140309-00:00:00"", TimeZone.getTimeZone(""US/Eastern"")));

        DurationHelper dh = new DurationHelper(""R2/2014-03-09T00:00:00/P1D"", testingClock);

        assertThat(dh.getCalendarAfter(testingClock.getCurrentCalendar())).isEqualTo(parseCalendar(""20140310-00:00:00"", TimeZone.getTimeZone(""US/Eastern"")));
    }

    @Test
    public void daylightSaving25HourDayEurope() throws Exception {
        Clock testingClock = new DefaultClockImpl();
        testingClock.setCurrentCalendar(parseCalendar(""20131027-00:00:00"", TimeZone.getTimeZone(""Europe/Amsterdam"")));

        DurationHelper dh = new DurationHelper(""R2/2013-10-27T00:00:00/P1D"", testingClock);

        assertThat(dh.getCalendarAfter(testingClock.getCurrentCalendar())).isEqualTo(parseCalendar(""20131028-00:00:00"", TimeZone.getTimeZone(""Europe/Amsterdam"")));
    }

    @Test
    public void daylightSaving23HourDayEurope() throws Exception {
        Clock testingClock = new DefaultClockImpl();
        testingClock.setCurrentCalendar(parseCalendar(""20140330-00:00:00"", TimeZone.getTimeZone(""Europe/Amsterdam"")));

        DurationHelper dh = new DurationHelper(""R2/2014-03-30T00:00:00/P1D"", testingClock);

        assertThat(dh.getCalendarAfter(testingClock.getCurrentCalendar())).isEqualTo(parseCalendar(""20140331-00:00:00"", TimeZone.getTimeZone(""Europe/Amsterdam"")));
    }

    private Date parse(String str) throws Exception {
        SimpleDateFormat simpleDateFormat = new SimpleDateFormat(""yyyyMMdd-HH:mm:ss"");
        return simpleDateFormat.parse(str);
    }

    private Calendar parseCalendarWithOffset(String str) throws Exception {

        Calendar cal = Calendar.getInstance();
        cal.setTime(DateUtils.parseDate(str, ""yyyyMMdd-HH:mm:ss ZZ""));
        return cal;
    }

    private Calendar parseCalendar(String str, TimeZone timeZone) throws Exception {
        return parseCalendar(str, timeZone, ""yyyyMMdd-HH:mm:ss"");
    }

    private Calendar parseCalendar(String str, TimeZone timeZone, String format) throws Exception {
        Calendar date = new GregorianCalendar(timeZone);
        SimpleDateFormat simpleDateFormat = new SimpleDateFormat(format);
        simpleDateFormat.setTimeZone(timeZone);
        date.setTime(simpleDateFormat.parse(str));
        return date;
    }
}
","/*
 * Copyright 2010-2020 Alfresco Software, Ltd.
 *
 * Licensed under the Apache License, Version 2.0 (the ""License"");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.activiti.engine.impl.calendar;

import static java.util.Arrays.asList;

import java.text.DateFormat;
import java.text.SimpleDateFormat;
import java.util.Calendar;
import java.util.Date;
import java.util.GregorianCalendar;
import java.util.List;
import java.util.Locale;

import javax.xml.datatype.DatatypeFactory;
import javax.xml.datatype.Duration;

import org.activiti.engine.ActivitiIllegalArgumentException;
import org.activiti.engine.api.internal.Internal;
import org.activiti.engine.impl.util.TimeZoneUtil;
import org.activiti.engine.runtime.ClockReader;
import org.joda.time.DateTimeZone;
import org.joda.time.format.ISODateTimeFormat;

@Internal
public class DurationHelper {

    private Calendar start;
    private Calendar end;
    private Duration period;
    private boolean isRepeat;
    private int times;
    private int maxIterations = -1;
    private boolean repeatWithNoBounds;

    private DatatypeFactory datatypeFactory;

    public Calendar getStart() {
        return start;
    }

    public Calendar getEnd() {
        return end;
    }

    public Duration getPeriod() {
        return period;
    }

    public boolean isRepeat() {
        return isRepeat;
    }

    public int getTimes() {
        return times;
    }

    protected ClockReader clockReader;

    public DurationHelper(String expressionS,
                          int maxIterations,
                          ClockReader clockReader) throws Exception {
        this.clockReader = clockReader;
        this.maxIterations = maxIterations;
        List<String> expression = asList(expressionS.split(""/""));
        datatypeFactory = DatatypeFactory.newInstance();

        if (expression.size() > 3 || expression.isEmpty()) {
            throw new ActivitiIllegalArgumentException(""Cannot parse duration"");
        }
        if (expression.get(0).startsWith(""R"")) {
            isRepeat = true;
            times = expression.get(0).length() == 1 ? Integer.MAX_VALUE - 1 : Integer.parseInt(expression.get(0).substring(1));

            if (expression.get(0).equals(""R"")) { // R without params
                repeatWithNoBounds = true;
            }

            expression = expression.subList(1,
                                            expression.size());
        }

        if (isDuration(expression.get(0))) {
            period = parsePeriod(expression.get(0));
            end = expression.size() == 1 ? null : parseDate(expression.get(1));
        } else {
            start = parseDate(expression.get(0));
            if (isDuration(expression.get(1))) {
                period = parsePeriod(expression.get(1));
            } else {
                end = parseDate(expression.get(1));
                period = datatypeFactory.newDuration(end.getTimeInMillis() - start.getTimeInMillis());
            }
        }
        if (start == null) {
            start = clockReader.getCurrentCalendar();
        }
    }

    public DurationHelper(String expressionS,
                          ClockReader clockReader) throws Exception {
        this(expressionS,
             -1,
             clockReader);
    }

    public Calendar getCalendarAfter() {
        return getCalendarAfter(clockReader.getCurrentCalendar());
    }

    public Calendar getCalendarAfter(Calendar time) {
        if (isRepeat) {
            return getDateAfterRepeat(time);
        }
        // TODO: is this correct?
        if (end != null) {
            return end;
        }
        return add(start,
                   period);
    }

    public Boolean isValidDate(Date newTimer) {
        return end == null || end.getTime().after(newTimer) || end.getTime().equals(newTimer);
    }

    public Date getDateAfter() {
        Calendar date = getCalendarAfter();

        return date == null ? null : date.getTime();
    }

    private Calendar getDateAfterRepeat(Calendar date) {
        Calendar current = TimeZoneUtil.convertToTimeZone(start,
                                                          date.getTimeZone());

        if (repeatWithNoBounds) {

            while (current.before(date) || current.equals(date)) { // As long as current date is not past the engine date, we keep looping
                Calendar newTime = add(current,
                                       period);
                if (newTime.equals(current) || newTime.before(current)) {
                    break;
                }
                current = newTime;
            }
        } else {

            int maxLoops = times;
            if (maxIterations > 0) {
                maxLoops = maxIterations - times;
            }
            for (int i = 0; i < maxLoops + 1 && !current.after(date); i++) {
                current = add(current,
                              period);
            }
        }
        return current.before(date) ? date : TimeZoneUtil.convertToTimeZone(current,
                                                                            clockReader.getCurrentTimeZone());
    }

    protected Calendar add(Calendar date,
                           Duration duration) {
        Calendar calendar = (Calendar) date.clone();

        // duration.addTo does not account for daylight saving time (xerces),
        // reversing order of addition fixes the problem
        calendar.add(Calendar.SECOND,
                     duration.getSeconds() * duration.getSign());
        calendar.add(Calendar.MINUTE,
                     duration.getMinutes() * duration.getSign());
        calendar.add(Calendar.HOUR,
                     duration.getHours() * duration.getSign());
        calendar.add(Calendar.DAY_OF_MONTH,
                     duration.getDays() * duration.getSign());
        calendar.add(Calendar.MONTH,
                     duration.getMonths() * duration.getSign());
        calendar.add(Calendar.YEAR,
                     duration.getYears() * duration.getSign());

        return calendar;
    }

    protected Calendar parseDate(String date) throws Exception {
        Calendar dateCalendar = null;
        try {
            dateCalendar = ISODateTimeFormat.dateTimeParser().withZone(DateTimeZone.forTimeZone(
                    clockReader.getCurrentTimeZone())).parseDateTime(date).toCalendar(null);
        } catch (IllegalArgumentException e) {
            // try to parse a java.util.date to string back to a java.util.date
            dateCalendar = new GregorianCalendar();
            DateFormat DATE_FORMAT = new SimpleDateFormat(""EEE MMM dd kk:mm:ss z yyyy"",
                                                          Locale.ENGLISH);
            dateCalendar.setTime(DATE_FORMAT.parse(date));
        }

        return dateCalendar;
    }

    protected Duration parsePeriod(String period) throws Exception {
        return datatypeFactory.newDuration(period);
    }

    protected boolean isDuration(String time) {
        return time.startsWith(""P"");
    }
}
","['Assertion Roulette', 'Eager Test', 'Lazy Test']","['Assertion Roulette', 'Eager Test']",0,1,2,14
16901_21_dubbo_testgetcustomport,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/16901_test.java,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/16901_actual.java,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the ""License""); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.dubbo.registry.multicast;

import org.apache.dubbo.common.URL;
import org.apache.dubbo.common.utils.NetUtils;
import org.apache.dubbo.registry.NotifyListener;

import java.net.InetAddress;
import java.net.MulticastSocket;
import java.net.UnknownHostException;
import java.util.List;
import java.util.Map;
import java.util.Random;
import java.util.Set;

import org.junit.jupiter.api.Assertions;
import org.junit.jupiter.api.BeforeEach;
import org.junit.jupiter.api.Test;

import static org.apache.dubbo.common.constants.RegistryConstants.EMPTY_PROTOCOL;
import static org.hamcrest.CoreMatchers.is;
import static org.hamcrest.MatcherAssert.assertThat;
import static org.junit.jupiter.api.Assertions.assertEquals;
import static org.junit.jupiter.api.Assertions.assertFalse;
import static org.junit.jupiter.api.Assertions.assertTrue;

class MulticastRegistryTest {

    private String service = ""org.apache.dubbo.test.injvmServie"";
    private URL registryUrl = URL.valueOf(""multicast://239.239.239.239/"");
    private URL serviceUrl = URL.valueOf(""dubbo://"" + NetUtils.getLocalHost() + ""/"" + service + ""?methods=test1,test2"");
    private URL adminUrl = URL.valueOf(""dubbo://"" + NetUtils.getLocalHost() + ""/*"");
    private URL consumerUrl = URL.valueOf(""subscribe://"" + NetUtils.getLocalHost() + ""/"" + service + ""?arg1=1&arg2=2"");
    private MulticastRegistry registry = new MulticastRegistry(registryUrl);

    @BeforeEach
    void setUp() {
        registry.register(serviceUrl);
    }

    /**
     * Test method for {@link org.apache.dubbo.registry.multicast.MulticastRegistry#MulticastRegistry(URL)}.
     */
    @Test
    void testUrlError() {
        Assertions.assertThrows(UnknownHostException.class, () -> {
            try {
                URL errorUrl = URL.valueOf(""multicast://mullticast.local/"");
                new MulticastRegistry(errorUrl);
            } catch (IllegalStateException e) {
                throw e.getCause();
            }
        });
    }

    /**
     * Test method for {@link org.apache.dubbo.registry.multicast.MulticastRegistry#MulticastRegistry(URL)}.
     */
    @Test
    void testAnyHost() {
        Assertions.assertThrows(IllegalStateException.class, () -> {
            URL errorUrl = URL.valueOf(""multicast://0.0.0.0/"");
            new MulticastRegistry(errorUrl);
        });
    }

    /**
     * Test method for {@link org.apache.dubbo.registry.multicast.MulticastRegistry#MulticastRegistry(URL)}.
     */
    @Test
    void testGetCustomPort() {
        int port = NetUtils.getAvailablePort(20880 + new Random().nextInt(10000));
        URL customPortUrl = URL.valueOf(""multicast://239.239.239.239:"" + port);
        MulticastRegistry multicastRegistry = new MulticastRegistry(customPortUrl);
        assertThat(multicastRegistry.getUrl().getPort(), is(port));
    }

    /**
     * Test method for {@link org.apache.dubbo.registry.multicast.MulticastRegistry#getRegistered()}.
     */
    @Test
    void testRegister() {
        Set<URL> registered;
        // clear first
        registered = registry.getRegistered();
        for (URL url : registered) {
            registry.unregister(url);
        }

        for (int i = 0; i < 2; i++) {
            registry.register(serviceUrl);
            registered = registry.getRegistered();
            assertTrue(registered.contains(serviceUrl));
        }
        // confirm only 1 register success
        registered = registry.getRegistered();
        assertEquals(1, registered.size());
    }

    /**
     * Test method for {@link org.apache.dubbo.registry.multicast.MulticastRegistry#unregister(URL)}.
     */
    @Test
    void testUnregister() {
        Set<URL> registered;

        // register first
        registry.register(serviceUrl);
        registered = registry.getRegistered();
        assertTrue(registered.contains(serviceUrl));

        // then unregister
        registered = registry.getRegistered();
        registry.unregister(serviceUrl);
        assertFalse(registered.contains(serviceUrl));
    }

    /**
     * Test method for
     * {@link org.apache.dubbo.registry.multicast.MulticastRegistry#subscribe(URL url, org.apache.dubbo.registry.NotifyListener)}
     * .
     */
    @Test
    void testSubscribe() {
        // verify listener
        final URL[] notifyUrl = new URL[1];
        for (int i = 0; i < 10; i++) {
            registry.register(serviceUrl);
            registry.subscribe(consumerUrl, urls -> {
                notifyUrl[0] = urls.get(0);

                Map<URL, Set<NotifyListener>> subscribed = registry.getSubscribed();
                assertEquals(consumerUrl, subscribed.keySet().iterator().next());
            });
            if (!EMPTY_PROTOCOL.equalsIgnoreCase(notifyUrl[0].getProtocol())) {
                break;
            }
        }
        assertEquals(serviceUrl.toFullString(), notifyUrl[0].toFullString());
    }

    /**
     * Test method for {@link org.apache.dubbo.registry.multicast.MulticastRegistry#unsubscribe(URL, NotifyListener)}
     */
    @Test
    void testUnsubscribe() {
        // subscribe first
        registry.subscribe(consumerUrl, new NotifyListener() {
            @Override
            public void notify(List<URL> urls) {
                // do nothing
            }
        });

        // then unsubscribe
        registry.unsubscribe(consumerUrl, new NotifyListener() {
            @Override
            public void notify(List<URL> urls) {
                Map<URL, Set<NotifyListener>> subscribed = registry.getSubscribed();
                Set<NotifyListener> listeners = subscribed.get(consumerUrl);
                assertTrue(listeners.isEmpty());

                Map<URL, Set<URL>> received = registry.getReceived();
                assertTrue(received.get(consumerUrl).isEmpty());
            }
        });
    }

    /**
     * Test method for {@link MulticastRegistry#isAvailable()}
     */
    @Test
    void testAvailability() {
        int port = NetUtils.getAvailablePort(20880 + new Random().nextInt(10000));
        MulticastRegistry registry = new MulticastRegistry(URL.valueOf(""multicast://224.5.6.8:"" + port));
        assertTrue(registry.isAvailable());
    }

    /**
     * Test method for {@link MulticastRegistry#destroy()}
     */
    @Test
    void testDestroy() {
        MulticastSocket socket = registry.getMulticastSocket();
        assertFalse(socket.isClosed());

        // then destroy, the multicast socket will be closed
        registry.destroy();
        socket = registry.getMulticastSocket();
        assertTrue(socket.isClosed());
    }

    /**
     * Test method for {@link org.apache.dubbo.registry.multicast.MulticastRegistry#MulticastRegistry(URL)}
     */
    @Test
    void testDefaultPort() {
        MulticastRegistry multicastRegistry = new MulticastRegistry(URL.valueOf(""multicast://224.5.6.7""));
        try {
            MulticastSocket multicastSocket = multicastRegistry.getMulticastSocket();
            Assertions.assertEquals(1234, multicastSocket.getLocalPort());
        } finally {
            multicastRegistry.destroy();
        }
    }

    /**
     * Test method for {@link org.apache.dubbo.registry.multicast.MulticastRegistry#MulticastRegistry(URL)}
     */
    @Test
    void testCustomedPort() {
        int port = NetUtils.getAvailablePort(20880 + new Random().nextInt(10000));
        MulticastRegistry multicastRegistry = new MulticastRegistry(URL.valueOf(""multicast://224.5.6.7:"" + port));
        try {
            MulticastSocket multicastSocket = multicastRegistry.getMulticastSocket();
            assertEquals(port, multicastSocket.getLocalPort());
        } finally {
            multicastRegistry.destroy();
        }
    }

    @Test
    void testMulticastAddress() {
        InetAddress multicastAddress = null;
        MulticastSocket multicastSocket = null;
        try {
            // ipv4 multicast address
            multicastAddress = InetAddress.getByName(""224.55.66.77"");
            multicastSocket = new MulticastSocket(2345);
            multicastSocket.setLoopbackMode(false);
            NetUtils.setInterface(multicastSocket, false);
            multicastSocket.joinGroup(multicastAddress);
        } catch (Exception e) {
            Assertions.fail(e);
        } finally {
            if (multicastSocket != null) {
                multicastSocket.close();
            }
        }

        // multicast ipv6 address,
        try {
            multicastAddress = InetAddress.getByName(""ff01::1"");

            multicastSocket = new MulticastSocket();
            multicastSocket.setLoopbackMode(false);
            NetUtils.setInterface(multicastSocket, true);
            multicastSocket.joinGroup(multicastAddress);
        } catch (Throwable t) {
            t.printStackTrace();
        } finally {
            if (multicastSocket != null) {
                multicastSocket.close();
            }
        }
    }
}
","/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the ""License""); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.dubbo.registry.multicast;

import org.apache.dubbo.common.URL;
import org.apache.dubbo.common.logger.ErrorTypeAwareLogger;
import org.apache.dubbo.common.logger.LoggerFactory;
import org.apache.dubbo.common.utils.CollectionUtils;
import org.apache.dubbo.common.utils.ConcurrentHashSet;
import org.apache.dubbo.common.utils.ExecutorUtil;
import org.apache.dubbo.common.utils.NamedThreadFactory;
import org.apache.dubbo.common.utils.NetUtils;
import org.apache.dubbo.common.utils.UrlUtils;
import org.apache.dubbo.registry.NotifyListener;
import org.apache.dubbo.registry.support.FailbackRegistry;
import org.apache.dubbo.rpc.model.ApplicationModel;

import java.io.IOException;
import java.net.DatagramPacket;
import java.net.Inet4Address;
import java.net.InetAddress;
import java.net.InetSocketAddress;
import java.net.MulticastSocket;
import java.net.Socket;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashSet;
import java.util.List;
import java.util.Map;
import java.util.Set;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.ConcurrentMap;
import java.util.concurrent.Executors;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.ScheduledFuture;
import java.util.concurrent.TimeUnit;

import static org.apache.dubbo.common.constants.CommonConstants.ANY_VALUE;
import static org.apache.dubbo.common.constants.CommonConstants.DEFAULT_TIMEOUT;
import static org.apache.dubbo.common.constants.CommonConstants.TIMEOUT_KEY;
import static org.apache.dubbo.common.constants.LoggerCodeConstants.REGISTRY_SOCKET_EXCEPTION;
import static org.apache.dubbo.common.constants.RegistryConstants.DYNAMIC_KEY;
import static org.apache.dubbo.common.constants.RegistryConstants.EMPTY_PROTOCOL;
import static org.apache.dubbo.common.constants.RegistryConstants.OVERRIDE_PROTOCOL;
import static org.apache.dubbo.common.constants.RegistryConstants.ROUTE_PROTOCOL;
import static org.apache.dubbo.registry.Constants.CONSUMER_PROTOCOL;
import static org.apache.dubbo.registry.Constants.DEFAULT_SESSION_TIMEOUT;
import static org.apache.dubbo.registry.Constants.REGISTER;
import static org.apache.dubbo.registry.Constants.REGISTER_KEY;
import static org.apache.dubbo.registry.Constants.SESSION_TIMEOUT_KEY;
import static org.apache.dubbo.registry.Constants.SUBSCRIBE;
import static org.apache.dubbo.registry.Constants.UNREGISTER;
import static org.apache.dubbo.registry.Constants.UNSUBSCRIBE;

/**
 * MulticastRegistry
 */
public class MulticastRegistry extends FailbackRegistry {

    // logging output
    private static final ErrorTypeAwareLogger logger = LoggerFactory.getErrorTypeAwareLogger(MulticastRegistry.class);

    private static final int DEFAULT_MULTICAST_PORT = 1234;

    private final InetAddress multicastAddress;

    private final MulticastSocket multicastSocket;

    private final int multicastPort;

    private final ConcurrentMap<URL, Set<URL>> received = new ConcurrentHashMap<>();

    private final ScheduledExecutorService cleanExecutor =
            Executors.newScheduledThreadPool(1, new NamedThreadFactory(""DubboMulticastRegistryCleanTimer"", true));

    private final ScheduledFuture<?> cleanFuture;

    private final int cleanPeriod;

    private final ApplicationModel applicationModel;

    private volatile boolean admin = false;

    public MulticastRegistry(URL url, ApplicationModel applicationModel) {
        super(url);
        this.applicationModel = applicationModel;
        if (url.isAnyHost()) {
            throw new IllegalStateException(""registry address == null"");
        }
        try {
            multicastAddress = InetAddress.getByName(url.getHost());
            checkMulticastAddress(multicastAddress);

            multicastPort = url.getPort() <= 0 ? DEFAULT_MULTICAST_PORT : url.getPort();
            multicastSocket = new MulticastSocket(multicastPort);
            NetUtils.joinMulticastGroup(multicastSocket, multicastAddress);
            Thread thread = new Thread(
                    () -> {
                        byte[] buf = new byte[2048];
                        DatagramPacket recv = new DatagramPacket(buf, buf.length);
                        while (!multicastSocket.isClosed()) {
                            try {
                                multicastSocket.receive(recv);
                                String msg = new String(recv.getData()).trim();
                                int i = msg.indexOf('\n');
                                if (i > 0) {
                                    msg = msg.substring(0, i).trim();
                                }
                                receive(msg, (InetSocketAddress) recv.getSocketAddress());
                                Arrays.fill(buf, (byte) 0);
                            } catch (Throwable e) {
                                if (!multicastSocket.isClosed()) {
                                    logger.error(REGISTRY_SOCKET_EXCEPTION, """", """", e.getMessage(), e);
                                }
                            }
                        }
                    },
                    ""DubboMulticastRegistryReceiver"");
            thread.setDaemon(true);
            thread.start();
        } catch (IOException e) {
            throw new IllegalStateException(e.getMessage(), e);
        }
        this.cleanPeriod = url.getParameter(SESSION_TIMEOUT_KEY, DEFAULT_SESSION_TIMEOUT);
        if (url.getParameter(""clean"", true)) {
            this.cleanFuture = cleanExecutor.scheduleWithFixedDelay(
                    () -> {
                        try {
                            clean(); // Remove the expired
                        } catch (Throwable t) { // Defensive fault tolerance
                            logger.error(
                                    REGISTRY_SOCKET_EXCEPTION,
                                    """",
                                    """",
                                    ""Unexpected exception occur at clean expired provider, cause: "" + t.getMessage(),
                                    t);
                        }
                    },
                    cleanPeriod,
                    cleanPeriod,
                    TimeUnit.MILLISECONDS);
        } else {
            this.cleanFuture = null;
        }
    }

    public MulticastRegistry(URL url) {
        this(url, url.getOrDefaultApplicationModel());
    }

    private void checkMulticastAddress(InetAddress multicastAddress) {
        if (!multicastAddress.isMulticastAddress()) {
            String message = ""Invalid multicast address "" + multicastAddress;
            if (multicastAddress instanceof Inet4Address) {
                throw new IllegalArgumentException(
                        message + "", "" + ""ipv4 multicast address scope: 224.0.0.0 - 239.255.255.255."");
            } else {
                throw new IllegalArgumentException(
                        message + "", "" + ""ipv6 multicast address must start with ff, "" + ""for example: ff01::1"");
            }
        }
    }

    /**
     * Remove the expired providers, only when ""clean"" parameter is true.
     */
    private void clean() {
        if (admin) {
            for (Set<URL> providers : new HashSet<Set<URL>>(received.values())) {
                for (URL url : new HashSet<URL>(providers)) {
                    if (isExpired(url)) {
                        if (logger.isWarnEnabled()) {
                            logger.warn(REGISTRY_SOCKET_EXCEPTION, """", """", ""Clean expired provider "" + url);
                        }
                        doUnregister(url);
                    }
                }
            }
        }
    }

    private boolean isExpired(URL url) {
        if (!url.getParameter(DYNAMIC_KEY, true)
                || url.getPort() <= 0
                || CONSUMER_PROTOCOL.equals(url.getProtocol())
                || ROUTE_PROTOCOL.equals(url.getProtocol())
                || OVERRIDE_PROTOCOL.equals(url.getProtocol())) {
            return false;
        }
        try (Socket socket = new Socket(url.getHost(), url.getPort())) {
        } catch (Throwable e) {
            try {
                Thread.sleep(100);
            } catch (Throwable e2) {
            }
            try (Socket socket2 = new Socket(url.getHost(), url.getPort())) {
            } catch (Throwable e2) {
                return true;
            }
        }
        return false;
    }

    private void receive(String msg, InetSocketAddress remoteAddress) {
        if (logger.isInfoEnabled()) {
            logger.info(""Receive multicast message: "" + msg + "" from "" + remoteAddress);
        }
        if (applicationModel.isDestroyed()) {
            logger.info(""The applicationModel is destroyed, skip"");
            return;
        }
        if (msg.startsWith(REGISTER)) {
            URL url = URL.valueOf(msg.substring(REGISTER.length()).trim());
            registered(url);
        } else if (msg.startsWith(UNREGISTER)) {
            URL url = URL.valueOf(msg.substring(UNREGISTER.length()).trim());
            unregistered(url);
        } else if (msg.startsWith(SUBSCRIBE)) {
            URL url = URL.valueOf(msg.substring(SUBSCRIBE.length()).trim());
            Set<URL> urls = getRegistered();
            if (CollectionUtils.isNotEmpty(urls)) {
                for (URL u : urls) {
                    if (UrlUtils.isMatch(url, u)) {
                        String host = remoteAddress != null && remoteAddress.getAddress() != null
                                ? remoteAddress.getAddress().getHostAddress()
                                : url.getIp();
                        if (url.getParameter(""unicast"", true) // Whether the consumer's machine has only one process
                                && !NetUtils.getLocalHost()
                                        .equals(host)) { // Multiple processes in the same machine cannot be unicast
                            // with unicast or there will be only one process receiving
                            // information
                            unicast(REGISTER + "" "" + u.toFullString(), host);
                        } else {
                            multicast(REGISTER + "" "" + u.toFullString());
                        }
                    }
                }
            }
        } /* else if (msg.startsWith(UNSUBSCRIBE)) {
          }*/
    }

    private void multicast(String msg) {
        if (logger.isInfoEnabled()) {
            logger.info(""Send multicast message: "" + msg + "" to "" + multicastAddress + "":"" + multicastPort);
        }
        try {
            byte[] data = (msg + ""\n"").getBytes();
            DatagramPacket hi = new DatagramPacket(data, data.length, multicastAddress, multicastPort);
            multicastSocket.send(hi);
        } catch (Exception e) {
            throw new IllegalStateException(e.getMessage(), e);
        }
    }

    private void unicast(String msg, String host) {
        if (logger.isInfoEnabled()) {
            logger.info(""Send unicast message: "" + msg + "" to "" + host + "":"" + multicastPort);
        }
        try {
            byte[] data = (msg + ""\n"").getBytes();
            DatagramPacket hi = new DatagramPacket(data, data.length, InetAddress.getByName(host), multicastPort);
            multicastSocket.send(hi);
        } catch (Exception e) {
            throw new IllegalStateException(e.getMessage(), e);
        }
    }

    @Override
    public void doRegister(URL url) {
        multicast(REGISTER + "" "" + url.toFullString());
    }

    @Override
    public void doUnregister(URL url) {
        multicast(UNREGISTER + "" "" + url.toFullString());
    }

    @Override
    public void doSubscribe(URL url, final NotifyListener listener) {
        if (ANY_VALUE.equals(url.getServiceInterface())) {
            admin = true;
        }
        multicast(SUBSCRIBE + "" "" + url.toFullString());
        synchronized (listener) {
            try {
                listener.wait(url.getParameter(TIMEOUT_KEY, DEFAULT_TIMEOUT));
            } catch (InterruptedException e) {
            }
        }
    }

    @Override
    public void doUnsubscribe(URL url, NotifyListener listener) {
        if (!ANY_VALUE.equals(url.getServiceInterface()) && url.getParameter(REGISTER_KEY, true)) {
            unregister(url);
        }
        multicast(UNSUBSCRIBE + "" "" + url.toFullString());
    }

    @Override
    public boolean isAvailable() {
        try {
            return multicastSocket != null;
        } catch (Throwable t) {
            return false;
        }
    }

    /**
     * Remove the expired providers(if clean is true), leave the multicast group and close the multicast socket.
     */
    @Override
    public void destroy() {
        super.destroy();
        try {
            ExecutorUtil.cancelScheduledFuture(cleanFuture);
        } catch (Throwable t) {
            logger.warn(REGISTRY_SOCKET_EXCEPTION, """", """", t.getMessage(), t);
        }
        try {
            multicastSocket.leaveGroup(multicastAddress);
            multicastSocket.close();
        } catch (Throwable t) {
            logger.warn(REGISTRY_SOCKET_EXCEPTION, """", """", t.getMessage(), t);
        }
        ExecutorUtil.gracefulShutdown(cleanExecutor, cleanPeriod);
    }

    protected void registered(URL url) {
        for (Map.Entry<URL, Set<NotifyListener>> entry : getSubscribed().entrySet()) {
            URL key = entry.getKey();
            if (UrlUtils.isMatch(key, url)) {
                Set<URL> urls = received.computeIfAbsent(key, k -> new ConcurrentHashSet<>());
                urls.add(url);
                List<URL> list = toList(urls);
                for (final NotifyListener listener : entry.getValue()) {
                    notify(key, listener, list);
                    synchronized (listener) {
                        listener.notify();
                    }
                }
            }
        }
    }

    protected void unregistered(URL url) {
        for (Map.Entry<URL, Set<NotifyListener>> entry : getSubscribed().entrySet()) {
            URL key = entry.getKey();
            if (UrlUtils.isMatch(key, url)) {
                Set<URL> urls = received.get(key);
                if (urls != null) {
                    urls.remove(url);
                }
                if (urls == null || urls.isEmpty()) {
                    if (urls == null) {
                        urls = new ConcurrentHashSet<>();
                    }
                    URL empty = url.setProtocol(EMPTY_PROTOCOL);
                    urls.add(empty);
                }
                List<URL> list = toList(urls);
                for (NotifyListener listener : entry.getValue()) {
                    notify(key, listener, list);
                }
            }
        }
    }

    protected void subscribed(URL url, NotifyListener listener) {
        List<URL> urls = lookup(url);
        notify(url, listener, urls);
    }

    private List<URL> toList(Set<URL> urls) {
        List<URL> list = new ArrayList<>();
        if (CollectionUtils.isNotEmpty(urls)) {
            list.addAll(urls);
        }
        return list;
    }

    @Override
    public void register(URL url) {
        super.register(url);
        registered(url);
    }

    @Override
    public void unregister(URL url) {
        super.unregister(url);
        unregistered(url);
    }

    @Override
    public void subscribe(URL url, NotifyListener listener) {
        super.subscribe(url, listener);
        subscribed(url, listener);
    }

    @Override
    public void unsubscribe(URL url, NotifyListener listener) {
        super.unsubscribe(url, listener);
        received.remove(url);
    }

    @Override
    public List<URL> lookup(URL url) {
        List<URL> urls = new ArrayList<>();
        Map<String, List<URL>> notifiedUrls = getNotified().get(url);
        if (notifiedUrls != null && notifiedUrls.size() > 0) {
            for (List<URL> values : notifiedUrls.values()) {
                urls.addAll(values);
            }
        }
        if (urls.isEmpty()) {
            List<URL> cacheUrls = getCacheUrls(url);
            if (CollectionUtils.isNotEmpty(cacheUrls)) {
                urls.addAll(cacheUrls);
            }
        }
        if (urls.isEmpty()) {
            for (URL u : getRegistered()) {
                if (UrlUtils.isMatch(url, u)) {
                    urls.add(u);
                }
            }
        }
        if (ANY_VALUE.equals(url.getServiceInterface())) {
            for (URL u : getSubscribed().keySet()) {
                if (UrlUtils.isMatch(url, u)) {
                    urls.add(u);
                }
            }
        }
        return urls;
    }

    public MulticastSocket getMulticastSocket() {
        return multicastSocket;
    }

    public Map<URL, Set<URL>> getReceived() {
        return received;
    }
}
","['Assertion Roulette', 'Conditional Test Logic', 'Exception Catching Throwing', 'Eager Test', 'Lazy Test', 'Unknown Test']",['Assertion Roulette'],0,5,1,11
18022_58.0_esper_testincorrectremove,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/18022_test.java,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/18022_actual.java,"/*
 ***************************************************************************************
 *  Copyright (C) 2006 EsperTech, Inc. All rights reserved.                            *
 *  http://www.espertech.com/esper                                                     *
 *  http://www.espertech.com                                                           *
 *  ---------------------------------------------------------------------------------- *
 *  The software in this package is published under the terms of the GPL license       *
 *  a copy of which has been included with this distribution in the license.txt file.  *
 ***************************************************************************************
 */
package com.espertech.esper.runtime.internal.schedulesvcimpl;

import com.espertech.esper.common.internal.epl.expression.time.abacus.TimeAbacusMilliseconds;
import com.espertech.esper.common.internal.schedule.*;
import com.espertech.esper.common.internal.type.ScheduleUnit;
import com.espertech.esper.runtime.internal.timer.TimeSourceServiceImpl;
import junit.framework.TestCase;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.time.ZoneId;
import java.util.Calendar;
import java.util.Collection;
import java.util.LinkedList;
import java.util.TimeZone;

public class TestSchedulingServiceImpl extends TestCase {
    private SchedulingServiceImpl service;

    private long slots[][];
    private SupportScheduleCallback callbacks[];

    public void setUp() {
        service = new SchedulingServiceImpl(-1, new TimeSourceServiceImpl(), ZoneId.systemDefault());

        // 2-by-2 table of buckets and slots
        ScheduleBucket[] buckets = new ScheduleBucket[3];
        slots = new long[buckets.length][2];
        for (int i = 0; i < buckets.length; i++) {
            buckets[i] = new ScheduleBucket(i);
            slots[i] = new long[2];
            for (int j = 0; j < slots[i].length; j++) {
                slots[i][j] = buckets[i].allocateSlot();
            }
        }

        callbacks = new SupportScheduleCallback[5];
        for (int i = 0; i < callbacks.length; i++) {
            callbacks[i] = new SupportScheduleCallback();
        }
    }

    public void testAddTwice() {
        service.add(100, callbacks[0], slots[0][0]);
        assertTrue(service.isScheduled(callbacks[0]));
        service.add(100, callbacks[0], slots[0][0]);

        service.add(ScheduleComputeHelper.computeNextOccurance(new ScheduleSpec(), service.getTime(), TimeZone.getDefault(), TimeAbacusMilliseconds.INSTANCE), callbacks[1], slots[0][0]);
        service.add(ScheduleComputeHelper.computeNextOccurance(new ScheduleSpec(), service.getTime(), TimeZone.getDefault(), TimeAbacusMilliseconds.INSTANCE), callbacks[1], slots[0][0]);
    }

    public void testTrigger() {
        long startTime = 0;

        service.setTime(0);

        // Add callbacks
        service.add(20, callbacks[3], slots[1][1]);
        service.add(20, callbacks[2], slots[1][0]);
        service.add(20, callbacks[1], slots[0][1]);
        service.add(21, callbacks[0], slots[0][0]);
        assertTrue(service.isScheduled(callbacks[3]));
        assertTrue(service.isScheduled(callbacks[0]));

        // Evaluate before the within time, expect not results
        startTime += 19;
        service.setTime(startTime);
        evaluateSchedule();
        checkCallbacks(callbacks, new Integer[]{0, 0, 0, 0, 0});
        assertTrue(service.isScheduled(callbacks[3]));

        // Evaluate exactly on the within time, expect a result
        startTime += 1;
        service.setTime(startTime);
        evaluateSchedule();
        checkCallbacks(callbacks, new Integer[]{0, 1, 2, 3, 0});
        assertFalse(service.isScheduled(callbacks[3]));

        // Evaluate after already evaluated once, no result
        startTime += 1;
        service.setTime(startTime);
        evaluateSchedule();
        checkCallbacks(callbacks, new Integer[]{4, 0, 0, 0, 0});
        assertFalse(service.isScheduled(callbacks[3]));

        startTime += 1;
        service.setTime(startTime);
        evaluateSchedule();
        assertEquals(0, callbacks[3].clearAndGetOrderTriggered());

        // Adding the same callback more than once should cause an exception
        service.add(20, callbacks[0], slots[0][0]);
        service.add(28, callbacks[0], slots[0][0]);
        service.remove(callbacks[0], slots[0][0]);

        service.add(20, callbacks[2], slots[1][0]);
        service.add(25, callbacks[1], slots[0][1]);
        service.remove(callbacks[1], slots[0][1]);
        service.add(21, callbacks[0], slots[0][0]);
        service.add(21, callbacks[3], slots[1][1]);
        service.add(20, callbacks[1], slots[0][1]);
        SupportScheduleCallback.setCallbackOrderNum(0);

        startTime += 20;
        service.setTime(startTime);
        evaluateSchedule();
        checkCallbacks(callbacks, new Integer[]{0, 1, 2, 0, 0});

        startTime += 1;
        service.setTime(startTime);
        evaluateSchedule();
        checkCallbacks(callbacks, new Integer[]{3, 0, 0, 4, 0});

        service.setTime(startTime + Integer.MAX_VALUE);
        evaluateSchedule();
        checkCallbacks(callbacks, new Integer[]{0, 0, 0, 0, 0});
    }

    public void testWaitAndSpecTogether() {
        Calendar calendar = Calendar.getInstance();
        calendar.set(2004, 11, 9, 15, 27, 10);
        calendar.set(Calendar.MILLISECOND, 500);
        long startTime = calendar.getTimeInMillis();

        service.setTime(startTime);

        // Add a specification
        ScheduleSpec spec = new ScheduleSpec();
        spec.addValue(ScheduleUnit.MONTHS, 12);
        spec.addValue(ScheduleUnit.DAYS_OF_MONTH, 9);
        spec.addValue(ScheduleUnit.HOURS, 15);
        spec.addValue(ScheduleUnit.MINUTES, 27);
        spec.addValue(ScheduleUnit.SECONDS, 20);

        service.add(ScheduleComputeHelper.computeDeltaNextOccurance(spec, service.getTime(), TimeZone.getDefault(), TimeAbacusMilliseconds.INSTANCE), callbacks[3], slots[1][1]);

        spec.addValue(ScheduleUnit.SECONDS, 15);
        service.add(ScheduleComputeHelper.computeDeltaNextOccurance(spec, service.getTime(), TimeZone.getDefault(), TimeAbacusMilliseconds.INSTANCE), callbacks[4], slots[2][0]);

        // Add some more callbacks
        service.add(5000, callbacks[0], slots[0][0]);
        service.add(10000, callbacks[1], slots[0][1]);
        service.add(15000, callbacks[2], slots[1][0]);

        // Now send a times reflecting various seconds later and check who got a callback
        service.setTime(startTime + 1000);
        SupportScheduleCallback.setCallbackOrderNum(0);
        evaluateSchedule();
        checkCallbacks(callbacks, new Integer[]{0, 0, 0, 0, 0});

        service.setTime(startTime + 2000);
        evaluateSchedule();
        checkCallbacks(callbacks, new Integer[]{0, 0, 0, 0, 0});

        service.setTime(startTime + 4000);
        evaluateSchedule();
        checkCallbacks(callbacks, new Integer[]{0, 0, 0, 0, 0});

        service.setTime(startTime + 5000);
        evaluateSchedule();
        checkCallbacks(callbacks, new Integer[]{1, 0, 0, 0, 2});

        service.setTime(startTime + 9000);
        evaluateSchedule();
        checkCallbacks(callbacks, new Integer[]{0, 0, 0, 0, 0});

        service.setTime(startTime + 10000);
        evaluateSchedule();
        checkCallbacks(callbacks, new Integer[]{0, 3, 0, 4, 0});

        service.setTime(startTime + 11000);
        evaluateSchedule();
        checkCallbacks(callbacks, new Integer[]{0, 0, 0, 0, 0});

        service.setTime(startTime + 15000);
        evaluateSchedule();
        checkCallbacks(callbacks, new Integer[]{0, 0, 5, 0, 0});

        service.setTime(startTime + Integer.MAX_VALUE);
        evaluateSchedule();
        checkCallbacks(callbacks, new Integer[]{0, 0, 0, 0, 0});
    }

    public void testIncorrectRemove() {
        SchedulingServiceImpl evaluator = new SchedulingServiceImpl(-1, new TimeSourceServiceImpl(), ZoneId.systemDefault());
        SupportScheduleCallback callback = new SupportScheduleCallback();
        evaluator.remove(callback, 0);
    }

    private void checkCallbacks(SupportScheduleCallback callbacks[], Integer[] results) {
        assertTrue(callbacks.length == results.length);

        for (int i = 0; i < callbacks.length; i++) {
            assertEquals((int) results[i], (int) callbacks[i].clearAndGetOrderTriggered());
        }
    }

    private void evaluateSchedule() {
        Collection<ScheduleHandle> handles = new LinkedList<ScheduleHandle>();
        service.evaluate(handles);

        for (ScheduleHandle handle : handles) {
            ScheduleHandleCallback cb = (ScheduleHandleCallback) handle;
            cb.scheduledTrigger();
        }
    }

    public static class SupportScheduleCallback implements ScheduleHandle, ScheduleHandleCallback {
        private static int orderAllCallbacks;

        private int orderTriggered = 0;

        public void scheduledTrigger() {
            log.debug("".scheduledTrigger"");
            orderAllCallbacks++;
            orderTriggered = orderAllCallbacks;
        }

        public int clearAndGetOrderTriggered() {
            int result = orderTriggered;
            orderTriggered = 0;
            return result;
        }

        public static void setCallbackOrderNum(int orderAllCallbacks) {
            SupportScheduleCallback.orderAllCallbacks = orderAllCallbacks;
        }

        public int getStatementId() {
            return 1;
        }

        public int getAgentInstanceId() {
            return 0;
        }

        private static final Logger log = LoggerFactory.getLogger(SupportScheduleCallback.class);
    }
}
","/*
 ***************************************************************************************
 *  Copyright (C) 2006 EsperTech, Inc. All rights reserved.                            *
 *  http://www.espertech.com/esper                                                     *
 *  http://www.espertech.com                                                           *
 *  ---------------------------------------------------------------------------------- *
 *  The software in this package is published under the terms of the GPL license       *
 *  a copy of which has been included with this distribution in the license.txt file.  *
 ***************************************************************************************
 */
package com.espertech.esper.runtime.internal.schedulesvcimpl;

import com.espertech.esper.common.client.util.DateTime;
import com.espertech.esper.common.internal.schedule.ScheduleHandle;
import com.espertech.esper.common.internal.schedule.ScheduleServiceException;
import com.espertech.esper.common.internal.schedule.TimeSourceService;
import com.espertech.esper.runtime.internal.metrics.instrumentation.InstrumentationHelper;
import com.espertech.esper.runtime.internal.metrics.jmx.JmxGetter;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.time.ZoneId;
import java.time.format.DateTimeFormatter;
import java.util.*;

/**
 * Implements the schedule service by simply keeping a sorted set of long millisecond
 * values and a set of handles for each.
 * <p>
 * Synchronized since statement creation and event evaluation by multiple (event send) threads
 * can lead to callbacks added/removed asynchronously.
 */
public final class SchedulingServiceImpl implements SchedulingServiceSPI {
    private final int stageId;
    private final DateTimeFormatter defaultDateTimeFormatter;

    // Map of time and handle
    private final SortedMap<Long, SortedMap<Long, ScheduleHandle>> timeHandleMap;

    // Map of handle and handle list for faster removal
    private final Map<ScheduleHandle, SortedMap<Long, ScheduleHandle>> handleSetMap;

    // Current time - used for evaluation as well as for adding new handles
    private volatile long currentTime;

    /**
     * Constructor.
     *
     * @param timeSourceService time source provider
     * @param stageId stage id or -1 when not applicable
     * @param defaultFormatterTimeZone time zone for audit formatter
     */
    public SchedulingServiceImpl(int stageId, TimeSourceService timeSourceService, ZoneId defaultFormatterTimeZone) {
        this.stageId = stageId;
        this.defaultDateTimeFormatter = DateTimeFormatter.ofPattern(DateTime.DEFAULT_XMLLIKE_DATE_FORMAT).withZone(defaultFormatterTimeZone);
        this.timeHandleMap = new TreeMap<Long, SortedMap<Long, ScheduleHandle>>();
        this.handleSetMap = new HashMap<ScheduleHandle, SortedMap<Long, ScheduleHandle>>();
        // initialize time to just before now as there is a check for duplicate external time events
        this.currentTime = timeSourceService.getTimeMillis() - 1;
    }

    public void destroy() {
        log.debug(""Destroying scheduling service"");
        handleSetMap.clear();
        timeHandleMap.clear();
    }

    public long getTime() {
        // note that this.currentTime is volatile
        return this.currentTime;
    }

    public synchronized final void setTime(long currentTime) {
        this.currentTime = currentTime;
    }

    public synchronized final void add(long afterTime, ScheduleHandle handle, long slot)
            throws ScheduleServiceException {
        if (InstrumentationHelper.ENABLED) {
            InstrumentationHelper.get().qScheduleAdd(currentTime, afterTime, handle, slot);
        }

        if (handleSetMap.containsKey(handle)) {
            remove(handle, slot);
        }

        long triggerOnTime = currentTime + afterTime;
        addTrigger(slot, handle, triggerOnTime);

        if (InstrumentationHelper.ENABLED) {
            InstrumentationHelper.get().aScheduleAdd();
        }
    }

    public synchronized final void remove(ScheduleHandle handle, long slot) {
        if (InstrumentationHelper.ENABLED) {
            InstrumentationHelper.get().qScheduleRemove(handle, slot);
        }

        SortedMap<Long, ScheduleHandle> handleSet = handleSetMap.get(handle);
        if (handleSet == null) {
            // If it already has been removed then that's fine;
            // Such could be the case when 2 timers fireStatementStopped at the same time, and one stops the other
            return;
        }
        handleSet.remove(slot);
        handleSetMap.remove(handle);

        if (InstrumentationHelper.ENABLED) {
            InstrumentationHelper.get().aScheduleRemove();
        }
    }

    public synchronized final void evaluate(Collection<ScheduleHandle> handles) {
        if (InstrumentationHelper.ENABLED) {
            InstrumentationHelper.get().qScheduleEval(currentTime);
        }

        // Get the values on or before the current time - to get those that are exactly on the
        // current time we just add one to the current time for getting the head map
        SortedMap<Long, SortedMap<Long, ScheduleHandle>> headMap = timeHandleMap.headMap(currentTime + 1);

        if (headMap.isEmpty()) {
            if (InstrumentationHelper.ENABLED) {
                InstrumentationHelper.get().aScheduleEval(handles);
            }
            return;
        }

        // First determine all triggers to shoot
        List<Long> removeKeys = new ArrayList<Long>();
        for (Map.Entry<Long, SortedMap<Long, ScheduleHandle>> entry : headMap.entrySet()) {
            Long key = entry.getKey();
            SortedMap<Long, ScheduleHandle> value = entry.getValue();
            removeKeys.add(key);
            for (ScheduleHandle handle : value.values()) {
                handles.add(handle);
            }
        }

        // Next remove all handles
        for (Map.Entry<Long, SortedMap<Long, ScheduleHandle>> entry : headMap.entrySet()) {
            for (ScheduleHandle handle : entry.getValue().values()) {
                handleSetMap.remove(handle);
            }
        }

        // Remove all triggered msec values
        for (Long key : removeKeys) {
            timeHandleMap.remove(key);
        }

        if (InstrumentationHelper.ENABLED) {
            InstrumentationHelper.get().aScheduleEval(handles);
        }
    }

    public void transfer(Set<Integer> statementIds, SchedulingServiceSPI schedulingService) {
        long currentTime = getTime();
        long targetTime = schedulingService.getTime();
        for (Map.Entry<Long, SortedMap<Long, ScheduleHandle>> schedule : timeHandleMap.entrySet()) {
            for (Map.Entry<Long, ScheduleHandle> entry : schedule.getValue().entrySet()) {
                if (statementIds.contains(entry.getValue().getStatementId())) {
                    long relative = ScheduleTransferHelper.computeTransferTime(currentTime, targetTime, schedule.getKey());
                    remove(entry.getValue(), entry.getKey());
                    schedulingService.add(relative, entry.getValue(), entry.getKey());
                }
            }
        }
    }

    public void init() {
        // no action required
    }

    private void addTrigger(long slot, ScheduleHandle handle, long triggerTime) {
        SortedMap<Long, ScheduleHandle> handleSet = timeHandleMap.get(triggerTime);
        if (handleSet == null) {
            handleSet = new TreeMap<Long, ScheduleHandle>();
            timeHandleMap.put(triggerTime, handleSet);
        }
        handleSet.put(slot, handle);
        handleSetMap.put(handle, handleSet);
    }

    @JmxGetter(name = ""TimeHandleCount"", description = ""Number of outstanding time evaluations"")
    public int getTimeHandleCount() {
        return timeHandleMap.size();
    }

    @JmxGetter(name = ""FurthestTimeHandle"", description = ""Furthest outstanding time evaluation"")
    public String getFurthestTimeHandleDate() {
        Long handle = getFurthestTimeHandle();
        if (handle != null) {
            return DateTime.print(handle);
        }
        return null;
    }

    @JmxGetter(name = ""NearestTimeHandle"", description = ""Nearest outstanding time evaluation"")
    public String getNearestTimeHandleDate() {
        Long handle = getNearestTimeHandle();
        if (handle != null) {
            return DateTime.print(handle);
        }
        return null;
    }

    public Long getFurthestTimeHandle() {
        if (!timeHandleMap.isEmpty()) {
            return timeHandleMap.lastKey();
        }
        return null;
    }

    public int getScheduleHandleCount() {
        return handleSetMap.size();
    }

    public boolean isScheduled(ScheduleHandle handle) {
        return handleSetMap.containsKey(handle);
    }

    @Override
    public synchronized Long getNearestTimeHandle() {
        if (timeHandleMap.isEmpty()) {
            return null;
        }
        for (Map.Entry<Long, SortedMap<Long, ScheduleHandle>> entry : timeHandleMap.entrySet()) {
            if (entry.getValue().isEmpty()) {
                continue;
            }
            return entry.getKey();
        }
        return null;
    }

    public void visitSchedules(ScheduleVisitor visitor) {
        ScheduleVisit visit = new ScheduleVisit();
        for (Map.Entry<Long, SortedMap<Long, ScheduleHandle>> entry : timeHandleMap.entrySet()) {
            visit.setTimestamp(entry.getKey());

            for (Map.Entry<Long, ScheduleHandle> inner : entry.getValue().entrySet()) {
                visit.setStatementId(inner.getValue().getStatementId());
                visit.setAgentInstanceId(inner.getValue().getAgentInstanceId());
                visitor.visit(visit);
            }
        }
    }

    public DateTimeFormatter getDefaultFormatter() {
        return defaultDateTimeFormatter;
    }

    private static final Logger log = LoggerFactory.getLogger(SchedulingServiceImpl.class);
}
","['Assertion Roulette', 'General Fixture', 'Unknown Test']","['Assertion Roulette', 'Eager Test', 'Magic Number Test', 'Mystery Guest', 'Redundant Print', 'Sleepy Test']",5,2,1,12
14359_11_wikidata-toolkit_iterateoverallstatements,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/14359_test.java,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/14359_actual.java,"package org.wikidata.wdtk.datamodel.implementation;

import static org.junit.Assert.assertEquals;
import static org.junit.Assert.assertFalse;
import static org.junit.Assert.assertNotEquals;
import static org.junit.Assert.assertTrue;

import java.io.IOException;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Collections;
import java.util.Iterator;
import java.util.List;

import org.junit.Test;
import org.wikidata.wdtk.datamodel.helpers.Datamodel;
import org.wikidata.wdtk.datamodel.helpers.DatamodelMapper;
import org.wikidata.wdtk.datamodel.interfaces.Claim;
import org.wikidata.wdtk.datamodel.interfaces.DatatypeIdValue;
import org.wikidata.wdtk.datamodel.interfaces.FormDocument;
import org.wikidata.wdtk.datamodel.interfaces.FormIdValue;
import org.wikidata.wdtk.datamodel.interfaces.ItemIdValue;
import org.wikidata.wdtk.datamodel.interfaces.MonolingualTextValue;
import org.wikidata.wdtk.datamodel.interfaces.PropertyDocument;
import org.wikidata.wdtk.datamodel.interfaces.Statement;
import org.wikidata.wdtk.datamodel.interfaces.StatementGroup;
import org.wikidata.wdtk.datamodel.interfaces.StatementRank;

/*
 * #%L
 * Wikidata Toolkit Data Model
 * %%
 * Copyright (C) 2014 Wikidata Toolkit Developers
 * %%
 * Licensed under the Apache License, Version 2.0 (the ""License"");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 * 
 *      http://www.apache.org/licenses/LICENSE-2.0
 * 
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * #L%
 */

import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.databind.ObjectMapper;

public class FormDocumentImplTest {

	private final ObjectMapper mapper = new DatamodelMapper(""http://example.com/entity/"");

	private final FormIdValue fid = new FormIdValueImpl(""L42-F1"", ""http://example.com/entity/"");
	private final List<ItemIdValue> gramFeatures = Arrays.asList(
			new ItemIdValueImpl(""Q2"", ""http://example.com/entity/""),
			new ItemIdValueImpl(""Q1"", ""http://example.com/entity/"")
	);
	private final Statement s = new StatementImpl(""MyId"", StatementRank.NORMAL,
			new SomeValueSnakImpl(new PropertyIdValueImpl(""P42"", ""http://example.com/entity/"")),
			Collections.emptyList(), Collections.emptyList(), fid);
	private final List<StatementGroup> statementGroups = Collections.singletonList(
			new StatementGroupImpl(Collections.singletonList(s))
	);
	private final MonolingualTextValue rep = new TermImpl(""en"", ""rep"");
	private final List<MonolingualTextValue> repList = Collections.singletonList(rep);

	private final FormDocument fd1 = new FormDocumentImpl(fid, repList, gramFeatures, statementGroups, 1234);
	private final FormDocument fd2 = new FormDocumentImpl(fid, repList, gramFeatures, statementGroups, 1234);

	private final String JSON_FORM = ""{\""type\"":\""form\"",\""id\"":\""L42-F1\"",\""grammaticalFeatures\"":[\""Q1\"",\""Q2\""],\""representations\"":{\""en\"":{\""language\"":\""en\"",\""value\"":\""rep\""}},\""claims\"":{\""P42\"":[{\""rank\"":\""normal\"",\""id\"":\""MyId\"",\""mainsnak\"":{\""property\"":\""P42\"",\""snaktype\"":\""somevalue\""},\""type\"":\""statement\""}]},\""lastrevid\"":1234}"";

	@Test
	public void fieldsAreCorrect() {
		assertEquals(fd1.getEntityId(), fid);
		assertEquals(fd1.getRepresentations(), Collections.singletonMap(rep.getLanguageCode(), rep));
		assertEquals(fd1.getGrammaticalFeatures(), gramFeatures);
		assertEquals(fd1.getStatementGroups(), statementGroups);
	}

	@Test
	public void equalityBasedOnContent() {
		FormDocument irDiffRepresentations = new FormDocumentImpl(fid, Collections.singletonList(new MonolingualTextValueImpl(""fr"", ""bar"")), gramFeatures, statementGroups, 1234);
		FormDocument irDiffGramFeatures = new FormDocumentImpl(fid, repList, Collections.emptyList(), statementGroups, 1234);
		FormDocument irDiffStatementGroups = new FormDocumentImpl(fid, repList, gramFeatures, Collections.emptyList(), 1234);
		FormDocument irDiffRevisions = new FormDocumentImpl(fid, repList, gramFeatures, statementGroups, 1235);
		PropertyDocument pr = new PropertyDocumentImpl(
				new PropertyIdValueImpl(""P42"", ""foo""),
				repList, Collections.emptyList(), Collections.emptyList(),
				Collections.emptyList(),
				new DatatypeIdImpl(DatatypeIdValue.DT_STRING), 1234);
		FormDocument irDiffFormIdValue = new FormDocumentImpl(
				new FormIdValueImpl(""L42-F2"", ""http://example.com/entity/""),
				repList, gramFeatures, Collections.emptyList(), 1235);

		assertEquals(fd1, fd1);
		assertEquals(fd1, fd2);
		assertNotEquals(fd1, irDiffRepresentations);
		assertNotEquals(fd1, irDiffGramFeatures);
		assertNotEquals(fd1, irDiffStatementGroups);
		assertNotEquals(fd1, irDiffRevisions);
		assertNotEquals(irDiffStatementGroups, irDiffFormIdValue);
		assertNotEquals(fd1, pr);
		assertNotEquals(fd1, null);
		assertNotEquals(fd1, this);
	}

	@Test
	public void hashBasedOnContent() {
		assertEquals(fd1.hashCode(), fd2.hashCode());
	}

	@Test(expected = NullPointerException.class)
	public void idNotNull() {
		new FormDocumentImpl(null, repList, gramFeatures, statementGroups, 1234);
	}

	@Test
	public void representationsNull() {
		assertEquals(Collections.emptyMap(), new FormDocumentImpl(fid,  null, gramFeatures, statementGroups, 1234).getRepresentations());
	}

	@Test
	public void representationsEmpty() {
		assertEquals(Collections.emptyMap(), new FormDocumentImpl(fid, Collections.emptyList(), gramFeatures, statementGroups, 1234).getRepresentations());
	}

	@Test
	public void grammaticalFeaturesCanBeNull() {
		FormDocument doc = new FormDocumentImpl(fid, repList, null, statementGroups, 1234);
		assertTrue(doc.getGrammaticalFeatures().isEmpty());
	}

	@Test
	public void statementGroupsCanBeNull() {
		FormDocument doc = new FormDocumentImpl(fid, repList, gramFeatures, null, 1234);
		assertTrue(doc.getStatementGroups().isEmpty());
	}

	@Test(expected = IllegalArgumentException.class)
	public void statementGroupsUseSameSubject() {
		FormIdValue iid2 = new FormIdValueImpl(""Q23"", ""http://example.org/"");
		Statement s2 = new StatementImpl(""MyId"", StatementRank.NORMAL,
				new SomeValueSnakImpl(new PropertyIdValueImpl(""P42"", ""http://wikibase.org/entity/"")),
				Collections.emptyList(),  Collections.emptyList(), iid2);
		StatementGroup sg2 = new StatementGroupImpl(Collections.singletonList(s2));

		List<StatementGroup> statementGroups2 = new ArrayList<>();
		statementGroups2.add(statementGroups.get(0));
		statementGroups2.add(sg2);

		new FormDocumentImpl(fid, repList, gramFeatures, statementGroups2, 1234);
	}

	@Test
	public void iterateOverAllStatements() {
		Iterator<Statement> statements = fd1.getAllStatements();

		assertTrue(statements.hasNext());
		assertEquals(s, statements.next());
		assertFalse(statements.hasNext());
	}

	@Test
	public void testWithEntityId() {
		assertEquals(FormIdValue.NULL, fd1.withEntityId(FormIdValue.NULL).getEntityId());
		FormIdValue id = Datamodel.makeWikidataFormIdValue(""L123-F45"");
		assertEquals(id, fd1.withEntityId(id).getEntityId());
	}

	@Test
	public void testWithRevisionId() {
		assertEquals(1235L, fd1.withRevisionId(1235L).getRevisionId());
		assertEquals(fd1, fd1.withRevisionId(1325L).withRevisionId(fd1.getRevisionId()));
	}

	@Test
	public void testWithRepresentationInNewLanguage() {
		MonolingualTextValue newRepresentation = new MonolingualTextValueImpl(""Foo"", ""fr"");
		FormDocument withRepresentation = fd1.withRepresentation(newRepresentation);
		assertEquals(newRepresentation, withRepresentation.getRepresentations().get(""fr""));
	}

	@Test
	public void testWithNewGrammaticalFeatures() {
		ItemIdValue newGrammaticalFeature = new ItemIdValueImpl(""Q3"", ""http://example.com/entity/"");
		FormDocument withGrammaticalFeature = fd1.withGrammaticalFeature(newGrammaticalFeature);
		assertTrue(withGrammaticalFeature.getGrammaticalFeatures().containsAll(gramFeatures));
		assertTrue(withGrammaticalFeature.getGrammaticalFeatures().contains(newGrammaticalFeature));
	}

	@Test
	public void testWithExistingGrammaticalFeatures() {
		ItemIdValue newGrammaticalFeature = new ItemIdValueImpl(""Q2"", ""http://example.com/entity/"");
		FormDocument withGrammaticalFeature = fd1.withGrammaticalFeature(newGrammaticalFeature);
		assertEquals(fd1, withGrammaticalFeature);
	}

	@Test
	public void testAddStatement() {
		Statement fresh = new StatementImpl(""MyFreshId"", StatementRank.NORMAL,
				new SomeValueSnakImpl(new PropertyIdValueImpl(""P29"", ""http://example.com/entity/"")),
				Collections.emptyList(), Collections.emptyList(), fid);
		Claim claim = fresh.getClaim();
		assertFalse(fd1.hasStatementValue(
				claim.getMainSnak().getPropertyId(),
				claim.getValue()));
		FormDocument withStatement = fd1.withStatement(fresh);
		assertTrue(withStatement.hasStatementValue(
				claim.getMainSnak().getPropertyId(),
				claim.getValue()));
	}

	@Test
	public void testDeleteStatements() {
		Statement toRemove = statementGroups.get(0).getStatements().get(0);
		FormDocument withoutStatement = fd1.withoutStatementIds(Collections.singleton(toRemove.getStatementId()));
		assertNotEquals(withoutStatement, fd1);
	}

	@Test
	public void testFormToJson() throws JsonProcessingException {
		JsonComparator.compareJsonStrings(JSON_FORM, mapper.writeValueAsString(fd1));
	}

	@Test
	public void testFormToJava() throws IOException {
		assertEquals(fd1, mapper.readValue(JSON_FORM, FormDocumentImpl.class));
	}

}
","package org.wikidata.wdtk.datamodel.implementation;

/*
 * #%L
 * Wikidata Toolkit Data Model
 * %%
 * Copyright (C) 2014 Wikidata Toolkit Developers
 * %%
 * Licensed under the Apache License, Version 2.0 (the ""License"");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 * 
 *      http://www.apache.org/licenses/LICENSE-2.0
 * 
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * #L%
 */

import com.fasterxml.jackson.annotation.*;
import com.fasterxml.jackson.databind.annotation.JsonDeserialize;
import org.wikidata.wdtk.datamodel.helpers.Equality;
import org.wikidata.wdtk.datamodel.helpers.Hash;
import org.wikidata.wdtk.datamodel.helpers.ToString;
import org.wikidata.wdtk.datamodel.interfaces.*;

import java.util.*;


/**
 * Jackson implementation of {@link FormDocument}.
 *
 * @author Thomas Pellissier Tanon
 */
@JsonIgnoreProperties(ignoreUnknown = true)
@JsonTypeInfo(use = JsonTypeInfo.Id.NONE)
public class FormDocumentImpl extends StatementDocumentImpl implements FormDocument {

	private final List<ItemIdValue> grammaticalFeatures;

	private final Map<String,MonolingualTextValue> representations;

	/**
	 * Constructor.
	 *
	 * @param id
	 *            the id of the le that data is about
	 * @param representations
	 *            the list of representations of this lexeme, with at most one
	 *            lemma for each language code
	 * @param grammaticalFeatures
	 *            the grammatical features of the lexeme
	 * @param statements
	 *            the list of statement groups of this lexeme; all of them must
	 *            have the given id as their subject
	 * @param revisionId
	 *            the revision ID or 0 if not known; see
	 *            {@link EntityDocument#getRevisionId()}
	 */
	FormDocumentImpl(
			FormIdValue id,
			List<MonolingualTextValue> representations,
			List<ItemIdValue> grammaticalFeatures,
			List<StatementGroup> statements,
			long revisionId) {
		super(id, statements, revisionId);
		this.representations = (representations == null || representations.isEmpty()) ? Collections.emptyMap() : constructTermMap(representations);
		this.grammaticalFeatures = (grammaticalFeatures == null) ? Collections.emptyList() : grammaticalFeatures;
		this.grammaticalFeatures.sort(Comparator.comparing(EntityIdValue::getId));
	}

	/**
	 * Constructor. Creates an object that can be populated during JSON
	 * deserialization. Should only be used by Jackson for this very purpose.
	 */
	@JsonCreator
	FormDocumentImpl(
			@JsonProperty(""id"") String jsonId,
			@JsonProperty(""representations"") @JsonDeserialize(contentAs=TermImpl.class) Map<String, MonolingualTextValue> representations,
			@JsonProperty(""grammaticalFeatures"") List<String> grammaticalFeatures,
			@JsonProperty(""claims"") Map<String, List<StatementImpl.PreStatement>> claims,
			@JsonProperty(""lastrevid"") long revisionId,
			@JacksonInject(""siteIri"") String siteIri) {
		super(jsonId, claims, revisionId, siteIri);
		this.representations = (representations == null) ? Collections.emptyMap() : representations;
		this.grammaticalFeatures = (grammaticalFeatures == null || grammaticalFeatures.isEmpty())
				? Collections.emptyList()
				: constructGrammaticalFeatures(grammaticalFeatures, siteIri);
	}

	/**
	 * Copy constructor, used when creating modified copies of forms.
	 */
	private FormDocumentImpl(
			FormIdValue id,
			Map<String,MonolingualTextValue> representations,
			List<ItemIdValue> grammaticalFeatures,
			Map<String, List<Statement>> statements,
			long revisionId) {
		super(id, statements, revisionId);
		this.representations = representations;
		this.grammaticalFeatures = grammaticalFeatures;
	}

	private static Map<String, MonolingualTextValue> constructTermMap(List<MonolingualTextValue> terms) {
		Map<String, MonolingualTextValue> map = new HashMap<>();
		for(MonolingualTextValue term : terms) {
			String language = term.getLanguageCode();
			if(map.containsKey(language)) {
				throw new IllegalArgumentException(""Multiple terms provided for the same language."");
			}
			// We need to make sure the terms are of the right type, otherwise they will not
			// be serialized correctly.
			map.put(language, toTerm(term));
		}
		return map;
	}

	private static MonolingualTextValue toTerm(MonolingualTextValue term) {
		return (term instanceof TermImpl) ? term : new TermImpl(term.getLanguageCode(), term.getText());
	}

	private List<ItemIdValue> constructGrammaticalFeatures(List<String> grammaticalFeatures, String siteIri) {
		List<ItemIdValue> output = new ArrayList<>(grammaticalFeatures.size());
		for(String grammaticalFeature : grammaticalFeatures) {
			output.add(new ItemIdValueImpl(grammaticalFeature, siteIri));
		}
		return output;
	}

	@JsonIgnore
	@Override
	public FormIdValue getEntityId() {
		return new FormIdValueImpl(entityId, siteIri);
	}

	@JsonIgnore
	@Override
	public List<ItemIdValue> getGrammaticalFeatures() {
		return grammaticalFeatures;
	}

	@JsonProperty(""grammaticalFeatures"")
	List<String> getJsonGrammaticalFeatures() {
		if (grammaticalFeatures.isEmpty()) {
			return Collections.emptyList();
		}
		List<String> output = new ArrayList<>(grammaticalFeatures.size());
		for(ItemIdValue feature : grammaticalFeatures) {
			output.add(feature.getId());
		}
		return output;
	}

	@JsonProperty(""type"")
	String getType() {
		return EntityDocumentImpl.JSON_TYPE_FORM;
	}

	@JsonProperty(""representations"")
	@Override
	public Map<String, MonolingualTextValue> getRepresentations() {
		return representations;
	}

	@Override
	public int hashCode() {
		return Hash.hashCode(this);
	}

	@Override
	public boolean equals(Object obj) {
		return Equality.equalsFormDocument(this, obj);
	}

	@Override
	public String toString() {
		return ToString.toString(this);
	}
	
	@Override
	public FormDocument withEntityId(FormIdValue newEntityId) {
		return new FormDocumentImpl(newEntityId, representations, grammaticalFeatures, claims, revisionId);
	}

	@Override
	public FormDocument withRevisionId(long newRevisionId) {
		return new FormDocumentImpl(getEntityId(),
				representations, grammaticalFeatures,
				claims, newRevisionId);
	}

	@Override
	public FormDocument withRepresentation(MonolingualTextValue representation) {
		Map<String, MonolingualTextValue> newRepresentations = new HashMap<>(representations);
		newRepresentations.put(representation.getLanguageCode(), toTerm(representation));
		return new FormDocumentImpl(getEntityId(), newRepresentations, grammaticalFeatures, claims, revisionId);
	}

	@Override
	public FormDocument withGrammaticalFeature(ItemIdValue grammaticalFeature) {
		if (grammaticalFeatures.contains(grammaticalFeature)) {
			return this;
		}
		List<ItemIdValue> newGrammaticalFeatures = new ArrayList<>(grammaticalFeatures);
		newGrammaticalFeatures.add(grammaticalFeature);
		return new FormDocumentImpl(getEntityId(), representations, newGrammaticalFeatures, claims, revisionId);
	}

	@Override
	public FormDocument withStatement(Statement statement) {
		Map<String, List<Statement>> newGroups = addStatementToGroups(statement, claims);
		return new FormDocumentImpl(getEntityId(),
				representations, grammaticalFeatures,
				newGroups, revisionId);
	}

	@Override
	public FormDocument withoutStatementIds(Set<String> statementIds) {
		Map<String, List<Statement>> newGroups = removeStatements(statementIds, claims);
		return new FormDocumentImpl(getEntityId(),
				representations, grammaticalFeatures,
				newGroups, revisionId);
	}
}
","['Assertion Roulette', 'Redundant Assertion']",['Assertion Roulette'],0,1,1,15
45544_71_spring-ws_handlesimplewsdl11definitionwithtransformlocation,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/45544_test.java,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/45544_actual.java,"/*
 * Copyright 2005-2022 the original author or authors.
 *
 * Licensed under the Apache License, Version 2.0 (the ""License"");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *	   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.springframework.ws.transport.http;

import static org.assertj.core.api.Assertions.*;
import static org.easymock.EasyMock.*;

import jakarta.servlet.http.HttpServletResponse;

import java.io.ByteArrayInputStream;
import java.io.InputStream;
import java.net.URI;

import javax.xml.parsers.DocumentBuilder;
import javax.xml.parsers.DocumentBuilderFactory;

import org.junit.jupiter.api.BeforeEach;
import org.junit.jupiter.api.Test;
import org.springframework.core.io.ClassPathResource;
import org.springframework.mock.web.MockHttpServletRequest;
import org.springframework.mock.web.MockHttpServletResponse;
import org.springframework.ws.wsdl.WsdlDefinition;
import org.springframework.ws.wsdl.wsdl11.SimpleWsdl11Definition;
import org.springframework.xml.DocumentBuilderFactoryUtils;
import org.springframework.xml.transform.StringSource;
import org.w3c.dom.Document;
import org.xmlunit.assertj.XmlAssert;

public class WsdlDefinitionHandlerAdapterTest {

	private WsdlDefinitionHandlerAdapter adapter;

	private WsdlDefinition definitionMock;

	private MockHttpServletRequest request;

	private MockHttpServletResponse response;

	@BeforeEach
	public void setUp() throws Exception {

		adapter = new WsdlDefinitionHandlerAdapter();
		definitionMock = createMock(WsdlDefinition.class);
		adapter.afterPropertiesSet();
		request = new MockHttpServletRequest();
		response = new MockHttpServletResponse();
	}

	@Test
	public void handleGet() throws Exception {

		request.setMethod(HttpTransportConstants.METHOD_GET);
		String definition = ""<definition xmlns='http://schemas.xmlsoap.org/wsdl/'/>"";
		expect(definitionMock.getSource()).andReturn(new StringSource(definition));

		replay(definitionMock);

		adapter.handle(request, response, definitionMock);

		XmlAssert.assertThat(response.getContentAsString()).and(definition).ignoreWhitespace().areIdentical();

		verify(definitionMock);
	}

	@Test
	public void handleNonGet() throws Exception {

		request.setMethod(HttpTransportConstants.METHOD_POST);

		replay(definitionMock);

		adapter.handle(request, response, definitionMock);

		assertThat(response.getStatus()).isEqualTo(HttpServletResponse.SC_METHOD_NOT_ALLOWED);

		verify(definitionMock);
	}

	@Test
	public void transformLocations() throws Exception {

		adapter.setTransformLocations(true);
		request.setMethod(HttpTransportConstants.METHOD_GET);
		request.setScheme(""http"");
		request.setServerName(""example.com"");
		request.setServerPort(8080);
		request.setContextPath(""/context"");
		request.setServletPath(""/service.wsdl"");
		request.setPathInfo(null);
		request.setRequestURI(""/context/service.wsdl"");

		replay(definitionMock);

		DocumentBuilderFactory documentBuilderFactory = DocumentBuilderFactoryUtils.newInstance();
		documentBuilderFactory.setNamespaceAware(true);
		DocumentBuilder documentBuilder = documentBuilderFactory.newDocumentBuilder();
		Document result = documentBuilder.parse(getClass().getResourceAsStream(""wsdl11-input.wsdl""));
		adapter.transformLocations(result, request);
		Document expectedDocument = documentBuilder.parse(getClass().getResourceAsStream(""wsdl11-expected.wsdl""));

		XmlAssert.assertThat(result).and(expectedDocument).ignoreWhitespace().areIdentical();

		verify(definitionMock);
	}

	@Test
	public void transformLocationFullUrl() throws Exception {

		request.setScheme(""http"");
		request.setServerName(""example.com"");
		request.setServerPort(8080);
		request.setContextPath(""/context"");
		request.setPathInfo(""/service.wsdl"");
		request.setRequestURI(""/context/service.wsdl"");
		String oldLocation = ""http://localhost:8080/context/service"";

		String result = adapter.transformLocation(oldLocation, request);

		assertThat(result).isNotNull();
		assertThat(new URI(result)).isEqualTo(new URI(""http://example.com:8080/context/service""));
	}

	@Test
	public void transformLocationEmptyContextFullUrl() throws Exception {

		request.setScheme(""http"");
		request.setServerName(""example.com"");
		request.setServerPort(8080);
		request.setContextPath("""");
		request.setRequestURI(""/service.wsdl"");
		String oldLocation = ""http://localhost:8080/service"";

		String result = adapter.transformLocation(oldLocation, request);

		assertThat(result).isNotNull();
		assertThat(new URI(result)).isEqualTo(new URI(""http://example.com:8080/service""));
	}

	@Test
	public void transformLocationRelativeUrl() throws Exception {

		request.setScheme(""http"");
		request.setServerName(""example.com"");
		request.setServerPort(8080);
		request.setContextPath(""/context"");
		request.setPathInfo(""/service.wsdl"");
		request.setRequestURI(""/context/service.wsdl"");
		String oldLocation = ""/service"";

		String result = adapter.transformLocation(oldLocation, request);

		assertThat(result).isNotNull();
		assertThat(new URI(result)).isEqualTo(new URI(""http://example.com:8080/context/service""));
	}

	@Test
	public void transformLocationEmptyContextRelativeUrl() throws Exception {

		request.setScheme(""http"");
		request.setServerName(""example.com"");
		request.setServerPort(8080);
		request.setContextPath("""");
		request.setRequestURI(""/service.wsdl"");
		String oldLocation = ""/service"";

		String result = adapter.transformLocation(oldLocation, request);

		assertThat(result).isNotNull();
		assertThat(new URI(result)).isEqualTo(new URI(""http://example.com:8080/service""));
	}

	@Test
	public void handleSimpleWsdl11DefinitionWithoutTransformLocations() throws Exception {

		adapter.setTransformLocations(false);
		request.setMethod(HttpTransportConstants.METHOD_GET);
		request.setScheme(""http"");
		request.setServerName(""example.com"");
		request.setServerPort(8080);
		request.setContextPath(""/context"");
		request.setServletPath(""/service.wsdl"");
		request.setPathInfo(null);
		request.setRequestURI(""/context/service.wsdl"");

		SimpleWsdl11Definition definition = new SimpleWsdl11Definition(
				new ClassPathResource(""echo-input.wsdl"", getClass()));

		adapter.handle(request, response, definition);

		InputStream inputStream = new ByteArrayInputStream(response.getContentAsByteArray());
		DocumentBuilderFactory documentBuilderFactory = DocumentBuilderFactoryUtils.newInstance();
		documentBuilderFactory.setNamespaceAware(true);
		DocumentBuilder documentBuilder = documentBuilderFactory.newDocumentBuilder();
		Document resultingDocument = documentBuilder.parse(inputStream);

		documentBuilder = documentBuilderFactory.newDocumentBuilder();
		Document expectedDocument = documentBuilder.parse(getClass().getResourceAsStream(""echo-input.wsdl""));

		XmlAssert.assertThat(resultingDocument).and(expectedDocument).ignoreWhitespace().areIdentical();
	}

	@Test
	public void handleSimpleWsdl11DefinitionWithTransformLocation() throws Exception {

		adapter.setTransformLocations(true);
		adapter.setTransformSchemaLocations(true);

		request.setMethod(HttpTransportConstants.METHOD_GET);
		request.setScheme(""http"");
		request.setServerName(""example.com"");
		request.setServerPort(80);
		request.setContextPath(""/context"");
		request.setServletPath(""/service.wsdl"");
		request.setPathInfo(null);
		request.setRequestURI(""/context/service.wsdl"");

		SimpleWsdl11Definition definition = new SimpleWsdl11Definition(
				new ClassPathResource(""echo-input.wsdl"", getClass()));

		adapter.handle(request, response, definition);

		InputStream inputStream = new ByteArrayInputStream(response.getContentAsByteArray());
		DocumentBuilderFactory documentBuilderFactory = DocumentBuilderFactoryUtils.newInstance();
		documentBuilderFactory.setNamespaceAware(true);
		DocumentBuilder documentBuilder = documentBuilderFactory.newDocumentBuilder();
		Document resultingDocument = documentBuilder.parse(inputStream);

		documentBuilder = documentBuilderFactory.newDocumentBuilder();
		Document expectedDocument = documentBuilder.parse(getClass().getResourceAsStream(""echo-expected.wsdl""));

		XmlAssert.assertThat(resultingDocument).and(expectedDocument).ignoreWhitespace().areIdentical();
	}

	@Test
	public void handlesForwardedHeadersInRequest() {

		// given
		request.setScheme(""http"");
		request.setServerName(""example.com"");
		request.setServerPort(80);
		request.setContextPath(""/context"");
		request.setPathInfo(""/service.wsdl"");

		request.addHeader(""X-Forwarded-Proto"", ""https"");
		request.addHeader(""X-Forwarded-Host"", ""loadbalancer.com"");
		request.addHeader(""X-Forwarded-Port"", ""8080"");

		// when
		String result = adapter.transformLocation(""/service"", request);

		// then
		assertThat(URI.create(""https://loadbalancer.com:8080/context/service"")).isEqualTo(URI.create(result));
	}

	@Test
	public void handlesNoForwardedHeadersInRequest() {

		// given
		request.setScheme(""http"");
		request.setServerName(""example.com"");
		request.setServerPort(80);
		request.setContextPath(""/context"");
		request.setPathInfo(""/service.wsdl"");

		// when
		String result = adapter.transformLocation(""/service"", request);

		// then
		assertThat(URI.create(""http://example.com:80/context/service"")).isEqualTo(URI.create(result));
	}

}
","/*
 * Copyright 2005-2022 the original author or authors.
 *
 * Licensed under the Apache License, Version 2.0 (the ""License"");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.springframework.ws.transport.http;

import jakarta.servlet.http.HttpServletRequest;
import jakarta.servlet.http.HttpServletResponse;

import java.util.HashMap;
import java.util.Map;

import javax.xml.transform.Source;
import javax.xml.transform.Transformer;
import javax.xml.transform.dom.DOMResult;
import javax.xml.transform.dom.DOMSource;
import javax.xml.transform.stream.StreamResult;

import org.springframework.beans.factory.InitializingBean;
import org.springframework.web.servlet.HandlerAdapter;
import org.springframework.web.servlet.ModelAndView;
import org.springframework.ws.wsdl.WsdlDefinition;
import org.springframework.xml.xpath.XPathExpression;
import org.springframework.xml.xpath.XPathExpressionFactory;
import org.w3c.dom.Document;

/**
 * Adapter to use the {@code WsdlDefinition} interface with the generic {@code DispatcherServlet}.
 * <p>
 * Reads the source from the mapped {@code WsdlDefinition} implementation, and writes that as the result to the
 * {@code HttpServletResponse}.
 * <p>
 * If the property {@code transformLocations} is set to {@code true}, this adapter will change {@code location}
 * attributes in the WSDL definition to reflect the URL of the incoming request. If the location field in the original
 * WSDL is an absolute path, the scheme, hostname, and port will be changed. If the location is a relative path, the
 * scheme, hostname, port, and context path will be prepended. This behavior can be customized by overriding the
 * {@code transformLocation()} method.
 * <p>
 * For instance, if the location attribute defined in the WSDL is
 * {@code http://localhost:8080/context/services/myService}, and the request URI for the WSDL is
 * {@code http://example.com/context/myService.wsdl}, the location will be changed to
 * {@code http://example.com/context/services/myService}.
 * <p>
 * If the location attribute defined in the WSDL is {@code /services/myService}, and the request URI for the WSDL is
 * {@code http://example.com:8080/context/myService.wsdl}, the location will be changed to
 * {@code http://example.com:8080/context/services/myService}.
 * <p>
 * When {@code transformLocations} is enabled, all {@code location} attributes found in the WSDL definition are changed
 * by default. This behavior can be customized by changing the {@code locationExpression} property, which is an XPath
 * expression that matches the attributes to change.
 *
 * @author Arjen Poutsma
 * @see WsdlDefinition
 * @see #setTransformLocations(boolean)
 * @see #setLocationExpression(String)
 * @see #transformLocation(String,jakarta.servlet.http.HttpServletRequest)
 * @since 1.0.0
 */
public class WsdlDefinitionHandlerAdapter extends LocationTransformerObjectSupport
		implements HandlerAdapter, InitializingBean {

	/** Default XPath expression used for extracting all {@code location} attributes from the WSDL definition. */
	public static final String DEFAULT_LOCATION_EXPRESSION = ""//@location"";

	/** Default XPath expression used for extracting all {@code schemaLocation} attributes from the WSDL definition. */
	public static final String DEFAULT_SCHEMA_LOCATION_EXPRESSION = ""//@schemaLocation"";

	private static final String CONTENT_TYPE = ""text/xml"";

	private Map<String, String> expressionNamespaces = new HashMap<String, String>();

	private String locationExpression = DEFAULT_LOCATION_EXPRESSION;

	private String schemaLocationExpression = DEFAULT_SCHEMA_LOCATION_EXPRESSION;

	private XPathExpression locationXPathExpression;

	private XPathExpression schemaLocationXPathExpression;

	private boolean transformLocations = false;

	private boolean transformSchemaLocations = false;

	/**
	 * Sets the XPath expression used for extracting the {@code location} attributes from the WSDL 1.1 definition.
	 * <p>
	 * Defaults to {@code DEFAULT_LOCATION_EXPRESSION}.
	 */
	public void setLocationExpression(String locationExpression) {
		this.locationExpression = locationExpression;
	}

	/**
	 * Sets the XPath expression used for extracting the {@code schemaLocation} attributes from the WSDL 1.1 definition.
	 * <p>
	 * Defaults to {@code DEFAULT_SCHEMA_LOCATION_EXPRESSION}.
	 */
	public void setSchemaLocationExpression(String schemaLocationExpression) {
		this.schemaLocationExpression = schemaLocationExpression;
	}

	/**
	 * Sets whether relative address locations in the WSDL are to be transformed using the request URI of the incoming
	 * {@code HttpServletRequest}. Defaults to {@code false}.
	 */
	public void setTransformLocations(boolean transformLocations) {
		this.transformLocations = transformLocations;
	}

	/**
	 * Sets whether relative address schema locations in the WSDL are to be transformed using the request URI of the
	 * incoming {@code HttpServletRequest}. Defaults to {@code false}.
	 */
	public void setTransformSchemaLocations(boolean transformSchemaLocations) {
		this.transformSchemaLocations = transformSchemaLocations;
	}

	@Override
	public long getLastModified(HttpServletRequest request, Object handler) {
		Source definitionSource = ((WsdlDefinition) handler).getSource();
		return LastModifiedHelper.getLastModified(definitionSource);
	}

	@Override
	public ModelAndView handle(HttpServletRequest request, HttpServletResponse response, Object handler)
			throws Exception {
		if (HttpTransportConstants.METHOD_GET.equals(request.getMethod())) {
			WsdlDefinition definition = (WsdlDefinition) handler;

			Transformer transformer = createTransformer();
			Source definitionSource = definition.getSource();

			if (transformLocations || transformSchemaLocations) {
				DOMResult domResult = new DOMResult();
				transformer.transform(definitionSource, domResult);
				Document definitionDocument = (Document) domResult.getNode();
				if (transformLocations) {
					transformLocations(definitionDocument, request);
				}
				if (transformSchemaLocations) {
					transformSchemaLocations(definitionDocument, request);
				}
				definitionSource = new DOMSource(definitionDocument);
			}

			response.setContentType(CONTENT_TYPE);
			StreamResult responseResult = new StreamResult(response.getOutputStream());
			transformer.transform(definitionSource, responseResult);
		} else {
			response.setStatus(HttpServletResponse.SC_METHOD_NOT_ALLOWED);
		}
		return null;
	}

	@Override
	public boolean supports(Object handler) {
		return handler instanceof WsdlDefinition;
	}

	@Override
	public void afterPropertiesSet() throws Exception {
		locationXPathExpression = XPathExpressionFactory.createXPathExpression(locationExpression, expressionNamespaces);
		schemaLocationXPathExpression = XPathExpressionFactory.createXPathExpression(schemaLocationExpression,
				expressionNamespaces);
	}

	/**
	 * Transforms all {@code location} attributes to reflect the server name given {@code HttpServletRequest}. Determines
	 * the suitable attributes by evaluating the defined XPath expression, and delegates to {@code transformLocation} to
	 * do the transformation for all attributes that match.
	 * <p>
	 * This method is only called when the {@code transformLocations} property is true.
	 *
	 * @see #setLocationExpression(String)
	 * @see #setTransformLocations(boolean)
	 * @see #transformLocation(String,jakarta.servlet.http.HttpServletRequest)
	 */
	protected void transformLocations(Document definitionDocument, HttpServletRequest request) throws Exception {
		transformLocations(locationXPathExpression, definitionDocument, request);
	}

	/**
	 * Transforms all {@code schemaLocation} attributes to reflect the server name given {@code HttpServletRequest}.
	 * Determines the suitable attributes by evaluating the defined XPath expression, and delegates to
	 * {@code transformLocation} to do the transformation for all attributes that match.
	 * <p>
	 * This method is only called when the {@code transformSchemaLocations} property is true.
	 *
	 * @see #setSchemaLocationExpression(String)
	 * @see #setTransformSchemaLocations(boolean)
	 * @see #transformLocation(String,jakarta.servlet.http.HttpServletRequest)
	 */
	protected void transformSchemaLocations(Document definitionDocument, HttpServletRequest request) throws Exception {
		transformLocations(schemaLocationXPathExpression, definitionDocument, request);
	}

}
","['Assertion Roulette', 'General Fixture']","['Assertion Roulette', 'Eager Test']",1,1,1,14
4030_18.0_admiral_testreturninitialhostlistwhennomatchingcontainerdescnamewithantiafinity,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/4030_test.java,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/4030_actual.java,"/*
 * Copyright (c) 2016-2020 VMware, Inc. All Rights Reserved.
 *
 * This product is licensed to you under the Apache License, Version 2.0 (the ""License"").
 * You may not use this product except in compliance with the License.
 *
 * This product may include a number of subcomponents with separate copyright notices
 * and license terms. Your use of these subcomponents is subject to the terms and
 * conditions of the subcomponent's license, as noted in the LICENSE file.
 */

package com.vmware.admiral.request.allocation.filter;

import static com.vmware.admiral.request.allocation.filter.AffinityConstraint.AffinityConstraintType.ANTI_AFFINITY_PREFIX;
import static com.vmware.admiral.request.allocation.filter.AffinityConstraint.AffinityConstraintType.SOFT;
import static org.junit.Assert.assertEquals;
import static org.junit.Assert.assertNotNull;
import static org.junit.Assert.assertNull;

import java.util.Arrays;
import java.util.Collections;
import java.util.UUID;

import org.junit.Test;

import com.vmware.admiral.compute.container.CompositeComponentFactoryService;
import com.vmware.admiral.compute.container.ContainerDescriptionService;
import com.vmware.admiral.compute.container.ContainerDescriptionService.ContainerDescription;
import com.vmware.admiral.compute.container.ContainerService.ContainerState;
import com.vmware.admiral.request.util.TestRequestStateFactory;
import com.vmware.admiral.request.utils.RequestUtils;
import com.vmware.xenon.common.UriUtils;

public class ServiceAntiAffinityHostFilterTest extends BaseAffinityHostFilterTest {
    private static final String CONTAINER_NAME1 = ""test-container1"";
    private static final String CONTAINER_NAME2 = ""test-container2"";
    private static final String CONTAINER_NAME3 = ""test-container3"";

    @Override
    public void setUp() throws Throwable {
        super.setUp();
        throwErrorOnFilter = true;
    }

    @Test
    public void testReturnInitialHostListWhenNoContainerDescWithAntiAfinity() throws Throwable {
        ContainerDescription desc = createDescriptions(CONTAINER_NAME1);
        assertNull(desc.affinity);
        filter = new ServiceAntiAffinityHostFilter(host, desc);

        filter(expectedLinks);
    }

    @Test
    public void testReturnInitialHostListWhenNoMatchingContainerDescNameWithAntiAfinity()
            throws Throwable {
        ContainerDescription desc1 = createDescriptions(CONTAINER_NAME1);
        createContainer(desc1, initialHostLinks.get(0));

        String[] anti_affinity = new String[] { ANTI_AFFINITY_PREFIX
                + ""not-existing-container-name"" };
        ContainerDescription desc = createDescriptions(anti_affinity);

        filter = new ServiceAntiAffinityHostFilter(host, desc);

        filter(initialHostLinks);
    }

    @Test
    public void testReturnInitialHostListWhenContainerWithSameNameInAntiAffinityButDifferentContextId()
            throws Throwable {
        ContainerDescription desc1 = createDescriptions(CONTAINER_NAME1);
        createContainerWithDifferentContextId(desc1, initialHostLinks.get(0));

        String[] anti_affinity = new String[] { ANTI_AFFINITY_PREFIX + CONTAINER_NAME1 };
        ContainerDescription desc = createDescriptions(anti_affinity);

        filter = new ServiceAntiAffinityHostFilter(host, desc);

        filter(initialHostLinks);
    }

    @Test
    public void testDoNotSelectContainerHostWhenContainerAlreadyProvisionedAndSameHostAndContextId()
            throws Throwable {
        ContainerDescription desc1 = createDescriptions(CONTAINER_NAME1);
        ContainerState container = createContainer(desc1, initialHostLinks.get(0));

        assertEquals(UriUtils.buildUriPath(
                CompositeComponentFactoryService.SELF_LINK, state.contextId),
                container.compositeComponentLink);

        String[] anti_affinity = new String[] { ANTI_AFFINITY_PREFIX + CONTAINER_NAME1 };
        ContainerDescription desc = createDescriptions(anti_affinity);

        filter = new ServiceAntiAffinityHostFilter(host, desc);

        expectedLinks.remove(container.parentLink);

        filter(expectedLinks);
    }

    @Test
    public void testMoreThanOneContainersWithDifferentHostsInAffinity() throws Throwable {
        ContainerDescription desc1 = createDescriptions(CONTAINER_NAME1);
        String hostLink1 = initialHostLinks.get(0);
        createContainer(desc1, hostLink1);

        ContainerDescription desc2 = createDescriptions(CONTAINER_NAME2);
        String hostLink2 = initialHostLinks.get(1);
        createContainer(desc2, hostLink2);

        String[] anti_affinity = new String[] {
                ANTI_AFFINITY_PREFIX + CONTAINER_NAME1,
                ANTI_AFFINITY_PREFIX + CONTAINER_NAME2 };
        ContainerDescription desc = createDescriptions(anti_affinity);

        filter = new ServiceAntiAffinityHostFilter(host, desc);

        expectedLinks.remove(hostLink1);
        expectedLinks.remove(hostLink2);

        filter(expectedLinks);
    }

    @Test
    public void testProvisionContainerWhenAnotherAlreadyProvisionedAndHasAntiAffinityRules()
            throws Throwable {

        //Create a container A which has anti affinity to B. Deploy A, then deploy B. B should NOT be placed on the same host as A
        ContainerDescription desc1 = createDescriptions(CONTAINER_NAME1, new String[] {
                ANTI_AFFINITY_PREFIX + CONTAINER_NAME2 });
        String hostLink2 = initialHostLinks.get(1);
        createContainer(desc1, hostLink2);

        ContainerDescription desc2 = createDescriptions(CONTAINER_NAME2);

        filter = new ServiceAntiAffinityHostFilter(host, desc2);

        expectedLinks.remove(hostLink2);

        state.addCustomProperty(RequestUtils.CLUSTERING_OPERATION_CUSTOM_PROP, ""true"");
        filter(expectedLinks);
    }

    @Test
    public void testSoftAntiAfinity() throws Throwable {
        ContainerDescription desc1 = createDescriptions(CONTAINER_NAME1);
        String hostLink1 = initialHostLinks.get(0);
        createContainer(desc1, hostLink1);

        ContainerDescription desc2 = createDescriptions(CONTAINER_NAME2);
        String hostLink2 = initialHostLinks.get(1);
        createContainer(desc2, hostLink2);

        ContainerDescription desc3 = createDescriptions(CONTAINER_NAME3);
        String hostLink3 = initialHostLinks.get(2);
        createContainer(desc3, hostLink3);

        String[] anti_affinity = new String[] {
                ANTI_AFFINITY_PREFIX + CONTAINER_NAME1,
                ANTI_AFFINITY_PREFIX + CONTAINER_NAME2,
                ANTI_AFFINITY_PREFIX + CONTAINER_NAME3 };
        ContainerDescription desc = createDescriptions(anti_affinity);

        filter = new ServiceAntiAffinityHostFilter(host, desc);

        // filter(Collections.emptyList());

        // when the last host is soft affinity:
        anti_affinity = new String[] {
                ANTI_AFFINITY_PREFIX + CONTAINER_NAME1,
                ANTI_AFFINITY_PREFIX + CONTAINER_NAME2,
                ANTI_AFFINITY_PREFIX + CONTAINER_NAME3 + SOFT.getValue() };

        desc = createDescriptions(anti_affinity);

        filter = new ServiceAntiAffinityHostFilter(host, desc);

        // The third host is a soft constraint so since it is the only one left disregard the soft
        // constraint in that case
        filter(Arrays.asList(hostLink3));

        // when the last host left is soft container but there is another container on the same host
        // with hard constraint.:

        String containerName4 = ""containerName4"";
        ContainerDescription desc4 = createDescriptions(containerName4);
        createContainer(desc4, hostLink3);

        anti_affinity = new String[] {
                ANTI_AFFINITY_PREFIX + CONTAINER_NAME1,
                ANTI_AFFINITY_PREFIX + CONTAINER_NAME2,
                ANTI_AFFINITY_PREFIX + CONTAINER_NAME3 + SOFT.getValue(),
                ANTI_AFFINITY_PREFIX + containerName4 };

        desc = createDescriptions(anti_affinity);

        filter = new ServiceAntiAffinityHostFilter(host, desc);

        // The third host is a soft constraint but the fourth container points to the same host and
        // it is a hard constraint
        filter(Collections.emptyList());

        // filter even soft constraints when more than one host available

        anti_affinity = new String[] {
                ANTI_AFFINITY_PREFIX + CONTAINER_NAME1 + SOFT.getValue(),
                ANTI_AFFINITY_PREFIX + CONTAINER_NAME2 + SOFT.getValue() };

        desc = createDescriptions(anti_affinity);

        filter = new ServiceAntiAffinityHostFilter(host, desc);

        filter(Arrays.asList(hostLink3));

    }

    private ContainerDescription createDescriptions(String[] affinity) throws Throwable {
        return createDescriptions(""randomName"" + Math.random(), affinity);
    }

    private ContainerDescription createDescriptions(String name) throws Throwable {
        return createDescriptions(name, null);
    }

    private ContainerDescription createDescriptions(String name, String[] affinity)
            throws Throwable {
        // loop a few times to make sure the right host is not chosen by chance
        ContainerDescription desc = TestRequestStateFactory.createContainerDescription();
        desc.documentSelfLink = UUID.randomUUID().toString();
        desc.name = name;
        desc.affinity = affinity;
        desc = doPost(desc, ContainerDescriptionService.FACTORY_LINK);
        assertNotNull(desc);
        addForDeletion(desc);

        return desc;
    }
}
","/*
 * Copyright (c) 2016 VMware, Inc. All Rights Reserved.
 *
 * This product is licensed to you under the Apache License, Version 2.0 (the ""License"").
 * You may not use this product except in compliance with the License.
 *
 * This product may include a number of subcomponents with separate copyright notices
 * and license terms. Your use of these subcomponents is subject to the terms and
 * conditions of the subcomponent's license, as noted in the LICENSE file.
 */

package com.vmware.admiral.request.allocation.filter;

import java.util.Arrays;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

import com.vmware.admiral.compute.container.ContainerDescriptionService.ContainerDescription;
import com.vmware.admiral.request.PlacementHostSelectionTaskService.PlacementHostSelectionTaskState;
import com.vmware.xenon.common.ServiceHost;
import com.vmware.xenon.common.Utils;
import com.vmware.xenon.services.common.QueryTask;

/**
 * A filter implementing {@link HostSelectionFilter} in order to provide host selection in case the
 * {@link ContainerDescription}s has <code>affinity</code> definitions. The goal of this filter is
 * to place the cluster nodes on either same or different hosts based on the affinity rules
 * definitions.
 */
public class ServiceAntiAffinityHostFilter extends ServiceAffinityHostFilter {
    protected static final boolean AFFINITY = false;

    public ServiceAntiAffinityHostFilter(final ServiceHost host, final ContainerDescription desc) {
        super(host, desc, desc.affinity, AFFINITY);
    }

    @Override
    public Map<String, AffinityConstraint> getAffinityConstraints() {
        return extractAffinityConstraints(affinityNames, AFFINITY);
    }

    @Override
    protected QueryTask getDescQuery() {
        //Get all container descriptions whose names are in the affinity constraints of this one
        //and all container descriptions who have affinity to this one

        return getBidirectionalDescQuery(ContainerDescription.FIELD_NAME_AFFINITY,
                AffinityConstraint.AffinityConstraintType.ANTI_AFFINITY_PREFIX
                        + containerDescriptionName + ""*"", getAffinity());
    }

    @Override
    protected Map<String, HostSelection> applyAffinityConstraints(
            final PlacementHostSelectionTaskState state,
            final Map<String, HostSelection> initHostSelectionMap,
            final Map<String, HostSelection> filteredHostSelectionMap) {

        if (filteredHostSelectionMap.isEmpty()) {
            return initHostSelectionMap;
        } else {
            final List<Map<String, HostSelection>> result = seperateHostsByAffinitiConstraints(
                    filteredHostSelectionMap, AFFINITY);
            final Map<String, HostSelection> hardAfinitiHosts = result.get(0);
            final Map<String, HostSelection> softAfinitiHosts = result.get(1);

            final Map<String, HostSelection> hostSelectionMap = new HashMap<>(initHostSelectionMap);
            for (HostSelection hostSelection : hardAfinitiHosts.values()) {
                hostSelectionMap.remove(hostSelection.hostLink);
            }

            if (hostSelectionMap.isEmpty()) {
                final String errMsg = String.format(
                        ""No host from %s matches anti-affinity rules %s."",
                        initHostSelectionMap.keySet(), Arrays.asList(affinityNames));
                throw new HostSelectionFilterException(errMsg, ""request.service.anti-affinity.filter.no.host"",
                        initHostSelectionMap.keySet(), Arrays.asList(affinityNames));
            }

            for (HostSelection hostSelection : softAfinitiHosts.values()) {
                if (hostSelectionMap.size() == 1) {
                    break;
                }

                hostSelectionMap.remove(hostSelection.hostLink);
            }

            return hostSelectionMap;
        }
    }

    @Override
    protected void completeWhenNoContainerDescriptionsFound(
            final PlacementHostSelectionTaskState state,
            final Map<String, HostSelection> filteredHostSelectionMap,
            final Map<String, DescName> containerDescLinksWithNames,
            final HostSelectionFilterCompletion callback) {
        Utils.logWarning(""No ContainerDescriptions found for anti-affinity with container desc %s"",
                containerDescriptionName);
        callback.complete(filteredHostSelectionMap, null);
    }

    @Override
    protected boolean hasOutgoingAffinities() {
        if (affinityNames == null || affinityNames.length == 0) {
            return false;
        } else {
            return Arrays.stream(affinityNames).anyMatch(affinity -> affinity.startsWith(""!""));
        }
    }

}
",['Unknown Test'],"['Assertion Roulette', 'Magic Number Test', 'Eager Test', 'Lazy Test', 'Sensitive Equality']",5,1,0,12
29121_4.0_hadoop_testhsjobblockfornormalsizejobshouldnotdisplaywarningmessage,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/29121_test.java,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/29121_actual.java,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * ""License""); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.hadoop.mapreduce.v2.hs.webapp;

import org.apache.commons.io.output.ByteArrayOutputStream;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.mapreduce.Counters;
import org.apache.hadoop.mapreduce.v2.api.records.*;
import org.apache.hadoop.mapreduce.v2.api.records.impl.pb.JobIdPBImpl;
import org.apache.hadoop.mapreduce.v2.app.job.Task;
import org.apache.hadoop.mapreduce.v2.app.webapp.AMParams;
import org.apache.hadoop.mapreduce.v2.hs.CompletedJob;
import org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager;
import org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.HistoryFileInfo;
import org.apache.hadoop.mapreduce.v2.hs.JobHistory;
import org.apache.hadoop.mapreduce.v2.hs.UnparsedJob;
import org.apache.hadoop.mapreduce.v2.jobhistory.JHAdminConfig;
import org.apache.hadoop.yarn.api.records.ApplicationId;
import org.apache.hadoop.yarn.util.StringHelper;
import org.apache.hadoop.yarn.webapp.ResponseInfo;
import org.apache.hadoop.yarn.webapp.SubView;
import org.apache.hadoop.yarn.webapp.view.BlockForTest;
import org.apache.hadoop.yarn.webapp.view.HtmlBlock;
import org.apache.hadoop.yarn.webapp.view.HtmlBlockForTest;

import org.junit.Assert;
import org.junit.Test;

import java.io.IOException;
import java.io.OutputStream;
import java.io.PrintWriter;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.Map;

import static org.mockito.ArgumentMatchers.any;
import static org.mockito.Mockito.mock;
import static org.mockito.Mockito.when;

/**
 * Test the HsJobBlock generated for oversized jobs in JHS.
 */
public class TestHsJobBlock {

  @Test
  public void testHsJobBlockForOversizeJobShouldDisplayWarningMessage() {
    int maxAllowedTaskNum = 100;

    Configuration config = new Configuration();
    config.setInt(JHAdminConfig.MR_HS_LOADED_JOBS_TASKS_MAX, maxAllowedTaskNum);

    JobHistory jobHistory =
        new JobHistoryStubWithAllOversizeJobs(maxAllowedTaskNum);
    jobHistory.init(config);

    HsJobBlock jobBlock = new HsJobBlock(jobHistory) {
      // override this so that job block can fetch a job id.
      @Override
      public Map<String, String> moreParams() {
        Map<String, String> map = new HashMap<>();
        map.put(AMParams.JOB_ID, ""job_0000_0001"");
        return map;
      }
    };

    // set up the test block to render HsJobBLock to
    OutputStream outputStream = new ByteArrayOutputStream();
    HtmlBlock.Block block = createBlockToCreateTo(outputStream);

    jobBlock.render(block);

    block.getWriter().flush();
    String out = outputStream.toString();
    Assert.assertTrue(""Should display warning message for jobs that have too "" +
        ""many tasks"", out.contains(""Any job larger than "" + maxAllowedTaskNum +
            "" will not be loaded""));
  }

  @Test
  public void testHsJobBlockForNormalSizeJobShouldNotDisplayWarningMessage() {

    Configuration config = new Configuration();
    config.setInt(JHAdminConfig.MR_HS_LOADED_JOBS_TASKS_MAX, -1);

    JobHistory jobHistory = new JobHitoryStubWithAllNormalSizeJobs();
    jobHistory.init(config);

    HsJobBlock jobBlock = new HsJobBlock(jobHistory) {
      // override this so that the job block can fetch a job id.
      @Override
      public Map<String, String> moreParams() {
        Map<String, String> map = new HashMap<>();
        map.put(AMParams.JOB_ID, ""job_0000_0001"");
        return map;
      }

      // override this to avoid view context lookup in render()
      @Override
      public ResponseInfo info(String about) {
        return new ResponseInfo().about(about);
      }

      // override this to avoid view context lookup in render()
      @Override
      public String url(String... parts) {
        return StringHelper.ujoin("""", parts);
      }
    };

    // set up the test block to render HsJobBLock to
    OutputStream outputStream = new ByteArrayOutputStream();
    HtmlBlock.Block block = createBlockToCreateTo(outputStream);

    jobBlock.render(block);

    block.getWriter().flush();
    String out = outputStream.toString();

    Assert.assertTrue(""Should display job overview for the job."",
        out.contains(""ApplicationMaster""));
  }

  private static HtmlBlock.Block createBlockToCreateTo(
      OutputStream outputStream) {
    PrintWriter printWriter = new PrintWriter(outputStream);
    HtmlBlock html = new HtmlBlockForTest();
    return new BlockForTest(html, printWriter, 10, false) {
      @Override
      protected void subView(Class<? extends SubView> cls) {
      }
    };
  };

  /**
   * A JobHistory stub that treat all jobs as oversized and therefore will
   * not parse their job history files but return a UnparseJob instance.
   */
  static class JobHistoryStubWithAllOversizeJobs extends JobHistory {
    private final int maxAllowedTaskNum;

    public JobHistoryStubWithAllOversizeJobs(int maxAllowedTaskNum) {
      this.maxAllowedTaskNum = maxAllowedTaskNum;
    }

    @Override
    protected HistoryFileManager createHistoryFileManager() {
      HistoryFileManager historyFileManager;
      try {
        HistoryFileInfo historyFileInfo =
            createUnparsedJobHistoryFileInfo(maxAllowedTaskNum);

        historyFileManager = mock(HistoryFileManager.class);
        when(historyFileManager.getFileInfo(any(JobId.class))).thenReturn(
            historyFileInfo);
      } catch (IOException ex) {
        // this should never happen
        historyFileManager = super.createHistoryFileManager();
      }
      return historyFileManager;
    }

    private static HistoryFileInfo createUnparsedJobHistoryFileInfo(
        int maxAllowedTaskNum) throws IOException {
      HistoryFileInfo fileInfo = mock(HistoryFileInfo.class);

      // create an instance of UnparsedJob for a large job
      UnparsedJob unparsedJob = mock(UnparsedJob.class);
      when(unparsedJob.getMaxTasksAllowed()).thenReturn(maxAllowedTaskNum);
      when(unparsedJob.getTotalMaps()).thenReturn(maxAllowedTaskNum);
      when(unparsedJob.getTotalReduces()).thenReturn(maxAllowedTaskNum);

      when(fileInfo.loadJob()).thenReturn(unparsedJob);

      return fileInfo;
    }
  }

  /**
   * A JobHistory stub that treats all jobs as normal size and therefore will
   * return a CompletedJob on HistoryFileInfo.loadJob().
   */
  static class JobHitoryStubWithAllNormalSizeJobs extends  JobHistory {
    @Override
    public HistoryFileManager createHistoryFileManager() {
      HistoryFileManager historyFileManager;
      try {
        HistoryFileInfo historyFileInfo = createParsedJobHistoryFileInfo();

        historyFileManager = mock(HistoryFileManager.class);
        when(historyFileManager.getFileInfo(any(JobId.class))).thenReturn(
            historyFileInfo);
      } catch (IOException ex) {
        // this should never happen
        historyFileManager = super.createHistoryFileManager();
      }
      return historyFileManager;

    }

    private static HistoryFileInfo createParsedJobHistoryFileInfo()
        throws IOException {
      HistoryFileInfo fileInfo = mock(HistoryFileInfo.class);
      CompletedJob job = createFakeCompletedJob();
      when(fileInfo.loadJob()).thenReturn(job);
      return fileInfo;
    }


    private static CompletedJob createFakeCompletedJob() {
      CompletedJob job = mock(CompletedJob.class);

      when(job.getTotalMaps()).thenReturn(0);
      when(job.getCompletedMaps()).thenReturn(0);
      when(job.getTotalReduces()).thenReturn(0);
      when(job.getCompletedReduces()).thenReturn(0);

      JobId jobId = createFakeJobId();
      when(job.getID()).thenReturn(jobId);

      JobReport jobReport = mock(JobReport.class);
      when(jobReport.getSubmitTime()).thenReturn(-1L);
      when(jobReport.getStartTime()).thenReturn(-1L);
      when(jobReport.getFinishTime()).thenReturn(-1L);
      when(job.getReport()).thenReturn(jobReport);

      when(job.getAMInfos()).thenReturn(new ArrayList<AMInfo>());
      when(job.getDiagnostics()).thenReturn(new ArrayList<String>());
      when(job.getName()).thenReturn(""fake completed job"");
      when(job.getQueueName()).thenReturn(""default"");
      when(job.getUserName()).thenReturn(""junit"");
      when(job.getState()).thenReturn(JobState.ERROR);
      when(job.getAllCounters()).thenReturn(new Counters());
      when(job.getTasks()).thenReturn(new HashMap<TaskId, Task>());

      return job;
    }

    private static JobId createFakeJobId() {
      JobId jobId = new JobIdPBImpl();
      jobId.setId(0);

      ApplicationId appId = mock(ApplicationId.class);
      when(appId.getClusterTimestamp()).thenReturn(0L);
      when(appId.getId()).thenReturn(0);

      jobId.setAppId(appId);

      return jobId;
    }
  }

}
","/**
* Licensed to the Apache Software Foundation (ASF) under one
* or more contributor license agreements.  See the NOTICE file
* distributed with this work for additional information
* regarding copyright ownership.  The ASF licenses this file
* to you under the Apache License, Version 2.0 (the
* ""License""); you may not use this file except in compliance
* with the License.  You may obtain a copy of the License at
*
*     http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an ""AS IS"" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
*/

package org.apache.hadoop.mapreduce.v2.hs.webapp;

import static org.apache.hadoop.mapreduce.v2.app.webapp.AMParams.JOB_ID;
import static org.apache.hadoop.yarn.webapp.view.JQueryUI._EVEN;
import static org.apache.hadoop.yarn.webapp.view.JQueryUI._INFO_WRAP;
import static org.apache.hadoop.yarn.webapp.view.JQueryUI._ODD;
import static org.apache.hadoop.yarn.webapp.view.JQueryUI._TH;

import java.util.Date;
import java.util.List;

import org.apache.hadoop.mapreduce.TaskID;
import org.apache.hadoop.mapreduce.v2.api.records.AMInfo;
import org.apache.hadoop.mapreduce.v2.api.records.JobId;
import org.apache.hadoop.mapreduce.v2.app.AppContext;
import org.apache.hadoop.mapreduce.v2.app.job.Job;
import org.apache.hadoop.mapreduce.v2.app.webapp.dao.ConfEntryInfo;
import org.apache.hadoop.mapreduce.v2.hs.UnparsedJob;
import org.apache.hadoop.mapreduce.v2.hs.webapp.dao.AMAttemptInfo;
import org.apache.hadoop.mapreduce.v2.hs.webapp.dao.JobInfo;
import org.apache.hadoop.mapreduce.v2.jobhistory.JHAdminConfig;
import org.apache.hadoop.mapreduce.v2.util.MRApps;
import org.apache.hadoop.mapreduce.v2.util.MRApps.TaskAttemptStateUI;
import org.apache.hadoop.mapreduce.v2.util.MRWebAppUtil;
import org.apache.hadoop.util.StringUtils;
import org.apache.hadoop.yarn.util.Times;
import org.apache.hadoop.yarn.webapp.ResponseInfo;
import org.apache.hadoop.yarn.webapp.hamlet2.Hamlet;
import org.apache.hadoop.yarn.webapp.hamlet2.Hamlet.DIV;
import org.apache.hadoop.yarn.webapp.hamlet2.Hamlet.TABLE;
import org.apache.hadoop.yarn.webapp.view.HtmlBlock;
import org.apache.hadoop.yarn.webapp.view.InfoBlock;

import com.google.inject.Inject;

/**
 * Render a block of HTML for a give job.
 */
public class HsJobBlock extends HtmlBlock {
  final AppContext appContext;

  @Inject HsJobBlock(AppContext appctx) {
    appContext = appctx;
  }

  /*
   * (non-Javadoc)
   * @see org.apache.hadoop.yarn.webapp.view.HtmlBlock#render(org.apache.hadoop.yarn.webapp.view.HtmlBlock.Block)
   */
  @Override protected void render(Block html) {
    String jid = $(JOB_ID);
    if (jid.isEmpty()) {
      html.
        p().__(""Sorry, can't do anything without a JobID."").__();
      return;
    }
    JobId jobID = MRApps.toJobID(jid);
    Job j = appContext.getJob(jobID);
    if (j == null) {
      html.p().__(""Sorry, "", jid, "" not found."").__();
      return;
    }
    if(j instanceof UnparsedJob) {
      final int taskCount = j.getTotalMaps() + j.getTotalReduces();
      UnparsedJob oversizedJob = (UnparsedJob) j;
      html.p().__(""The job has a total of "" + taskCount + "" tasks. "")
          .__(""Any job larger than "" + oversizedJob.getMaxTasksAllowed() +
              "" will not be loaded."").__();
      html.p().__(""You can either use the CLI tool: 'mapred job -history'""
          + "" to view large jobs or adjust the property "" +
          JHAdminConfig.MR_HS_LOADED_JOBS_TASKS_MAX + ""."").__();
      return;
    }
    List<AMInfo> amInfos = j.getAMInfos();
    JobInfo job = new JobInfo(j);
    ResponseInfo infoBlock = info(""Job Overview"").
        __(""Job Name:"", job.getName()).
        __(""User Name:"", job.getUserName()).
        __(""Queue:"", job.getQueueName()).
        __(""State:"", job.getState()).
        __(""Uberized:"", job.isUber()).
        __(""Submitted:"", new Date(job.getSubmitTime())).
        __(""Started:"", job.getStartTimeStr()).
        __(""Finished:"", new Date(job.getFinishTime())).
        __(""Elapsed:"", StringUtils.formatTime(
            Times.elapsed(job.getStartTime(), job.getFinishTime(), false)));
    
    String amString =
        amInfos.size() == 1 ? ""ApplicationMaster"" : ""ApplicationMasters""; 
    
    // todo - switch to use JobInfo
    List<String> diagnostics = j.getDiagnostics();
    if(diagnostics != null && !diagnostics.isEmpty()) {
      StringBuilder b = new StringBuilder();
      for(String diag: diagnostics) {
        b.append(addTaskLinks(diag));
      }
      infoBlock._r(""Diagnostics:"", b.toString());
    }

    if(job.getNumMaps() > 0) {
      infoBlock.__(""Average Map Time"", StringUtils.formatTime(job.getAvgMapTime()));
    }
    if(job.getNumReduces() > 0) {
      infoBlock.__(""Average Shuffle Time"", StringUtils.formatTime(job.getAvgShuffleTime()));
      infoBlock.__(""Average Merge Time"", StringUtils.formatTime(job.getAvgMergeTime()));
      infoBlock.__(""Average Reduce Time"", StringUtils.formatTime(job.getAvgReduceTime()));
    }

    for (ConfEntryInfo entry : job.getAcls()) {
      infoBlock.__(""ACL ""+entry.getName()+"":"", entry.getValue());
    }
    DIV<Hamlet> div = html.
        __(InfoBlock.class).
      div(_INFO_WRAP);
    
      // MRAppMasters Table
        TABLE<DIV<Hamlet>> table = div.table(""#job"");
        table.
          tr().
            th(amString).
            __().
          tr().
            th(_TH, ""Attempt Number"").
            th(_TH, ""Start Time"").
            th(_TH, ""Node"").
            th(_TH, ""Logs"").
            __();
        boolean odd = false;
          for (AMInfo amInfo : amInfos) {
            AMAttemptInfo attempt = new AMAttemptInfo(amInfo,
                job.getId(), job.getUserName(), """", """");
            table.tr((odd = !odd) ? _ODD : _EVEN).
              td(String.valueOf(attempt.getAttemptId())).
              td(new Date(attempt.getStartTime()).toString()).
              td().a("".nodelink"", url(MRWebAppUtil.getYARNWebappScheme(),
                  attempt.getNodeHttpAddress()),
                  attempt.getNodeHttpAddress()).__().
              td().a("".logslink"", url(attempt.getLogsLink()),
                      ""logs"").__().
                __();
          }
          table.__();
          div.__();
          
        
        html.div(_INFO_WRAP).        
      
      // Tasks table
        table(""#job"").
          tr().
            th(_TH, ""Task Type"").
            th(_TH, ""Total"").
            th(_TH, ""Complete"").__().
          tr(_ODD).
            th().
              a(url(""tasks"", jid, ""m""), ""Map"").__().
            td(String.valueOf(String.valueOf(job.getMapsTotal()))).
            td(String.valueOf(String.valueOf(job.getMapsCompleted()))).__().
          tr(_EVEN).
            th().
              a(url(""tasks"", jid, ""r""), ""Reduce"").__().
            td(String.valueOf(String.valueOf(job.getReducesTotal()))).
            td(String.valueOf(String.valueOf(job.getReducesCompleted()))).__()
          .__().

        // Attempts table
        table(""#job"").
        tr().
          th(_TH, ""Attempt Type"").
          th(_TH, ""Failed"").
          th(_TH, ""Killed"").
          th(_TH, ""Successful"").__().
        tr(_ODD).
          th(""Maps"").
          td().a(url(""attempts"", jid, ""m"",
              TaskAttemptStateUI.FAILED.toString()), 
              String.valueOf(job.getFailedMapAttempts())).__().
          td().a(url(""attempts"", jid, ""m"",
              TaskAttemptStateUI.KILLED.toString()), 
              String.valueOf(job.getKilledMapAttempts())).__().
          td().a(url(""attempts"", jid, ""m"",
              TaskAttemptStateUI.SUCCESSFUL.toString()), 
              String.valueOf(job.getSuccessfulMapAttempts())).__().
            __().
        tr(_EVEN).
          th(""Reduces"").
          td().a(url(""attempts"", jid, ""r"",
              TaskAttemptStateUI.FAILED.toString()), 
              String.valueOf(job.getFailedReduceAttempts())).__().
          td().a(url(""attempts"", jid, ""r"",
              TaskAttemptStateUI.KILLED.toString()), 
              String.valueOf(job.getKilledReduceAttempts())).__().
          td().a(url(""attempts"", jid, ""r"",
              TaskAttemptStateUI.SUCCESSFUL.toString()), 
              String.valueOf(job.getSuccessfulReduceAttempts())).__().
            __().
            __().
            __();
  }

  static String addTaskLinks(String text) {
    return TaskID.taskIdPattern.matcher(text).replaceAll(
        ""<a href=\""/jobhistory/task/$0\"">$0</a>"");
  }
}
",['Lazy Test'],"['Assertion Roulette', 'Eager Test', 'Magic Number Test', 'Mystery Guest', 'Sleepy Test', 'Sensitive Equality']",6,1,0,12
51224_37.0_jimfs_testasyncclose_write,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/51224_test.java,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/51224_actual.java,"/*
 * Copyright 2013 Google Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the ""License"");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.google.common.jimfs;

import static com.google.common.jimfs.TestUtils.buffer;
import static com.google.common.jimfs.TestUtils.regularFile;
import static com.google.common.truth.Truth.assertThat;
import static java.nio.file.StandardOpenOption.READ;
import static java.nio.file.StandardOpenOption.WRITE;
import static java.util.concurrent.TimeUnit.MILLISECONDS;
import static java.util.concurrent.TimeUnit.SECONDS;
import static org.junit.Assert.assertEquals;
import static org.junit.Assert.assertFalse;
import static org.junit.Assert.assertNotNull;
import static org.junit.Assert.assertSame;
import static org.junit.Assert.fail;

import com.google.common.collect.ImmutableSet;
import com.google.common.util.concurrent.Runnables;
import com.google.common.util.concurrent.SettableFuture;
import com.google.common.util.concurrent.Uninterruptibles;
import java.io.IOException;
import java.nio.ByteBuffer;
import java.nio.channels.AsynchronousCloseException;
import java.nio.channels.AsynchronousFileChannel;
import java.nio.channels.ClosedChannelException;
import java.nio.channels.CompletionHandler;
import java.nio.channels.FileLock;
import java.nio.file.OpenOption;
import java.util.concurrent.ExecutionException;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.concurrent.Future;
import org.junit.Test;
import org.junit.runner.RunWith;
import org.junit.runners.JUnit4;

/**
 * Tests for {@link JimfsAsynchronousFileChannel}.
 *
 * @author Colin Decker
 */
@RunWith(JUnit4.class)
public class JimfsAsynchronousFileChannelTest {

  private static JimfsAsynchronousFileChannel channel(
      RegularFile file, ExecutorService executor, OpenOption... options) throws IOException {
    JimfsFileChannel channel =
        new JimfsFileChannel(
            file,
            Options.getOptionsForChannel(ImmutableSet.copyOf(options)),
            new FileSystemState(new FakeFileTimeSource(), Runnables.doNothing()));
    return new JimfsAsynchronousFileChannel(channel, executor);
  }

  /**
   * Just tests the main read/write methods... the methods all delegate to the non-async channel
   * anyway.
   */
  @Test
  public void testAsyncChannel() throws Throwable {
    RegularFile file = regularFile(15);
    ExecutorService executor = Executors.newSingleThreadExecutor();
    JimfsAsynchronousFileChannel channel = channel(file, executor, READ, WRITE);

    try {
      assertEquals(15, channel.size());

      assertSame(channel, channel.truncate(5));
      assertEquals(5, channel.size());

      file.write(5, new byte[5], 0, 5);
      checkAsyncRead(channel);
      checkAsyncWrite(channel);
      checkAsyncLock(channel);

      channel.close();
      assertFalse(channel.isOpen());
    } finally {
      executor.shutdown();
    }
  }

  @Test
  public void testClosedChannel() throws Throwable {
    RegularFile file = regularFile(15);
    ExecutorService executor = Executors.newSingleThreadExecutor();

    try {
      JimfsAsynchronousFileChannel channel = channel(file, executor, READ, WRITE);
      channel.close();

      assertClosed(channel.read(ByteBuffer.allocate(10), 0));
      assertClosed(channel.write(ByteBuffer.allocate(10), 15));
      assertClosed(channel.lock());
      assertClosed(channel.lock(0, 10, true));
    } finally {
      executor.shutdown();
    }
  }

  @Test
  public void testAsyncClose_write() throws Throwable {
    RegularFile file = regularFile(15);
    ExecutorService executor = Executors.newFixedThreadPool(4);

    try {
      JimfsAsynchronousFileChannel channel = channel(file, executor, READ, WRITE);

      file.writeLock().lock(); // cause another thread trying to write to block

      // future-returning write
      Future<Integer> future = channel.write(ByteBuffer.allocate(10), 0);

      // completion handler write
      SettableFuture<Integer> completionHandlerFuture = SettableFuture.create();
      channel.write(ByteBuffer.allocate(10), 0, null, setFuture(completionHandlerFuture));

      // Despite this 10ms sleep to allow plenty of time, it's possible, though very rare, for a
      // race to cause the channel to be closed before the asynchronous calls get to the initial
      // check that the channel is open, causing ClosedChannelException to be thrown rather than
      // AsynchronousCloseException. This is not a problem in practice, just a quirk of how these
      // tests work and that we don't have a way of waiting for the operations to get past that
      // check.
      Uninterruptibles.sleepUninterruptibly(10, MILLISECONDS);

      channel.close();

      assertAsynchronousClose(future);
      assertAsynchronousClose(completionHandlerFuture);
    } finally {
      executor.shutdown();
    }
  }

  @Test
  public void testAsyncClose_read() throws Throwable {
    RegularFile file = regularFile(15);
    ExecutorService executor = Executors.newFixedThreadPool(2);

    try {
      JimfsAsynchronousFileChannel channel = channel(file, executor, READ, WRITE);

      file.writeLock().lock(); // cause another thread trying to read to block

      // future-returning read
      Future<Integer> future = channel.read(ByteBuffer.allocate(10), 0);

      // completion handler read
      SettableFuture<Integer> completionHandlerFuture = SettableFuture.create();
      channel.read(ByteBuffer.allocate(10), 0, null, setFuture(completionHandlerFuture));

      // Despite this 10ms sleep to allow plenty of time, it's possible, though very rare, for a
      // race to cause the channel to be closed before the asynchronous calls get to the initial
      // check that the channel is open, causing ClosedChannelException to be thrown rather than
      // AsynchronousCloseException. This is not a problem in practice, just a quirk of how these
      // tests work and that we don't have a way of waiting for the operations to get past that
      // check.
      Uninterruptibles.sleepUninterruptibly(10, MILLISECONDS);

      channel.close();

      assertAsynchronousClose(future);
      assertAsynchronousClose(completionHandlerFuture);
    } finally {
      executor.shutdown();
    }
  }

  private static void checkAsyncRead(AsynchronousFileChannel channel) throws Throwable {
    ByteBuffer buf = buffer(""1234567890"");
    assertEquals(10, (int) channel.read(buf, 0).get());

    buf.flip();

    SettableFuture<Integer> future = SettableFuture.create();
    channel.read(buf, 0, null, setFuture(future));

    assertThat(future.get(10, SECONDS)).isEqualTo(10);
  }

  private static void checkAsyncWrite(AsynchronousFileChannel asyncChannel) throws Throwable {
    ByteBuffer buf = buffer(""1234567890"");
    assertEquals(10, (int) asyncChannel.write(buf, 0).get());

    buf.flip();
    SettableFuture<Integer> future = SettableFuture.create();
    asyncChannel.write(buf, 0, null, setFuture(future));

    assertThat(future.get(10, SECONDS)).isEqualTo(10);
  }

  private static void checkAsyncLock(AsynchronousFileChannel channel) throws Throwable {
    assertNotNull(channel.lock().get());
    assertNotNull(channel.lock(0, 10, true).get());

    SettableFuture<FileLock> future = SettableFuture.create();
    channel.lock(0, 10, true, null, setFuture(future));

    assertNotNull(future.get(10, SECONDS));
  }

  /**
   * Returns a {@code CompletionHandler} that sets the appropriate result or exception on the given
   * {@code future} on completion.
   */
  private static <T> CompletionHandler<T, Object> setFuture(final SettableFuture<T> future) {
    return new CompletionHandler<T, Object>() {
      @Override
      public void completed(T result, Object attachment) {
        future.set(result);
      }

      @Override
      public void failed(Throwable exc, Object attachment) {
        future.setException(exc);
      }
    };
  }

  /** Assert that the future fails, with the failure caused by {@code ClosedChannelException}. */
  private static void assertClosed(Future<?> future) throws Throwable {
    try {
      future.get(10, SECONDS);
      fail(""ChannelClosedException was not thrown"");
    } catch (ExecutionException expected) {
      assertThat(expected.getCause()).isInstanceOf(ClosedChannelException.class);
    }
  }

  /**
   * Assert that the future fails, with the failure caused by either {@code
   * AsynchronousCloseException} or (rarely) {@code ClosedChannelException}.
   */
  private static void assertAsynchronousClose(Future<?> future) throws Throwable {
    try {
      future.get(10, SECONDS);
      fail(""no exception was thrown"");
    } catch (ExecutionException expected) {
      Throwable t = expected.getCause();
      if (!(t instanceof AsynchronousCloseException || t instanceof ClosedChannelException)) {
        fail(
            ""expected AsynchronousCloseException (or in rare cases ClosedChannelException); got ""
                + t);
      }
    }
  }
}
","/*
 * Copyright 2013 Google Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the ""License"");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.google.common.jimfs;

import static com.google.common.base.Preconditions.checkArgument;
import static com.google.common.base.Preconditions.checkNotNull;

import com.google.common.util.concurrent.Futures;
import com.google.common.util.concurrent.ListenableFuture;
import com.google.common.util.concurrent.ListeningExecutorService;
import com.google.common.util.concurrent.MoreExecutors;
import com.google.common.util.concurrent.SettableFuture;
import com.google.errorprone.annotations.CanIgnoreReturnValue;
import java.io.IOException;
import java.nio.ByteBuffer;
import java.nio.channels.AsynchronousFileChannel;
import java.nio.channels.ClosedChannelException;
import java.nio.channels.CompletionHandler;
import java.nio.channels.FileLock;
import java.util.concurrent.Callable;
import java.util.concurrent.ExecutionException;
import java.util.concurrent.ExecutorService;
import org.checkerframework.checker.nullness.qual.Nullable;

/**
 * {@link AsynchronousFileChannel} implementation that delegates to a {@link JimfsFileChannel}.
 *
 * @author Colin Decker
 */
final class JimfsAsynchronousFileChannel extends AsynchronousFileChannel {

  private final JimfsFileChannel channel;
  private final ListeningExecutorService executor;

  public JimfsAsynchronousFileChannel(JimfsFileChannel channel, ExecutorService executor) {
    this.channel = checkNotNull(channel);
    this.executor = MoreExecutors.listeningDecorator(executor);
  }

  @Override
  public long size() throws IOException {
    return channel.size();
  }

  private <R, A> void addCallback(
      ListenableFuture<R> future, CompletionHandler<R, ? super A> handler, @Nullable A attachment) {
    future.addListener(new CompletionHandlerCallback<>(future, handler, attachment), executor);
  }

  @Override
  @CanIgnoreReturnValue
  public AsynchronousFileChannel truncate(long size) throws IOException {
    channel.truncate(size);
    return this;
  }

  @Override
  public void force(boolean metaData) throws IOException {
    channel.force(metaData);
  }

  @Override
  public <A> void lock(
      long position,
      long size,
      boolean shared,
      @Nullable A attachment,
      CompletionHandler<FileLock, ? super A> handler) {
    checkNotNull(handler);
    addCallback(lock(position, size, shared), handler, attachment);
  }

  @Override
  public ListenableFuture<FileLock> lock(
      final long position, final long size, final boolean shared) {
    Util.checkNotNegative(position, ""position"");
    Util.checkNotNegative(size, ""size"");
    if (!isOpen()) {
      return closedChannelFuture();
    }
    if (shared) {
      channel.checkReadable();
    } else {
      channel.checkWritable();
    }
    return executor.submit(
        new Callable<FileLock>() {
          @Override
          public FileLock call() throws IOException {
            return tryLock(position, size, shared);
          }
        });
  }

  @Override
  public FileLock tryLock(long position, long size, boolean shared) throws IOException {
    Util.checkNotNegative(position, ""position"");
    Util.checkNotNegative(size, ""size"");
    channel.checkOpen();
    if (shared) {
      channel.checkReadable();
    } else {
      channel.checkWritable();
    }
    return new JimfsFileChannel.FakeFileLock(this, position, size, shared);
  }

  @Override
  public <A> void read(
      ByteBuffer dst,
      long position,
      @Nullable A attachment,
      CompletionHandler<Integer, ? super A> handler) {
    addCallback(read(dst, position), handler, attachment);
  }

  @Override
  public ListenableFuture<Integer> read(final ByteBuffer dst, final long position) {
    checkArgument(!dst.isReadOnly(), ""dst may not be read-only"");
    Util.checkNotNegative(position, ""position"");
    if (!isOpen()) {
      return closedChannelFuture();
    }
    channel.checkReadable();
    return executor.submit(
        new Callable<Integer>() {
          @Override
          public Integer call() throws IOException {
            return channel.read(dst, position);
          }
        });
  }

  @Override
  public <A> void write(
      ByteBuffer src,
      long position,
      @Nullable A attachment,
      CompletionHandler<Integer, ? super A> handler) {
    addCallback(write(src, position), handler, attachment);
  }

  @Override
  public ListenableFuture<Integer> write(final ByteBuffer src, final long position) {
    Util.checkNotNegative(position, ""position"");
    if (!isOpen()) {
      return closedChannelFuture();
    }
    channel.checkWritable();
    return executor.submit(
        new Callable<Integer>() {
          @Override
          public Integer call() throws IOException {
            return channel.write(src, position);
          }
        });
  }

  @Override
  public boolean isOpen() {
    return channel.isOpen();
  }

  @Override
  public void close() throws IOException {
    channel.close();
  }

  /** Immediate future indicating that the channel is closed. */
  private static <V> ListenableFuture<V> closedChannelFuture() {
    SettableFuture<V> future = SettableFuture.create();
    future.setException(new ClosedChannelException());
    return future;
  }

  /** Runnable callback that wraps a {@link CompletionHandler} and an attachment. */
  private static final class CompletionHandlerCallback<R, A> implements Runnable {

    private final ListenableFuture<R> future;
    private final CompletionHandler<R, ? super A> completionHandler;
    private final @Nullable A attachment;

    private CompletionHandlerCallback(
        ListenableFuture<R> future,
        CompletionHandler<R, ? super A> completionHandler,
        @Nullable A attachment) {
      this.future = checkNotNull(future);
      this.completionHandler = checkNotNull(completionHandler);
      this.attachment = attachment;
    }

    @Override
    public void run() {
      R result;
      try {
        result = Futures.getDone(future);
      } catch (ExecutionException e) {
        onFailure(e.getCause());
        return;
      } catch (RuntimeException | Error e) {
        onFailure(e);
        return;
      }

      onSuccess(result);
    }

    private void onSuccess(R result) {
      completionHandler.completed(result, attachment);
    }

    private void onFailure(Throwable t) {
      completionHandler.failed(t, attachment);
    }
  }
}
",['Assertion Roulette'],"['Assertion Roulette', 'Eager Test', 'Lazy Test', 'Magic Number Test', 'Redundant Print', 'Sensitive Equality']",5,0,1,13
51014_43.0_timely_testpersistencewithvisibility,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/51014_test.java,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/51014_actual.java,"package timely.server.integration;

import static com.google.common.util.concurrent.Uninterruptibles.sleepUninterruptibly;
import static org.junit.Assert.assertEquals;
import static org.junit.Assert.assertTrue;
import static timely.server.test.TestConfiguration.WAIT_SECONDS;

import java.nio.ByteBuffer;
import java.util.EnumSet;
import java.util.Map;
import java.util.concurrent.TimeUnit;

import org.apache.accumulo.core.data.Key;
import org.apache.accumulo.core.data.Value;
import org.apache.accumulo.core.iterators.IteratorUtil.IteratorScope;
import org.apache.accumulo.core.security.Authorizations;
import org.junit.After;
import org.junit.Assert;
import org.junit.Before;
import org.junit.Rule;
import org.junit.Test;
import org.junit.experimental.categories.Category;
import org.junit.runner.RunWith;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.beans.factory.annotation.Qualifier;
import org.springframework.boot.test.context.SpringBootTest;
import org.springframework.test.context.ActiveProfiles;
import org.springframework.test.context.junit4.SpringRunner;

import timely.api.request.MetricRequest;
import timely.client.tcp.TcpClient;
import timely.model.Metric;
import timely.model.Tag;
import timely.server.test.TestCaptureRequestHandler;
import timely.test.IntegrationTest;
import timely.test.TimelyTestRule;

@Category(IntegrationTest.class)
@RunWith(SpringRunner.class)
@SpringBootTest(webEnvironment = SpringBootTest.WebEnvironment.NONE)
@ActiveProfiles({""oneWaySsl""})
public class TcpClientIT extends OneWaySSLBase {

    private static final Long TEST_TIME = (System.currentTimeMillis() / 1000) * 1000;

    @Autowired
    @Rule
    public TimelyTestRule testRule;

    @Autowired
    @Qualifier(""tcp"")
    public TestCaptureRequestHandler tcpRequests;

    private String hostIp;
    private int tcpPort;

    @Before
    public void setup() {
        super.setup();
        hostIp = serverProperties.getIp();
        tcpPort = serverProperties.getTcpPort();
        tcpRequests.clear();
    }

    @After
    public void cleanup() {
        super.cleanup();
    }

    @Test
    public void testPut() throws Exception {
        try (TcpClient client = new TcpClient(hostIp, tcpPort)) {
            client.open();
            client.write(""put sys.cpu.user "" + TEST_TIME + "" 1.0 tag1=value1 tag2=value2\n"");
            client.flush();
            while (1 != tcpRequests.getCount()) {
                Thread.sleep(5);
            }
            Assert.assertEquals(1, tcpRequests.getResponses().size());
            Assert.assertEquals(MetricRequest.class, tcpRequests.getResponses().get(0).getClass());
            final MetricRequest actual = (MetricRequest) tcpRequests.getResponses().get(0);
            // @formatter:off
            final MetricRequest expected = new MetricRequest(
                    Metric.newBuilder()
                            .name(""sys.cpu.user"")
                            .value(TEST_TIME, 1.0D)
                            .tag(new Tag(""tag1"", ""value1""))
                            .tag(new Tag(""tag2"", ""value2""))
                            .build()
            );
            // @formatter:on
            Assert.assertEquals(expected, actual);
        }
    }

    @Test
    public void testPutMultiple() throws Exception {

        try (TcpClient client = new TcpClient(hostIp, tcpPort)) {
            client.open();
            // @formatter:off
            client.write(""put sys.cpu.user "" + TEST_TIME + "" 1.0 tag1=value1 tag2=value2\n""
                       + ""put sys.cpu.idle "" + (TEST_TIME + 1) + "" 1.0 tag3=value3 tag4=value4\n"");
            client.flush();
            while (2 != tcpRequests.getCount()) {
                Thread.sleep(5);
            }
            Assert.assertEquals(2, tcpRequests.getResponses().size());
            Assert.assertEquals(MetricRequest.class, tcpRequests.getResponses().get(0).getClass());
            MetricRequest actual = (MetricRequest) tcpRequests.getResponses().get(0);
            MetricRequest expected = new MetricRequest(
                    Metric.newBuilder()
                            .name(""sys.cpu.user"")
                            .value(TEST_TIME, 1.0D)
                            .tag(new Tag(""tag1"", ""value1""))
                            .tag(new Tag(""tag2"", ""value2""))
                            .build()
            );
            Assert.assertEquals(expected, actual);

            Assert.assertEquals(MetricRequest.class, tcpRequests.getResponses().get(1).getClass());
            actual = (MetricRequest) tcpRequests.getResponses().get(1);
            expected = new MetricRequest(
                    Metric.newBuilder()
                        .name(""sys.cpu.idle"")
                        .value(TEST_TIME + 1, 1.0D)
                        .tag(new Tag(""tag3"", ""value3""))
                        .tag(new Tag(""tag4"", ""value4""))
                        .build()
            );
            // @formatter:on
            Assert.assertEquals(expected, actual);
        }
    }

    @Test
    public void testPutInvalidTimestamp() throws Exception {
        try (TcpClient client = new TcpClient(hostIp, tcpPort)) {
            client.open();
            client.write(""put sys.cpu.user "" + TEST_TIME + ""Z"" + "" 1.0 tag1=value1 tag2=value2\n"");
            client.flush();
            sleepUninterruptibly(WAIT_SECONDS, TimeUnit.SECONDS);
            Assert.assertEquals(0, tcpRequests.getCount());
        }
    }

    @Test
    public void testPersistence() throws Exception {
        // @formatter:off
        put(""sys.cpu.user "" + TEST_TIME + "" 1.0 tag1=value1 tag2=value2"",
                   ""sys.cpu.idle "" + (TEST_TIME + 1) + "" 1.0 tag3=value3 tag4=value4"",
                   ""sys.cpu.idle "" + (TEST_TIME + 2) + "" 1.0 tag3=value3 tag4=value4"");
        // @formatter:on
        sleepUninterruptibly(WAIT_SECONDS, TimeUnit.SECONDS);
        assertTrue(accumuloClient.namespaceOperations().exists(""timely""));
        assertTrue(accumuloClient.tableOperations().exists(""timely.metrics""));
        assertTrue(accumuloClient.tableOperations().exists(""timely.meta""));
        int count = 0;
        for (final Map.Entry<Key,Value> entry : accumuloClient.createScanner(""timely.metrics"", Authorizations.EMPTY)) {
            log.debug(""Entry: "" + entry);
            final double value = ByteBuffer.wrap(entry.getValue().get()).getDouble();
            assertEquals(1.0, value, 1e-9);
            count++;
        }
        assertEquals(6, count);
        count = 0;
        for (final Map.Entry<Key,Value> entry : accumuloClient.createScanner(""timely.meta"", Authorizations.EMPTY)) {
            log.debug(""Meta entry: "" + entry);
            count++;
        }
        assertEquals(10, count);
        // count w/out versioning iterator to make sure that the optimization
        // for writing is working
        accumuloClient.tableOperations().removeIterator(""timely.meta"", ""vers"", EnumSet.of(IteratorScope.scan));
        // wait for zookeeper propagation
        sleepUninterruptibly(WAIT_SECONDS, TimeUnit.SECONDS);
        count = 0;
        for (final Map.Entry<Key,Value> entry : accumuloClient.createScanner(""timely.meta"", Authorizations.EMPTY)) {
            log.debug(""Meta no vers iter: "" + entry);
            count++;
        }
        assertEquals(15, count);
    }

    @Test
    public void testPersistenceWithVisibility() throws Exception {
        // @formatter:off
        put(""sys.cpu.user "" + TEST_TIME + "" 1.0 tag1=value1 tag2=value2"",
                   ""sys.cpu.idle "" + (TEST_TIME + 1) + "" 1.0 tag3=value3 tag4=value4 viz=(A|B)"",
                   ""sys.cpu.idle "" + (TEST_TIME + 2) + "" 1.0 tag3=value3 tag4=value4 viz=(C&B)"");
        // @formatter:on
        dataStore.flush();
        sleepUninterruptibly(WAIT_SECONDS, TimeUnit.SECONDS);
        int count = 0;
        for (final Map.Entry<Key,Value> entry : accumuloClient.createScanner(timelyProperties.getMetricsTable(), Authorizations.EMPTY)) {
            final double value = ByteBuffer.wrap(entry.getValue().get()).getDouble();
            assertEquals(1.0, value, 1e-9);
            count++;
        }
        assertEquals(2, count);
        count = 0;
        Authorizations auth1 = new Authorizations(""A"");
        for (final Map.Entry<Key,Value> entry : accumuloClient.createScanner(timelyProperties.getMetricsTable(), auth1)) {
            final double value = ByteBuffer.wrap(entry.getValue().get()).getDouble();
            assertEquals(1.0, value, 1e-9);
            count++;
        }
        assertEquals(4, count);
        count = 0;
        Authorizations auth2 = new Authorizations(""B"", ""C"");
        for (final Map.Entry<Key,Value> entry : accumuloClient.createScanner(timelyProperties.getMetricsTable(), auth2)) {
            final double value = ByteBuffer.wrap(entry.getValue().get()).getDouble();
            assertEquals(1.0, value, 1e-9);
            count++;
        }
        assertEquals(6, count);
    }

    protected void put(String... lines) throws Exception {
        StringBuffer format = new StringBuffer();
        for (String line : lines) {
            format.append(""put "");
            format.append(line);
            format.append(""\n"");
        }
        try (TcpClient client = new TcpClient(hostIp, tcpPort)) {
            client.open();
            client.write(format.toString());
            client.flush();
        }
    }
}
","package timely.server.integration;

import static com.google.common.util.concurrent.Uninterruptibles.sleepUninterruptibly;
import static org.junit.Assert.assertEquals;
import static org.junit.Assert.assertTrue;
import static timely.server.test.TestConfiguration.WAIT_SECONDS;

import java.nio.ByteBuffer;
import java.util.EnumSet;
import java.util.Map;
import java.util.concurrent.TimeUnit;

import org.apache.accumulo.core.data.Key;
import org.apache.accumulo.core.data.Value;
import org.apache.accumulo.core.iterators.IteratorUtil.IteratorScope;
import org.apache.accumulo.core.security.Authorizations;
import org.junit.After;
import org.junit.Assert;
import org.junit.Before;
import org.junit.Rule;
import org.junit.Test;
import org.junit.experimental.categories.Category;
import org.junit.runner.RunWith;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.beans.factory.annotation.Qualifier;
import org.springframework.boot.test.context.SpringBootTest;
import org.springframework.test.context.ActiveProfiles;
import org.springframework.test.context.junit4.SpringRunner;

import timely.api.request.MetricRequest;
import timely.client.tcp.TcpClient;
import timely.model.Metric;
import timely.model.Tag;
import timely.server.test.TestCaptureRequestHandler;
import timely.test.IntegrationTest;
import timely.test.TimelyTestRule;

@Category(IntegrationTest.class)
@RunWith(SpringRunner.class)
@SpringBootTest(webEnvironment = SpringBootTest.WebEnvironment.NONE)
@ActiveProfiles({""oneWaySsl""})
public class TcpClientIT extends OneWaySSLBase {

    private static final Long TEST_TIME = (System.currentTimeMillis() / 1000) * 1000;

    @Autowired
    @Rule
    public TimelyTestRule testRule;

    @Autowired
    @Qualifier(""tcp"")
    public TestCaptureRequestHandler tcpRequests;

    private String hostIp;
    private int tcpPort;

    @Before
    public void setup() {
        super.setup();
        hostIp = serverProperties.getIp();
        tcpPort = serverProperties.getTcpPort();
        tcpRequests.clear();
    }

    @After
    public void cleanup() {
        super.cleanup();
    }

    @Test
    public void testPut() throws Exception {
        try (TcpClient client = new TcpClient(hostIp, tcpPort)) {
            client.open();
            client.write(""put sys.cpu.user "" + TEST_TIME + "" 1.0 tag1=value1 tag2=value2\n"");
            client.flush();
            while (1 != tcpRequests.getCount()) {
                Thread.sleep(5);
            }
            Assert.assertEquals(1, tcpRequests.getResponses().size());
            Assert.assertEquals(MetricRequest.class, tcpRequests.getResponses().get(0).getClass());
            final MetricRequest actual = (MetricRequest) tcpRequests.getResponses().get(0);
            // @formatter:off
            final MetricRequest expected = new MetricRequest(
                    Metric.newBuilder()
                            .name(""sys.cpu.user"")
                            .value(TEST_TIME, 1.0D)
                            .tag(new Tag(""tag1"", ""value1""))
                            .tag(new Tag(""tag2"", ""value2""))
                            .build()
            );
            // @formatter:on
            Assert.assertEquals(expected, actual);
        }
    }

    @Test
    public void testPutMultiple() throws Exception {

        try (TcpClient client = new TcpClient(hostIp, tcpPort)) {
            client.open();
            // @formatter:off
            client.write(""put sys.cpu.user "" + TEST_TIME + "" 1.0 tag1=value1 tag2=value2\n""
                       + ""put sys.cpu.idle "" + (TEST_TIME + 1) + "" 1.0 tag3=value3 tag4=value4\n"");
            client.flush();
            while (2 != tcpRequests.getCount()) {
                Thread.sleep(5);
            }
            Assert.assertEquals(2, tcpRequests.getResponses().size());
            Assert.assertEquals(MetricRequest.class, tcpRequests.getResponses().get(0).getClass());
            MetricRequest actual = (MetricRequest) tcpRequests.getResponses().get(0);
            MetricRequest expected = new MetricRequest(
                    Metric.newBuilder()
                            .name(""sys.cpu.user"")
                            .value(TEST_TIME, 1.0D)
                            .tag(new Tag(""tag1"", ""value1""))
                            .tag(new Tag(""tag2"", ""value2""))
                            .build()
            );
            Assert.assertEquals(expected, actual);

            Assert.assertEquals(MetricRequest.class, tcpRequests.getResponses().get(1).getClass());
            actual = (MetricRequest) tcpRequests.getResponses().get(1);
            expected = new MetricRequest(
                    Metric.newBuilder()
                        .name(""sys.cpu.idle"")
                        .value(TEST_TIME + 1, 1.0D)
                        .tag(new Tag(""tag3"", ""value3""))
                        .tag(new Tag(""tag4"", ""value4""))
                        .build()
            );
            // @formatter:on
            Assert.assertEquals(expected, actual);
        }
    }

    @Test
    public void testPutInvalidTimestamp() throws Exception {
        try (TcpClient client = new TcpClient(hostIp, tcpPort)) {
            client.open();
            client.write(""put sys.cpu.user "" + TEST_TIME + ""Z"" + "" 1.0 tag1=value1 tag2=value2\n"");
            client.flush();
            sleepUninterruptibly(WAIT_SECONDS, TimeUnit.SECONDS);
            Assert.assertEquals(0, tcpRequests.getCount());
        }
    }

    @Test
    public void testPersistence() throws Exception {
        // @formatter:off
        put(""sys.cpu.user "" + TEST_TIME + "" 1.0 tag1=value1 tag2=value2"",
                   ""sys.cpu.idle "" + (TEST_TIME + 1) + "" 1.0 tag3=value3 tag4=value4"",
                   ""sys.cpu.idle "" + (TEST_TIME + 2) + "" 1.0 tag3=value3 tag4=value4"");
        // @formatter:on
        sleepUninterruptibly(WAIT_SECONDS, TimeUnit.SECONDS);
        assertTrue(accumuloClient.namespaceOperations().exists(""timely""));
        assertTrue(accumuloClient.tableOperations().exists(""timely.metrics""));
        assertTrue(accumuloClient.tableOperations().exists(""timely.meta""));
        int count = 0;
        for (final Map.Entry<Key,Value> entry : accumuloClient.createScanner(""timely.metrics"", Authorizations.EMPTY)) {
            log.debug(""Entry: "" + entry);
            final double value = ByteBuffer.wrap(entry.getValue().get()).getDouble();
            assertEquals(1.0, value, 1e-9);
            count++;
        }
        assertEquals(6, count);
        count = 0;
        for (final Map.Entry<Key,Value> entry : accumuloClient.createScanner(""timely.meta"", Authorizations.EMPTY)) {
            log.debug(""Meta entry: "" + entry);
            count++;
        }
        assertEquals(10, count);
        // count w/out versioning iterator to make sure that the optimization
        // for writing is working
        accumuloClient.tableOperations().removeIterator(""timely.meta"", ""vers"", EnumSet.of(IteratorScope.scan));
        // wait for zookeeper propagation
        sleepUninterruptibly(WAIT_SECONDS, TimeUnit.SECONDS);
        count = 0;
        for (final Map.Entry<Key,Value> entry : accumuloClient.createScanner(""timely.meta"", Authorizations.EMPTY)) {
            log.debug(""Meta no vers iter: "" + entry);
            count++;
        }
        assertEquals(15, count);
    }

    @Test
    public void testPersistenceWithVisibility() throws Exception {
        // @formatter:off
        put(""sys.cpu.user "" + TEST_TIME + "" 1.0 tag1=value1 tag2=value2"",
                   ""sys.cpu.idle "" + (TEST_TIME + 1) + "" 1.0 tag3=value3 tag4=value4 viz=(A|B)"",
                   ""sys.cpu.idle "" + (TEST_TIME + 2) + "" 1.0 tag3=value3 tag4=value4 viz=(C&B)"");
        // @formatter:on
        dataStore.flush();
        sleepUninterruptibly(WAIT_SECONDS, TimeUnit.SECONDS);
        int count = 0;
        for (final Map.Entry<Key,Value> entry : accumuloClient.createScanner(timelyProperties.getMetricsTable(), Authorizations.EMPTY)) {
            final double value = ByteBuffer.wrap(entry.getValue().get()).getDouble();
            assertEquals(1.0, value, 1e-9);
            count++;
        }
        assertEquals(2, count);
        count = 0;
        Authorizations auth1 = new Authorizations(""A"");
        for (final Map.Entry<Key,Value> entry : accumuloClient.createScanner(timelyProperties.getMetricsTable(), auth1)) {
            final double value = ByteBuffer.wrap(entry.getValue().get()).getDouble();
            assertEquals(1.0, value, 1e-9);
            count++;
        }
        assertEquals(4, count);
        count = 0;
        Authorizations auth2 = new Authorizations(""B"", ""C"");
        for (final Map.Entry<Key,Value> entry : accumuloClient.createScanner(timelyProperties.getMetricsTable(), auth2)) {
            final double value = ByteBuffer.wrap(entry.getValue().get()).getDouble();
            assertEquals(1.0, value, 1e-9);
            count++;
        }
        assertEquals(6, count);
    }

    protected void put(String... lines) throws Exception {
        StringBuffer format = new StringBuffer();
        for (String line : lines) {
            format.append(""put "");
            format.append(line);
            format.append(""\n"");
        }
        try (TcpClient client = new TcpClient(hostIp, tcpPort)) {
            client.open();
            client.write(format.toString());
            client.flush();
        }
    }
}
","['Assertion Roulette', 'Conditional Test Logic', 'General Fixture', 'Wait And See', 'Lazy Assert']","['Eager Test', 'Sleepy Test', 'Magic Number Test', 'Sensitive Equality', 'Redundant Print', 'Lazy Test']",6,5,0,9
25109_4.0_hadoop_testappattemptpagenaturalsorttype,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/25109_test.java,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/25109_actual.java,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * ""License""); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.hadoop.yarn.server.applicationhistoryservice.webapp;

import java.util.Map;

import com.google.inject.Injector;
import org.junit.jupiter.api.BeforeEach;
import org.junit.jupiter.api.Test;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.yarn.api.ApplicationBaseProtocol;
import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;
import org.apache.hadoop.yarn.api.records.ApplicationId;
import org.apache.hadoop.yarn.api.records.ContainerId;
import org.apache.hadoop.yarn.api.records.YarnApplicationState;
import org.apache.hadoop.yarn.conf.YarnConfiguration;
import org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryClientService;
import org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryManager;
import org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryManagerImpl;
import org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryStore;
import org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryStoreTestUtils;
import org.apache.hadoop.yarn.server.applicationhistoryservice.MemoryApplicationHistoryStore;
import org.apache.hadoop.yarn.util.StringHelper;
import org.apache.hadoop.yarn.webapp.YarnWebParams;
import org.apache.hadoop.yarn.webapp.test.WebAppTests;

import static org.apache.hadoop.yarn.webapp.Params.TITLE;
import static org.junit.jupiter.api.Assertions.assertEquals;
import static org.junit.jupiter.api.Assertions.assertTrue;
import static org.mockito.Mockito.mock;

public class TestAHSWebApp extends ApplicationHistoryStoreTestUtils {

  public void setApplicationHistoryStore(ApplicationHistoryStore store) {
    this.store = store;
  }

  @BeforeEach
  public void setup() {
    store = new MemoryApplicationHistoryStore();
  }

  @Test
  void testAppControllerIndex() throws Exception {
    ApplicationHistoryManager ahManager = mock(ApplicationHistoryManager.class);
    Injector injector =
        WebAppTests.createMockInjector(ApplicationHistoryManager.class,
            ahManager);
    AHSController controller = injector.getInstance(AHSController.class);
    controller.index();
    assertEquals(""Application History"", controller.get(TITLE, ""unknown""));
  }

  @Test
  void testView() throws Exception {
    Injector injector =
        WebAppTests.createMockInjector(ApplicationBaseProtocol.class,
            mockApplicationHistoryClientService(5, 1, 1));
    AHSView ahsViewInstance = injector.getInstance(AHSView.class);

    ahsViewInstance.render();
    WebAppTests.flushOutput(injector);

    ahsViewInstance.set(YarnWebParams.APP_STATE,
        YarnApplicationState.FAILED.toString());
    ahsViewInstance.render();
    WebAppTests.flushOutput(injector);

    ahsViewInstance.set(YarnWebParams.APP_STATE, StringHelper.cjoin(
        YarnApplicationState.FAILED.toString(), YarnApplicationState.KILLED));
    ahsViewInstance.render();
    WebAppTests.flushOutput(injector);
  }

  @Test
  void testAPPViewNaturalSortType() throws Exception {
    Injector injector =
        WebAppTests.createMockInjector(ApplicationBaseProtocol.class,
            mockApplicationHistoryClientService(5, 1, 1));
    AHSView ahsViewInstance = injector.getInstance(AHSView.class);

    ahsViewInstance.render();
    WebAppTests.flushOutput(injector);
    Map<String, String> moreParams =
        ahsViewInstance.context().requestContext().moreParams();
    String appTableColumnsMeta = moreParams.get(""ui.dataTables.apps.init"");
    assertTrue(appTableColumnsMeta.indexOf(""natural"") != -1);
  }

  @Test
  void testAboutPage() throws Exception {
    Injector injector =
        WebAppTests.createMockInjector(ApplicationBaseProtocol.class,
            mockApplicationHistoryClientService(0, 0, 0));
    AboutPage aboutPageInstance = injector.getInstance(AboutPage.class);

    aboutPageInstance.render();
    WebAppTests.flushOutput(injector);

    aboutPageInstance.render();
    WebAppTests.flushOutput(injector);
  }

  @Test
  void testAppPage() throws Exception {
    Injector injector =
        WebAppTests.createMockInjector(ApplicationBaseProtocol.class,
            mockApplicationHistoryClientService(1, 5, 1));
    AppPage appPageInstance = injector.getInstance(AppPage.class);

    appPageInstance.render();
    WebAppTests.flushOutput(injector);

    appPageInstance.set(YarnWebParams.APPLICATION_ID, ApplicationId
        .newInstance(0, 1).toString());
    appPageInstance.render();
    WebAppTests.flushOutput(injector);
  }

  @Test
  void testAppPageNaturalSortType() throws Exception {
    Injector injector =
        WebAppTests.createMockInjector(ApplicationBaseProtocol.class,
            mockApplicationHistoryClientService(1, 5, 1));
    AppPage appPageInstance = injector.getInstance(AppPage.class);

    appPageInstance.render();
    WebAppTests.flushOutput(injector);
    Map<String, String> moreParams =
        appPageInstance.context().requestContext().moreParams();
    String attemptsTableColumnsMeta =
        moreParams.get(""ui.dataTables.attempts.init"");
    assertTrue(attemptsTableColumnsMeta.indexOf(""natural"") != -1);
  }

  @Test
  void testAppAttemptPage() throws Exception {
    Injector injector =
        WebAppTests.createMockInjector(ApplicationBaseProtocol.class,
            mockApplicationHistoryClientService(1, 1, 5));
    AppAttemptPage appAttemptPageInstance =
        injector.getInstance(AppAttemptPage.class);

    appAttemptPageInstance.render();
    WebAppTests.flushOutput(injector);

    appAttemptPageInstance.set(YarnWebParams.APPLICATION_ATTEMPT_ID,
        ApplicationAttemptId.newInstance(ApplicationId.newInstance(0, 1), 1)
            .toString());
    appAttemptPageInstance.render();
    WebAppTests.flushOutput(injector);
  }

  @Test
  void testAppAttemptPageNaturalSortType() throws Exception {
    Injector injector =
        WebAppTests.createMockInjector(ApplicationBaseProtocol.class,
            mockApplicationHistoryClientService(1, 1, 5));
    AppAttemptPage appAttemptPageInstance =
        injector.getInstance(AppAttemptPage.class);
    appAttemptPageInstance.render();
    WebAppTests.flushOutput(injector);
    Map<String, String> moreParams =
        appAttemptPageInstance.context().requestContext().moreParams();
    String tableColumnsMeta = moreParams.get(""ui.dataTables.containers.init"");
    assertTrue(tableColumnsMeta.indexOf(""natural"") != -1);
  }

  @Test
  void testContainerPage() throws Exception {
    Injector injector =
        WebAppTests.createMockInjector(ApplicationBaseProtocol.class,
            mockApplicationHistoryClientService(1, 1, 1));
    ContainerPage containerPageInstance =
        injector.getInstance(ContainerPage.class);

    containerPageInstance.render();
    WebAppTests.flushOutput(injector);

    containerPageInstance.set(
        YarnWebParams.CONTAINER_ID,
        ContainerId
            .newContainerId(
                ApplicationAttemptId.newInstance(ApplicationId.newInstance(0, 1), 1),
                1).toString());
    containerPageInstance.render();
    WebAppTests.flushOutput(injector);
  }

  ApplicationHistoryClientService mockApplicationHistoryClientService(int numApps,
      int numAppAttempts, int numContainers) throws Exception {
    ApplicationHistoryManager ahManager =
        new MockApplicationHistoryManagerImpl(store);
    ApplicationHistoryClientService historyClientService =
        new ApplicationHistoryClientService(ahManager);
    for (int i = 1; i <= numApps; ++i) {
      ApplicationId appId = ApplicationId.newInstance(0, i);
      writeApplicationStartData(appId);
      for (int j = 1; j <= numAppAttempts; ++j) {
        ApplicationAttemptId appAttemptId =
            ApplicationAttemptId.newInstance(appId, j);
        writeApplicationAttemptStartData(appAttemptId);
        for (int k = 1; k <= numContainers; ++k) {
          ContainerId containerId = ContainerId.newContainerId(appAttemptId, k);
          writeContainerStartData(containerId);
          writeContainerFinishData(containerId);
        }
        writeApplicationAttemptFinishData(appAttemptId);
      }
      writeApplicationFinishData(appId);
    }
    return historyClientService;
  }

  class MockApplicationHistoryManagerImpl extends ApplicationHistoryManagerImpl {

    MockApplicationHistoryManagerImpl(ApplicationHistoryStore store) {
      super();
      init(new YarnConfiguration());
      start();
    }

    @Override
    protected ApplicationHistoryStore createApplicationHistoryStore(
        Configuration conf) {
      return store;
    }
  };
}
","/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * ""License""); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hadoop.yarn.server.applicationhistoryservice.webapp;

import static org.apache.hadoop.yarn.util.StringHelper.pajoin;

import org.apache.hadoop.yarn.api.ApplicationBaseProtocol;
import org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryClientService;
import org.apache.hadoop.yarn.server.timeline.TimelineDataManager;
import org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices;
import org.apache.hadoop.yarn.webapp.GenericExceptionHandler;
import org.apache.hadoop.yarn.webapp.WebApp;
import org.apache.hadoop.yarn.webapp.YarnJacksonJaxbJsonProvider;
import org.apache.hadoop.yarn.webapp.YarnWebParams;

public class AHSWebApp extends WebApp implements YarnWebParams {

  private final ApplicationHistoryClientService historyClientService;
  private TimelineDataManager timelineDataManager;

  public AHSWebApp(TimelineDataManager timelineDataManager,
      ApplicationHistoryClientService historyClientService) {
    this.timelineDataManager = timelineDataManager;
    this.historyClientService = historyClientService;
  }

  public ApplicationHistoryClientService getApplicationHistoryClientService() {
    return historyClientService;
  }

  public TimelineDataManager getTimelineDataManager() {
    return timelineDataManager;
  }

  @Override
  public void setup() {
    bind(YarnJacksonJaxbJsonProvider.class);
    bind(AHSWebServices.class);
    bind(TimelineWebServices.class);
    bind(GenericExceptionHandler.class);
    bind(ApplicationBaseProtocol.class).toInstance(historyClientService);
    bind(TimelineDataManager.class).toInstance(timelineDataManager);
    route(""/"", AHSController.class);
    route(""/about"", AHSController.class, ""about"");
    route(pajoin(""/apps"", APP_STATE), AHSController.class);
    route(pajoin(""/app"", APPLICATION_ID), AHSController.class, ""app"");
    route(pajoin(""/appattempt"", APPLICATION_ATTEMPT_ID), AHSController.class,
      ""appattempt"");
    route(pajoin(""/container"", CONTAINER_ID), AHSController.class, ""container"");
    route(
      pajoin(""/logs"", NM_NODENAME, CONTAINER_ID, ENTITY_STRING, APP_OWNER,
        CONTAINER_LOG_TYPE), AHSController.class, ""logs"");
    route(""/errors-and-warnings"", AHSController.class, ""errorsAndWarnings"");
  }
}",['Unknown Test'],"['Assertion Roulette', 'Lazy Test', 'Redundant Print', 'Sleepy Test', 'Eager Test']",5,1,0,13
4583_18.0_admiral_testgetkubeconfigwithbearertoken,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/4583_test.java,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/4583_actual.java,"/*
 * Copyright (c) 2018-2020 VMware, Inc. All Rights Reserved.
 *
 * This product is licensed to you under the Apache License, Version 2.0 (the ""License"").
 * You may not use this product except in compliance with the License.
 *
 * This product may include a number of subcomponents with separate copyright notices
 * and license terms. Your use of these subcomponents is subject to the terms and
 * conditions of the subcomponent's license, as noted in the LICENSE file.
 */

package com.vmware.admiral.adapter.pks.service;

import static com.vmware.admiral.adapter.pks.PKSConstants.PKS_ENDPOINT_PROP_NAME;
import static com.vmware.admiral.compute.ComputeConstants.HOST_AUTH_CREDENTIALS_PROP_NAME;
import static com.vmware.admiral.compute.ContainerHostService.CONTAINER_HOST_TYPE_PROP_NAME;
import static org.junit.Assert.assertEquals;
import static org.junit.Assert.assertTrue;
import static org.junit.Assert.fail;

import java.net.URI;
import java.util.HashMap;
import java.util.UUID;

import org.junit.Before;
import org.junit.Test;

import com.vmware.admiral.compute.ContainerHostService.ContainerHostType;
import com.vmware.admiral.compute.container.ComputeBaseTest;
import com.vmware.photon.controller.model.resources.ComputeService;
import com.vmware.photon.controller.model.resources.ComputeService.ComputeState;
import com.vmware.photon.controller.model.security.util.AuthCredentialsType;
import com.vmware.xenon.common.Operation;
import com.vmware.xenon.common.Service.Action;
import com.vmware.xenon.common.UriUtils;
import com.vmware.xenon.services.common.AuthCredentialsService;
import com.vmware.xenon.services.common.AuthCredentialsService.AuthCredentialsServiceState;

public class KubeConfigContentServiceTest extends ComputeBaseTest {

    private static final String KUBE_CONFIG_JSON = ""{\""clusters\"":[{\""name\"":\""cluster2\"",\""cluster\"":{\""server\"":\""https://mshipkovenski-test:8443\"",\""certificate-authority-data\"":\""cert\""}}],\""contexts\"":[{\""name\"":\""cluster2\"",\""context\"":{\""cluster\"":\""cluster2\"",\""user\"":\""bdf17412-f7ee-4df0-bf74-161d8b663d3c\""}}],\""users\"":[{\""name\"":\""bdf17412-f7ee-4df0-bf74-161d8b663d3c\"",\""user\"":{\""token\"":\""token\""}}],\""current-context\"":\""cluster2\"",\""apiVersion\"":\""v1\"",\""kind\"":\""Config\""}"";
    private static final String KUBE_CONFIG_YAML = ""---\nclusters:\n- name: \""cluster2\""\n  cluster:\n    server: \""hostname\""\n    certificate-authority-data: \""cert\""\ncontexts:\n- name: \""cluster2\""\n  context:\n    cluster: \""cluster2\""\n    user: \""bdf17412-f7ee-4df0-bf74-161d8b663d3c\""\nusers:\n- name: \""bdf17412-f7ee-4df0-bf74-161d8b663d3c\""\n  user:\n    token: \""token\""\ncurrent-context: \""cluster2\""\napiVersion: \""v1\""\nkind: \""Config\""\n"";

    @Before
    public void setUp() throws Throwable {
        host.startService(Operation.createPost(UriUtils.buildUri(host,
                KubeConfigContentService.class)), new KubeConfigContentService());

        waitForServiceAvailability(KubeConfigContentService.SELF_LINK);
        waitForServiceAvailability(AuthCredentialsService.FACTORY_LINK);
        waitForServiceAvailability(ComputeService.FACTORY_LINK);
    }

    @Test
    public void testGetKubeConfigWithBearerToken() throws Throwable {
        String authCredentialsLink = createCredentials(AuthCredentialsType.Bearer, true)
                .documentSelfLink;
        String hostLink = createCompute(authCredentialsLink, true, true).documentSelfLink;

        URI serviceUri = UriUtils.buildUri(host, KubeConfigContentService.SELF_LINK,
                UriUtils.buildUriQuery(""hostLink"", hostLink));

        verifyOperation(Operation.createGet(serviceUri), o -> {
            assertEquals(""attachment; filename=\""kubeconfig\"""",
                    o.getResponseHeader(""Content-Disposition""));
            assertEquals(KUBE_CONFIG_YAML, o.getBody(String.class));
        });
    }

    @Test
    public void testGetKubeConfigWithCertificateAndKey() throws Throwable {
        assertTrue(UUID.randomUUID().toString().matches(""[-a-z0-9]+""));
        String authCredentialsLink = createCredentials(AuthCredentialsType.PublicKey, false)
                .documentSelfLink;
        String hostLink = createCompute(authCredentialsLink, true, false).documentSelfLink;

        URI serviceUri = UriUtils.buildUri(host, KubeConfigContentService.SELF_LINK,
                UriUtils.buildUriQuery(""hostLink"", hostLink));

        verifyOperation(Operation.createGet(serviceUri), o -> {
            assertEquals(""attachment; filename=\""kubeconfig\"""",
                    o.getResponseHeader(""Content-Disposition""));
        });
    }

    @Test
    public void testShouldFailWhenHostTypeNotKubernetes() throws Throwable {
        String authCredentialsLink = createCredentials(AuthCredentialsType.Bearer, true)
                .documentSelfLink;
        String hostLink = createCompute(authCredentialsLink, false, true).documentSelfLink;
        URI serviceUri = UriUtils.buildUri(host, KubeConfigContentService.SELF_LINK,
                UriUtils.buildUriQuery(""hostLink"", hostLink));
        try {
            doOperation(null, serviceUri, true, Action.GET);
            fail(""Operation should have failed: only k8s hosts are supported"");
        } catch (Exception e) {
            assertEquals(""host type must be KUBERNETES"", e.getMessage());
        }
    }

    @Test
    public void testShouldFailWhenKubeConfigContentIsMissing() throws Throwable {
        String authCredentialsLink = createCredentials(AuthCredentialsType.Bearer, false)
                .documentSelfLink;
        String hostLink = createCompute(authCredentialsLink, true, true).documentSelfLink;
        URI serviceUri = UriUtils.buildUri(host, KubeConfigContentService.SELF_LINK,
                UriUtils.buildUriQuery(""hostLink"", hostLink));
        try {
            doOperation(null, serviceUri, true, Action.GET);
            fail(""Operation should have failed: kubeconfig not set"");
        } catch (Exception e) {
            assertEquals(""KubeConfig cannot be retrieved"", e.getMessage());
        }
    }

    @Test
    public void testShouldFailWhenHostLinkParamIsMissing() throws Throwable {
        URI serviceUri = UriUtils.buildUri(host, KubeConfigContentService.SELF_LINK);
        try {
            doOperation(null, serviceUri, true, Action.GET);
            fail(""Operation should have failed: hostLink query param not set"");
        } catch (Exception e) {
            assertEquals(""'hostLink' is required"", e.getMessage());
        }
    }

    @Test
    public void testShouldFailWhenHostAuthTypeNotSupported() throws Throwable {
        String authCredentialsLink = createCredentials(AuthCredentialsType.PublicKeyCA, false)
                .documentSelfLink;
        String hostLink = createCompute(authCredentialsLink, true, false).documentSelfLink;
        URI serviceUri = UriUtils.buildUri(host, KubeConfigContentService.SELF_LINK,
                UriUtils.buildUriQuery(""hostLink"", hostLink));
        try {
            doOperation(null, serviceUri, true, Action.GET);
            fail(""Operation should have failed: host auth type not supported"");
        } catch (Exception e) {
            assertEquals(""Unsupported credentials type"", e.getMessage());
        }
    }

    private AuthCredentialsServiceState createCredentials(AuthCredentialsType type,
            boolean setKubeConfig) throws Throwable {

        AuthCredentialsServiceState credentials = new AuthCredentialsServiceState();
        if (AuthCredentialsType.Bearer == type) {
            credentials.type = AuthCredentialsType.Bearer.toString();
            credentials.publicKey = ""token"";
        } else if (AuthCredentialsType.PublicKey == type) {
            credentials.type = AuthCredentialsType.PublicKey.toString();
            credentials.publicKey = ""certificate"";
            credentials.privateKey = ""privateKey"";
        } else {
            credentials.type = type.toString();
        }

        if (setKubeConfig) {
            credentials.customProperties = new HashMap<>();
            credentials.customProperties.put(""__kubeConfig"", KUBE_CONFIG_JSON);
        }

        return doPost(credentials, AuthCredentialsService.FACTORY_LINK);
    }

    private ComputeState createCompute(String authCredentialsLink, boolean isKubernetesHost,
            boolean pksManaged) throws Throwable {

        ComputeState kubernetesHost = new ComputeState();
        kubernetesHost.address = ""hostname"";
        kubernetesHost.descriptionLink = ""description"";
        kubernetesHost.customProperties = new HashMap<>();
        kubernetesHost.customProperties.put(HOST_AUTH_CREDENTIALS_PROP_NAME,
                authCredentialsLink);
        if (isKubernetesHost) {
            kubernetesHost.customProperties.put(CONTAINER_HOST_TYPE_PROP_NAME,
                    ContainerHostType.KUBERNETES.name());
        } else {
            kubernetesHost.customProperties.put(CONTAINER_HOST_TYPE_PROP_NAME,
                    ContainerHostType.DOCKER.name());
        }
        if (pksManaged) {
            kubernetesHost.customProperties.put(PKS_ENDPOINT_PROP_NAME,
                    ""/endpoint/link"");
        }
        return doPost(kubernetesHost, ComputeService.FACTORY_LINK);
    }
}
","/*
 * Copyright (c) 2018 VMware, Inc. All Rights Reserved.
 *
 * This product is licensed to you under the Apache License, Version 2.0 (the ""License"").
 * You may not use this product except in compliance with the License.
 *
 * This product may include a number of subcomponents with separate copyright notices
 * and license terms. Your use of these subcomponents is subject to the terms and
 * conditions of the subcomponent's license, as noted in the LICENSE file.
 */

package com.vmware.admiral.adapter.pks.service;

import static com.vmware.admiral.common.util.UriUtilsExtended.MEDIA_TYPE_APPLICATION_YAML;
import static com.vmware.admiral.compute.ComputeConstants.HOST_AUTH_CREDENTIALS_PROP_NAME;

import java.io.IOException;
import java.util.Map;
import java.util.function.BiConsumer;
import java.util.function.Consumer;

import com.vmware.admiral.adapter.pks.PKSConstants;
import com.vmware.admiral.common.ManagementUriParts;
import com.vmware.admiral.common.util.AssertUtil;
import com.vmware.admiral.common.util.YamlMapper;
import com.vmware.admiral.compute.ContainerHostUtil;
import com.vmware.admiral.compute.content.kubernetes.KubernetesUtil;
import com.vmware.admiral.compute.kubernetes.entities.config.KubeConfig;
import com.vmware.photon.controller.model.resources.ComputeService.ComputeState;
import com.vmware.xenon.common.Operation;
import com.vmware.xenon.common.StatelessService;
import com.vmware.xenon.common.UriUtils;
import com.vmware.xenon.common.Utils;
import com.vmware.xenon.services.common.AuthCredentialsService.AuthCredentialsServiceState;

/**
 * Service that produces kubeconfig files for user authorization against kubernetes hosts.
 */
public class KubeConfigContentService extends StatelessService {

    public static final String SELF_LINK = ManagementUriParts.PKS_KUBE_CONFIG_CONTENT;

    public static final String KUBERNETES_HOST_LINK_PARAM_NAME = ""hostLink"";

    private static final String CONTENT_DISPOSITION_HEADER = ""Content-Disposition"";
    private static final String CONTENT_DISPOSITION_ATTACHMENT = ""attachment; filename=\""kubeconfig\"""";

    @Override
    public void handleRequest(Operation op) {
        if (op.getAction() != Action.GET) {
            Operation.failActionNotSupported(op);
            return;
        }

        handleGet(op);
    }

    @Override
    public void handleGet(Operation op) {
        try {
            Map<String, String> queryParams = UriUtils.parseUriQueryParams(op.getUri());

            String hostLink = queryParams.get(KUBERNETES_HOST_LINK_PARAM_NAME);
            AssertUtil.assertNotNullOrEmpty(hostLink, KUBERNETES_HOST_LINK_PARAM_NAME);

            getHostAndCredentials(op, hostLink,
                    (compute, credentials) -> constructKubeConfig(op, compute, credentials));
        } catch (Exception x) {
            logSevere(x);
            op.fail(x);
        }
    }

    private void constructKubeConfig(Operation op, ComputeState kubernetesHost,
            AuthCredentialsServiceState credentials) {

        try {
            String kubeConfigJson = constructKubeConfigJson(kubernetesHost, credentials);
            String kubeConfigYaml = serializeContent(kubeConfigJson);
            op.setBody(kubeConfigYaml);
            op.setContentType(MEDIA_TYPE_APPLICATION_YAML);
            op.addResponseHeader(CONTENT_DISPOSITION_HEADER, CONTENT_DISPOSITION_ATTACHMENT);

            op.complete();
        } catch (Exception e) {
            op.fail(e);
        }
    }


    private String constructKubeConfigJson(ComputeState kubernetesHost,
            AuthCredentialsServiceState credentials) throws Exception {

        KubeConfig kubeConfig = null;

        if (KubernetesUtil.isPKSManagedHost(kubernetesHost)) {
            if (credentials.customProperties == null
                    || !credentials.customProperties.containsKey(PKSConstants.KUBE_CONFIG_PROP_NAME)) {
                throw new IllegalStateException(""KubeConfig cannot be retrieved"");
            }

            String kubeConfigJson = credentials.customProperties
                    .get(PKSConstants.KUBE_CONFIG_PROP_NAME);
            kubeConfig = Utils.fromJson(kubeConfigJson, KubeConfig.class);
            // overwrite cluster address to cover the case when the cluster was added by
            // master IP instead of master host name
            kubeConfig.clusters.get(0).cluster.server = kubernetesHost.address;
        } else {
            kubeConfig = KubernetesUtil.constructKubeConfig(kubernetesHost.address, credentials);
        }

        return Utils.toJson(kubeConfig);
    }

    private String serializeContent(String kubeConfig) throws IOException {
        return YamlMapper.fromJsonToYaml(kubeConfig);
    }

    private void getHostAndCredentials(Operation op, String hostLink,
            BiConsumer<ComputeState, AuthCredentialsServiceState> consumer) {

        getCompute(op, hostLink, (host) -> {
            if (!ContainerHostUtil.isKubernetesHost(host)) {
                op.fail(new IllegalArgumentException(""host type must be KUBERNETES""));
                return;
            }

            String credentialsLink = host.customProperties.get(HOST_AUTH_CREDENTIALS_PROP_NAME);

            if (credentialsLink == null) {
                op.fail(new IllegalStateException(""Missing credentials link""));
                return;
            }

            Operation.createGet(this, credentialsLink).setCompletion((o, e) -> {
                if (e != null) {
                    op.fail(e);
                    return;
                }

                AuthCredentialsServiceState credentials = o
                        .getBody(AuthCredentialsServiceState.class);

                consumer.accept(host, credentials);
            }).sendWith(this);
        });
    }

    private void getCompute(Operation op, String hostLink,
            Consumer<ComputeState> consumer) {

        Operation.createGet(this, hostLink).setCompletion((o, e) -> {
            if (e != null) {
                op.fail(e);
                return;
            }

            consumer.accept(o.getBody(ComputeState.class));
        }).sendWith(this);
    }
}
","['Assertion Roulette', 'Exception Catching Throwing', 'Sensitive Equality']","['Assertion Roulette', 'Lazy Test']",1,2,1,13
1887_47.0_rxjava2-extras_testretrywhenmultipleretriesworkonsingledelay,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/1887_test.java,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/1887_actual.java,"package com.github.davidmoten.rx2;

import static org.junit.Assert.assertEquals;
import static org.junit.Assert.assertFalse;

import java.sql.SQLException;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicInteger;

import org.junit.Test;
import org.reactivestreams.Subscription;

import com.github.davidmoten.junit.Asserts;
import com.github.davidmoten.rx2.RetryWhen.ErrorAndDuration;

import io.reactivex.Flowable;
import io.reactivex.functions.Consumer;
import io.reactivex.functions.Predicate;
import io.reactivex.schedulers.Schedulers;
import io.reactivex.schedulers.TestScheduler;
import io.reactivex.subscribers.TestSubscriber;

public class RetryWhenTest {

    @Test
    public void isUtilityClass() {
        Asserts.assertIsUtilityClass(RetryWhen.class);
    }

    @Test
    public void testExponentialBackoff() {
        Exception ex = new IllegalArgumentException(""boo"");
        TestSubscriber<Integer> ts = TestSubscriber.create();
        final AtomicInteger logCalls = new AtomicInteger();
        Consumer<ErrorAndDuration> log = new Consumer<ErrorAndDuration>() {

            @Override
            public void accept(ErrorAndDuration e) {
                System.out.println(""WARN: "" + e.throwable().getMessage());
                System.out.println(""waiting for "" + e.durationMs() + ""ms"");
                logCalls.incrementAndGet();
            }
        };
        Flowable.just(1, 2, 3)
                // force error after 3 emissions
                .concatWith(Flowable.<Integer>error(ex))
                // retry with backoff
                .retryWhen(RetryWhen.maxRetries(5).action(log).exponentialBackoff(10, TimeUnit.MILLISECONDS).build())
                // go
                .subscribe(ts);

        // check results
        ts.awaitTerminalEvent();
        ts.assertError(ex);
        ts.assertValues(1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3);
        assertEquals(5, logCalls.get());
    }

    @Test
    public void testWithScheduler() {
        Exception ex = new IllegalArgumentException(""boo"");
        TestSubscriber<Integer> ts = TestSubscriber.create();
        TestScheduler scheduler = new TestScheduler();
        Flowable.just(1, 2)
                // force error after 3 emissions
                .concatWith(Flowable.<Integer>error(ex))
                // retry with backoff
                .retryWhen(RetryWhen.maxRetries(2).action(log).exponentialBackoff(1, TimeUnit.MINUTES)
                        .scheduler(scheduler).build())
                // go
                .subscribe(ts);
        ts.assertValues(1, 2);
        ts.assertNotComplete();
        scheduler.advanceTimeBy(1, TimeUnit.MINUTES);
        ts.assertValues(1, 2, 1, 2);
        ts.assertNotComplete();
        // next wait is 2 seconds so advancing by 1 should do nothing
        scheduler.advanceTimeBy(1, TimeUnit.MINUTES);
        ts.assertValues(1, 2, 1, 2);
        ts.assertNotComplete();
        scheduler.advanceTimeBy(1, TimeUnit.MINUTES);
        ts.assertValues(1, 2, 1, 2, 1, 2);
        ts.assertError(ex);
    }

    @SuppressWarnings(""unchecked"")
    @Test
    public void testRetryWhenSpecificExceptionFails() {
        Exception ex = new IllegalArgumentException(""boo"");
        TestSubscriber<Integer> ts = TestSubscriber.create();
        TestScheduler scheduler = new TestScheduler();
        Flowable.just(1, 2)
                // force error after 3 emissions
                .concatWith(Flowable.<Integer>error(ex))
                // retry with backoff
                .retryWhen(RetryWhen.maxRetries(2).action(log).exponentialBackoff(1, TimeUnit.MINUTES)
                        .scheduler(scheduler).failWhenInstanceOf(IllegalArgumentException.class).build())
                // go
                .subscribe(ts);
        ts.assertValues(1, 2);
        ts.assertError(ex);
    }

    @SuppressWarnings(""unchecked"")
    @Test
    public void testRetryWhenSpecificExceptionFailsBecauseIsNotInstanceOf() {
        Exception ex = new IllegalArgumentException(""boo"");
        TestSubscriber<Integer> ts = TestSubscriber.create();
        TestScheduler scheduler = new TestScheduler();
        Flowable.just(1, 2)
                // force error after 3 emissions
                .concatWith(Flowable.<Integer>error(ex))
                // retry with backoff
                .retryWhen(RetryWhen.maxRetries(2).action(log).exponentialBackoff(1, TimeUnit.MINUTES)
                        .scheduler(scheduler).retryWhenInstanceOf(SQLException.class).build())
                // go
                .subscribe(ts);
        ts.assertValues(1, 2);
        ts.assertError(ex);
    }
    
    @SuppressWarnings(""unchecked"")
    @Test
    public void testRetryWhenSubClassWorksWithInstanceOf() {
        Exception ex = new IllegalArgumentException(""boo"");
        final AtomicInteger count = new AtomicInteger();
        Flowable.error(ex)
                // force error after 3 emissions
                .doOnSubscribe(new Consumer<Subscription>() {
					@Override
					public void accept(Subscription sub) throws Exception {
						count.incrementAndGet();
					}
				}) //
                // retry with backoff
                .retryWhen(RetryWhen //
                		.maxRetries(2) //
                		.scheduler(Schedulers.trampoline()) //
                        .retryWhenInstanceOf(Exception.class) //
                        .build())
                // go
                .test() //
                .assertNoValues() //
                .assertError(ex);
        assertEquals(3, count.get());
    }

    @SuppressWarnings(""unchecked"")
    @Test
    public void testRetryWhenSpecificExceptionAllowed() {
        Exception ex = new IllegalArgumentException(""boo"");
        TestSubscriber<Integer> ts = TestSubscriber.create();
        TestScheduler scheduler = new TestScheduler();
        Flowable.just(1, 2)
                // force error after 3 emissions
                .concatWith(Flowable.<Integer>error(ex))
                // retry with backoff
                .retryWhen(RetryWhen.maxRetries(2).action(log).exponentialBackoff(1, TimeUnit.MINUTES)
                        .scheduler(scheduler).retryWhenInstanceOf(IllegalArgumentException.class).build())
                // go
                .subscribe(ts);
        ts.assertValues(1, 2);
        ts.assertNotComplete();
    }

    private static final Consumer<ErrorAndDuration> log = new Consumer<ErrorAndDuration>() {

        @Override
        public void accept(ErrorAndDuration e) {
            System.out.println(""WARN: "" + e.throwable().getMessage());
            System.out.println(""waiting for "" + e.durationMs() + ""ms"");
        }
    };

    @Test
    public void testRetryWhenSpecificExceptionAllowedUsePredicateReturnsTrue() {
        Exception ex = new IllegalArgumentException(""boo"");
        TestSubscriber<Integer> ts = TestSubscriber.create();
        TestScheduler scheduler = new TestScheduler();
        Predicate<Throwable> predicate = new Predicate<Throwable>() {
            @Override
            public boolean test(Throwable t) {
                return t instanceof IllegalArgumentException;
            }
        };
        Flowable.just(1, 2)
                // force error after 3 emissions
                .concatWith(Flowable.<Integer>error(ex))
                // retry with backoff
                .retryWhen(RetryWhen.maxRetries(2).action(log).exponentialBackoff(1, TimeUnit.MINUTES)
                        .scheduler(scheduler).retryIf(predicate).build())
                // go
                .subscribe(ts);
        ts.assertValues(1, 2);
        ts.assertNotComplete();
    }

    @Test
    public void testRetryWhenSpecificExceptionAllowedUsePredicateReturnsFalse() {
        Exception ex = new IllegalArgumentException(""boo"");
        TestSubscriber<Integer> ts = TestSubscriber.create();
        TestScheduler scheduler = new TestScheduler();
        Predicate<Throwable> predicate = Predicates.alwaysFalse();
        Flowable.just(1, 2)
                // force error after 3 emissions
                .concatWith(Flowable.<Integer>error(ex))
                // retry with backoff
                .retryWhen(RetryWhen.maxRetries(2).action(log).exponentialBackoff(1, TimeUnit.MINUTES)
                        .scheduler(scheduler).retryIf(predicate).build())
                // go
                .subscribe(ts);
        ts.assertValues(1, 2);
        ts.assertError(ex);
    }

    @Test
    public void testRetryWhenMultipleRetriesWorkOnSingleDelay() {
        AtomicInteger count = new AtomicInteger();
        TestSubscriber<Object> ts = TestSubscriber.create();
        Exception exception = new Exception(""boo"");
        Flowable.error(exception) //
                .doOnSubscribe(Consumers.increment(count)) //
                .retryWhen(RetryWhen //
                        .delay(1, TimeUnit.MILLISECONDS) //
                        .scheduler(Schedulers.trampoline()) //
                        .maxRetries(10).build()) //
                .subscribe(ts);
        ts.assertTerminated();
        assertFalse(ts.errors().isEmpty());
        assertEquals(exception, ts.errors().get(0));
        assertEquals(11, count.get());
    }

}
","package com.github.davidmoten.rx2;

import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;
import java.util.concurrent.TimeUnit;

import com.github.davidmoten.guavamini.Optional;
import com.github.davidmoten.guavamini.Preconditions;

import io.reactivex.Flowable;
import io.reactivex.Scheduler;
import io.reactivex.functions.BiFunction;
import io.reactivex.functions.Consumer;
import io.reactivex.functions.Function;
import io.reactivex.functions.Predicate;
import io.reactivex.internal.functions.Functions;
import io.reactivex.schedulers.Schedulers;

/**
 * Provides builder for the {@link Function} parameter of
 * {@link Flowable#retryWhen(Function)}. For example:
 * 
 * <pre>
 * o.retryWhen(RetryWhen.maxRetries(4).delay(10, TimeUnit.SECONDS).action(log).build());
 * </pre>
 * 
 * <p>
 * or
 * </p>
 * 
 * <pre>
 * o.retryWhen(RetryWhen.exponentialBackoff(100, TimeUnit.MILLISECONDS).maxRetries(10).build());
 * </pre>
 */
public final class RetryWhen {

    private RetryWhen() {
        // prevent instantiation
    }

    private static final long NO_MORE_DELAYS = -1;

    private static Function<Flowable<? extends Throwable>, Flowable<Object>> notificationHandler(
            final Flowable<Long> delays, final Scheduler scheduler, final Consumer<? super ErrorAndDuration> action,
            final List<Class<? extends Throwable>> retryExceptions,
            final List<Class<? extends Throwable>> failExceptions,
            final Predicate<? super Throwable> exceptionPredicate) {

        final Function<ErrorAndDuration, Flowable<ErrorAndDuration>> checkExceptions = createExceptionChecker(
                retryExceptions, failExceptions, exceptionPredicate);

        return createNotificationHandler(delays, scheduler, action, checkExceptions);
    }

    private static Function<Flowable<? extends Throwable>, Flowable<Object>> createNotificationHandler(
            final Flowable<Long> delays, final Scheduler scheduler, final Consumer<? super ErrorAndDuration> action,
            final Function<ErrorAndDuration, Flowable<ErrorAndDuration>> checkExceptions) {
        return new Function<Flowable<? extends Throwable>, Flowable<Object>>() {

            @SuppressWarnings(""unchecked"")
            @Override
            public Flowable<Object> apply(Flowable<? extends Throwable> errors) {
                // TODO remove this cast when rxjava 2.0.3 released because
                // signature of retryWhen
                // will be fixed
                return (Flowable<Object>) (Flowable<?>) errors
                        // zip with delays, use -1 to signal completion
                        .zipWith(delays.concatWith(Flowable.just(NO_MORE_DELAYS)), TO_ERROR_AND_DURATION)
                        // check retry and non-retry exceptions
                        .flatMap(checkExceptions)
                        // perform user action (for example log that a
                        // delay is happening)
                        .doOnNext(callActionExceptForLast(action))
                        // delay the time in ErrorAndDuration
                        .flatMap(delay(scheduler));
            }
        };
    }

    private static Consumer<ErrorAndDuration> callActionExceptForLast(final Consumer<? super ErrorAndDuration> action) {
        return new Consumer<ErrorAndDuration>() {
            @Override
            public void accept(ErrorAndDuration e) throws Exception {
                if (e.durationMs() != NO_MORE_DELAYS) {
                    action.accept(e);
                }
            }
        };
    }

    // TODO unit test
    private static Function<ErrorAndDuration, Flowable<ErrorAndDuration>> createExceptionChecker(
            final List<Class<? extends Throwable>> retryExceptions,
            final List<Class<? extends Throwable>> failExceptions,
            final Predicate<? super Throwable> exceptionPredicate) {
        return new Function<ErrorAndDuration, Flowable<ErrorAndDuration>>() {

            @Override
            public Flowable<ErrorAndDuration> apply(ErrorAndDuration e) throws Exception {
                if (!exceptionPredicate.test(e.throwable()))
                    return Flowable.error(e.throwable());
                for (Class<? extends Throwable> cls : failExceptions) {
                    if (cls.isAssignableFrom(e.throwable().getClass()))
                        return Flowable.error(e.throwable());
                }
                if (retryExceptions.size() > 0) {
                    for (Class<? extends Throwable> cls : retryExceptions) {
                        if (cls.isAssignableFrom(e.throwable().getClass()))
                            return Flowable.just(e);
                    }
                    return Flowable.error(e.throwable());
                } else {
                    return Flowable.just(e);
                }
            }
        };
    }

    private static BiFunction<Throwable, Long, ErrorAndDuration> TO_ERROR_AND_DURATION = new BiFunction<Throwable, Long, ErrorAndDuration>() {
        @Override
        public ErrorAndDuration apply(Throwable throwable, Long durationMs) {
            return new ErrorAndDuration(throwable, durationMs);
        }
    };

    private static Function<ErrorAndDuration, Flowable<ErrorAndDuration>> delay(final Scheduler scheduler) {
        return new Function<ErrorAndDuration, Flowable<ErrorAndDuration>>() {
            @Override
            public Flowable<ErrorAndDuration> apply(ErrorAndDuration e) {
                if (e.durationMs() == NO_MORE_DELAYS)
                    return Flowable.error(e.throwable());
                else
                    return Flowable.timer(e.durationMs(), TimeUnit.MILLISECONDS, scheduler)
                            .map(com.github.davidmoten.rx2.Functions.constant(e));
            }
        };
    }

    // Builder factory methods

    public static Builder retryWhenInstanceOf(Class<? extends Throwable>... classes) {
        return new Builder().retryWhenInstanceOf(classes);
    }

    public static Builder failWhenInstanceOf(Class<? extends Throwable>... classes) {
        return new Builder().failWhenInstanceOf(classes);
    }

    public static Builder retryIf(Predicate<Throwable> predicate) {
        return new Builder().retryIf(predicate);
    }

    public static Builder delays(Flowable<Long> delays, TimeUnit unit) {
        return new Builder().delays(delays, unit);
    }

    public static Builder delaysInt(Flowable<Integer> delays, TimeUnit unit) {
        return new Builder().delaysInt(delays, unit);
    }

    public static Builder delay(long delay, final TimeUnit unit) {
        return new Builder().delay(delay, unit);
    }

    public static Builder maxRetries(int maxRetries) {
        return new Builder().maxRetries(maxRetries);
    }

    public static Builder scheduler(Scheduler scheduler) {
        return new Builder().scheduler(scheduler);
    }

    public static Builder action(Consumer<? super ErrorAndDuration> action) {
        return new Builder().action(action);
    }

    public static Builder exponentialBackoff(final long firstDelay, final TimeUnit unit, final double factor) {
        return new Builder().exponentialBackoff(firstDelay, unit, factor);
    }

    public static Builder exponentialBackoff(long firstDelay, TimeUnit unit) {
        return new Builder().exponentialBackoff(firstDelay, unit);
    }

    public static final class Builder {

        private final List<Class<? extends Throwable>> retryExceptions = new ArrayList<Class<? extends Throwable>>();
        private final List<Class<? extends Throwable>> failExceptions = new ArrayList<Class<? extends Throwable>>();
        private Predicate<? super Throwable> exceptionPredicate = Functions.alwaysTrue();

        private Flowable<Long> delays = Flowable.just(0L).repeat();
        private Optional<Integer> maxRetries = Optional.absent();
        private Optional<Scheduler> scheduler = Optional.absent();
        private Consumer<? super ErrorAndDuration> action = Consumers.doNothing();

        private Builder() {
            // must use static factory method to instantiate
        }

        public Builder retryWhenInstanceOf(Class<? extends Throwable>... classes) {
            retryExceptions.addAll(Arrays.asList(classes));
            return this;
        }

        public Builder failWhenInstanceOf(Class<? extends Throwable>... classes) {
            failExceptions.addAll(Arrays.asList(classes));
            return this;
        }

        public Builder retryIf(Predicate<Throwable> predicate) {
            this.exceptionPredicate = predicate;
            return this;
        }

        public Builder delays(Flowable<Long> delays, TimeUnit unit) {
            this.delays = delays.map(toMillis(unit));
            return this;
        }

        private static class ToLongHolder {
            static final Function<Integer, Long> INSTANCE = new Function<Integer, Long>() {
                @Override
                public Long apply(Integer n) {
                    if (n == null) {
                        return null;
                    } else {
                        return n.longValue();
                    }
                }
            };
        }

        public Builder delaysInt(Flowable<Integer> delays, TimeUnit unit) {
            return delays(delays.map(ToLongHolder.INSTANCE), unit);
        }

        public Builder delay(Long delay, final TimeUnit unit) {
            this.delays = Flowable.just(delay).map(toMillis(unit)).repeat();
            return this;
        }

        private static Function<Long, Long> toMillis(final TimeUnit unit) {
            return new Function<Long, Long>() {

                @Override
                public Long apply(Long t) {
                    return unit.toMillis(t);
                }
            };
        }

        public Builder maxRetries(int maxRetries) {
            this.maxRetries = Optional.of(maxRetries);
            return this;
        }

        public Builder scheduler(Scheduler scheduler) {
            this.scheduler = Optional.of(scheduler);
            return this;
        }

        public Builder action(Consumer<? super ErrorAndDuration> action) {
            this.action = action;
            return this;
        }

        public Builder exponentialBackoff(final long firstDelay, final long maxDelay, final TimeUnit unit,
                final double factor) {

            delays = Flowable.range(1, Integer.MAX_VALUE)
                    // make exponential
                    .map(new Function<Integer, Long>() {
                        @Override
                        public Long apply(Integer n) {
                            long delayMs = Math.round(Math.pow(factor, n - 1) * unit.toMillis(firstDelay));
                            if (maxDelay == -1) {
                                return delayMs;
                            } else {
                                long maxDelayMs = unit.toMillis(maxDelay);
                                return Math.min(maxDelayMs, delayMs);
                            }
                        }
                    });
            return this;
        }

        public Builder exponentialBackoff(final long firstDelay, final TimeUnit unit, final double factor) {
            return exponentialBackoff(firstDelay, -1, unit, factor);
        }

        public Builder exponentialBackoff(long firstDelay, TimeUnit unit) {
            return exponentialBackoff(firstDelay, unit, 2);
        }

        public Function<Flowable<? extends Throwable>, Flowable<Object>> build() {
            Preconditions.checkNotNull(delays);
            if (maxRetries.isPresent()) {
                delays = delays.take(maxRetries.get());
            }
            Scheduler s = scheduler.isPresent() ? scheduler.get() : Schedulers.computation();
            return notificationHandler(delays, s, action, retryExceptions, failExceptions,
                    exceptionPredicate);
        }

    }

    public static final class ErrorAndDuration {

        private final Throwable throwable;
        private final long durationMs;

        public ErrorAndDuration(Throwable throwable, long durationMs) {
            this.throwable = throwable;
            this.durationMs = durationMs;
        }

        public Throwable throwable() {
            return throwable;
        }

        public long durationMs() {
            return durationMs;
        }

    }
}
","['Assertion Roulette', 'Unknown Test']","['Assertion Roulette', 'Eager Test', 'Redundant Print', 'Magic Number Test', 'Lazy Test', 'Sensitive Equality', 'Sleepy Test', 'Resource Optimism', 'Unknown Test', 'Redundant Assertion', 'Assertion Roulette', 'Lazy Test', 'Redundant Print']",8,0,2,11
40650_45.0_hbase_testupdateheartbeatandunlockfortable,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/40650_test.java,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/40650_actual.java,"/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * ""License""); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hadoop.hbase.procedure2;

import static org.junit.Assert.assertEquals;

import java.io.IOException;
import java.util.ArrayList;
import java.util.concurrent.atomic.AtomicBoolean;
import java.util.concurrent.atomic.AtomicLong;
import org.apache.hadoop.hbase.HBaseClassTestRule;
import org.apache.hadoop.hbase.HBaseCommonTestingUtil;
import org.apache.hadoop.hbase.procedure2.store.NoopProcedureStore;
import org.apache.hadoop.hbase.procedure2.store.ProcedureStore;
import org.apache.hadoop.hbase.testclassification.MasterTests;
import org.apache.hadoop.hbase.testclassification.SmallTests;
import org.apache.hadoop.hbase.util.Threads;
import org.junit.After;
import org.junit.Before;
import org.junit.ClassRule;
import org.junit.Test;
import org.junit.experimental.categories.Category;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

@Category({ MasterTests.class, SmallTests.class })
public class TestProcedureSuspended {
  @ClassRule
  public static final HBaseClassTestRule CLASS_RULE =
    HBaseClassTestRule.forClass(TestProcedureSuspended.class);

  private static final Logger LOG = LoggerFactory.getLogger(TestProcedureSuspended.class);

  private static final int PROCEDURE_EXECUTOR_SLOTS = 1;
  private static final Procedure NULL_PROC = null;

  private ProcedureExecutor<TestProcEnv> procExecutor;
  private ProcedureStore procStore;

  private HBaseCommonTestingUtil htu;

  @Before
  public void setUp() throws IOException {
    htu = new HBaseCommonTestingUtil();

    procStore = new NoopProcedureStore();
    procExecutor = new ProcedureExecutor<>(htu.getConfiguration(), new TestProcEnv(), procStore);
    procStore.start(PROCEDURE_EXECUTOR_SLOTS);
    ProcedureTestingUtility.initAndStartWorkers(procExecutor, PROCEDURE_EXECUTOR_SLOTS, true);
  }

  @After
  public void tearDown() throws IOException {
    procExecutor.stop();
    procStore.stop(false);
  }

  @Test
  public void testSuspendWhileHoldingLocks() {
    final AtomicBoolean lockA = new AtomicBoolean(false);
    final AtomicBoolean lockB = new AtomicBoolean(false);

    final TestLockProcedure p1keyA = new TestLockProcedure(lockA, ""keyA"", false, true);
    final TestLockProcedure p2keyA = new TestLockProcedure(lockA, ""keyA"", false, true);
    final TestLockProcedure p3keyB = new TestLockProcedure(lockB, ""keyB"", false, true);

    procExecutor.submitProcedure(p1keyA);
    procExecutor.submitProcedure(p2keyA);
    procExecutor.submitProcedure(p3keyB);

    // first run p1, p3 are able to run p2 is blocked by p1
    waitAndAssertTimestamp(p1keyA, 1, 1);
    waitAndAssertTimestamp(p2keyA, 0, -1);
    waitAndAssertTimestamp(p3keyB, 1, 2);
    assertEquals(true, lockA.get());
    assertEquals(true, lockB.get());

    // release p3
    p3keyB.setThrowSuspend(false);
    procExecutor.getScheduler().addFront(p3keyB);
    waitAndAssertTimestamp(p1keyA, 1, 1);
    waitAndAssertTimestamp(p2keyA, 0, -1);
    waitAndAssertTimestamp(p3keyB, 2, 3);
    assertEquals(true, lockA.get());

    // wait until p3 is fully completed
    ProcedureTestingUtility.waitProcedure(procExecutor, p3keyB);
    assertEquals(false, lockB.get());

    // rollback p2 and wait until is fully completed
    p1keyA.setTriggerRollback(true);
    procExecutor.getScheduler().addFront(p1keyA);
    ProcedureTestingUtility.waitProcedure(procExecutor, p1keyA);

    // p2 should start and suspend
    waitAndAssertTimestamp(p1keyA, 4, 60000);
    waitAndAssertTimestamp(p2keyA, 1, 7);
    waitAndAssertTimestamp(p3keyB, 2, 3);
    assertEquals(true, lockA.get());

    // wait until p2 is fully completed
    p2keyA.setThrowSuspend(false);
    procExecutor.getScheduler().addFront(p2keyA);
    ProcedureTestingUtility.waitProcedure(procExecutor, p2keyA);
    waitAndAssertTimestamp(p1keyA, 4, 60000);
    waitAndAssertTimestamp(p2keyA, 2, 8);
    waitAndAssertTimestamp(p3keyB, 2, 3);
    assertEquals(false, lockA.get());
    assertEquals(false, lockB.get());
  }

  @Test
  public void testYieldWhileHoldingLocks() {
    final AtomicBoolean lock = new AtomicBoolean(false);

    final TestLockProcedure p1 = new TestLockProcedure(lock, ""key"", true, false);
    final TestLockProcedure p2 = new TestLockProcedure(lock, ""key"", true, false);

    procExecutor.submitProcedure(p1);
    procExecutor.submitProcedure(p2);

    // try to execute a bunch of yield on p1, p2 should be blocked
    while (p1.getTimestamps().size() < 100) {
      Threads.sleep(10);
    }

    assertEquals(0, p2.getTimestamps().size());

    // wait until p1 is completed
    p1.setThrowYield(false);
    ProcedureTestingUtility.waitProcedure(procExecutor, p1);

    // try to execute a bunch of yield on p2
    while (p2.getTimestamps().size() < 100) {
      Threads.sleep(10);
    }

    assertEquals(p1.getTimestamps().get(p1.getTimestamps().size() - 1).longValue() + 1,
      p2.getTimestamps().get(0).longValue());

    // wait until p2 is completed
    p1.setThrowYield(false);
    ProcedureTestingUtility.waitProcedure(procExecutor, p1);
  }

  private void waitAndAssertTimestamp(TestLockProcedure proc, int size, int lastTs) {
    final ArrayList<Long> timestamps = proc.getTimestamps();
    while (timestamps.size() < size) {
      Threads.sleep(10);
    }

    LOG.info(proc + "" -> "" + timestamps);
    assertEquals(size, timestamps.size());
    if (size > 0) {
      assertEquals(lastTs, timestamps.get(timestamps.size() - 1).longValue());
    }
  }

  public static class TestLockProcedure extends Procedure<TestProcEnv> {
    private final ArrayList<Long> timestamps = new ArrayList<>();
    private final String key;

    private boolean triggerRollback = false;
    private boolean throwSuspend = false;
    private boolean throwYield = false;
    private AtomicBoolean lock = null;
    private boolean hasLock = false;

    public TestLockProcedure(final AtomicBoolean lock, final String key, final boolean throwYield,
      final boolean throwSuspend) {
      this.lock = lock;
      this.key = key;
      this.throwYield = throwYield;
      this.throwSuspend = throwSuspend;
    }

    public void setThrowYield(final boolean throwYield) {
      this.throwYield = throwYield;
    }

    public void setThrowSuspend(final boolean throwSuspend) {
      this.throwSuspend = throwSuspend;
    }

    public void setTriggerRollback(final boolean triggerRollback) {
      this.triggerRollback = triggerRollback;
    }

    @Override
    protected Procedure[] execute(final TestProcEnv env)
      throws ProcedureYieldException, ProcedureSuspendedException {
      LOG.info(""EXECUTE "" + this + "" suspend "" + (lock != null));
      timestamps.add(env.nextTimestamp());
      if (triggerRollback) {
        setFailure(getClass().getSimpleName(), new Exception(""injected failure""));
      } else if (throwYield) {
        throw new ProcedureYieldException();
      } else if (throwSuspend) {
        throw new ProcedureSuspendedException();
      }
      return null;
    }

    @Override
    protected void rollback(final TestProcEnv env) {
      LOG.info(""ROLLBACK "" + this);
      timestamps.add(env.nextTimestamp() * 10000);
    }

    @Override
    protected LockState acquireLock(final TestProcEnv env) {
      hasLock = lock.compareAndSet(false, true);
      if (hasLock) {
        LOG.info(""ACQUIRE LOCK "" + this + "" "" + (hasLock));
        return LockState.LOCK_ACQUIRED;
      }
      return LockState.LOCK_YIELD_WAIT;
    }

    @Override
    protected void releaseLock(final TestProcEnv env) {
      LOG.info(""RELEASE LOCK "" + this + "" "" + hasLock);
      lock.set(false);
    }

    @Override
    protected boolean holdLock(final TestProcEnv env) {
      return true;
    }

    public ArrayList<Long> getTimestamps() {
      return timestamps;
    }

    @Override
    protected void toStringClassDetails(StringBuilder builder) {
      builder.append(getClass().getName());
      builder.append(""("" + key + "")"");
    }

    @Override
    protected boolean abort(TestProcEnv env) {
      return false;
    }

    @Override
    protected void serializeStateData(ProcedureStateSerializer serializer) throws IOException {
    }

    @Override
    protected void deserializeStateData(ProcedureStateSerializer serializer) throws IOException {
    }
  }

  private static class TestProcEnv {
    public final AtomicLong timestamp = new AtomicLong(0);

    public long nextTimestamp() {
      return timestamp.incrementAndGet();
    }
  }
}
","/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * ""License""); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hadoop.hbase.master.locking;

import java.io.IOException;
import java.util.concurrent.CountDownLatch;
import java.util.concurrent.atomic.AtomicBoolean;
import java.util.concurrent.atomic.AtomicLong;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.TableName;
import org.apache.hadoop.hbase.client.RegionInfo;
import org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv;
import org.apache.hadoop.hbase.master.procedure.TableProcedureInterface;
import org.apache.hadoop.hbase.procedure2.LockType;
import org.apache.hadoop.hbase.procedure2.Procedure;
import org.apache.hadoop.hbase.procedure2.ProcedureEvent;
import org.apache.hadoop.hbase.procedure2.ProcedureStateSerializer;
import org.apache.hadoop.hbase.procedure2.ProcedureSuspendedException;
import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;
import org.apache.yetus.audience.InterfaceAudience;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil;
import org.apache.hadoop.hbase.shaded.protobuf.generated.LockServiceProtos;
import org.apache.hadoop.hbase.shaded.protobuf.generated.LockServiceProtos.LockProcedureData;
import org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos;

/**
 * Procedure to allow blessed clients and external admin tools to take our internal Schema locks
 * used by the procedure framework isolating procedures doing creates/deletes etc. on
 * table/namespace/regions. This procedure when scheduled, acquires specified locks, suspends itself
 * and waits for:
 * <ul>
 * <li>Call to unlock: if lock request came from the process itself, say master chore.</li>
 * <li>Timeout : if lock request came from RPC. On timeout, evaluates if it should continue holding
 * the lock or not based on last heartbeat timestamp.</li>
 * </ul>
 */
@InterfaceAudience.Private
public final class LockProcedure extends Procedure<MasterProcedureEnv>
  implements TableProcedureInterface {
  private static final Logger LOG = LoggerFactory.getLogger(LockProcedure.class);

  public static final int DEFAULT_REMOTE_LOCKS_TIMEOUT_MS = 30000; // timeout in ms
  public static final String REMOTE_LOCKS_TIMEOUT_MS_CONF =
    ""hbase.master.procedure.remote.locks.timeout.ms"";
  // 10 min. Same as old ZK lock timeout.
  public static final int DEFAULT_LOCAL_MASTER_LOCKS_TIMEOUT_MS = 600000;
  public static final String LOCAL_MASTER_LOCKS_TIMEOUT_MS_CONF =
    ""hbase.master.procedure.local.master.locks.timeout.ms"";

  private String namespace;
  private TableName tableName;
  private RegionInfo[] regionInfos;
  private LockType type;
  // underlying namespace/table/region lock.
  private LockInterface lock;
  private TableOperationType opType;
  private String description;
  // True when recovery of master lock from WALs
  private boolean recoveredMasterLock;

  private final ProcedureEvent<LockProcedure> event = new ProcedureEvent<>(this);
  // True if this proc acquired relevant locks. This value is for client checks.
  private final AtomicBoolean locked = new AtomicBoolean(false);
  // Last system time (in ms) when client sent the heartbeat.
  // Initialize to system time for non-null value in case of recovery.
  private final AtomicLong lastHeartBeat = new AtomicLong();
  // Set to true when unlock request is received.
  private final AtomicBoolean unlock = new AtomicBoolean(false);
  // decreased when locks are acquired. Only used for local (with master process) purposes.
  // Setting latch to non-null value increases default timeout to
  // DEFAULT_LOCAL_MASTER_LOCKS_TIMEOUT_MS (10 min) so that there is no need to heartbeat.
  private final CountDownLatch lockAcquireLatch;

  private volatile boolean suspended = false;

  @Override
  public TableName getTableName() {
    return tableName;
  }

  @Override
  public TableOperationType getTableOperationType() {
    return opType;
  }

  private interface LockInterface {
    boolean acquireLock(MasterProcedureEnv env);

    void releaseLock(MasterProcedureEnv env);
  }

  public LockProcedure() {
    lockAcquireLatch = null;
  }

  private LockProcedure(final Configuration conf, final LockType type, final String description,
    final CountDownLatch lockAcquireLatch) {
    this.type = type;
    this.description = description;
    this.lockAcquireLatch = lockAcquireLatch;
    if (lockAcquireLatch == null) {
      setTimeout(conf.getInt(REMOTE_LOCKS_TIMEOUT_MS_CONF, DEFAULT_REMOTE_LOCKS_TIMEOUT_MS));
    } else {
      setTimeout(
        conf.getInt(LOCAL_MASTER_LOCKS_TIMEOUT_MS_CONF, DEFAULT_LOCAL_MASTER_LOCKS_TIMEOUT_MS));
    }
  }

  /**
   * Constructor for namespace lock.
   * @param lockAcquireLatch if not null, the latch is decreased when lock is acquired.
   */
  public LockProcedure(final Configuration conf, final String namespace, final LockType type,
    final String description, final CountDownLatch lockAcquireLatch)
    throws IllegalArgumentException {
    this(conf, type, description, lockAcquireLatch);

    if (namespace.isEmpty()) {
      throw new IllegalArgumentException(""Empty namespace"");
    }

    this.namespace = namespace;
    this.lock = setupNamespaceLock();
  }

  /**
   * Constructor for table lock.
   * @param lockAcquireLatch if not null, the latch is decreased when lock is acquired.
   */
  public LockProcedure(final Configuration conf, final TableName tableName, final LockType type,
    final String description, final CountDownLatch lockAcquireLatch)
    throws IllegalArgumentException {
    this(conf, type, description, lockAcquireLatch);

    this.tableName = tableName;
    this.lock = setupTableLock();
  }

  /**
   * Constructor for region lock(s).
   * @param lockAcquireLatch if not null, the latch is decreased when lock is acquired. Useful for
   *                         locks acquired locally from master process.
   * @throws IllegalArgumentException if all regions are not from same table.
   */
  public LockProcedure(final Configuration conf, final RegionInfo[] regionInfos,
    final LockType type, final String description, final CountDownLatch lockAcquireLatch)
    throws IllegalArgumentException {
    this(conf, type, description, lockAcquireLatch);

    // Build RegionInfo from region names.
    if (regionInfos.length == 0) {
      throw new IllegalArgumentException(""No regions specified for region lock"");
    }

    // check all regions belong to same table.
    final TableName regionTable = regionInfos[0].getTable();
    for (int i = 1; i < regionInfos.length; ++i) {
      if (!regionInfos[i].getTable().equals(regionTable)) {
        throw new IllegalArgumentException(""All regions should be from same table"");
      }
    }

    this.regionInfos = regionInfos;
    this.lock = setupRegionLock();
  }

  private boolean hasHeartbeatExpired() {
    return EnvironmentEdgeManager.currentTime() - lastHeartBeat.get() >= getTimeout();
  }

  /**
   * Updates timeout deadline for the lock.
   */
  public void updateHeartBeat() {
    lastHeartBeat.set(EnvironmentEdgeManager.currentTime());
    if (LOG.isDebugEnabled()) {
      LOG.debug(""Heartbeat "" + toString());
    }
  }

  /**
   * Re run the procedure after every timeout to write new WAL entries so we don't hold back old
   * WALs.
   * @return false, so procedure framework doesn't mark this procedure as failure.
   */
  @Override
  protected synchronized boolean setTimeoutFailure(final MasterProcedureEnv env) {
    synchronized (event) {
      if (LOG.isDebugEnabled()) LOG.debug(""Timeout failure "" + this.event);
      if (!event.isReady()) { // Maybe unlock() awakened the event.
        setState(ProcedureProtos.ProcedureState.RUNNABLE);
        if (LOG.isDebugEnabled()) LOG.debug(""Calling wake on "" + this.event);
        event.wake(env.getProcedureScheduler());
      }
    }
    return false; // false: do not mark the procedure as failed.
  }

  // Can be called before procedure gets scheduled, in which case, the execute() will finish
  // immediately and release the underlying locks.
  public void unlock(final MasterProcedureEnv env) {
    unlock.set(true);
    locked.set(false);
    // Maybe timeout already awakened the event and the procedure has finished.
    synchronized (event) {
      if (!event.isReady() && suspended) {
        setState(ProcedureProtos.ProcedureState.RUNNABLE);
        event.wake(env.getProcedureScheduler());
        suspended = false;
      }
    }
  }

  @Override
  protected Procedure<MasterProcedureEnv>[] execute(final MasterProcedureEnv env)
    throws ProcedureSuspendedException {
    // Local master locks don't store any state, so on recovery, simply finish this procedure
    // immediately.
    if (recoveredMasterLock) return null;
    if (lockAcquireLatch != null) {
      lockAcquireLatch.countDown();
    }
    if (unlock.get() || hasHeartbeatExpired()) {
      locked.set(false);
      LOG.debug((unlock.get() ? ""UNLOCKED "" : ""TIMED OUT "") + toString());
      return null;
    }
    synchronized (event) {
      event.suspend();
      event.suspendIfNotReady(this);
      setState(ProcedureProtos.ProcedureState.WAITING_TIMEOUT);
      suspended = true;
    }
    throw new ProcedureSuspendedException();
  }

  @Override
  protected void rollback(final MasterProcedureEnv env) {
    throw new UnsupportedOperationException();
  }

  @Override
  protected boolean abort(final MasterProcedureEnv env) {
    unlock(env);
    return true;
  }

  @Override
  protected void serializeStateData(ProcedureStateSerializer serializer) throws IOException {
    final LockProcedureData.Builder builder = LockProcedureData.newBuilder()
      .setLockType(LockServiceProtos.LockType.valueOf(type.name())).setDescription(description);
    if (regionInfos != null) {
      for (int i = 0; i < regionInfos.length; ++i) {
        builder.addRegionInfo(ProtobufUtil.toRegionInfo(regionInfos[i]));
      }
    } else if (namespace != null) {
      builder.setNamespace(namespace);
    } else if (tableName != null) {
      builder.setTableName(ProtobufUtil.toProtoTableName(tableName));
    }
    if (lockAcquireLatch != null) {
      builder.setIsMasterLock(true);
    }
    serializer.serialize(builder.build());
  }

  @Override
  protected void deserializeStateData(ProcedureStateSerializer serializer) throws IOException {
    final LockProcedureData state = serializer.deserialize(LockProcedureData.class);
    type = LockType.valueOf(state.getLockType().name());
    description = state.getDescription();
    if (state.getRegionInfoCount() > 0) {
      regionInfos = new RegionInfo[state.getRegionInfoCount()];
      for (int i = 0; i < state.getRegionInfoCount(); ++i) {
        regionInfos[i] = ProtobufUtil.toRegionInfo(state.getRegionInfo(i));
      }
    } else if (state.hasNamespace()) {
      namespace = state.getNamespace();
    } else if (state.hasTableName()) {
      tableName = ProtobufUtil.toTableName(state.getTableName());
    }
    recoveredMasterLock = state.getIsMasterLock();
    this.lock = setupLock();
  }

  @Override
  protected LockState acquireLock(final MasterProcedureEnv env) {
    boolean ret = lock.acquireLock(env);
    locked.set(ret);
    if (ret) {
      if (LOG.isDebugEnabled()) {
        LOG.debug(""LOCKED "" + toString());
      }
      lastHeartBeat.set(EnvironmentEdgeManager.currentTime());
      return LockState.LOCK_ACQUIRED;
    }
    LOG.warn(""Failed acquire LOCK "" + toString() + ""; YIELDING"");
    return LockState.LOCK_EVENT_WAIT;
  }

  @Override
  protected void releaseLock(final MasterProcedureEnv env) {
    lock.releaseLock(env);
  }

  /**
   * On recovery, re-execute from start to acquire the locks. Need to explicitly set it to RUNNABLE
   * because the procedure might have been in WAITING_TIMEOUT state when crash happened. In which
   * case, it'll be sent back to timeout queue on recovery, which we don't want since we want to
   * require locks.
   */
  @Override
  protected void beforeReplay(MasterProcedureEnv env) {
    setState(ProcedureProtos.ProcedureState.RUNNABLE);
  }

  @Override
  protected void toStringClassDetails(final StringBuilder builder) {
    super.toStringClassDetails(builder);
    if (regionInfos != null) {
      builder.append("" regions="");
      for (int i = 0; i < regionInfos.length; ++i) {
        if (i > 0) builder.append("","");
        builder.append(regionInfos[i].getShortNameToLog());
      }
    } else if (namespace != null) {
      builder.append("", namespace="").append(namespace);
    } else if (tableName != null) {
      builder.append("", tableName="").append(tableName);
    }
    builder.append("", type="").append(type);
  }

  public LockType getType() {
    return type;
  }

  private LockInterface setupLock() throws IllegalArgumentException {
    if (regionInfos != null) {
      return setupRegionLock();
    } else if (namespace != null) {
      return setupNamespaceLock();
    } else if (tableName != null) {
      return setupTableLock();
    } else {
      LOG.error(""Unknown level specified in "" + toString());
      throw new IllegalArgumentException(""no namespace/table/region provided"");
    }
  }

  private LockInterface setupNamespaceLock() throws IllegalArgumentException {
    this.tableName = TableProcedureInterface.DUMMY_NAMESPACE_TABLE_NAME;
    switch (type) {
      case EXCLUSIVE:
        this.opType = TableOperationType.EDIT;
        return new NamespaceExclusiveLock();
      case SHARED:
        LOG.error(""Shared lock on namespace not supported for "" + toString());
        throw new IllegalArgumentException(""Shared lock on namespace not supported"");
      default:
        LOG.error(""Unexpected lock type "" + toString());
        throw new IllegalArgumentException(""Wrong lock type: "" + type.toString());
    }
  }

  private LockInterface setupTableLock() throws IllegalArgumentException {
    switch (type) {
      case EXCLUSIVE:
        this.opType = TableOperationType.EDIT;
        return new TableExclusiveLock();
      case SHARED:
        this.opType = TableOperationType.READ;
        return new TableSharedLock();
      default:
        LOG.error(""Unexpected lock type "" + toString());
        throw new IllegalArgumentException(""Wrong lock type:"" + type.toString());
    }
  }

  private LockInterface setupRegionLock() throws IllegalArgumentException {
    this.tableName = regionInfos[0].getTable();
    switch (type) {
      case EXCLUSIVE:
        this.opType = TableOperationType.REGION_EDIT;
        return new RegionExclusiveLock();
      default:
        LOG.error(""Only exclusive lock supported on regions for "" + toString());
        throw new IllegalArgumentException(""Only exclusive lock supported on regions."");
    }
  }

  public String getDescription() {
    return description;
  }

  public boolean isLocked() {
    return locked.get();
  }

  @Override
  public boolean holdLock(final MasterProcedureEnv env) {
    return true;
  }

  ///////////////////////
  // LOCK IMPLEMENTATIONS
  ///////////////////////

  private class TableExclusiveLock implements LockInterface {
    @Override
    public boolean acquireLock(final MasterProcedureEnv env) {
      // We invert return from waitNamespaceExclusiveLock; it returns true if you HAVE TO WAIT
      // to get the lock and false if you don't; i.e. you got the lock.
      return !env.getProcedureScheduler().waitTableExclusiveLock(LockProcedure.this, tableName);
    }

    @Override
    public void releaseLock(final MasterProcedureEnv env) {
      env.getProcedureScheduler().wakeTableExclusiveLock(LockProcedure.this, tableName);
    }
  }

  private class TableSharedLock implements LockInterface {
    @Override
    public boolean acquireLock(final MasterProcedureEnv env) {
      // We invert return from waitNamespaceExclusiveLock; it returns true if you HAVE TO WAIT
      // to get the lock and false if you don't; i.e. you got the lock.
      return !env.getProcedureScheduler().waitTableSharedLock(LockProcedure.this, tableName);
    }

    @Override
    public void releaseLock(final MasterProcedureEnv env) {
      env.getProcedureScheduler().wakeTableSharedLock(LockProcedure.this, tableName);
    }
  }

  private class NamespaceExclusiveLock implements LockInterface {
    @Override
    public boolean acquireLock(final MasterProcedureEnv env) {
      // We invert return from waitNamespaceExclusiveLock; it returns true if you HAVE TO WAIT
      // to get the lock and false if you don't; i.e. you got the lock.
      return !env.getProcedureScheduler().waitNamespaceExclusiveLock(LockProcedure.this, namespace);
    }

    @Override
    public void releaseLock(final MasterProcedureEnv env) {
      env.getProcedureScheduler().wakeNamespaceExclusiveLock(LockProcedure.this, namespace);
    }
  }

  private class RegionExclusiveLock implements LockInterface {
    @Override
    public boolean acquireLock(final MasterProcedureEnv env) {
      // We invert return from waitNamespaceExclusiveLock; it returns true if you HAVE TO WAIT
      // to get the lock and false if you don't; i.e. you got the lock.
      return !env.getProcedureScheduler().waitRegions(LockProcedure.this, tableName, regionInfos);
    }

    @Override
    public void releaseLock(final MasterProcedureEnv env) {
      env.getProcedureScheduler().wakeRegions(LockProcedure.this, tableName, regionInfos);
    }
  }
}
","['Assertion Roulette', 'Conditional Test Logic', 'General Fixture']","['Eager Test', 'Magic Number Test', 'Mystery Guest', 'Redundant Print', 'Sleepy Test']",5,3,0,12
42612_10_helios_jobdeployedwithhistorylastusedrecentlynotreaped,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/42612_test.java,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/42612_actual.java,"/*-
 * -\-\-
 * Helios Services
 * --
 * Copyright (C) 2016 Spotify AB
 * --
 * Licensed under the Apache License, Version 2.0 (the ""License"");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 * 
 *      http://www.apache.org/licenses/LICENSE-2.0
 * 
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * -/-/-
 */

package com.spotify.helios.master.reaper;

import static java.util.Collections.emptyList;
import static java.util.Collections.emptyMap;
import static java.util.concurrent.TimeUnit.HOURS;
import static org.mockito.Mockito.mock;
import static org.mockito.Mockito.never;
import static org.mockito.Mockito.timeout;
import static org.mockito.Mockito.verify;
import static org.mockito.Mockito.when;

import com.google.common.collect.ImmutableList;
import com.google.common.collect.ImmutableMap;
import com.google.common.collect.Lists;
import com.spotify.helios.common.Clock;
import com.spotify.helios.common.descriptors.Deployment;
import com.spotify.helios.common.descriptors.DeploymentGroup;
import com.spotify.helios.common.descriptors.Goal;
import com.spotify.helios.common.descriptors.Job;
import com.spotify.helios.common.descriptors.JobId;
import com.spotify.helios.common.descriptors.JobStatus;
import com.spotify.helios.common.descriptors.RolloutOptions;
import com.spotify.helios.common.descriptors.TaskStatus;
import com.spotify.helios.common.descriptors.TaskStatus.State;
import com.spotify.helios.common.descriptors.TaskStatusEvent;
import com.spotify.helios.master.MasterModel;
import java.util.ArrayList;
import java.util.Collections;
import java.util.List;
import java.util.Map;
import org.joda.time.Instant;
import org.junit.Rule;
import org.junit.Test;
import org.junit.rules.TestName;

public class OldJobReaperTest {

  @Rule
  public TestName name = new TestName();

  private static final long RETENTION_DAYS = 1;
  private static final Job DUMMY_JOB = Job.newBuilder().build();

  /**
   * A job not deployed, with history, and last used too long ago should BE reaped.
   */
  @Test
  public void jobNotDeployedWithHistoryLastUsedTooLongAgoReaped() throws Exception {
    final MasterModel masterModel = mock(MasterModel.class);
    testReap(name.getMethodName(), emptyMap(),
        events(ImmutableList.of(HOURS.toMillis(20), HOURS.toMillis(22))), null, masterModel, true);
  }

  /**
   * A job not deployed, with history, and last used recently should NOT BE reaped.
   */
  @Test
  public void jobNotDeployedWithHistoryLastUsedRecentlyNotReaped() throws Exception {
    final MasterModel masterModel = mock(MasterModel.class);
    testReap(name.getMethodName(), emptyMap(),
        events(ImmutableList.of(HOURS.toMillis(20), HOURS.toMillis(40))), null, masterModel, false);
  }

  /**
   * A job not deployed, without history, and without a creation date should BE reaped.
   */
  @Test
  public void jobNotDeployedWithoutHistoryWithCreateDateReaped() throws Exception {
    final MasterModel masterModel = mock(MasterModel.class);
    testReap(name.getMethodName(), emptyMap(), emptyList(), null, masterModel, true);
  }

  /**
   * A job not deployed, without history, and created before retention time should BE reaped.
   */
  @Test
  public void jobNotDeployedWithoutHistoryCreateBeforeRetentionReaped() throws Exception {
    final MasterModel masterModel = mock(MasterModel.class);
    testReap(name.getMethodName(), emptyMap(), emptyList(), HOURS.toMillis(23), masterModel, true);
  }

  /**
   * A job not deployed, without history, created after retention time should NOT BE reaped.
   */
  @Test
  public void jobNotDeployedWithoutHistoryCreateAfterRetentionNotReaped() throws Exception {
    final MasterModel masterModel = mock(MasterModel.class);
    testReap(name.getMethodName(), emptyMap(), emptyList(), HOURS.toMillis(25), masterModel, false);
  }

  /**
   * A job deployed and without history should NOT BE reaped.
   */
  @Test
  public void jobDeployedWithoutHistoryNotReaped() throws Exception {
    final MasterModel masterModel = mock(MasterModel.class);
    testReap(name.getMethodName(), deployments(JobId.fromString(name.getMethodName()), 2),
        emptyList(), null, masterModel, false);
  }

  /**
   * A job deployed, with history, and last used too long ago should NOT BE reaped.
   */
  @Test
  public void jobDeployedWithHistoryLastUsedTooLongAgoNotReaped() throws Exception {
    final MasterModel masterModel = mock(MasterModel.class);
    testReap(name.getMethodName(), deployments(JobId.fromString(name.getMethodName()), 3),
        events(ImmutableList.of(HOURS.toMillis(20), HOURS.toMillis(22))), null, masterModel, false);
  }

  /**
   * A job deployed, with history, and last used recently should NOT BE reaped.
   */
  @Test
  public void jobDeployedWithHistoryLastUsedRecentlyNotReaped() throws Exception {
    final MasterModel masterModel = mock(MasterModel.class);
    testReap(name.getMethodName(), deployments(JobId.fromString(name.getMethodName()), 3),
        events(ImmutableList.of(HOURS.toMillis(20), HOURS.toMillis(40))), null, masterModel, false);
  }

  /**
   * A job not deployed, with history, last used too long ago and part of a deployment group
   * should NOT BE reaped.
   */
  @Test
  public void jobDeployedWithHistoryLastUsedTooLongAgoInDgNotReaped() throws Exception {
    final MasterModel masterModel = mock(MasterModel.class);

    when(masterModel.getDeploymentGroups())
        .thenReturn(ImmutableMap.of(
            ""testdg"",
            new DeploymentGroup(""name"", new ArrayList<>(), JobId.fromString(name.getMethodName()),
                RolloutOptions.getDefault(), DeploymentGroup.RollingUpdateReason.MANUAL)));

    testReap(name.getMethodName(), emptyMap(),
        events(ImmutableList.of(HOURS.toMillis(2), HOURS.toMillis(3))), null, masterModel, false);
  }

  /**
   * A job not deployed, with history, last used too long ago and part of a deployment group
   * should BE reaped.
   */
  @Test
  public void jobDeployedWithHistoryLastUsedTooLongAgoNotInDgReaped() throws Exception {
    final MasterModel masterModel = mock(MasterModel.class);

    when(masterModel.getDeploymentGroups())
        .thenReturn(ImmutableMap.of(
            ""testdg"",
            new DeploymentGroup(""name"", new ArrayList<>(), JobId.fromString(""framazama""),
                RolloutOptions.getDefault(), DeploymentGroup.RollingUpdateReason.MANUAL)));

    testReap(name.getMethodName(), emptyMap(),
        events(ImmutableList.of(HOURS.toMillis(2), HOURS.toMillis(3))), null, masterModel, true);
  }

  private void testReap(
      final String jobName,
      final Map<String, Deployment> deployments,
      final List<TaskStatusEvent> history,
      final Long created,
      final MasterModel masterModel,
      final boolean expectReap) throws Exception {

    final Job.Builder jobBuilder = Job
        .newBuilder()
        .setName(jobName);

    if (created != null) {
      jobBuilder.setCreated(created);
    }

    final Job job = jobBuilder.build();

    final JobStatus jobStatus = JobStatus
        .newBuilder()
        .setDeployments(deployments)
        .build();

    final Clock clock = mock(Clock.class);
    when(clock.now()).thenReturn(new Instant(HOURS.toMillis(48)));

    when(masterModel.getJobs())
        .thenReturn(ImmutableMap.of(job.getId(), job));

    when(masterModel.getJobHistory(job.getId())).thenReturn(history);
    when(masterModel.getJobStatus(job.getId())).thenReturn(jobStatus);

    final OldJobReaper reaper = new OldJobReaper(masterModel, RETENTION_DAYS, clock, 100, 0);
    reaper.startAsync().awaitRunning();

    // Wait 100ms to give the reaper enough time to process all the jobs before verifying :(
    Thread.sleep(100);

    if (expectReap) {
      verify(masterModel, timeout(500)).removeJob(job.getId(), Job.EMPTY_TOKEN);
    } else {
      verify(masterModel, never()).removeJob(job.getId(), Job.EMPTY_TOKEN);
    }
  }

  private List<TaskStatusEvent> events(final List<Long> timestamps) {
    final ImmutableList.Builder<TaskStatusEvent> builder = ImmutableList.builder();

    // First sort by timestamps ascending
    final List<Long> copy = Lists.newArrayList(timestamps);
    Collections.sort(copy);

    for (final Long timestamp : timestamps) {
      final TaskStatus taskStatus = TaskStatus.newBuilder()
          .setJob(DUMMY_JOB)
          .setGoal(Goal.START)
          .setState(State.RUNNING)
          .build();
      builder.add(new TaskStatusEvent(taskStatus, timestamp, """"));
    }

    return builder.build();
  }

  private Map<String, Deployment> deployments(final JobId jobId, final int numHosts) {
    final ImmutableMap.Builder<String, Deployment> builder = ImmutableMap.builder();

    for (int i = 0; i < numHosts; i++) {
      builder.put(""host"" + i, Deployment.of(jobId, Goal.START));
    }
    return builder.build();
  }
}
","/*-
 * -\-\-
 * Helios Services
 * --
 * Copyright (C) 2016 Spotify AB
 * --
 * Licensed under the Apache License, Version 2.0 (the ""License"");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 * 
 *      http://www.apache.org/licenses/LICENSE-2.0
 * 
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * -/-/-
 */

package com.spotify.helios.master.reaper;

import static com.google.common.base.Preconditions.checkArgument;

import com.google.common.annotations.VisibleForTesting;
import com.spotify.helios.common.Clock;
import com.spotify.helios.common.SystemClock;
import com.spotify.helios.common.descriptors.Deployment;
import com.spotify.helios.common.descriptors.DeploymentGroup;
import com.spotify.helios.common.descriptors.Job;
import com.spotify.helios.common.descriptors.JobId;
import com.spotify.helios.common.descriptors.JobStatus;
import com.spotify.helios.common.descriptors.TaskStatusEvent;
import com.spotify.helios.master.MasterModel;
import java.util.List;
import java.util.Map;
import java.util.Random;
import java.util.Set;
import java.util.concurrent.TimeUnit;
import java.util.stream.Collectors;
import org.joda.time.format.DateTimeFormat;
import org.joda.time.format.DateTimeFormatter;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Removes old jobs that haven't been deployed for a while.
 * The logic for whether a job should be reaped depends on whether it's deployed, its last history
 * event, its creation date, and the specified number of retention days.
 *
 * <p>1. A job that is deployed should NOT BE reaped, regardless of its history or creation date.
 * 2. A job that is not deployed, with history, and an event *before* the number of retention
 * days, should BE reaped.
 * 3. A job that is not deployed, with history, and an event *after* the number of retention
 * days should NOT BE reaped. For example: a job created long ago but deployed recently.
 * 4. A job that is not deployed, without history, and without a creation date should BE
 * reaped. Only really old versions of Helios create jobs without dates.
 * 5. A job that is not deployed, without history, but with a creation date before the number of
 * retention days should BE reaped only if the job is NOT part of a deployment group.
 * 6. A job that is not deployed, without history, and with a creation date after the number of
 * retention days should NOT BE reaped.
 *
 * <p>Note that the --disable-job-history flag in {@link com.spotify.helios.agent.AgentParser}
 * controls whether the Helios agent should write job history to the data store. If this is
 * disabled, scenarios two and three above will never match. In this case, a job created a long
 * time ago but deployed recently may be reaped once it's undeployed even if the user needs it
 * again in the future.
 */
public class OldJobReaper extends RateLimitedService<Job> {

  private static final double PERMITS_PER_SECOND = 0.2; // one permit every 5 seconds
  private static final Clock SYSTEM_CLOCK = new SystemClock();
  private static final int DELAY = 60 * 24; // 1 day in minutes
  private static final TimeUnit TIME_UNIT = TimeUnit.MINUTES;
  private static final DateTimeFormatter DATE_FORMATTER =
      DateTimeFormat.forPattern(""YYYY-MM-dd HH:mm:ss"");

  private static final Logger log = LoggerFactory.getLogger(OldJobReaper.class);

  private final MasterModel masterModel;
  private final long retentionDays;
  private final long retentionMillis;
  private final Clock clock;

  private Set<JobId> jobsInDeploymentGroups;

  public OldJobReaper(final MasterModel masterModel, final long retentionDays) {
    this(masterModel, retentionDays, SYSTEM_CLOCK, PERMITS_PER_SECOND, new Random().nextInt(DELAY));
  }

  @VisibleForTesting
  OldJobReaper(final MasterModel masterModel,
               final long retentionDays,
               final Clock clock,
               final double permitsPerSecond,
               final int initialDelay) {
    super(permitsPerSecond, initialDelay, DELAY, TIME_UNIT);
    this.masterModel = masterModel;
    checkArgument(retentionDays > 0);
    this.retentionDays = retentionDays;
    this.retentionMillis = TimeUnit.DAYS.toMillis(retentionDays);
    this.clock = clock;
  }

  @Override
  Iterable<Job> collectItems() {
    return masterModel.getJobs().values();
  }

  @Override
  protected void beforeIteration() {
    jobsInDeploymentGroups = masterModel.getDeploymentGroups()
        .values()
        .stream()
        .map(DeploymentGroup::getJobId)
        .collect(Collectors.toSet());
  }

  @Override
  void processItem(final Job job) {
    final JobId jobId = job.getId();

    try {
      final JobStatus jobStatus = masterModel.getJobStatus(jobId);
      if (jobStatus == null) {
        log.warn(""Couldn't find job status for {} because job has already been deleted. Skipping."",
            jobId);
        return;
      }

      final Map<String, Deployment> deployments = jobStatus.getDeployments();
      final List<TaskStatusEvent> events = masterModel.getJobHistory(jobId);

      boolean reap;

      if (deployments.isEmpty()) {
        if (events.isEmpty()) {
          final Long created = job.getCreated();
          if (created == null) {
            log.info(""Marked job '{}' for reaping (not deployed, no history, no creation date)"",
                jobId);
            reap = true;
          } else if ((clock.now().getMillis() - created) > retentionMillis) {
            log.info(""Marked job '{}' for reaping (not deployed, no history, creation date ""
                     + ""of {} before retention time of {} days)"",
                jobId, DATE_FORMATTER.print(created), retentionDays);
            reap = true;
          } else {
            log.info(""NOT reaping job '{}' (not deployed, no history, creation date of {} after ""
                     + ""retention time of {} days)"",
                jobId, DATE_FORMATTER.print(created), retentionDays);
            reap = false;
          }
        } else {
          // Get the last event which is the most recent
          final TaskStatusEvent event = events.get(events.size() - 1);
          final String eventDate = DATE_FORMATTER.print(event.getTimestamp());
          // Calculate the amount of time in milliseconds that has elapsed since the last event
          final long unusedDurationMillis = clock.now().getMillis() - event.getTimestamp();

          // A job not deployed, with history, job is not part of a deployment group, and last used
          //  too long ago should BE reaped
          // A job not deployed, with history, job is part of a deployment group, or last used
          //  recently should NOT BE reaped
          if (unusedDurationMillis > retentionMillis && !jobsInDeploymentGroups.contains(jobId)) {
            log.info(""Marked job '{}' for reaping (not deployed, has history whose last event ""
                     + ""on {} was before the retention time of {} days)"",
                jobId, eventDate, retentionDays);
            reap = true;
          } else {
            log.info(""NOT reaping job '{}' (not deployed, has history whose last event ""
                     + ""on {} was after the retention time of {} days)"",
                jobId, eventDate, retentionDays);
            reap = false;
          }
        }
      } else {
        // A job that's deployed should NOT BE reaped regardless of its history or creation date
        reap = false;
      }

      if (reap) {
        try {
          log.info(""reaping old job '{}'"", job.getId());
          masterModel.removeJob(jobId, job.getToken());
        } catch (Exception e) {
          log.warn(""Failed to reap old job '{}'"", jobId, e);
        }
      }
    } catch (Exception e) {
      log.warn(""Failed to determine if job '{}' should be reaped"", jobId, e);
    }
  }
}
","['Conditional Test Logic', 'Wait And See', 'Eager Test', 'Unknown Test']","['Assertion Roulette', 'Lazy Test']",2,4,0,11
31005_4.0_hadoop_testcreateshutdown,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/31005_test.java,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/31005_actual.java,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * ""License""); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hadoop.net.unix;

import static org.junit.Assert.assertFalse;

import java.util.ArrayList;
import java.util.Random;
import java.util.concurrent.CountDownLatch;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.concurrent.locks.ReentrantLock;

import org.junit.After;
import org.junit.Assume;
import org.junit.Before;
import org.junit.Test;

import org.apache.hadoop.thirdparty.com.google.common.util.concurrent.Uninterruptibles;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class TestDomainSocketWatcher {
  static final Logger LOG =
      LoggerFactory.getLogger(TestDomainSocketWatcher.class);

  private Throwable trappedException = null;

  @Before
  public void before() {
    Assume.assumeTrue(DomainSocket.getLoadingFailureReason() == null);
  }

  @After
  public void after() {
    if (trappedException != null) {
      throw new IllegalStateException(
          ""DomainSocketWatcher thread terminated with unexpected exception."",
          trappedException);
    }
  }

  /**
   * Test that we can create a DomainSocketWatcher and then shut it down.
   */
  @Test(timeout=60000)
  public void testCreateShutdown() throws Exception {
    DomainSocketWatcher watcher = newDomainSocketWatcher(10000000);
    watcher.close();
  }

  /**
   * Test that we can get notifications out a DomainSocketWatcher.
   */
  @Test(timeout=180000)
  public void testDeliverNotifications() throws Exception {
    DomainSocketWatcher watcher = newDomainSocketWatcher(10000000);
    DomainSocket pair[] = DomainSocket.socketpair();
    final CountDownLatch latch = new CountDownLatch(1);
    watcher.add(pair[1], new DomainSocketWatcher.Handler() {
      @Override
      public boolean handle(DomainSocket sock) {
        latch.countDown();
        return true;
      }
    });
    pair[0].close();
    latch.await();
    watcher.close();
  }

  /**
   * Test that a java interruption can stop the watcher thread
   */
  @Test(timeout=60000)
  public void testInterruption() throws Exception {
    final DomainSocketWatcher watcher = newDomainSocketWatcher(10);
    watcher.watcherThread.interrupt();
    Uninterruptibles.joinUninterruptibly(watcher.watcherThread);
    watcher.close();
  }

  /**
   * Test that domain sockets are closed when the watcher is closed.
   */
  @Test(timeout=300000)
  public void testCloseSocketOnWatcherClose() throws Exception {
    final DomainSocketWatcher watcher = newDomainSocketWatcher(10000000);
    DomainSocket pair[] = DomainSocket.socketpair();
    watcher.add(pair[1], new DomainSocketWatcher.Handler() {
      @Override
      public boolean handle(DomainSocket sock) {
        return true;
      }
    });
    watcher.close();
    Uninterruptibles.joinUninterruptibly(watcher.watcherThread);
    assertFalse(pair[1].isOpen());
  }
  
  @Test(timeout=300000)
  public void testStress() throws Exception {
    final int SOCKET_NUM = 250;
    final ReentrantLock lock = new ReentrantLock();
    final DomainSocketWatcher watcher = newDomainSocketWatcher(10000000);
    final ArrayList<DomainSocket[]> pairs = new ArrayList<DomainSocket[]>();
    final AtomicInteger handled = new AtomicInteger(0);

    final Thread adderThread = new Thread(new Runnable() {
      @Override
      public void run() {
        try {
          for (int i = 0; i < SOCKET_NUM; i++) {
            DomainSocket pair[] = DomainSocket.socketpair();
            watcher.add(pair[1], new DomainSocketWatcher.Handler() {
              @Override
              public boolean handle(DomainSocket sock) {
                handled.incrementAndGet();
                return true;
              }
            });
            lock.lock();
            try {
              pairs.add(pair);
            } finally {
              lock.unlock();
            }
          }
        } catch (Throwable e) {
          LOG.error(e.toString());
          throw new RuntimeException(e);
        }
      }
    });
    
    final Thread removerThread = new Thread(new Runnable() {
      @Override
      public void run() {
        final Random random = new Random();
        try {
          while (handled.get() != SOCKET_NUM) {
            lock.lock();
            try {
              if (!pairs.isEmpty()) {
                int idx = random.nextInt(pairs.size());
                DomainSocket pair[] = pairs.remove(idx);
                if (random.nextBoolean()) {
                  pair[0].close();
                } else {
                  watcher.remove(pair[1]);
                }
              }
            } finally {
              lock.unlock();
            }
          }
        } catch (Throwable e) {
          LOG.error(e.toString());
          throw new RuntimeException(e);
        }
      }
    });

    adderThread.start();
    removerThread.start();
    Uninterruptibles.joinUninterruptibly(adderThread);
    Uninterruptibles.joinUninterruptibly(removerThread);
    watcher.close();
  }

  @Test(timeout = 300000)
  public void testStressInterruption() throws Exception {
    final int SOCKET_NUM = 250;
    final ReentrantLock lock = new ReentrantLock();
    final DomainSocketWatcher watcher = newDomainSocketWatcher(10);
    final ArrayList<DomainSocket[]> pairs = new ArrayList<DomainSocket[]>();
    final AtomicInteger handled = new AtomicInteger(0);

    final Thread adderThread = new Thread(new Runnable() {
      @Override
      public void run() {
        try {
          for (int i = 0; i < SOCKET_NUM; i++) {
            DomainSocket pair[] = DomainSocket.socketpair();
            watcher.add(pair[1], new DomainSocketWatcher.Handler() {
              @Override
              public boolean handle(DomainSocket sock) {
                handled.incrementAndGet();
                return true;
              }
            });
            lock.lock();
            try {
              pairs.add(pair);
            } finally {
              lock.unlock();
            }
            TimeUnit.MILLISECONDS.sleep(1);
          }
        } catch (Throwable e) {
          LOG.error(e.toString());
          throw new RuntimeException(e);
        }
      }
    });

    final Thread removerThread = new Thread(new Runnable() {
      @Override
      public void run() {
        final Random random = new Random();
        try {
          while (handled.get() != SOCKET_NUM) {
            lock.lock();
            try {
              if (!pairs.isEmpty()) {
                int idx = random.nextInt(pairs.size());
                DomainSocket pair[] = pairs.remove(idx);
                if (random.nextBoolean()) {
                  pair[0].close();
                } else {
                  watcher.remove(pair[1]);
                }
                TimeUnit.MILLISECONDS.sleep(1);
              }
            } finally {
              lock.unlock();
            }
          }
        } catch (Throwable e) {
          LOG.error(e.toString());
          throw new RuntimeException(e);
        }
      }
    });

    adderThread.start();
    removerThread.start();
    TimeUnit.MILLISECONDS.sleep(100);
    watcher.watcherThread.interrupt();
    Uninterruptibles.joinUninterruptibly(adderThread);
    Uninterruptibles.joinUninterruptibly(removerThread);
    Uninterruptibles.joinUninterruptibly(watcher.watcherThread);
  }

  /**
   * Creates a new DomainSocketWatcher and tracks its thread for termination due
   * to an unexpected exception.  At the end of each test, if there was an
   * unexpected exception, then that exception is thrown to force a failure of
   * the test.
   *
   * @param interruptCheckPeriodMs interrupt check period passed to
   *     DomainSocketWatcher
   * @return new DomainSocketWatcher
   * @throws Exception if there is any failure
   */
  private DomainSocketWatcher newDomainSocketWatcher(int interruptCheckPeriodMs)
      throws Exception {
    DomainSocketWatcher watcher = new DomainSocketWatcher(
        interruptCheckPeriodMs, getClass().getSimpleName());
    watcher.watcherThread.setUncaughtExceptionHandler(
        new Thread.UncaughtExceptionHandler() {
          @Override
          public void uncaughtException(Thread thread, Throwable t) {
            trappedException = t;
          }
        });
    return watcher;
  }
}
","/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * ""License""); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hadoop.net.unix;

import java.io.Closeable;
import java.io.EOFException;

import org.apache.hadoop.classification.InterfaceAudience;
import org.apache.hadoop.io.IOUtils;

import java.io.IOException;
import java.nio.channels.ClosedChannelException;
import java.util.Iterator;
import java.util.LinkedList;
import java.util.TreeMap;
import java.util.Map;
import java.util.concurrent.locks.Condition;
import java.util.concurrent.locks.ReentrantLock;

import org.apache.commons.lang3.SystemUtils;
import org.apache.hadoop.util.NativeCodeLoader;
import org.apache.hadoop.classification.VisibleForTesting;
import org.apache.hadoop.util.Preconditions;

import org.apache.hadoop.thirdparty.com.google.common.util.concurrent.Uninterruptibles;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * The DomainSocketWatcher watches a set of domain sockets to see when they
 * become readable, or closed.  When one of those events happens, it makes a
 * callback.
 *
 * See {@link DomainSocket} for more information about UNIX domain sockets.
 */
@InterfaceAudience.LimitedPrivate(""HDFS"")
public final class DomainSocketWatcher implements Closeable {
  static {
    if (SystemUtils.IS_OS_WINDOWS) {
      loadingFailureReason = ""UNIX Domain sockets are not available on Windows."";
    } else if (!NativeCodeLoader.isNativeCodeLoaded()) {
      loadingFailureReason = ""libhadoop cannot be loaded."";
    } else {
      String problem;
      try {
        anchorNative();
        problem = null;
      } catch (Throwable t) {
        problem = ""DomainSocketWatcher#anchorNative got error: "" +
          t.getMessage();
      }
      loadingFailureReason = problem;
    }
  }

  static final Logger LOG = LoggerFactory.getLogger(DomainSocketWatcher.class);

  /**
   * The reason why DomainSocketWatcher is not available, or null if it is
   * available.
   */
  private final static String loadingFailureReason;

  /**
   * Initializes the native library code.
   */
  private static native void anchorNative();

  public static String getLoadingFailureReason() {
    return loadingFailureReason;
  }

  public interface Handler {
    /**
     * Handles an event on a socket.  An event may be the socket becoming
     * readable, or the remote end being closed.
     *
     * @param sock    The socket that the event occurred on.
     * @return        Whether we should close the socket.
     */
    boolean handle(DomainSocket sock);
  }

  /**
   * Handler for {DomainSocketWatcher#notificationSockets[1]}
   */
  private class NotificationHandler implements Handler {
    public boolean handle(DomainSocket sock) {
      assert(lock.isHeldByCurrentThread());
      try {
        kicked = false;
        if (LOG.isTraceEnabled()) {
          LOG.trace(this + "": NotificationHandler: doing a read on "" +
            sock.fd);
        }
        if (sock.getInputStream().read() == -1) {
          if (LOG.isTraceEnabled()) {
            LOG.trace(this + "": NotificationHandler: got EOF on "" + sock.fd);
          }
          throw new EOFException();
        }
        if (LOG.isTraceEnabled()) {
          LOG.trace(this + "": NotificationHandler: read succeeded on "" +
            sock.fd);
        }
        return false;
      } catch (IOException e) {
        if (LOG.isTraceEnabled()) {
          LOG.trace(this + "": NotificationHandler: setting closed to "" +
              ""true for "" + sock.fd);
        }
        closed = true;
        return true;
      }
    }
  }

  private static class Entry {
    final DomainSocket socket;
    final Handler handler;

    Entry(DomainSocket socket, Handler handler) {
      this.socket = socket;
      this.handler = handler;
    }

    DomainSocket getDomainSocket() {
      return socket;
    }

    Handler getHandler() {
      return handler;
    }
  }

  /**
   * The FdSet is a set of file descriptors that gets passed to poll(2).
   * It contains a native memory segment, so that we don't have to copy
   * in the poll0 function.
   */
  private static class FdSet {
    private long data;

    private native static long alloc0();

    FdSet() {
      data = alloc0();
    }

    /**
     * Add a file descriptor to the set.
     *
     * @param fd   The file descriptor to add.
     */
    native void add(int fd);

    /**
     * Remove a file descriptor from the set.
     *
     * @param fd   The file descriptor to remove.
     */
    native void remove(int fd);

    /**
     * Get an array containing all the FDs marked as readable.
     * Also clear the state of all FDs.
     *
     * @return     An array containing all of the currently readable file
     *             descriptors.
     */
    native int[] getAndClearReadableFds();

    /**
     * Close the object and de-allocate the memory used.
     */
    native void close();
  }

  /**
   * Lock which protects toAdd, toRemove, and closed.
   */
  private final ReentrantLock lock = new ReentrantLock();

  /**
   * Condition variable which indicates that toAdd and toRemove have been
   * processed.
   */
  private final Condition processedCond = lock.newCondition();

  /**
   * Entries to add.
   */
  private final LinkedList<Entry> toAdd =
      new LinkedList<Entry>();

  /**
   * Entries to remove.
   */
  private final TreeMap<Integer, DomainSocket> toRemove =
      new TreeMap<Integer, DomainSocket>();

  /**
   * Maximum length of time to go between checking whether the interrupted
   * bit has been set for this thread.
   */
  private final int interruptCheckPeriodMs;

  /**
   * A pair of sockets used to wake up the thread after it has called poll(2).
   */
  private final DomainSocket notificationSockets[];

  /**
   * Whether or not this DomainSocketWatcher is closed.
   */
  private boolean closed = false;
  
  /**
   * True if we have written a byte to the notification socket. We should not
   * write anything else to the socket until the notification handler has had a
   * chance to run. Otherwise, our thread might block, causing deadlock. 
   * See HADOOP-11333 for details.
   */
  private boolean kicked = false;

  public DomainSocketWatcher(int interruptCheckPeriodMs, String src)
      throws IOException {
    if (loadingFailureReason != null) {
      throw new UnsupportedOperationException(loadingFailureReason);
    }
    Preconditions.checkArgument(interruptCheckPeriodMs > 0);
    this.interruptCheckPeriodMs = interruptCheckPeriodMs;
    notificationSockets = DomainSocket.socketpair();
    watcherThread.setDaemon(true);
    watcherThread.setName(src + "" DomainSocketWatcher"");
    watcherThread
        .setUncaughtExceptionHandler(new Thread.UncaughtExceptionHandler() {
          @Override
          public void uncaughtException(Thread thread, Throwable t) {
            LOG.error(thread + "" terminating on unexpected exception"", t);
          }
        });
    watcherThread.start();
  }

  /**
   * Close the DomainSocketWatcher and wait for its thread to terminate.
   *
   * If there is more than one close, all but the first will be ignored.
   */
  @Override
  public void close() throws IOException {
    lock.lock();
    try {
      if (closed) return;
      if (LOG.isDebugEnabled()) {
        LOG.debug(this + "": closing"");
      }
      closed = true;
    } finally {
      lock.unlock();
    }
    // Close notificationSockets[0], so that notificationSockets[1] gets an EOF
    // event.  This will wake up the thread immediately if it is blocked inside
    // the select() system call.
    notificationSockets[0].close();
    // Wait for the select thread to terminate.
    Uninterruptibles.joinUninterruptibly(watcherThread);
  }

  @VisibleForTesting
  public boolean isClosed() {
    lock.lock();
    try {
      return closed;
    } finally {
      lock.unlock();
    }
  }

  /**
   * Add a socket.
   *
   * @param sock     The socket to add.  It is an error to re-add a socket that
   *                   we are already watching.
   * @param handler  The handler to associate with this socket.  This may be
   *                   called any time after this function is called.
   */
  public void add(DomainSocket sock, Handler handler) {
    lock.lock();
    try {
      if (closed) {
        handler.handle(sock);
        IOUtils.cleanupWithLogger(LOG, sock);
        return;
      }
      Entry entry = new Entry(sock, handler);
      try {
        sock.refCount.reference();
      } catch (ClosedChannelException e1) {
        // If the socket is already closed before we add it, invoke the
        // handler immediately.  Then we're done.
        handler.handle(sock);
        return;
      }
      toAdd.add(entry);
      kick();
      while (true) {
        processedCond.awaitUninterruptibly();
        if (!toAdd.contains(entry)) {
          break;
        }
      }
    } finally {
      lock.unlock();
    }
  }

  /**
   * Remove a socket.  Its handler will be called.
   *
   * @param sock     The socket to remove.
   */
  public void remove(DomainSocket sock) {
    lock.lock();
    try {
      if (closed) return;
      toRemove.put(sock.fd, sock);
      kick();
      while (true) {
        processedCond.awaitUninterruptibly();
        if (!toRemove.containsKey(sock.fd)) {
          break;
        }
      }
    } finally {
      lock.unlock();
    }
  }

  /**
   * Wake up the DomainSocketWatcher thread.
   */
  private void kick() {
    assert(lock.isHeldByCurrentThread());
    
    if (kicked) {
      return;
    }
    
    try {
      notificationSockets[0].getOutputStream().write(0);
      kicked = true;
    } catch (IOException e) {
      if (!closed) {
        LOG.error(this + "": error writing to notificationSockets[0]"", e);
      }
    }
  }

  /**
   * Send callback and return whether or not the domain socket was closed as a
   * result of processing.
   *
   * @param caller reason for call
   * @param entries mapping of file descriptor to entry
   * @param fdSet set of file descriptors
   * @param fd file descriptor
   * @return true if the domain socket was closed as a result of processing
   */
  private boolean sendCallback(String caller, TreeMap<Integer, Entry> entries,
      FdSet fdSet, int fd) {
    if (LOG.isTraceEnabled()) {
      LOG.trace(this + "": "" + caller + "" starting sendCallback for fd "" + fd);
    }
    Entry entry = entries.get(fd);
    Preconditions.checkNotNull(entry,
        this + "": fdSet contained "" + fd + "", which we were "" +
        ""not tracking."");
    DomainSocket sock = entry.getDomainSocket();
    if (entry.getHandler().handle(sock)) {
      if (LOG.isTraceEnabled()) {
        LOG.trace(this + "": "" + caller + "": closing fd "" + fd +
            "" at the request of the handler."");
      }
      if (toRemove.remove(fd) != null) {
        if (LOG.isTraceEnabled()) {
          LOG.trace(this + "": "" + caller + "" : sendCallback processed fd "" +
            fd  + "" in toRemove."");
        }
      }
      try {
        sock.refCount.unreferenceCheckClosed();
      } catch (IOException e) {
        Preconditions.checkArgument(false,
            this + "": file descriptor "" + sock.fd + "" was closed while "" +
            ""still in the poll(2) loop."");
      }
      IOUtils.cleanupWithLogger(LOG, sock);
      fdSet.remove(fd);
      return true;
    } else {
      if (LOG.isTraceEnabled()) {
        LOG.trace(this + "": "" + caller + "": sendCallback not "" +
            ""closing fd "" + fd);
      }
      return false;
    }
  }

  /**
   * Send callback, and if the domain socket was closed as a result of
   * processing, then also remove the entry for the file descriptor.
   *
   * @param caller reason for call
   * @param entries mapping of file descriptor to entry
   * @param fdSet set of file descriptors
   * @param fd file descriptor
   */
  private void sendCallbackAndRemove(String caller,
      TreeMap<Integer, Entry> entries, FdSet fdSet, int fd) {
    if (sendCallback(caller, entries, fdSet, fd)) {
      entries.remove(fd);
    }
  }

  @VisibleForTesting
  final Thread watcherThread = new Thread(new Runnable() {
    @Override
    public void run() {
      if (LOG.isDebugEnabled()) {
        LOG.debug(this + "": starting with interruptCheckPeriodMs = "" +
            interruptCheckPeriodMs);
      }
      final TreeMap<Integer, Entry> entries = new TreeMap<Integer, Entry>();
      FdSet fdSet = new FdSet();
      addNotificationSocket(entries, fdSet);
      try {
        while (true) {
          lock.lock();
          try {
            for (int fd : fdSet.getAndClearReadableFds()) {
              sendCallbackAndRemove(""getAndClearReadableFds"", entries, fdSet,
                  fd);
            }
            if (!(toAdd.isEmpty() && toRemove.isEmpty())) {
              // Handle pending additions (before pending removes).
              for (Iterator<Entry> iter = toAdd.iterator(); iter.hasNext(); ) {
                Entry entry = iter.next();
                iter.remove();
                DomainSocket sock = entry.getDomainSocket();
                Entry prevEntry = entries.put(sock.fd, entry);
                Preconditions.checkState(prevEntry == null,
                    this + "": tried to watch a file descriptor that we "" +
                    ""were already watching: "" + sock);
                if (LOG.isTraceEnabled()) {
                  LOG.trace(this + "": adding fd "" + sock.fd);
                }
                fdSet.add(sock.fd);
              }
              // Handle pending removals
              while (true) {
                Map.Entry<Integer, DomainSocket> entry = toRemove.firstEntry();
                if (entry == null) break;
                sendCallbackAndRemove(""handlePendingRemovals"",
                    entries, fdSet, entry.getValue().fd);
              }
              processedCond.signalAll();
            }
            // Check if the thread should terminate.  Doing this check now is
            // easier than at the beginning of the loop, since we know toAdd and
            // toRemove are now empty and processedCond has been notified if it
            // needed to be.
            if (closed) {
              if (LOG.isDebugEnabled()) {
                LOG.debug(toString() + "" thread terminating."");
              }
              return;
            }
            // Check if someone sent our thread an InterruptedException while we
            // were waiting in poll().
            if (Thread.interrupted()) {
              throw new InterruptedException();
            }
          } finally {
            lock.unlock();
          }
          doPoll0(interruptCheckPeriodMs, fdSet);
        }
      } catch (InterruptedException e) {
        LOG.info(toString() + "" terminating on InterruptedException"");
      } catch (Throwable e) {
        LOG.error(toString() + "" terminating on exception"", e);
      } finally {
        lock.lock();
        try {
          kick(); // allow the handler for notificationSockets[0] to read a byte
          for (Entry entry : entries.values()) {
            // We do not remove from entries as we iterate, because that can
            // cause a ConcurrentModificationException.
            sendCallback(""close"", entries, fdSet, entry.getDomainSocket().fd);
          }
          entries.clear();
          fdSet.close();
          closed = true;
          if (!(toAdd.isEmpty() && toRemove.isEmpty())) {
            // Items in toAdd might not be added to entries, handle it here
            for (Iterator<Entry> iter = toAdd.iterator(); iter.hasNext();) {
              Entry entry = iter.next();
              entry.getDomainSocket().refCount.unreference();
              entry.getHandler().handle(entry.getDomainSocket());
              IOUtils.cleanupWithLogger(LOG, entry.getDomainSocket());
              iter.remove();
            }
            // Items in toRemove might not be really removed, handle it here
            while (true) {
              Map.Entry<Integer, DomainSocket> entry = toRemove.firstEntry();
              if (entry == null)
                break;
              sendCallback(""close"", entries, fdSet, entry.getValue().fd);
            }
          }
          processedCond.signalAll();
        } finally {
          lock.unlock();
        }
      }
    }
  });

  private void addNotificationSocket(final TreeMap<Integer, Entry> entries,
      FdSet fdSet) {
    entries.put(notificationSockets[1].fd, 
        new Entry(notificationSockets[1], new NotificationHandler()));
    try {
      notificationSockets[1].refCount.reference();
    } catch (IOException e) {
      throw new RuntimeException(e);
    }
    fdSet.add(notificationSockets[1].fd);
    if (LOG.isTraceEnabled()) {
      LOG.trace(this + "": adding notificationSocket "" +
          notificationSockets[1].fd + "", connected to "" +
          notificationSockets[0].fd);
    }
  }

  public String toString() {
    return ""DomainSocketWatcher("" + System.identityHashCode(this) + "")""; 
  }

  private static native int doPoll0(int maxWaitMs, FdSet readFds)
      throws IOException;
}
",[],"['Assertion Roulette', 'Eager Test', 'Magic Number Test', 'Redundant Print', 'Sleepy Test', 'Unknown Test']",6,0,0,14
35890_76.0_achilles_should_build_schema_for_entity_with_static_column,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/35890_test.java,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/35890_actual.java,"/*
 * Copyright (C) 2012-2021 DuyHai DOAN
 *
 * Licensed under the Apache License, Version 2.0 (the ""License"");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package info.archinnov.achilles.it;

import static org.assertj.core.api.Assertions.assertThat;

import org.junit.Test;

import com.datastax.driver.core.CodecRegistry;
import com.datastax.driver.core.ProtocolVersion;

import info.archinnov.achilles.generated.meta.entity.*;
import info.archinnov.achilles.generated.meta.udt.TestUDT_AchillesMeta;
import info.archinnov.achilles.internals.factory.TupleTypeFactory;
import info.archinnov.achilles.internals.factory.UserTypeFactory;
import info.archinnov.achilles.internals.schema.SchemaContext;
import info.archinnov.achilles.schema.SchemaGenerator;

public class TestSchemaGenerator extends AbstractTestUtil {

    private SchemaContext context = new SchemaContext(""my_ks"", false, false);

    @Test
    public void should_build_schema_for_entity_with_static_column() throws Exception {
        //Given
        final EntityWithStaticColumn_AchillesMeta meta = new EntityWithStaticColumn_AchillesMeta();

        //When
        final String actual = meta.generateSchema(context);

        //Then
        assertThat(actual.trim()).isEqualTo(readCodeBlockFromFile(
                ""schema/should_build_schema_for_entity_with_static_column.cql""));
    }

    @Test
    public void should_build_schema_for_entity_with_counter_column() throws Exception {
        //Given
        final EntityWithCounterColumn_AchillesMeta meta = new EntityWithCounterColumn_AchillesMeta();

        //When
        final String actual = meta.generateSchema(context);

        //Then
        assertThat(actual.trim()).isEqualTo(readCodeBlockFromFile(
                ""schema/should_build_schema_for_entity_with_counter_column.cql""));
    }

    @Test
    public void should_build_schema_for_entity_with_static_counter_column() throws Exception {
        //Given
        final EntityWithStaticCounterColumn_AchillesMeta meta = new EntityWithStaticCounterColumn_AchillesMeta();

        //When
        final String actual = meta.generateSchema(context);

        //Then
        assertThat(actual.trim()).isEqualTo(readCodeBlockFromFile(
                ""schema/should_build_schema_for_entity_with_static_counter_column.cql""));
    }

    @Test
    public void should_build_schema_for_child_entity() throws Exception {
        //Given
        final EntityAsChild_AchillesMeta meta = new EntityAsChild_AchillesMeta();

        //When
        final String actual = meta.generateSchema(context);

        //Then
        assertThat(actual.trim()).isEqualTo(readCodeBlockFromFile(
                ""schema/should_build_schema_for_child_entity.cql""));
    }

    @Test
    public void should_build_schema_for_complex_types() throws Exception {
        //Given
        final EntityWithComplexTypes_AchillesMeta meta = new EntityWithComplexTypes_AchillesMeta();
        final CodecRegistry codecRegistry = new CodecRegistry();
        TupleTypeFactory tupleTypeFactory = new TupleTypeFactory(ProtocolVersion.NEWEST_SUPPORTED, codecRegistry);
        UserTypeFactory userTypeFactory = new UserTypeFactory(ProtocolVersion.NEWEST_SUPPORTED, codecRegistry);

        meta.inject(userTypeFactory, tupleTypeFactory);

        //When
        final String actual = meta.generateSchema(context);

        //Then
        assertThat(actual.trim()).isEqualTo(readCodeBlockFromFile(
                ""schema/should_build_schema_for_complex_types.cql""));
    }

    @Test
    public void should_build_schema_for_complex_counter() throws Exception {
        //Given
        final EntityWithComplexCounters_AchillesMeta meta = new EntityWithComplexCounters_AchillesMeta();

        //When
        final String actual = meta.generateSchema(context);

        //Then
        assertThat(actual.trim()).isEqualTo(readCodeBlockFromFile(
                ""schema/should_build_schema_for_complex_counter.cql""));
    }

    @Test
    public void should_build_schema_for_complex_indices() throws Exception {
        //Given
        final EntityWithComplexIndices_AchillesMeta meta = new EntityWithComplexIndices_AchillesMeta();

        //When
        final String actual = meta.generateSchema(new SchemaContext(""my_ks"", false, true));

        //Then
        assertThat(actual.trim()).isEqualTo(readCodeBlockFromFile(
                ""schema/should_build_schema_for_complex_indices.cql""));
    }

    @Test
    public void should_build_schema_for_entity_with_static_annotations() throws Exception {
        //Given
        final EntityWithStaticAnnotations_AchillesMeta meta = new EntityWithStaticAnnotations_AchillesMeta();

        //When
        final String actual = meta.generateSchema(context);

        //Then
        assertThat(actual.trim()).isEqualTo(readCodeBlockFromFile(
                ""schema/should_build_schema_for_entity_with_static_annotations.cql""));
    }

    @Test
    public void should_build_schema_for_udt() throws Exception {
        //Given
        final TestUDT_AchillesMeta meta = TestUDT_AchillesMeta.INSTANCE;

        //When
        final String actual = meta.generateSchema(new SchemaContext(""my_ks"", true, false));

        //Then
        assertThat(actual.trim()).isEqualTo(readCodeBlockFromFile(
                ""schema/should_build_schema_for_udt.cql""));
    }

    @Test
    public void should_use_schema_generator() throws Exception {
        //Given

        //When
        final String schema = SchemaGenerator.builder()
                .withKeyspace(""test"")
                .generateCustomTypes(true)
                .generate();

        //Then
        assertThat(schema.trim()).isEqualTo(readCodeBlockFromFile(
                ""schema/should_use_schema_generator.cql""));
    }

    @Test
    public void should_use_schema_generator_and_append_to_string() throws Exception {
        //Given
        StringBuilder builder = new StringBuilder();

        //When
        SchemaGenerator.builder().withKeyspace(""test"").generateTo(builder);

        //Then
        assertThat(builder.toString().trim()).isEqualTo(readCodeBlockFromFile(
                ""schema/should_use_schema_generator.cql""));
    }


    @Test
    public void should_build_schema_for_simple_entity() throws Exception {
        //Given
        final SimpleEntity_AchillesMeta meta = new SimpleEntity_AchillesMeta();

        //When
        final String actual = meta.generateSchema(context);

        //Then
        assertThat(actual.trim()).isEqualTo(readCodeBlockFromFile(
                ""schema/should_build_schema_for_simple_entity.cql""));
    }

    @Test
    public void should_build_schema_for_entity_with_simple_partition_key() throws Exception {
        //Given
        final EntityWithSimplePartitionKey_AchillesMeta meta = new EntityWithSimplePartitionKey_AchillesMeta();

        //When
        final String actual = meta.generateSchema(context);

        //Then
        assertThat(actual.trim()).isEqualTo(readCodeBlockFromFile(
                ""schema/should_build_schema_for_entity_with_simple_partition_key.cql""));
    }

    @Test
    public void should_build_schema_for_entity_with_composite_partition_key() throws Exception {
        //Given
        final EntityWithCompositePartitionKey_AchillesMeta meta = new EntityWithCompositePartitionKey_AchillesMeta();

        //When
        final String actual = meta.generateSchema(context);

        //Then
        assertThat(actual.trim()).isEqualTo(readCodeBlockFromFile(
                ""schema/should_build_schema_for_entity_with_composite_partition_key.cql""));
    }

    @Test
    public void should_build_schema_for_entity_with_clustering_columns() throws Exception {
        //Given
        final EntityWithClusteringColumns_AchillesMeta meta = new EntityWithClusteringColumns_AchillesMeta();

        //When
        final String actual = meta.generateSchema(context);

        //Then
        assertThat(actual.trim()).isEqualTo(readCodeBlockFromFile(
                ""schema/should_build_schema_for_entity_with_clustering_columns.cql""));
    }

    @Test
    public void should_build_schema_for_view() throws Exception {
        //Given
        final ViewSensorByType_AchillesMeta meta = new ViewSensorByType_AchillesMeta();
        final EntitySensor_AchillesMeta baseTableMeta = new EntitySensor_AchillesMeta();
        meta.setBaseClassProperty(baseTableMeta);

        //When
        final String actual = meta.generateSchema(context);

        //Then
        assertThat(actual.trim()).isEqualTo(readCodeBlockFromFile(
                ""schema/should_build_schema_for_view.cql""));
    }
}
","/*
 * Copyright (C) 2012-2021 DuyHai DOAN
 *
 * Licensed under the Apache License, Version 2.0 (the ""License"");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package info.archinnov.achilles.schema;

import static com.datastax.driver.core.ProtocolVersion.NEWEST_SUPPORTED;
import static com.google.common.collect.Sets.newHashSet;
import static info.archinnov.achilles.internals.parser.TypeUtils.ENTITY_META_PACKAGE;
import static info.archinnov.achilles.internals.parser.TypeUtils.UDT_META_PACKAGE;
import static info.archinnov.achilles.validation.Validator.validateNotBlank;
import static info.archinnov.achilles.validation.Validator.validateTrue;
import static java.lang.String.format;
import static java.util.stream.Collectors.toList;

import java.io.File;
import java.io.IOException;
import java.io.OutputStreamWriter;
import java.io.Writer;
import java.nio.file.Files;
import java.nio.file.Path;
import java.util.Comparator;
import java.util.List;
import java.util.Map;
import java.util.Optional;
import java.util.stream.Collectors;

import org.reflections.Reflections;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.datastax.driver.core.CodecRegistry;

import info.archinnov.achilles.internals.factory.TupleTypeFactory;
import info.archinnov.achilles.internals.factory.UserTypeFactory;
import info.archinnov.achilles.internals.metamodel.AbstractEntityProperty;
import info.archinnov.achilles.internals.metamodel.AbstractUDTClassProperty;
import info.archinnov.achilles.internals.metamodel.AbstractViewProperty;
import info.archinnov.achilles.internals.schema.SchemaContext;
import info.archinnov.achilles.type.tuples.Tuple2;

public class SchemaGenerator {

    private static final Logger LOGGER = LoggerFactory.getLogger(SchemaGenerator.class);

    private static final CodecRegistry CODEC_REGISTRY = new CodecRegistry();
    private static final TupleTypeFactory TUPLE_TYPE_FACTORY = new TupleTypeFactory(NEWEST_SUPPORTED, CODEC_REGISTRY);
    private static final UserTypeFactory USER_TYPE_FACTORY = new UserTypeFactory(NEWEST_SUPPORTED, CODEC_REGISTRY);
    private static final Comparator<Tuple2<String, Class<AbstractEntityProperty<?>>>> BY_NAME_ENTITY_CLASS_SORTER =
            (o1, o2) -> o1._1().compareTo(o2._1());
    private static final Comparator<Tuple2<String, Class<AbstractUDTClassProperty<?>>>> BY_NAME_UDT_CLASS_SORTER =
            (o1, o2) -> o1._1().compareTo(o2._1());
    private Optional<String> keyspace = Optional.empty();
    private boolean createIndex = true;
    private boolean createUdt = true;

    private SchemaGenerator(String keyspaceName) {
        this.keyspace = Optional.ofNullable(keyspaceName);
    }

    public static void main(String... args) throws IOException {
        if (args == null || args.length != 4) {
            System.out.println(displayUsage());
        } else {
            final String targetFile = args[1];
            final String keyspaceName = args[3];
            final Path path = new File(targetFile).toPath();
            Files.deleteIfExists(path);
            Files.createFile(path);
            final SchemaGenerator generator = new SchemaGenerator(keyspaceName);
            generator.createIndex = true;
            generator.createUdt = true;
            generator.generateTo(path);
        }
    }

    private static String displayUsage() {
        StringBuilder builder = new StringBuilder();
        builder.append(""*********************************************************************************************************************************************************************\n"");
        builder.append(""\n"");
        builder.append(""Usage for Schema Generator : \n"");
        builder.append(""\n"");
        builder.append(""java -cp ./your_compiled_entities.jar:./achilles-schema-generator-<version>-shaded.jar info.archinnov.achilles.schema.SchemaGenerator -target <schema_file> -keyspace <keyspace_name> \n"");
        builder.append(""\n"");
        builder.append(""*********************************************************************************************************************************************************************\n"");

        return builder.toString();
    }

    public static SchemaGenerator builder() {
        return new SchemaGenerator(null);
    }

    public SchemaGenerator withKeyspace(String keyspace) {
        validateNotBlank(keyspace, ""Provided keyspace for SchemaGenerator should not be blank"");
        this.keyspace = Optional.of(keyspace);
        return this;
    }

    public SchemaGenerator generateCustomTypes(boolean generateCustomTypes) {
        this.createUdt = generateCustomTypes;
        return this;
    }

    public SchemaGenerator withCustomTypes() {
        this.createUdt = true;
        return this;
    }

    public SchemaGenerator withoutCustomTypes() {
        this.createUdt = false;
        return this;
    }

    public SchemaGenerator generateIndices(boolean generateIndices) {
        this.createIndex = generateIndices;
        return this;
    }

    public SchemaGenerator withIndices() {
        this.createIndex = true;
        return this;
    }

    public SchemaGenerator withoutIndices() {
        this.createIndex = false;
        return this;
    }

    @SuppressWarnings("""")
    public String generate() {
        LOGGER.info(""Start generating schema file "");
        validateNotBlank(keyspace.orElse(""""), ""Keyspace should be provided to generate schema"");
        final SchemaContext context = new SchemaContext(keyspace.get(), createUdt, createIndex);
        ReflectionsHelper.registerUrlTypes("".mar"", "".jnilib"", "".zip"");
        Reflections reflections = new Reflections(newHashSet(ENTITY_META_PACKAGE, UDT_META_PACKAGE), this.getClass().getClassLoader());
        StringBuilder builder = new StringBuilder();

        final List<AbstractEntityProperty<?>> entityMetas = reflections
                .getSubTypesOf(AbstractEntityProperty.class)
                .stream()
                .map(x -> Tuple2.of(x.getCanonicalName(), (Class<AbstractEntityProperty<?>>) x))
                .sorted(BY_NAME_ENTITY_CLASS_SORTER)
                .map(x -> x._2())
                .map(SchemaGenerator::newInstanceForEntityProperty)
                .filter(x -> x != null)
                .collect(toList());

        //Inject keyspace to entity metas
        entityMetas.forEach(x -> x.injectKeyspace(keyspace.get()));

        LOGGER.info(format(""Found %s entity meta classes"", entityMetas.size()));

        //Generate UDT BEFORE tables
        if (context.createUdt) {
            LOGGER.info(format(""Generating schema for UDT""));
            final List<AbstractUDTClassProperty<?>> udtMetas = reflections
                    .getSubTypesOf(AbstractUDTClassProperty.class)
                    .stream()
                    .map(x -> Tuple2.of(x.getCanonicalName(), (Class<AbstractUDTClassProperty<?>>) x))
                    .sorted(BY_NAME_UDT_CLASS_SORTER)
                    .map(x -> x._2())
                    .map(SchemaGenerator::newInstanceForUDTProperty)
                    .filter(x -> x != null)
                    .collect(toList());

            LOGGER.info(format(""Found %s udt classes"", udtMetas.size()));

            for (AbstractUDTClassProperty<?> instance : udtMetas) {
                instance.injectKeyspace(keyspace.get());
                instance.inject(USER_TYPE_FACTORY, TUPLE_TYPE_FACTORY);
                builder.append(instance.generateSchema(context));
            }

        }

        final long viewCount = entityMetas
                .stream()
                .filter(AbstractEntityProperty::isView)
                .count();


        //Inject base table property into view property
        if (viewCount > 0) {
            final Map<Class<?>, AbstractEntityProperty<?>> entityPropertiesMap = entityMetas
                    .stream()
                    .filter(AbstractEntityProperty::isTable)
                    .collect(Collectors.toMap(x -> x.entityClass, x -> x));

            entityMetas
                    .stream()
                    .filter(AbstractEntityProperty::isView)
                    .map(x -> (AbstractViewProperty<?>)x)
                    .forEach(x -> x.setBaseClassProperty(entityPropertiesMap.get(x.getBaseEntityClass())));
        }

        for (AbstractEntityProperty<?> instance : entityMetas) {
            instance.inject(USER_TYPE_FACTORY, TUPLE_TYPE_FACTORY);
            builder.append(instance.generateSchema(context));
        }

        return builder.toString();
    }

    public void generateTo(Appendable appendable) throws IOException {
        final String schemaString = generate();
        appendable.append(schemaString);
    }

    public void generateTo(Path file) throws IOException {
        validateTrue(Files.exists(file), ""'%s' should exist"", file.toString());
        validateTrue(Files.isWritable(file), ""'%s' should have write permission"", file.toString());
        Writer writer = new OutputStreamWriter(Files.newOutputStream(file));
        generateTo(writer);
        writer.flush();
        writer.close();
    }

    public void generateTo(File file) throws IOException {
        generateTo(file.toPath());
    }

    public static AbstractEntityProperty<?> newInstanceForEntityProperty(Class<? extends AbstractEntityProperty<?>> clazz) {
        try {
            return clazz.newInstance();
        } catch (InstantiationException | IllegalAccessException e) {
            e.printStackTrace();
        }
        return null;
    }

    public static AbstractUDTClassProperty<?> newInstanceForUDTProperty(Class<? extends AbstractUDTClassProperty<?>> clazz) {
        try {
            return clazz.newInstance();
        } catch (InstantiationException | IllegalAccessException e) {
            e.printStackTrace();
        }
        return null;
    }
}
","['Sensitive Equality', 'Eager Test', 'Lazy Test']","['Assertion Roulette', 'Magic Number Test', 'Lazy Test']",2,2,1,13
46436_6.0_junit-quickcheck_onesourceofmanyatpositionmid,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/46436_test.java,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/46436_actual.java,"/*
 The MIT License

 Copyright (c) 2010-2021 Paul R. Holser, Jr.

 Permission is hereby granted, free of charge, to any person obtaining
 a copy of this software and associated documentation files (the
 ""Software""), to deal in the Software without restriction, including
 without limitation the rights to use, copy, modify, merge, publish,
 distribute, sublicense, and/or sell copies of the Software, and to
 permit persons to whom the Software is furnished to do so, subject to
 the following conditions:

 The above copyright notice and this permission notice shall be
 included in all copies or substantial portions of the Software.

 THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND,
 EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
 MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
 NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
 LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
 OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
 WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
*/

package com.pholser.junit.quickcheck.internal;

import static com.google.common.collect.Lists.newArrayList;
import static java.util.Arrays.asList;
import static java.util.Collections.emptyIterator;
import static java.util.Collections.emptyList;
import static java.util.Collections.singletonList;
import static org.junit.Assert.assertEquals;

import java.util.List;
import org.junit.Test;

public class CartesianIteratorTest {
    @Test public void noSources() {
        CartesianIterator<Integer> iter = new CartesianIterator<>(emptyList());

        List<List<Integer>> asList = newArrayList(iter);

        assertEquals(0, asList.size());
    }

    @Test public void singleSourceIsEmpty() {
        CartesianIterator<Integer> iter =
            new CartesianIterator<>(singletonList(emptyIterator()));

        List<List<Integer>> result = newArrayList(iter);

        assertEquals(0, result.size());
    }

    @Test public void multipleSourcesAreEmpty() {
        CartesianIterator<Integer> iter =
            new CartesianIterator<>(
                asList(emptyIterator(), emptyIterator(), emptyIterator()));

        List<List<Integer>> result = newArrayList(iter);
        assertEquals(0, result.size());
    }

    @Test public void onlyOneSourceIsEmpty() {
        CartesianIterator<Integer> iter =
            new CartesianIterator<>(
                asList(
                    asList(1, 2, 3).iterator(),
                    emptyIterator(),
                    asList(4, 5, 6, 7).iterator()));

        List<List<Integer>> result = newArrayList(iter);

        assertEquals(0, result.size());
    }

    @Test public void oneSourceOfOne() {
        CartesianIterator<Integer> iter =
            new CartesianIterator<>(
                singletonList(
                    singletonList(1).iterator()));

        List<List<Integer>> result = newArrayList(iter);

        assertEquals(singletonList(singletonList(1)), result);
    }

    @Test public void manySourcesOfOne() {
        CartesianIterator<Integer> iter =
            new CartesianIterator<>(
                asList(
                    singletonList(1).iterator(),
                    singletonList(2).iterator(),
                    singletonList(3).iterator(),
                    singletonList(4).iterator()));

        List<List<Integer>> result = newArrayList(iter);

        assertEquals(singletonList(asList(1, 2, 3, 4)), result);
    }

    @Test public void oneSourceOfManyAtPositionZero() {
        CartesianIterator<Integer> iter =
            new CartesianIterator<>(
                asList(
                    asList(1, 2, 3).iterator(),
                    singletonList(4).iterator(),
                    singletonList(5).iterator(),
                    singletonList(6).iterator()));

        List<List<Integer>> result = newArrayList(iter);

        assertEquals(
            asList(
                asList(1, 4, 5, 6),
                asList(2, 4, 5, 6),
                asList(3, 4, 5, 6)
            ),
            result);
    }

    @Test public void oneSourceOfManyAtPositionMid() {
        CartesianIterator<Integer> iter =
            new CartesianIterator<>(
                asList(
                    singletonList(1).iterator(),
                    singletonList(2).iterator(),
                    asList(3, 4, 5).iterator(),
                    singletonList(6).iterator()));

        List<List<Integer>> result = newArrayList(iter);

        assertEquals(
            asList(
                asList(1, 2, 3, 6),
                asList(1, 2, 4, 6),
                asList(1, 2, 5, 6)),
            result);
    }

    @Test public void oneSourceOfManyAtPositionLast() {
        CartesianIterator<Integer> iter =
            new CartesianIterator<>(
                asList(
                    singletonList(1).iterator(),
                    singletonList(2).iterator(),
                    singletonList(3).iterator(),
                    asList(4, 5, 6).iterator()));

        List<List<Integer>> result = newArrayList(iter);

        assertEquals(
            asList(
                asList(1, 2, 3, 4),
                asList(1, 2, 3, 5),
                asList(1, 2, 3, 6)),
            result);
    }

    @Test public void manySourcesOfMany() {
        CartesianIterator<Integer> iter =
            new CartesianIterator<>(
                asList(
                    asList(0, 1, 2, 3, 4).iterator(),
                    asList(5, 6).iterator(),
                    asList(7, 8, 9).iterator(),
                    asList(10, 11, 12).iterator()));

        List<List<Integer>> result = newArrayList(iter);

        assertEquals(
            asList(
                asList(0, 5, 7, 10), asList(1, 5, 7, 10), asList(2, 5, 7, 10), asList(3, 5, 7, 10), asList(4, 5, 7, 10),
                asList(0, 6, 7, 10), asList(1, 6, 7, 10), asList(2, 6, 7, 10), asList(3, 6, 7, 10), asList(4, 6, 7, 10),
                asList(0, 5, 8, 10), asList(1, 5, 8, 10), asList(2, 5, 8, 10), asList(3, 5, 8, 10), asList(4, 5, 8, 10),
                asList(0, 6, 8, 10), asList(1, 6, 8, 10), asList(2, 6, 8, 10), asList(3, 6, 8, 10), asList(4, 6, 8, 10),
                asList(0, 5, 9, 10), asList(1, 5, 9, 10), asList(2, 5, 9, 10), asList(3, 5, 9, 10), asList(4, 5, 9, 10),
                asList(0, 6, 9, 10), asList(1, 6, 9, 10), asList(2, 6, 9, 10), asList(3, 6, 9, 10), asList(4, 6, 9, 10),
                asList(0, 5, 7, 11), asList(1, 5, 7, 11), asList(2, 5, 7, 11), asList(3, 5, 7, 11), asList(4, 5, 7, 11),
                asList(0, 6, 7, 11), asList(1, 6, 7, 11), asList(2, 6, 7, 11), asList(3, 6, 7, 11), asList(4, 6, 7, 11),
                asList(0, 5, 8, 11), asList(1, 5, 8, 11), asList(2, 5, 8, 11), asList(3, 5, 8, 11), asList(4, 5, 8, 11),
                asList(0, 6, 8, 11), asList(1, 6, 8, 11), asList(2, 6, 8, 11), asList(3, 6, 8, 11), asList(4, 6, 8, 11),
                asList(0, 5, 9, 11), asList(1, 5, 9, 11), asList(2, 5, 9, 11), asList(3, 5, 9, 11), asList(4, 5, 9, 11),
                asList(0, 6, 9, 11), asList(1, 6, 9, 11), asList(2, 6, 9, 11), asList(3, 6, 9, 11), asList(4, 6, 9, 11),
                asList(0, 5, 7, 12), asList(1, 5, 7, 12), asList(2, 5, 7, 12), asList(3, 5, 7, 12), asList(4, 5, 7, 12),
                asList(0, 6, 7, 12), asList(1, 6, 7, 12), asList(2, 6, 7, 12), asList(3, 6, 7, 12), asList(4, 6, 7, 12),
                asList(0, 5, 8, 12), asList(1, 5, 8, 12), asList(2, 5, 8, 12), asList(3, 5, 8, 12), asList(4, 5, 8, 12),
                asList(0, 6, 8, 12), asList(1, 6, 8, 12), asList(2, 6, 8, 12), asList(3, 6, 8, 12), asList(4, 6, 8, 12),
                asList(0, 5, 9, 12), asList(1, 5, 9, 12), asList(2, 5, 9, 12), asList(3, 5, 9, 12), asList(4, 5, 9, 12),
                asList(0, 6, 9, 12), asList(1, 6, 9, 12), asList(2, 6, 9, 12), asList(3, 6, 9, 12), asList(4, 6, 9, 12)
            ),
            result);
    }

    @Test public void manySourcesOfEqualSize() {
        CartesianIterator<Integer> iter =
            new CartesianIterator<>(
                asList(
                    asList(0, 1, 2).iterator(),
                    asList(0, 1, 2).iterator(),
                    asList(0, 1, 2).iterator()));

        List<List<Integer>> result = newArrayList(iter);

        assertEquals(
            asList(
                asList(0, 0, 0), asList(1, 0, 0), asList(2, 0, 0),
                asList(0, 1, 0), asList(1, 1, 0), asList(2, 1, 0),
                asList(0, 2, 0), asList(1, 2, 0), asList(2, 2, 0),
                asList(0, 0, 1), asList(1, 0, 1), asList(2, 0, 1),
                asList(0, 1, 1), asList(1, 1, 1), asList(2, 1, 1),
                asList(0, 2, 1), asList(1, 2, 1), asList(2, 2, 1),
                asList(0, 0, 2), asList(1, 0, 2), asList(2, 0, 2),
                asList(0, 1, 2), asList(1, 1, 2), asList(2, 1, 2),
                asList(0, 2, 2), asList(1, 2, 2), asList(2, 2, 2)),
            result);
    }
}
","/*
 The MIT License

 Copyright (c) 2010-2021 Paul R. Holser, Jr.

 Permission is hereby granted, free of charge, to any person obtaining
 a copy of this software and associated documentation files (the
 ""Software""), to deal in the Software without restriction, including
 without limitation the rights to use, copy, modify, merge, publish,
 distribute, sublicense, and/or sell copies of the Software, and to
 permit persons to whom the Software is furnished to do so, subject to
 the following conditions:

 The above copyright notice and this permission notice shall be
 included in all copies or substantial portions of the Software.

 THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND,
 EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
 MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
 NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
 LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
 OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
 WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
*/

package com.pholser.junit.quickcheck.internal;

import static java.util.stream.Collectors.toList;

import java.util.ArrayList;
import java.util.Iterator;
import java.util.List;

public class CartesianIterator<T> implements Iterator<List<T>> {
    private final List<Buffer<T>> buffers;
    private final boolean allStartedAvailable;

    private int count;

    public CartesianIterator(List<Iterator<T>> sources) {
        this.buffers =
            sources.stream()
                .map(s -> new Buffer<>(s, new ArrayList<>()))
                .collect(toList());
        allStartedAvailable = sources.stream().allMatch(Iterator::hasNext);
    }

    @Override public boolean hasNext() {
        return allStartedAvailable
            && buffers.stream().anyMatch(Buffer::available);
    }

    @Override public List<T> next() {
        List<T> result = new ArrayList<>();
        int n = count;

        for (Buffer<T> each : buffers) {
            int divisor = each.divisor();

            result.add(each.get(n));

            n /= divisor;
        }

        ++count;
        return result;
    }

    private static class Buffer<T> {
        private final Iterator<T> source;
        private final List<T> consumed;
        private int index;

        Buffer(Iterator<T> source, List<T> consumed) {
            this.source = source;
            this.consumed = consumed;
        }

        boolean available() {
            return source.hasNext() || index < consumed.size() - 1;
        }

        int divisor() {
            return source.hasNext() ? consumed.size() + 1 : consumed.size();
        }

        T get(int n) {
            index = n % divisor();
            if (index == consumed.size()) {
                consumed.add(source.next());
            }

            return consumed.get(index);
        }
    }
}
",[],"['Assertion Roulette', 'Eager Test']",2,0,0,15
12786_15.0_wro4j_shouldinvokeregisteredcallbacks,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/12786_test.java,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/12786_actual.java,"/**
 * Copyright Alex Objelean
 */
package ro.isdc.wro.manager.callback;

import static org.junit.Assert.assertEquals;

import java.io.StringWriter;
import java.util.concurrent.Callable;

import org.apache.commons.io.output.WriterOutputStream;
import org.junit.After;
import org.junit.AfterClass;
import org.junit.Before;
import org.junit.BeforeClass;
import org.junit.Test;
import org.mockito.Mockito;

import jakarta.servlet.FilterConfig;
import jakarta.servlet.http.HttpServletRequest;
import jakarta.servlet.http.HttpServletResponse;
import ro.isdc.wro.WroRuntimeException;
import ro.isdc.wro.config.Context;
import ro.isdc.wro.http.support.DelegatingServletOutputStream;
import ro.isdc.wro.manager.WroManager;
import ro.isdc.wro.manager.factory.BaseWroManagerFactory;
import ro.isdc.wro.manager.factory.WroManagerFactory;
import ro.isdc.wro.model.WroModel;
import ro.isdc.wro.model.factory.WroModelFactory;
import ro.isdc.wro.model.group.Group;
import ro.isdc.wro.model.group.GroupExtractor;
import ro.isdc.wro.model.resource.Resource;
import ro.isdc.wro.model.resource.ResourceType;
import ro.isdc.wro.util.ObjectFactory;
import ro.isdc.wro.util.WroTestUtils;
import ro.isdc.wro.util.WroUtil;


/**
 * @author Alex Objelean
 */
public class TestLifecycleCallbackRegistry {
  private LifecycleCallbackRegistry registry;
  
  @BeforeClass
  public static void onBeforeClass() {
    assertEquals(0, Context.countActive());
  }
  
  @AfterClass
  public static void onAfterClass() {
    assertEquals(0, Context.countActive());
  }
  
  @Before
  public void setUp() {
    Context.set(Context.standaloneContext());
    registry = new LifecycleCallbackRegistry();
  }
  
  @After
  public void tearDown() {
    Context.unset();
  }
  
  @Test
  public void shouldInvokeRegisteredCallbacks() {
    final LifecycleCallback callback = Mockito.mock(LifecycleCallback.class);
    final Resource changedResource = Resource.create(""test.js"");
    registry.registerCallback(factoryFor(callback));
    
    registry.onBeforeModelCreated();
    Mockito.verify(callback).onBeforeModelCreated();
    
    registry.onAfterModelCreated();
    Mockito.verify(callback).onAfterModelCreated();
    
    registry.onBeforePreProcess();
    Mockito.verify(callback).onBeforePreProcess();
    
    registry.onAfterPreProcess();
    Mockito.verify(callback).onAfterPreProcess();
    
    registry.onBeforePostProcess();
    Mockito.verify(callback).onBeforePostProcess();
    
    registry.onAfterPostProcess();
    Mockito.verify(callback).onAfterPostProcess();
    
    registry.onBeforeMerge();
    Mockito.verify(callback).onBeforeMerge();
    
    registry.onAfterMerge();
    Mockito.verify(callback).onAfterMerge();
    
    registry.onProcessingComplete();
    Mockito.verify(callback).onProcessingComplete();
    
    registry.onResourceChanged(changedResource);
    Mockito.verify(callback).onResourceChanged(Mockito.eq(changedResource));
  }
  
  private ObjectFactory<LifecycleCallback> factoryFor(final LifecycleCallback callback) {
    return new ObjectFactory<LifecycleCallback>() {
      public LifecycleCallback create() {
        return callback;
      }
    };
  }
  
  @Test
  public void shouldCatchCallbacksExceptionsAndContinueExecution() {
    final LifecycleCallback failingCallback = Mockito.mock(LifecycleCallback.class);
    final LifecycleCallback simpleCallback = Mockito.spy(new LifecycleCallbackSupport());
    final Resource changedResource = Resource.create(""test.js"");
    
    Mockito.doThrow(new IllegalStateException()).when(failingCallback).onBeforeModelCreated();
    Mockito.doThrow(new IllegalStateException()).when(failingCallback).onAfterModelCreated();
    Mockito.doThrow(new IllegalStateException()).when(failingCallback).onBeforePreProcess();
    Mockito.doThrow(new IllegalStateException()).when(failingCallback).onAfterPreProcess();
    Mockito.doThrow(new IllegalStateException()).when(failingCallback).onBeforePostProcess();
    Mockito.doThrow(new IllegalStateException()).when(failingCallback).onAfterPostProcess();
    Mockito.doThrow(new IllegalStateException()).when(failingCallback).onBeforeMerge();
    Mockito.doThrow(new IllegalStateException()).when(failingCallback).onAfterMerge();
    Mockito.doThrow(new IllegalStateException()).when(failingCallback).onProcessingComplete();
    Mockito.doThrow(new IllegalStateException()).when(failingCallback).onResourceChanged(Mockito.any(Resource.class));
    
    registry.registerCallback(factoryFor(failingCallback));
    registry.registerCallback(factoryFor(simpleCallback));
    
    registry.onBeforeModelCreated();
    registry.onAfterModelCreated();
    registry.onBeforePreProcess();
    registry.onAfterPreProcess();
    registry.onBeforePostProcess();
    registry.onAfterPostProcess();
    registry.onBeforeMerge();
    registry.onAfterMerge();
    registry.onProcessingComplete();
    registry.onResourceChanged(changedResource);
    
    Mockito.verify(simpleCallback).onBeforeModelCreated();
    Mockito.verify(simpleCallback).onAfterModelCreated();
    Mockito.verify(simpleCallback).onBeforePreProcess();
    Mockito.verify(simpleCallback).onAfterPreProcess();
    Mockito.verify(simpleCallback).onBeforePostProcess();
    Mockito.verify(simpleCallback).onAfterPostProcess();
    Mockito.verify(simpleCallback).onBeforeMerge();
    Mockito.verify(simpleCallback).onAfterMerge();
    Mockito.verify(simpleCallback).onProcessingComplete();
    Mockito.verify(simpleCallback).onResourceChanged(Mockito.eq(changedResource));
  }
  
  /**
   * TODO: Simplify the test and move common usage to utility method.
   */
  @Test
  public void shouldInvokeCallbackWhenCallingProcess()
      throws Exception {
    final HttpServletRequest request = Mockito.mock(HttpServletRequest.class);
    final HttpServletResponse response = Mockito.mock(HttpServletResponse.class);
    
    Mockito.when(response.getOutputStream()).thenReturn(
        new DelegatingServletOutputStream(new WriterOutputStream(new StringWriter())));
    Mockito.when(request.getRequestURL()).thenReturn(new StringBuffer(""""));
    Mockito.when(request.getRequestURI()).thenReturn("""");
    Mockito.when(request.getServletPath()).thenReturn("""");
    Context.unset();
    Context.set(Context.webContext(request, response, Mockito.mock(FilterConfig.class)));
    
    final LifecycleCallback callback = Mockito.mock(LifecycleCallback.class);
    
    final String groupName = ""group"";
    
    final GroupExtractor groupExtractor = Mockito.mock(GroupExtractor.class);
    Mockito.when(groupExtractor.getGroupName(Mockito.any(HttpServletRequest.class))).thenReturn(groupName);
    Mockito.when(groupExtractor.getResourceType(Mockito.any(HttpServletRequest.class))).thenReturn(ResourceType.JS);
    
    final Group group = new Group(groupName);
    group.addResource(Resource.create(""classpath:1.js""));
    final WroModelFactory modelFactory = WroUtil.factoryFor(new WroModel().addGroup(group));
    
    final WroManagerFactory managerFactory = new BaseWroManagerFactory().setGroupExtractor(groupExtractor).setModelFactory(
        modelFactory);
    final WroManager manager = managerFactory.create();
    manager.registerCallback(new ObjectFactory<LifecycleCallback>() {
      public LifecycleCallback create() {
        return callback;
      }
    });
    manager.process();
    
    Mockito.verify(callback).onBeforeModelCreated();
    Mockito.verify(callback).onAfterModelCreated();
    Mockito.verify(callback, Mockito.atLeastOnce()).onBeforePreProcess();
    Mockito.verify(callback, Mockito.atLeastOnce()).onAfterPreProcess();
    Mockito.verify(callback).onBeforeMerge();
    Mockito.verify(callback).onAfterMerge();
    Mockito.verify(callback).onProcessingComplete();
    
    // This will fail because the callback is invoked also on processors which are skipped
    // Mockito.verifyNoMoreInteractions(callback);
  }
  
  @Test
  public void shouldBeThreadSafe()
      throws Exception {
    registry = new LifecycleCallbackRegistry() {
      @Override
      protected void onException(final Exception e) {
        throw WroRuntimeException.wrap(e);
      }
    };
    registry.registerCallback(new ObjectFactory<LifecycleCallback>() {
      public LifecycleCallback create() {
        return new PerformanceLoggerCallback();
      }
    });
    WroTestUtils.runConcurrently(new Callable<Void>() {
      public Void call()
          throws Exception {
        Context.set(Context.standaloneContext());
        try {
          registry.onBeforeModelCreated();
          Thread.sleep(10);
          registry.onAfterModelCreated();
          registry.onBeforeMerge();
          registry.onBeforePreProcess();
          registry.onAfterPreProcess();
          registry.onAfterMerge();
          registry.onBeforePostProcess();
          registry.onAfterPostProcess();
          registry.onProcessingComplete();
          return null;
        } finally {
          Context.unset();
        }
      }
    });
  }
}
","/*
 * Copyright (c) 2011. All rights reserved.
 */
package ro.isdc.wro.manager.callback;

import java.util.ArrayList;
import java.util.List;
import java.util.Map;
import java.util.concurrent.ConcurrentHashMap;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import ro.isdc.wro.config.Context;
import ro.isdc.wro.model.resource.Resource;
import ro.isdc.wro.util.Function;
import ro.isdc.wro.util.ObjectFactory;


/**
 * Register all available callbacks. The registry acts as a {@link LifecycleCallback} itself whose implementation
 * delegate the call to registered callbacks. The registry will handle any runtime exceptions thrown by callbacks, in
 * order to allow successful lifecycle execution.
 *
 * @author Alex Objelean
 * @since 1.4.3
 */
public class LifecycleCallbackRegistry
  implements LifecycleCallback {
  private static final Logger LOG = LoggerFactory.getLogger(LifecycleCallbackRegistry.class);

  /**
   * The list of registered callbacks.
   */
  private final List<ObjectFactory<LifecycleCallback>> callbackFactoryList = new ArrayList<ObjectFactory<LifecycleCallback>>();
  private final Map<String, List<LifecycleCallback>> map = new ConcurrentHashMap<String, List<LifecycleCallback>>();

  /**
   * Register a callback using a factory responsible for callback instantiation.
   *
   * @param callbackFactory
   *          the factory used to instantiate callbacks.
   */
  public void registerCallback(final ObjectFactory<LifecycleCallback> callbackFactory) {
    callbackFactoryList.add(callbackFactory);
  }

  private List<LifecycleCallback> getCallbacks() {
    final String key = Context.getCorrelationId();
    List<LifecycleCallback> callbacks = map.get(key);
    if (callbacks == null) {
      callbacks = initCallbacks();
      map.put(key, callbacks);
    }
    return callbacks;
  }

  protected List<LifecycleCallback> initCallbacks() {
    final List<LifecycleCallback> callbacks = new ArrayList<LifecycleCallback>();
    for (final ObjectFactory<LifecycleCallback> callbackFactory : callbackFactoryList) {
      callbacks.add(callbackFactory.create());
    }
    return callbacks;
  }

  /**
   * {@inheritDoc}
   */
  @Override
  public void onBeforeModelCreated() {
    forEachCallbackDo(new Function<LifecycleCallback, Void>() {
      @Override
      public Void apply(final LifecycleCallback input)
          throws Exception {
        input.onBeforeModelCreated();
        return null;
      }
    });
  }

  /**
   * {@inheritDoc}
   */
  @Override
  public void onAfterModelCreated() {
    forEachCallbackDo(new Function<LifecycleCallback, Void>() {
      @Override
      public Void apply(final LifecycleCallback input)
          throws Exception {
        input.onAfterModelCreated();
        return null;
      }
    });
  }

  /**
   * {@inheritDoc}
   */
  @Override
  public void onBeforePreProcess() {
    forEachCallbackDo(new Function<LifecycleCallback, Void>() {
      @Override
      public Void apply(final LifecycleCallback input)
          throws Exception {
        input.onBeforePreProcess();
        return null;
      }
    });
  }

  /**
   * {@inheritDoc}
   */
  @Override
  public void onAfterPreProcess() {
    forEachCallbackDo(new Function<LifecycleCallback, Void>() {
      @Override
      public Void apply(final LifecycleCallback input)
          throws Exception {
        input.onAfterPreProcess();
        return null;
      }
    });
  }

  /**
   * {@inheritDoc}
   */
  @Override
  public void onBeforePostProcess() {
    forEachCallbackDo(new Function<LifecycleCallback, Void>() {
      @Override
      public Void apply(final LifecycleCallback input)
          throws Exception {
        input.onBeforePostProcess();
        return null;
      }
    });
  }

  /**
   * {@inheritDoc}
   */
  @Override
  public void onAfterPostProcess() {
    forEachCallbackDo(new Function<LifecycleCallback, Void>() {
      @Override
      public Void apply(final LifecycleCallback input)
          throws Exception {
        input.onAfterPostProcess();
        return null;
      }
    });
  }

  /**
   * {@inheritDoc}
   */
  @Override
  public void onBeforeMerge() {
    forEachCallbackDo(new Function<LifecycleCallback, Void>() {
      @Override
      public Void apply(final LifecycleCallback input)
          throws Exception {
        input.onBeforeMerge();
        return null;
      }
    });
  }

  @Override
  public void onAfterMerge() {
    forEachCallbackDo(new Function<LifecycleCallback, Void>() {
      @Override
      public Void apply(final LifecycleCallback input)
          throws Exception {
        input.onAfterMerge();
        return null;
      }
    });
  }

  /**
   * {@inheritDoc}
   */
  @Override
  public void onProcessingComplete() {
    forEachCallbackDo(new Function<LifecycleCallback, Void>() {
      @Override
      public Void apply(final LifecycleCallback input)
          throws Exception {
        input.onProcessingComplete();
        return null;
      }
    });
  }

  @Override
  public void onResourceChanged(final Resource resource) {
    forEachCallbackDo(new Function<LifecycleCallback, Void>() {
      @Override
      public Void apply(final LifecycleCallback input)
          throws Exception {
        input.onResourceChanged(resource);
        return null;
      }
    });
  }

  private void forEachCallbackDo(final Function<LifecycleCallback, Void> func) {
    for (final LifecycleCallback callback : getCallbacks()) {
      try {
        func.apply(callback);
      } catch (final Exception e) {
        LOG.error(""Problem invoking callback"", e);
        onException(e);
      } finally {
        map.remove(Context.getCorrelationId());
      }
    }
  }

  /**
   * Invoked when a callback fails. By default exception is ignored.
   *
   * @param e
   *          {@link Exception} thrown by the fallback.
   */
  protected void onException(final Exception e) {
  }
}
","['General Fixture', 'Unknown Test']","['Assertion Roulette', 'Eager Test', 'Lazy Test', 'Magic Number Test', 'Redundant Print', 'Sleepy Test']",6,2,0,12
35315_30_joda-time_test_getvalue_long_long,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/35315_test.java,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/35315_actual.java,"/*
 *  Copyright 2001-2009 Stephen Colebourne
 *
 *  Licensed under the Apache License, Version 2.0 (the ""License"");
 *  you may not use this file except in compliance with the License.
 *  You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 *  Unless required by applicable law or agreed to in writing, software
 *  distributed under the License is distributed on an ""AS IS"" BASIS,
 *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 *  See the License for the specific language governing permissions and
 *  limitations under the License.
 */
package org.joda.time.field;

import java.io.ByteArrayInputStream;
import java.io.ByteArrayOutputStream;
import java.io.ObjectInputStream;
import java.io.ObjectOutputStream;

import junit.framework.TestCase;
import junit.framework.TestSuite;

import org.joda.time.DurationField;
import org.joda.time.DurationFieldType;
import org.joda.time.chrono.ISOChronology;

/**
 * This class is a Junit unit test for PeriodFormatterBuilder.
 *
 * @author Stephen Colebourne
 */
public class TestMillisDurationField extends TestCase {

    public static void main(String[] args) {
        junit.textui.TestRunner.run(suite());
    }

    public static TestSuite suite() {
        return new TestSuite(TestMillisDurationField.class);
    }

    public TestMillisDurationField(String name) {
        super(name);
    }

    @Override
    protected void setUp() throws Exception {
    }

    @Override
    protected void tearDown() throws Exception {
    }

    //-----------------------------------------------------------------------
    public void test_getType() {
        assertEquals(DurationFieldType.millis(), MillisDurationField.INSTANCE.getType());
    }

    public void test_getName() {
        assertEquals(""millis"", MillisDurationField.INSTANCE.getName());
    }
    
    public void test_isSupported() {
        assertEquals(true, MillisDurationField.INSTANCE.isSupported());
    }

    public void test_isPrecise() {
        assertEquals(true, MillisDurationField.INSTANCE.isPrecise());
    }

    public void test_getUnitMillis() {
        assertEquals(1, MillisDurationField.INSTANCE.getUnitMillis());
    }

    public void test_toString() {
        assertEquals(""DurationField[millis]"", MillisDurationField.INSTANCE.toString());
    }
    
    //-----------------------------------------------------------------------
    public void test_getValue_long() {
        assertEquals(0, MillisDurationField.INSTANCE.getValue(0L));
        assertEquals(1234, MillisDurationField.INSTANCE.getValue(1234L));
        assertEquals(-1234, MillisDurationField.INSTANCE.getValue(-1234L));
        try {
            MillisDurationField.INSTANCE.getValue(((long) (Integer.MAX_VALUE)) + 1L);
            fail();
        } catch (ArithmeticException ex) {}
    }

    public void test_getValueAsLong_long() {
        assertEquals(0L, MillisDurationField.INSTANCE.getValueAsLong(0L));
        assertEquals(1234L, MillisDurationField.INSTANCE.getValueAsLong(1234L));
        assertEquals(-1234L, MillisDurationField.INSTANCE.getValueAsLong(-1234L));
        assertEquals(((long) (Integer.MAX_VALUE)) + 1L, MillisDurationField.INSTANCE.getValueAsLong(((long) (Integer.MAX_VALUE)) + 1L));
    }

    public void test_getValue_long_long() {
        assertEquals(0, MillisDurationField.INSTANCE.getValue(0L, 567L));
        assertEquals(1234, MillisDurationField.INSTANCE.getValue(1234L, 567L));
        assertEquals(-1234, MillisDurationField.INSTANCE.getValue(-1234L, 567L));
        try {
            MillisDurationField.INSTANCE.getValue(((long) (Integer.MAX_VALUE)) + 1L, 567L);
            fail();
        } catch (ArithmeticException ex) {}
    }

    public void test_getValueAsLong_long_long() {
        assertEquals(0L, MillisDurationField.INSTANCE.getValueAsLong(0L, 567L));
        assertEquals(1234L, MillisDurationField.INSTANCE.getValueAsLong(1234L, 567L));
        assertEquals(-1234L, MillisDurationField.INSTANCE.getValueAsLong(-1234L, 567L));
        assertEquals(((long) (Integer.MAX_VALUE)) + 1L, MillisDurationField.INSTANCE.getValueAsLong(((long) (Integer.MAX_VALUE)) + 1L, 567L));
    }

    //-----------------------------------------------------------------------
    public void test_getMillis_int() {
        assertEquals(0, MillisDurationField.INSTANCE.getMillis(0));
        assertEquals(1234, MillisDurationField.INSTANCE.getMillis(1234));
        assertEquals(-1234, MillisDurationField.INSTANCE.getMillis(-1234));
    }

    public void test_getMillis_long() {
        assertEquals(0L, MillisDurationField.INSTANCE.getMillis(0L));
        assertEquals(1234L, MillisDurationField.INSTANCE.getMillis(1234L));
        assertEquals(-1234L, MillisDurationField.INSTANCE.getMillis(-1234L));
    }

    public void test_getMillis_int_long() {
        assertEquals(0, MillisDurationField.INSTANCE.getMillis(0, 567L));
        assertEquals(1234, MillisDurationField.INSTANCE.getMillis(1234, 567L));
        assertEquals(-1234, MillisDurationField.INSTANCE.getMillis(-1234, 567L));
    }

    public void test_getMillis_long_long() {
        assertEquals(0L, MillisDurationField.INSTANCE.getMillis(0L, 567L));
        assertEquals(1234L, MillisDurationField.INSTANCE.getMillis(1234L, 567L));
        assertEquals(-1234L, MillisDurationField.INSTANCE.getMillis(-1234L, 567L));
    }

    //-----------------------------------------------------------------------
    public void test_add_long_int() {
        assertEquals(567L, MillisDurationField.INSTANCE.add(567L, 0));
        assertEquals(567L + 1234L, MillisDurationField.INSTANCE.add(567L, 1234));
        assertEquals(567L - 1234L, MillisDurationField.INSTANCE.add(567L, -1234));
        try {
            MillisDurationField.INSTANCE.add(Long.MAX_VALUE, 1);
            fail();
        } catch (ArithmeticException ex) {}
    }

    public void test_add_long_long() {
        assertEquals(567L, MillisDurationField.INSTANCE.add(567L, 0L));
        assertEquals(567L + 1234L, MillisDurationField.INSTANCE.add(567L, 1234L));
        assertEquals(567L - 1234L, MillisDurationField.INSTANCE.add(567L, -1234L));
        try {
            MillisDurationField.INSTANCE.add(Long.MAX_VALUE, 1L);
            fail();
        } catch (ArithmeticException ex) {}
    }

    //-----------------------------------------------------------------------
    public void test_getDifference_long_int() {
        assertEquals(567, MillisDurationField.INSTANCE.getDifference(567L, 0L));
        assertEquals(567 - 1234, MillisDurationField.INSTANCE.getDifference(567L, 1234L));
        assertEquals(567 + 1234, MillisDurationField.INSTANCE.getDifference(567L, -1234L));
        try {
            MillisDurationField.INSTANCE.getDifference(Long.MAX_VALUE, 1L);
            fail();
        } catch (ArithmeticException ex) {}
    }

    public void test_getDifferenceAsLong_long_long() {
        assertEquals(567L, MillisDurationField.INSTANCE.getDifferenceAsLong(567L, 0L));
        assertEquals(567L - 1234L, MillisDurationField.INSTANCE.getDifferenceAsLong(567L, 1234L));
        assertEquals(567L + 1234L, MillisDurationField.INSTANCE.getDifferenceAsLong(567L, -1234L));
        try {
            MillisDurationField.INSTANCE.getDifferenceAsLong(Long.MAX_VALUE, -1L);
            fail();
        } catch (ArithmeticException ex) {}
    }

    //-----------------------------------------------------------------------
    public void test_compareTo() {
        assertEquals(0, MillisDurationField.INSTANCE.compareTo(MillisDurationField.INSTANCE));
        assertEquals(-1, MillisDurationField.INSTANCE.compareTo(ISOChronology.getInstance().seconds()));
        DurationField dummy = new PreciseDurationField(DurationFieldType.seconds(), 0);
        assertEquals(1, MillisDurationField.INSTANCE.compareTo(dummy));
//        try {
//            MillisDurationField.INSTANCE.compareTo("""");
//            fail();
//        } catch (ClassCastException ex) {}
        try {
            MillisDurationField.INSTANCE.compareTo(null);
            fail();
        } catch (NullPointerException ex) {}
    }

    //-----------------------------------------------------------------------
    public void testSerialization() throws Exception {
        DurationField test = MillisDurationField.INSTANCE;
        
        ByteArrayOutputStream baos = new ByteArrayOutputStream();
        ObjectOutputStream oos = new ObjectOutputStream(baos);
        oos.writeObject(test);
        oos.close();
        byte[] bytes = baos.toByteArray();
        
        ByteArrayInputStream bais = new ByteArrayInputStream(bytes);
        ObjectInputStream ois = new ObjectInputStream(bais);
        DurationField result = (DurationField) ois.readObject();
        ois.close();
        
        assertSame(test, result);
    }

}
","/*
 *  Copyright 2001-2009 Stephen Colebourne
 *
 *  Licensed under the Apache License, Version 2.0 (the ""License"");
 *  you may not use this file except in compliance with the License.
 *  You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 *  Unless required by applicable law or agreed to in writing, software
 *  distributed under the License is distributed on an ""AS IS"" BASIS,
 *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 *  See the License for the specific language governing permissions and
 *  limitations under the License.
 */
package org.joda.time.field;

import java.io.Serializable;

import org.joda.time.DurationField;
import org.joda.time.DurationFieldType;

/**
 * Duration field class representing a field with a fixed unit length of one
 * millisecond.
 * <p>
 * MillisDurationField is thread-safe and immutable.
 *
 * @author Brian S O'Neill
 * @since 1.0
 */
public final class MillisDurationField extends DurationField implements Serializable {

    /** Serialization lock. */
    private static final long serialVersionUID = 2656707858124633367L;

    /** Singleton instance. */
    public static final DurationField INSTANCE = new MillisDurationField();

    /**
     * Restricted constructor.
     */
    private MillisDurationField() {
        super();
    }
    
    //------------------------------------------------------------------------
    @Override
    public DurationFieldType getType() {
        return DurationFieldType.millis();
    }

    @Override
    public String getName() {
        return ""millis"";
    }

    /**
     * Returns true as this field is supported.
     * 
     * @return true always
     */
    @Override
    public boolean isSupported() {
        return true;
    }

    /**
     * Returns true as this field is precise.
     * 
     * @return true always
     */
    @Override
    public final boolean isPrecise() {
        return true;
    }

    /**
     * Returns the amount of milliseconds per unit value of this field.
     *
     * @return one always
     */
    @Override
    public final long getUnitMillis() {
        return 1;
    }

    //------------------------------------------------------------------------
    @Override
    public int getValue(long duration) {
        return FieldUtils.safeToInt(duration);
    }

    @Override
    public long getValueAsLong(long duration) {
        return duration;
    }

    @Override
    public int getValue(long duration, long instant) {
        return FieldUtils.safeToInt(duration);
    }

    @Override
    public long getValueAsLong(long duration, long instant) {
        return duration;
    }

    @Override
    public long getMillis(int value) {
        return value;
    }

    @Override
    public long getMillis(long value) {
        return value;
    }

    @Override
    public long getMillis(int value, long instant) {
        return value;
    }

    @Override
    public long getMillis(long value, long instant) {
        return value;
    }

    @Override
    public long add(long instant, int value) {
        return FieldUtils.safeAdd(instant, value);
    }

    @Override
    public long add(long instant, long value) {
        return FieldUtils.safeAdd(instant, value);
    }

    @Override
    public int getDifference(long minuendInstant, long subtrahendInstant) {
        return FieldUtils.safeToInt(FieldUtils.safeSubtract(minuendInstant, subtrahendInstant));
    }

    @Override
    public long getDifferenceAsLong(long minuendInstant, long subtrahendInstant) {
        return FieldUtils.safeSubtract(minuendInstant, subtrahendInstant);
    }

    //------------------------------------------------------------------------
    public int compareTo(DurationField otherField) {
        long otherMillis = otherField.getUnitMillis();
        long thisMillis = getUnitMillis();
        // cannot do (thisMillis - otherMillis) as can overflow
        if (thisMillis == otherMillis) {
            return 0;
        }
        if (thisMillis < otherMillis) {
            return -1;
        } else {
            return 1;
        }
    }

    @Override
    public boolean equals(Object obj) {
        if (obj instanceof MillisDurationField) {
            return getUnitMillis() == ((MillisDurationField) obj).getUnitMillis();
        }
        return false;
    }

    @Override
    public int hashCode() {
        return (int) getUnitMillis();
    }

    /**
     * Get a suitable debug string.
     * 
     * @return debug string
     */
    @Override
    public String toString() {
        return ""DurationField[millis]"";
    }

    /**
     * Deserialize to the singleton.
     */
    private Object readResolve() {
        return INSTANCE;
    }

}
","['Assertion Roulette', 'Exception Catching Throwing', 'Sensitive Equality']","['Assertion Roulette', 'Magic Number Test', 'Redundant Print', 'Exception Handling']",3,2,1,14
35715_30_joda-time_testprint_buffermethods,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/35715_test.java,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/35715_actual.java,"/*
 *  Copyright 2001-2005 Stephen Colebourne
 *
 *  Licensed under the Apache License, Version 2.0 (the ""License"");
 *  you may not use this file except in compliance with the License.
 *  You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 *  Unless required by applicable law or agreed to in writing, software
 *  distributed under the License is distributed on an ""AS IS"" BASIS,
 *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 *  See the License for the specific language governing permissions and
 *  limitations under the License.
 */
package org.joda.time.format;

import java.io.CharArrayWriter;
import java.util.Locale;
import java.util.TimeZone;

import junit.framework.TestCase;
import junit.framework.TestSuite;

import org.joda.time.Chronology;
import org.joda.time.DateTimeConstants;
import org.joda.time.DateTimeUtils;
import org.joda.time.DateTimeZone;
import org.joda.time.MutablePeriod;
import org.joda.time.Period;
import org.joda.time.PeriodType;
import org.joda.time.chrono.BuddhistChronology;
import org.joda.time.chrono.ISOChronology;

/**
 * This class is a Junit unit test for Period Formating.
 *
 * @author Stephen Colebourne
 */
public class TestPeriodFormatter extends TestCase {

    private static final DateTimeZone UTC = DateTimeZone.UTC;
    private static final DateTimeZone PARIS = DateTimeZone.forID(""Europe/Paris"");
    private static final DateTimeZone LONDON = DateTimeZone.forID(""Europe/London"");
    private static final DateTimeZone TOKYO = DateTimeZone.forID(""Asia/Tokyo"");
    private static final DateTimeZone NEWYORK = DateTimeZone.forID(""America/New_York"");
    private static final Chronology ISO_UTC = ISOChronology.getInstanceUTC();
    private static final Chronology ISO_PARIS = ISOChronology.getInstance(PARIS);
    private static final Chronology BUDDHIST_PARIS = BuddhistChronology.getInstance(PARIS);

    long y2002days = 365 + 365 + 366 + 365 + 365 + 365 + 366 + 365 + 365 + 365 + 
                     366 + 365 + 365 + 365 + 366 + 365 + 365 + 365 + 366 + 365 + 
                     365 + 365 + 366 + 365 + 365 + 365 + 366 + 365 + 365 + 365 +
                     366 + 365;
    // 2002-06-09
    private long TEST_TIME_NOW =
            (y2002days + 31L + 28L + 31L + 30L + 31L + 9L -1L) * DateTimeConstants.MILLIS_PER_DAY;

    private DateTimeZone originalDateTimeZone = null;
    private TimeZone originalTimeZone = null;
    private Locale originalLocale = null;
    private PeriodFormatter f = null;

    public static void main(String[] args) {
        junit.textui.TestRunner.run(suite());
    }

    public static TestSuite suite() {
        return new TestSuite(TestPeriodFormatter.class);
    }

    public TestPeriodFormatter(String name) {
        super(name);
    }

    @Override
    protected void setUp() throws Exception {
        DateTimeUtils.setCurrentMillisFixed(TEST_TIME_NOW);
        originalDateTimeZone = DateTimeZone.getDefault();
        originalTimeZone = TimeZone.getDefault();
        originalLocale = Locale.getDefault();
        DateTimeZone.setDefault(LONDON);
        TimeZone.setDefault(TimeZone.getTimeZone(""Europe/London""));
        Locale.setDefault(Locale.UK);
        f = ISOPeriodFormat.standard();
    }

    @Override
    protected void tearDown() throws Exception {
        DateTimeUtils.setCurrentMillisSystem();
        DateTimeZone.setDefault(originalDateTimeZone);
        TimeZone.setDefault(originalTimeZone);
        Locale.setDefault(originalLocale);
        originalDateTimeZone = null;
        originalTimeZone = null;
        originalLocale = null;
        f = null;
    }

    //-----------------------------------------------------------------------
    public void testPrint_simple() {
        Period p = new Period(1, 2, 3, 4, 5, 6, 7, 8);
        assertEquals(""P1Y2M3W4DT5H6M7.008S"", f.print(p));
    }

    //-----------------------------------------------------------------------
    public void testPrint_bufferMethods() throws Exception {
        Period p = new Period(1, 2, 3, 4, 5, 6, 7, 8);
        StringBuffer buf = new StringBuffer();
        f.printTo(buf, p);
        assertEquals(""P1Y2M3W4DT5H6M7.008S"", buf.toString());
        
        buf = new StringBuffer();
        try {
            f.printTo(buf, null);
            fail();
        } catch (IllegalArgumentException ex) {}
    }

    //-----------------------------------------------------------------------
    public void testPrint_writerMethods() throws Exception {
        Period p = new Period(1, 2, 3, 4, 5, 6, 7, 8);
        CharArrayWriter out = new CharArrayWriter();
        f.printTo(out, p);
        assertEquals(""P1Y2M3W4DT5H6M7.008S"", out.toString());
        
        out = new CharArrayWriter();
        try {
            f.printTo(out, null);
            fail();
        } catch (IllegalArgumentException ex) {}
    }

    //-----------------------------------------------------------------------
    public void testWithGetLocaleMethods() {
        PeriodFormatter f2 = f.withLocale(Locale.FRENCH);
        assertEquals(Locale.FRENCH, f2.getLocale());
        assertSame(f2, f2.withLocale(Locale.FRENCH));
        
        f2 = f.withLocale(null);
        assertEquals(null, f2.getLocale());
        assertSame(f2, f2.withLocale(null));
    }

    public void testWithGetParseTypeMethods() {
        PeriodFormatter f2 = f.withParseType(PeriodType.dayTime());
        assertEquals(PeriodType.dayTime(), f2.getParseType());
        assertSame(f2, f2.withParseType(PeriodType.dayTime()));
        
        f2 = f.withParseType(null);
        assertEquals(null, f2.getParseType());
        assertSame(f2, f2.withParseType(null));
    }

    public void testPrinterParserMethods() {
        Period p = new Period(1, 2, 3, 4, 5, 6, 7, 8);
        PeriodFormatter f2 = new PeriodFormatter(f.getPrinter(), f.getParser());
        assertEquals(f.getPrinter(), f2.getPrinter());
        assertEquals(f.getParser(), f2.getParser());
        assertEquals(true, f2.isPrinter());
        assertEquals(true, f2.isParser());
        assertNotNull(f2.print(p));
        assertNotNull(f2.parsePeriod(""P1Y2M3W4DT5H6M7.008S""));
        
        f2 = new PeriodFormatter(f.getPrinter(), null);
        assertEquals(f.getPrinter(), f2.getPrinter());
        assertEquals(null, f2.getParser());
        assertEquals(true, f2.isPrinter());
        assertEquals(false, f2.isParser());
        assertNotNull(f2.print(p));
        try {
            assertNotNull(f2.parsePeriod(""P1Y2M3W4DT5H6M7.008S""));
            fail();
        } catch (UnsupportedOperationException ex) {}
        
        f2 = new PeriodFormatter(null, f.getParser());
        assertEquals(null, f2.getPrinter());
        assertEquals(f.getParser(), f2.getParser());
        assertEquals(false, f2.isPrinter());
        assertEquals(true, f2.isParser());
        try {
            f2.print(p);
            fail();
        } catch (UnsupportedOperationException ex) {}
        assertNotNull(f2.parsePeriod(""P1Y2M3W4DT5H6M7.008S""));
    }

    //-----------------------------------------------------------------------
    public void testParsePeriod_simple() {
        Period expect = new Period(1, 2, 3, 4, 5, 6, 7, 8);
        assertEquals(expect, f.parsePeriod(""P1Y2M3W4DT5H6M7.008S""));
        
        try {
            f.parsePeriod(""ABC"");
            fail();
        } catch (IllegalArgumentException ex) {}
    }

    public void testParsePeriod_parseType() {
        Period expect = new Period(0, 0, 0, 4, 5, 6, 7, 8, PeriodType.dayTime());
        assertEquals(expect, f.withParseType(PeriodType.dayTime()).parsePeriod(""P4DT5H6M7.008S""));
        try {
            f.withParseType(PeriodType.dayTime()).parsePeriod(""P3W4DT5H6M7.008S"");
            fail();
        } catch (IllegalArgumentException ex) {}
    }

    //-----------------------------------------------------------------------
    public void testParseMutablePeriod_simple() {
        MutablePeriod expect = new MutablePeriod(1, 2, 3, 4, 5, 6, 7, 8);
        assertEquals(expect, f.parseMutablePeriod(""P1Y2M3W4DT5H6M7.008S""));
        
        try {
            f.parseMutablePeriod(""ABC"");
            fail();
        } catch (IllegalArgumentException ex) {}
    }

    //-----------------------------------------------------------------------
    public void testParseInto_simple() {
        MutablePeriod expect = new MutablePeriod(1, 2, 3, 4, 5, 6, 7, 8);
        MutablePeriod result = new MutablePeriod();
        assertEquals(20, f.parseInto(result, ""P1Y2M3W4DT5H6M7.008S"", 0));
        assertEquals(expect, result);
        
        try {
            f.parseInto(null, ""P1Y2M3W4DT5H6M7.008S"", 0);
            fail();
        } catch (IllegalArgumentException ex) {}
        
        assertEquals(~0, f.parseInto(result, ""ABC"", 0));
    }

}
","/*
 *  Copyright 2001-2005 Stephen Colebourne
 *
 *  Licensed under the Apache License, Version 2.0 (the ""License"");
 *  you may not use this file except in compliance with the License.
 *  You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 *  Unless required by applicable law or agreed to in writing, software
 *  distributed under the License is distributed on an ""AS IS"" BASIS,
 *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 *  See the License for the specific language governing permissions and
 *  limitations under the License.
 */
package org.joda.time.format;

import java.io.IOException;
import java.io.Writer;
import java.util.Locale;

import org.joda.time.MutablePeriod;
import org.joda.time.Period;
import org.joda.time.PeriodType;
import org.joda.time.ReadWritablePeriod;
import org.joda.time.ReadablePeriod;

/**
 * Controls the printing and parsing of a time period to and from a string.
 * <p>
 * This class is the main API for printing and parsing used by most applications.
 * Instances of this class are created via one of three factory classes:
 * <ul>
 * <li>{@link PeriodFormat} - formats by pattern and style</li>
 * <li>{@link ISOPeriodFormat} - ISO8601 formats</li>
 * <li>{@link PeriodFormatterBuilder} - complex formats created via method calls</li>
 * </ul>
 * <p>
 * An instance of this class holds a reference internally to one printer and
 * one parser. It is possible that one of these may be null, in which case the
 * formatter cannot print/parse. This can be checked via the {@link #isPrinter()}
 * and {@link #isParser()} methods.
 * <p>
 * The underlying printer/parser can be altered to behave exactly as required
 * by using a decorator modifier:
 * <ul>
 * <li>{@link #withLocale(Locale)} - returns a new formatter that uses the specified locale</li>
 * </ul>
 * This returns a new formatter (instances of this class are immutable).
 * <p>
 * The main methods of the class are the <code>printXxx</code> and
 * <code>parseXxx</code> methods. These are used as follows:
 * <pre>
 * // print using the default locale
 * String periodStr = formatter.print(period);
 * // print using the French locale
 * String periodStr = formatter.withLocale(Locale.FRENCH).print(period);
 * 
 * // parse using the French locale
 * Period date = formatter.withLocale(Locale.FRENCH).parsePeriod(str);
 * </pre>
 *
 * @author Brian S O'Neill
 * @author Stephen Colebourne
 * @since 1.0
 */
public class PeriodFormatter {

    /** The internal printer used to output the datetime. */
    private final PeriodPrinter iPrinter;
    /** The internal parser used to output the datetime. */
    private final PeriodParser iParser;
    /** The locale to use for printing and parsing. */
    private final Locale iLocale;
    /** The period type used in parsing. */
    private final PeriodType iParseType;

    /**
     * Creates a new formatter, however you will normally use the factory
     * or the builder.
     * 
     * @param printer  the internal printer, null if cannot print
     * @param parser  the internal parser, null if cannot parse
     */
    public PeriodFormatter(
            PeriodPrinter printer, PeriodParser parser) {
        super();
        iPrinter = printer;
        iParser = parser;
        iLocale = null;
        iParseType = null;
    }

    /**
     * Constructor.
     * 
     * @param printer  the internal printer, null if cannot print
     * @param parser  the internal parser, null if cannot parse
     * @param locale  the locale to use
     * @param type  the parse period type
     */
    PeriodFormatter(
            PeriodPrinter printer, PeriodParser parser,
            Locale locale, PeriodType type) {
        super();
        iPrinter = printer;
        iParser = parser;
        iLocale = locale;
        iParseType = type;
    }

    //-----------------------------------------------------------------------
    /**
     * Is this formatter capable of printing.
     * 
     * @return true if this is a printer
     */
    public boolean isPrinter() {
        return (iPrinter != null);
    }

    /**
     * Gets the internal printer object that performs the real printing work.
     * 
     * @return the internal printer
     */
    public PeriodPrinter getPrinter() {
        return iPrinter;
    }

    /**
     * Is this formatter capable of parsing.
     * 
     * @return true if this is a parser
     */
    public boolean isParser() {
        return (iParser != null);
    }

    /**
     * Gets the internal parser object that performs the real parsing work.
     * 
     * @return the internal parser
     */
    public PeriodParser getParser() {
        return iParser;
    }

    //-----------------------------------------------------------------------
    /**
     * Returns a new formatter with a different locale that will be used
     * for printing and parsing.
     * <p>
     * A PeriodFormatter is immutable, so a new instance is returned,
     * and the original is unaltered and still usable.
     * <p>
     * A null locale indicates that no specific locale override is in use.
     * 
     * @param locale  the locale to use
     * @return the new formatter
     */
    public PeriodFormatter withLocale(Locale locale) {
        if (locale == getLocale() || (locale != null && locale.equals(getLocale()))) {
            return this;
        }
        return new PeriodFormatter(iPrinter, iParser, locale, iParseType);
    }

    /**
     * Gets the locale that will be used for printing and parsing.
     * <p>
     * A null locale indicates that no specific locale override is in use.
     * 
     * @return the locale to use
     */
    public Locale getLocale() {
        return iLocale;
    }

    //-----------------------------------------------------------------------
    /**
     * Returns a new formatter with a different PeriodType for parsing.
     * <p>
     * A PeriodFormatter is immutable, so a new instance is returned,
     * and the original is unaltered and still usable.
     * 
     * @param type  the type to use in parsing
     * @return the new formatter
     */
    public PeriodFormatter withParseType(PeriodType type) {
        if (type == iParseType) {
            return this;
        }
        return new PeriodFormatter(iPrinter, iParser, iLocale, type);
    }

    /**
     * Gets the PeriodType that will be used for parsing.
     * 
     * @return the parse type to use
     */
    public PeriodType getParseType() {
        return iParseType;
    }

    //-----------------------------------------------------------------------
    /**
     * Prints a ReadablePeriod to a StringBuffer.
     *
     * @param buf  the formatted period is appended to this buffer
     * @param period  the period to format, not null
     */
    public void printTo(StringBuffer buf, ReadablePeriod period) {
        checkPrinter();
        checkPeriod(period);
        
        getPrinter().printTo(buf, period, iLocale);
    }

    /**
     * Prints a ReadablePeriod to a Writer.
     *
     * @param out  the formatted period is written out
     * @param period  the period to format, not null
     * @throws IOException if an IO error occurs
     */
    public void printTo(Writer out, ReadablePeriod period) throws IOException {
        checkPrinter();
        checkPeriod(period);
        
        getPrinter().printTo(out, period, iLocale);
    }

    /**
     * Prints a ReadablePeriod to a new String.
     *
     * @param period  the period to format, not null
     * @return the printed result
     */
    public String print(ReadablePeriod period) {
        checkPrinter();
        checkPeriod(period);
        
        PeriodPrinter printer = getPrinter();
        StringBuffer buf = new StringBuffer(printer.calculatePrintedLength(period, iLocale));
        printer.printTo(buf, period, iLocale);
        return buf.toString();
    }

    /**
     * Checks whether printing is supported.
     * 
     * @throws UnsupportedOperationException if printing is not supported
     */
    private void checkPrinter() {
        if (iPrinter == null) {
            throw new UnsupportedOperationException(""Printing not supported"");
        }
    }

    /**
     * Checks whether the period is non-null.
     * 
     * @throws IllegalArgumentException if the period is null
     */
    private void checkPeriod(ReadablePeriod period) {
        if (period == null) {
            throw new IllegalArgumentException(""Period must not be null"");
        }
    }

    //-----------------------------------------------------------------------
    /**
     * Parses a period from the given text, at the given position, saving the
     * result into the fields of the given ReadWritablePeriod. If the parse
     * succeeds, the return value is the new text position. Note that the parse
     * may succeed without fully reading the text.
     * <p>
     * The parse type of the formatter is not used by this method.
     * <p>
     * If it fails, the return value is negative, but the period may still be
     * modified. To determine the position where the parse failed, apply the
     * one's complement operator (~) on the return value.
     *
     * @param period  a period that will be modified
     * @param text  text to parse
     * @param position position to start parsing from
     * @return new position, if negative, parse failed. Apply complement
     * operator (~) to get position of failure
     * @throws IllegalArgumentException if any field is out of range
     */
    public int parseInto(ReadWritablePeriod period, String text, int position) {
        checkParser();
        checkPeriod(period);
        
        return getParser().parseInto(period, text, position, iLocale);
    }

    /**
     * Parses a period from the given text, returning a new Period.
     *
     * @param text  text to parse
     * @return parsed value in a Period object
     * @throws IllegalArgumentException if any field is out of range
     */
    public Period parsePeriod(String text) {
        checkParser();
        
        return parseMutablePeriod(text).toPeriod();
    }

    /**
     * Parses a period from the given text, returning a new MutablePeriod.
     *
     * @param text  text to parse
     * @return parsed value in a MutablePeriod object
     * @throws IllegalArgumentException if any field is out of range
     */
    public MutablePeriod parseMutablePeriod(String text) {
        checkParser();
        
        MutablePeriod period = new MutablePeriod(0, iParseType);
        int newPos = getParser().parseInto(period, text, 0, iLocale);
        if (newPos >= 0) {
            if (newPos >= text.length()) {
                return period;
            }
        } else {
            newPos = ~newPos;
        }
        throw new IllegalArgumentException(FormatUtils.createErrorMessage(text, newPos));
    }

    /**
     * Checks whether parsing is supported.
     * 
     * @throws UnsupportedOperationException if parsing is not supported
     */
    private void checkParser() {
        if (iParser == null) {
            throw new UnsupportedOperationException(""Parsing not supported"");
        }
    }

}
","['Assertion Roulette', 'Exception Catching Throwing', 'General Fixture', 'Sensitive Equality', 'Eager Test']","['Assertion Roulette', 'Conditional Test Logic', 'Lazy Test', 'Magic Number Test', 'Redundant Assertion', 'Resource Optimism', 'Sensitive Equality', 'Sleepy Test', 'Unknown Test']",7,3,2,8
35721_30_joda-time_testparseperiod_parsetype,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/35721_test.java,/Users/codermehraj/Documents/codes/thesis/code-test-pair-for-tsDetect/downloaded_files/35721_actual.java,"/*
 *  Copyright 2001-2005 Stephen Colebourne
 *
 *  Licensed under the Apache License, Version 2.0 (the ""License"");
 *  you may not use this file except in compliance with the License.
 *  You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 *  Unless required by applicable law or agreed to in writing, software
 *  distributed under the License is distributed on an ""AS IS"" BASIS,
 *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 *  See the License for the specific language governing permissions and
 *  limitations under the License.
 */
package org.joda.time.format;

import java.io.CharArrayWriter;
import java.util.Locale;
import java.util.TimeZone;

import junit.framework.TestCase;
import junit.framework.TestSuite;

import org.joda.time.Chronology;
import org.joda.time.DateTimeConstants;
import org.joda.time.DateTimeUtils;
import org.joda.time.DateTimeZone;
import org.joda.time.MutablePeriod;
import org.joda.time.Period;
import org.joda.time.PeriodType;
import org.joda.time.chrono.BuddhistChronology;
import org.joda.time.chrono.ISOChronology;

/**
 * This class is a Junit unit test for Period Formating.
 *
 * @author Stephen Colebourne
 */
public class TestPeriodFormatter extends TestCase {

    private static final DateTimeZone UTC = DateTimeZone.UTC;
    private static final DateTimeZone PARIS = DateTimeZone.forID(""Europe/Paris"");
    private static final DateTimeZone LONDON = DateTimeZone.forID(""Europe/London"");
    private static final DateTimeZone TOKYO = DateTimeZone.forID(""Asia/Tokyo"");
    private static final DateTimeZone NEWYORK = DateTimeZone.forID(""America/New_York"");
    private static final Chronology ISO_UTC = ISOChronology.getInstanceUTC();
    private static final Chronology ISO_PARIS = ISOChronology.getInstance(PARIS);
    private static final Chronology BUDDHIST_PARIS = BuddhistChronology.getInstance(PARIS);

    long y2002days = 365 + 365 + 366 + 365 + 365 + 365 + 366 + 365 + 365 + 365 + 
                     366 + 365 + 365 + 365 + 366 + 365 + 365 + 365 + 366 + 365 + 
                     365 + 365 + 366 + 365 + 365 + 365 + 366 + 365 + 365 + 365 +
                     366 + 365;
    // 2002-06-09
    private long TEST_TIME_NOW =
            (y2002days + 31L + 28L + 31L + 30L + 31L + 9L -1L) * DateTimeConstants.MILLIS_PER_DAY;

    private DateTimeZone originalDateTimeZone = null;
    private TimeZone originalTimeZone = null;
    private Locale originalLocale = null;
    private PeriodFormatter f = null;

    public static void main(String[] args) {
        junit.textui.TestRunner.run(suite());
    }

    public static TestSuite suite() {
        return new TestSuite(TestPeriodFormatter.class);
    }

    public TestPeriodFormatter(String name) {
        super(name);
    }

    @Override
    protected void setUp() throws Exception {
        DateTimeUtils.setCurrentMillisFixed(TEST_TIME_NOW);
        originalDateTimeZone = DateTimeZone.getDefault();
        originalTimeZone = TimeZone.getDefault();
        originalLocale = Locale.getDefault();
        DateTimeZone.setDefault(LONDON);
        TimeZone.setDefault(TimeZone.getTimeZone(""Europe/London""));
        Locale.setDefault(Locale.UK);
        f = ISOPeriodFormat.standard();
    }

    @Override
    protected void tearDown() throws Exception {
        DateTimeUtils.setCurrentMillisSystem();
        DateTimeZone.setDefault(originalDateTimeZone);
        TimeZone.setDefault(originalTimeZone);
        Locale.setDefault(originalLocale);
        originalDateTimeZone = null;
        originalTimeZone = null;
        originalLocale = null;
        f = null;
    }

    //-----------------------------------------------------------------------
    public void testPrint_simple() {
        Period p = new Period(1, 2, 3, 4, 5, 6, 7, 8);
        assertEquals(""P1Y2M3W4DT5H6M7.008S"", f.print(p));
    }

    //-----------------------------------------------------------------------
    public void testPrint_bufferMethods() throws Exception {
        Period p = new Period(1, 2, 3, 4, 5, 6, 7, 8);
        StringBuffer buf = new StringBuffer();
        f.printTo(buf, p);
        assertEquals(""P1Y2M3W4DT5H6M7.008S"", buf.toString());
        
        buf = new StringBuffer();
        try {
            f.printTo(buf, null);
            fail();
        } catch (IllegalArgumentException ex) {}
    }

    //-----------------------------------------------------------------------
    public void testPrint_writerMethods() throws Exception {
        Period p = new Period(1, 2, 3, 4, 5, 6, 7, 8);
        CharArrayWriter out = new CharArrayWriter();
        f.printTo(out, p);
        assertEquals(""P1Y2M3W4DT5H6M7.008S"", out.toString());
        
        out = new CharArrayWriter();
        try {
            f.printTo(out, null);
            fail();
        } catch (IllegalArgumentException ex) {}
    }

    //-----------------------------------------------------------------------
    public void testWithGetLocaleMethods() {
        PeriodFormatter f2 = f.withLocale(Locale.FRENCH);
        assertEquals(Locale.FRENCH, f2.getLocale());
        assertSame(f2, f2.withLocale(Locale.FRENCH));
        
        f2 = f.withLocale(null);
        assertEquals(null, f2.getLocale());
        assertSame(f2, f2.withLocale(null));
    }

    public void testWithGetParseTypeMethods() {
        PeriodFormatter f2 = f.withParseType(PeriodType.dayTime());
        assertEquals(PeriodType.dayTime(), f2.getParseType());
        assertSame(f2, f2.withParseType(PeriodType.dayTime()));
        
        f2 = f.withParseType(null);
        assertEquals(null, f2.getParseType());
        assertSame(f2, f2.withParseType(null));
    }

    public void testPrinterParserMethods() {
        Period p = new Period(1, 2, 3, 4, 5, 6, 7, 8);
        PeriodFormatter f2 = new PeriodFormatter(f.getPrinter(), f.getParser());
        assertEquals(f.getPrinter(), f2.getPrinter());
        assertEquals(f.getParser(), f2.getParser());
        assertEquals(true, f2.isPrinter());
        assertEquals(true, f2.isParser());
        assertNotNull(f2.print(p));
        assertNotNull(f2.parsePeriod(""P1Y2M3W4DT5H6M7.008S""));
        
        f2 = new PeriodFormatter(f.getPrinter(), null);
        assertEquals(f.getPrinter(), f2.getPrinter());
        assertEquals(null, f2.getParser());
        assertEquals(true, f2.isPrinter());
        assertEquals(false, f2.isParser());
        assertNotNull(f2.print(p));
        try {
            assertNotNull(f2.parsePeriod(""P1Y2M3W4DT5H6M7.008S""));
            fail();
        } catch (UnsupportedOperationException ex) {}
        
        f2 = new PeriodFormatter(null, f.getParser());
        assertEquals(null, f2.getPrinter());
        assertEquals(f.getParser(), f2.getParser());
        assertEquals(false, f2.isPrinter());
        assertEquals(true, f2.isParser());
        try {
            f2.print(p);
            fail();
        } catch (UnsupportedOperationException ex) {}
        assertNotNull(f2.parsePeriod(""P1Y2M3W4DT5H6M7.008S""));
    }

    //-----------------------------------------------------------------------
    public void testParsePeriod_simple() {
        Period expect = new Period(1, 2, 3, 4, 5, 6, 7, 8);
        assertEquals(expect, f.parsePeriod(""P1Y2M3W4DT5H6M7.008S""));
        
        try {
            f.parsePeriod(""ABC"");
            fail();
        } catch (IllegalArgumentException ex) {}
    }

    public void testParsePeriod_parseType() {
        Period expect = new Period(0, 0, 0, 4, 5, 6, 7, 8, PeriodType.dayTime());
        assertEquals(expect, f.withParseType(PeriodType.dayTime()).parsePeriod(""P4DT5H6M7.008S""));
        try {
            f.withParseType(PeriodType.dayTime()).parsePeriod(""P3W4DT5H6M7.008S"");
            fail();
        } catch (IllegalArgumentException ex) {}
    }

    //-----------------------------------------------------------------------
    public void testParseMutablePeriod_simple() {
        MutablePeriod expect = new MutablePeriod(1, 2, 3, 4, 5, 6, 7, 8);
        assertEquals(expect, f.parseMutablePeriod(""P1Y2M3W4DT5H6M7.008S""));
        
        try {
            f.parseMutablePeriod(""ABC"");
            fail();
        } catch (IllegalArgumentException ex) {}
    }

    //-----------------------------------------------------------------------
    public void testParseInto_simple() {
        MutablePeriod expect = new MutablePeriod(1, 2, 3, 4, 5, 6, 7, 8);
        MutablePeriod result = new MutablePeriod();
        assertEquals(20, f.parseInto(result, ""P1Y2M3W4DT5H6M7.008S"", 0));
        assertEquals(expect, result);
        
        try {
            f.parseInto(null, ""P1Y2M3W4DT5H6M7.008S"", 0);
            fail();
        } catch (IllegalArgumentException ex) {}
        
        assertEquals(~0, f.parseInto(result, ""ABC"", 0));
    }

}
","/*
 *  Copyright 2001-2005 Stephen Colebourne
 *
 *  Licensed under the Apache License, Version 2.0 (the ""License"");
 *  you may not use this file except in compliance with the License.
 *  You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 *  Unless required by applicable law or agreed to in writing, software
 *  distributed under the License is distributed on an ""AS IS"" BASIS,
 *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 *  See the License for the specific language governing permissions and
 *  limitations under the License.
 */
package org.joda.time.format;

import java.io.IOException;
import java.io.Writer;
import java.util.Locale;

import org.joda.time.MutablePeriod;
import org.joda.time.Period;
import org.joda.time.PeriodType;
import org.joda.time.ReadWritablePeriod;
import org.joda.time.ReadablePeriod;

/**
 * Controls the printing and parsing of a time period to and from a string.
 * <p>
 * This class is the main API for printing and parsing used by most applications.
 * Instances of this class are created via one of three factory classes:
 * <ul>
 * <li>{@link PeriodFormat} - formats by pattern and style</li>
 * <li>{@link ISOPeriodFormat} - ISO8601 formats</li>
 * <li>{@link PeriodFormatterBuilder} - complex formats created via method calls</li>
 * </ul>
 * <p>
 * An instance of this class holds a reference internally to one printer and
 * one parser. It is possible that one of these may be null, in which case the
 * formatter cannot print/parse. This can be checked via the {@link #isPrinter()}
 * and {@link #isParser()} methods.
 * <p>
 * The underlying printer/parser can be altered to behave exactly as required
 * by using a decorator modifier:
 * <ul>
 * <li>{@link #withLocale(Locale)} - returns a new formatter that uses the specified locale</li>
 * </ul>
 * This returns a new formatter (instances of this class are immutable).
 * <p>
 * The main methods of the class are the <code>printXxx</code> and
 * <code>parseXxx</code> methods. These are used as follows:
 * <pre>
 * // print using the default locale
 * String periodStr = formatter.print(period);
 * // print using the French locale
 * String periodStr = formatter.withLocale(Locale.FRENCH).print(period);
 * 
 * // parse using the French locale
 * Period date = formatter.withLocale(Locale.FRENCH).parsePeriod(str);
 * </pre>
 *
 * @author Brian S O'Neill
 * @author Stephen Colebourne
 * @since 1.0
 */
public class PeriodFormatter {

    /** The internal printer used to output the datetime. */
    private final PeriodPrinter iPrinter;
    /** The internal parser used to output the datetime. */
    private final PeriodParser iParser;
    /** The locale to use for printing and parsing. */
    private final Locale iLocale;
    /** The period type used in parsing. */
    private final PeriodType iParseType;

    /**
     * Creates a new formatter, however you will normally use the factory
     * or the builder.
     * 
     * @param printer  the internal printer, null if cannot print
     * @param parser  the internal parser, null if cannot parse
     */
    public PeriodFormatter(
            PeriodPrinter printer, PeriodParser parser) {
        super();
        iPrinter = printer;
        iParser = parser;
        iLocale = null;
        iParseType = null;
    }

    /**
     * Constructor.
     * 
     * @param printer  the internal printer, null if cannot print
     * @param parser  the internal parser, null if cannot parse
     * @param locale  the locale to use
     * @param type  the parse period type
     */
    PeriodFormatter(
            PeriodPrinter printer, PeriodParser parser,
            Locale locale, PeriodType type) {
        super();
        iPrinter = printer;
        iParser = parser;
        iLocale = locale;
        iParseType = type;
    }

    //-----------------------------------------------------------------------
    /**
     * Is this formatter capable of printing.
     * 
     * @return true if this is a printer
     */
    public boolean isPrinter() {
        return (iPrinter != null);
    }

    /**
     * Gets the internal printer object that performs the real printing work.
     * 
     * @return the internal printer
     */
    public PeriodPrinter getPrinter() {
        return iPrinter;
    }

    /**
     * Is this formatter capable of parsing.
     * 
     * @return true if this is a parser
     */
    public boolean isParser() {
        return (iParser != null);
    }

    /**
     * Gets the internal parser object that performs the real parsing work.
     * 
     * @return the internal parser
     */
    public PeriodParser getParser() {
        return iParser;
    }

    //-----------------------------------------------------------------------
    /**
     * Returns a new formatter with a different locale that will be used
     * for printing and parsing.
     * <p>
     * A PeriodFormatter is immutable, so a new instance is returned,
     * and the original is unaltered and still usable.
     * <p>
     * A null locale indicates that no specific locale override is in use.
     * 
     * @param locale  the locale to use
     * @return the new formatter
     */
    public PeriodFormatter withLocale(Locale locale) {
        if (locale == getLocale() || (locale != null && locale.equals(getLocale()))) {
            return this;
        }
        return new PeriodFormatter(iPrinter, iParser, locale, iParseType);
    }

    /**
     * Gets the locale that will be used for printing and parsing.
     * <p>
     * A null locale indicates that no specific locale override is in use.
     * 
     * @return the locale to use
     */
    public Locale getLocale() {
        return iLocale;
    }

    //-----------------------------------------------------------------------
    /**
     * Returns a new formatter with a different PeriodType for parsing.
     * <p>
     * A PeriodFormatter is immutable, so a new instance is returned,
     * and the original is unaltered and still usable.
     * 
     * @param type  the type to use in parsing
     * @return the new formatter
     */
    public PeriodFormatter withParseType(PeriodType type) {
        if (type == iParseType) {
            return this;
        }
        return new PeriodFormatter(iPrinter, iParser, iLocale, type);
    }

    /**
     * Gets the PeriodType that will be used for parsing.
     * 
     * @return the parse type to use
     */
    public PeriodType getParseType() {
        return iParseType;
    }

    //-----------------------------------------------------------------------
    /**
     * Prints a ReadablePeriod to a StringBuffer.
     *
     * @param buf  the formatted period is appended to this buffer
     * @param period  the period to format, not null
     */
    public void printTo(StringBuffer buf, ReadablePeriod period) {
        checkPrinter();
        checkPeriod(period);
        
        getPrinter().printTo(buf, period, iLocale);
    }

    /**
     * Prints a ReadablePeriod to a Writer.
     *
     * @param out  the formatted period is written out
     * @param period  the period to format, not null
     * @throws IOException if an IO error occurs
     */
    public void printTo(Writer out, ReadablePeriod period) throws IOException {
        checkPrinter();
        checkPeriod(period);
        
        getPrinter().printTo(out, period, iLocale);
    }

    /**
     * Prints a ReadablePeriod to a new String.
     *
     * @param period  the period to format, not null
     * @return the printed result
     */
    public String print(ReadablePeriod period) {
        checkPrinter();
        checkPeriod(period);
        
        PeriodPrinter printer = getPrinter();
        StringBuffer buf = new StringBuffer(printer.calculatePrintedLength(period, iLocale));
        printer.printTo(buf, period, iLocale);
        return buf.toString();
    }

    /**
     * Checks whether printing is supported.
     * 
     * @throws UnsupportedOperationException if printing is not supported
     */
    private void checkPrinter() {
        if (iPrinter == null) {
            throw new UnsupportedOperationException(""Printing not supported"");
        }
    }

    /**
     * Checks whether the period is non-null.
     * 
     * @throws IllegalArgumentException if the period is null
     */
    private void checkPeriod(ReadablePeriod period) {
        if (period == null) {
            throw new IllegalArgumentException(""Period must not be null"");
        }
    }

    //-----------------------------------------------------------------------
    /**
     * Parses a period from the given text, at the given position, saving the
     * result into the fields of the given ReadWritablePeriod. If the parse
     * succeeds, the return value is the new text position. Note that the parse
     * may succeed without fully reading the text.
     * <p>
     * The parse type of the formatter is not used by this method.
     * <p>
     * If it fails, the return value is negative, but the period may still be
     * modified. To determine the position where the parse failed, apply the
     * one's complement operator (~) on the return value.
     *
     * @param period  a period that will be modified
     * @param text  text to parse
     * @param position position to start parsing from
     * @return new position, if negative, parse failed. Apply complement
     * operator (~) to get position of failure
     * @throws IllegalArgumentException if any field is out of range
     */
    public int parseInto(ReadWritablePeriod period, String text, int position) {
        checkParser();
        checkPeriod(period);
        
        return getParser().parseInto(period, text, position, iLocale);
    }

    /**
     * Parses a period from the given text, returning a new Period.
     *
     * @param text  text to parse
     * @return parsed value in a Period object
     * @throws IllegalArgumentException if any field is out of range
     */
    public Period parsePeriod(String text) {
        checkParser();
        
        return parseMutablePeriod(text).toPeriod();
    }

    /**
     * Parses a period from the given text, returning a new MutablePeriod.
     *
     * @param text  text to parse
     * @return parsed value in a MutablePeriod object
     * @throws IllegalArgumentException if any field is out of range
     */
    public MutablePeriod parseMutablePeriod(String text) {
        checkParser();
        
        MutablePeriod period = new MutablePeriod(0, iParseType);
        int newPos = getParser().parseInto(period, text, 0, iLocale);
        if (newPos >= 0) {
            if (newPos >= text.length()) {
                return period;
            }
        } else {
            newPos = ~newPos;
        }
        throw new IllegalArgumentException(FormatUtils.createErrorMessage(text, newPos));
    }

    /**
     * Checks whether parsing is supported.
     * 
     * @throws UnsupportedOperationException if parsing is not supported
     */
    private void checkParser() {
        if (iParser == null) {
            throw new UnsupportedOperationException(""Parsing not supported"");
        }
    }

}
","['Assertion Roulette', 'Exception Catching Throwing', 'General Fixture', 'Sensitive Equality', 'Eager Test']",['Assertion Roulette'],0,4,1,12
