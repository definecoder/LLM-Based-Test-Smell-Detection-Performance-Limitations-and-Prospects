{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "from typing import List, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.schema import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IsSmellPresent(BaseModel):\n",
    "    isSmellPresent: bool\n",
    "\n",
    "class SmellReasoningAndRefactoredTestCode(BaseModel):\n",
    "    reasonWhySmellExists: str\n",
    "    refactoredTestCode: str\n",
    "\n",
    "class Smell(BaseModel):\n",
    "    smellName: str\n",
    "    smellDescription: str\n",
    "    detectionTechnique: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0,\n",
    "    api_key=\"OPENAI_API_KEY\",\n",
    ")\n",
    "\n",
    "structured_llm = llm.with_structured_output(IsSmellPresent)\n",
    "structured_llm_v2 = llm.with_structured_output(SmellReasoningAndRefactoredTestCode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "smellList = [\n",
    "    Smell(smellName=\"Assertion Roulette\", \n",
    "          smellDescription=\"Occurs when a test method has multiple non-documented assertions. Multiple assertion statements in a test method without a descriptive message impacts readability/understandability/maintainability as itâ€™s not possible to understand the reason for the failure of the test.\", \n",
    "          detectionTechnique=\"A test method contains more than one assertion statement without without an explanation/message (parameter in the assertion method).\"),\n",
    "    Smell(smellName=\"Conditional Test Logic\",\n",
    "          smellDescription=\"Test methods need to be simple and execute all statements in the production method. Conditions within the test method will alter the behavior of the test and its expected output, and would lead to situations where the test fails to detect defects in the production method since test statements were not executed as a condition was not met. Furthermore, conditional code within a test method negatively impacts the ease of comprehension by developers.\",\n",
    "          detectionTechnique=\"A test method that contains one or more control statements (i.e if, switch, conditional expression, for, foreach and while statement).\"),\n",
    "    Smell(smellName=\"Constructor Initialization\",\n",
    "          smellDescription=\"Ideally, the test suite should not have a constructor. Initialization of fields should be in the setUp() method. Developers who are unaware of the purpose of setUp() method would give rise to this smell by defining a constructor for the test suite.\",\n",
    "          detectionTechnique=\"A test class that contains a constructor declaration.\"),\n",
    "    Smell(smellName=\"Default Test\",\n",
    "          smellDescription=\"By default Android Studio creates default test classes when a project is created. These classes are meant to serve as an example for developers when wring unit tests and should either be removed or renamed. Having such files in the project will cause developers to start adding test methods into these files, making the default test class a container of all test cases. This also would possibly cause problems when the classes need to be renamed in the future.\",\n",
    "          detectionTechnique=\"A test class is named either `ExampleUnitTest' or `ExampleInstrumentedTest'.\"),\n",
    "    Smell(smellName=\"Duplicate Assert\",\n",
    "          smellDescription=\"This smell occurs when a test method tests for the same condition multiple times within the same test method. If the test method needs to test the same condition using different values, a new test method should be utilized; the name of the test method should be an indication of the test being performed. Possible situations that would give rise to this smell include: (1) developers grouping multiple conditions to test a single method; (2) developers performing debugging activities; and (3) an accidental copy-paste of code.\",\n",
    "          detectionTechnique=\"A test method that contains more than one assertion statement with the same parameters.\"),\n",
    "    Smell(smellName=\"Eager Test\",\n",
    "          smellDescription=\"Occurs when a test method invokes several methods of the production object. This smell results in difficulties in test comprehension and maintenance.\",\n",
    "          detectionTechnique=\"A test method contains multiple calls to multiple production methods.\"),\n",
    "    Smell(smellName=\"Empty Test\",\n",
    "          smellDescription=\"Occurs when a test method does not contain executable statements. Such methods are possibly created for debugging purposes and then forgotten about or contains commented out code. An empty test can be considered problematic and more dangerous than not having a test case at all since JUnit will indicate that the test passes even if there are no executable statements present in the method body. As such, developers introducing behavior-breaking changes into production class, will not be notified of the alternated outcomes as JUnit will report the test as passing.\",\n",
    "          detectionTechnique=\"A test method that does not contain a single executable statement.\"),\n",
    "    Smell(smellName=\"Exception Handling\",\n",
    "          smellDescription=\"This smell occurs when a test method explicitly a passing or failing of a test method is dependent on the production method throwing an exception. Developers should utilize JUnit's exception handling to automatically pass/fail the test instead of writing custom exception handling code or throwing an exception.\",\n",
    "          detectionTechnique=\"A test method that contains either a throw statement or a catch clause.\"),\n",
    "    Smell(smellName=\"General Fixture\",\n",
    "          smellDescription=\"Occurs when a test case fixture is too general and the test methods only access part of it. A test setup/fixture method that initializes fields that are not accessed by test methods indicates that the fixture is too generalized. A drawback of it being too general is that unnecessary work is being done when a test method is run.\",\n",
    "          detectionTechnique=\"Not all fields instantiated within the setUp method of a test class are utilized by all test methods in the same test class.\"),\n",
    "    Smell(smellName=\"Ignored Test\",\n",
    "          smellDescription=\"JUnit 4 provides developers with the ability to suppress test methods from running. However, these ignored test methods result in overhead since they add unnecessary overhead with regards to compilation time, and increases code complexity and comprehension.\",\n",
    "          detectionTechnique=\"A test method or class that contains the @Ignore annotation.\"),\n",
    "    Smell(smellName=\"Lazy Test\",\n",
    "          smellDescription=\"Occurs when multiple test methods invoke the same method of the production object.\",\n",
    "          detectionTechnique=\"Multiple test methods calling the same production method.\"),\n",
    "    Smell(smellName=\"Magic Number Test\",\n",
    "          smellDescription=\"Occurs when assert statements in a test method contain numeric literals (i.e., magic numbers) as parameters. Magic numbers do not indicate the meaning/purpose of the number. Hence, they should be replaced with constants or variables, thereby providing a descriptive name for the input.\",\n",
    "          detectionTechnique=\"An assertion method that contains a numeric literal as an argument.\"),\n",
    "    Smell(smellName=\"Mystery Guest\",\n",
    "          smellDescription=\"Occurs when a test method utilizes external resources (e.g. files, database, etc.). Use of external resources in test methods will result in stability and performance issues. Developers should use mock objects in place of external resources.\",\n",
    "          detectionTechnique=\"A test method containing object instances of files and databases classes.\"),\n",
    "    Smell(smellName=\"Redundant Print\",\n",
    "          smellDescription=\"Print statements in unit tests are redundant as unit tests are executed as part of an automated process with little to no human intervention. Print statements are possibly used by developers for traceability and debugging purposes and then forgotten.\",\n",
    "          detectionTechnique=\"A test method that invokes either the print or println or printf or write method of the System class.\"),\n",
    "    Smell(smellName=\"Redundant Assertion\",\n",
    "          smellDescription=\"This smell occurs when test methods contain assertion statements that are either always true or always false. This smell is introduced by developers for debugging purposes and then forgotten.\",\n",
    "          detectionTechnique=\"A test method that contains an assertion statement in which the expected and actual parameters are the same.\"),\n",
    "    Smell(smellName=\"Resource Optimism\",\n",
    "          smellDescription=\"This smell occurs when a test method makes an optimistic assumption that the external resource (e.g., File), utilized by the test method, exists.\",\n",
    "          detectionTechnique=\"A test method utilizes an instance of a File class without calling the exists(), isFile() or notExists() methods of the object.\"),\n",
    "    Smell(smellName=\"Sensitive Equality\",\n",
    "          smellDescription=\"Occurs when the toString method is used within a test method. Test methods verify objects by invoking the default toString() method of the object and comparing the output against an specific string. Changes to the implementation of toString() might result in failure. The correct approach is to implement a custom method within the object to perform this comparison.\",\n",
    "          detectionTechnique=\"A test method invokes the toString() method of an object.\"),\n",
    "    Smell(smellName=\"Sleepy Test\",\n",
    "          smellDescription=\"Explicitly causing a thread to sleep can lead to unexpected results as the processing time for a task can differ on different devices. Developers introduce this smell when they need to pause execution of statements in a test method for a certain duration (i.e. simulate an external event) and then continuing with execution.\",\n",
    "          detectionTechnique=\"A test method that invokes the Thread.sleep() method.\"),\n",
    "    Smell(smellName=\"Unknown Test\",\n",
    "          smellDescription=\"An assertion statement is used to declare an expected boolean condition for a test method. By examining the assertion statement it is possible to understand the purpose of the test method. However, It is possible for a test method to written sans an assertion statement, in such an instance JUnit will show the test method as passing if the statements within the test method did not result in an exception, when executed. New developers to the project will find it difficult in understanding the purpose of such test methods (more so if the name of the test method is not descriptive enough).\",\n",
    "          detectionTechnique=\"A test method that does not contain a single assertion statement and @Test(expected) annotation parameter.\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSmell(smellName: str) -> Smell:\n",
    "    for smell in smellList:\n",
    "        if smell.smellName == smellName:\n",
    "            return smell\n",
    "    return None\n",
    "\n",
    "def getSmellDescription(smell: Smell) -> str:    \n",
    "    description = f\"\"\"\n",
    "    The name of the test code smell is {smell.smellName}\n",
    "    Description of {smell.smellName} is : {smell.smellDescription}\n",
    "    It can be detected using: {smell.detectionTechnique}\n",
    "    \"\"\"\n",
    "    return description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectSmell(testCode: str, productionCode: str, smell: Smell) -> bool:\n",
    "    \n",
    "    smellDetails = getSmellDescription(smell)\n",
    "    \n",
    "    message = [\n",
    "        SystemMessage(\n",
    "            \"You are an expert in detecting one test smells. You will be given a test code block and a production code block. You have to identify if that code blocks contain the following test smell:\" + smellDetails\n",
    "        ),\n",
    "        HumanMessage(\"Here is the test code block:\" + testCode + \"Here is the production code block:\" + productionCode + \"Can you tell which test smell(s) the test code block contains\"),\n",
    "    ]\n",
    "    \n",
    "    response = structured_llm.invoke(message)    \n",
    "    \n",
    "    return response.isSmellPresent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refactorAndExplainSmell(testCode: str, productionCode: str, smell: Smell) -> SmellReasoningAndRefactoredTestCode:\n",
    "    \n",
    "    smellDetails = getSmellDescription(smell)\n",
    "    \n",
    "    message = [\n",
    "        SystemMessage(\n",
    "            \"You are an expert in explaining test smell reasoning and refactoring. You will be given a test code block and a production code block. The test code contains the following test smell:\" + smellDetails + \" You have to explain why the code contains this smell and also give a refactored version so that the code does not contains the test smell\"\n",
    "        ),\n",
    "        HumanMessage(\"Here is the test code block:\" + testCode + \"Here is the production code block:\" + productionCode + \"Can you tell why the test smell exists and the refactored test code?\"),\n",
    "    ]\n",
    "    \n",
    "    response = structured_llm_v2.invoke(message)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smellAgent(testCode: str, productionCode: str, smellName: str):\n",
    "    smell = getSmell(smellName)\n",
    "        \n",
    "    if(type(smell) == type(None)):\n",
    "        print(\"THE SMELL \" + str(smellName) + \" DOES NOT EXISTS\")\n",
    "        return\n",
    "    \n",
    "    isSmellPresent = detectSmell(testCode, productionCode, smell)\n",
    "    \n",
    "    if isSmellPresent == True:\n",
    "        res = refactorAndExplainSmell(testCode, productionCode, smell)\n",
    "        print(\"THE TEST CODE CONTAINS \" + smell.smellName)\n",
    "        print(\"\\n\\nREASON: \" + res.reasonWhySmellExists)\n",
    "        print(\"\\n\\nREFACTORED VERSION : \" + res.refactoredTestCode)\n",
    "    else:\n",
    "        print(\"THE TEST CODE DO NOT CONTAIN \" + smell.smellName)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE TEST CODE CONTAINS Assertion Roulette\n",
      "\n",
      "\n",
      "REASON: The test method `realCase` contains multiple assertions without any descriptive messages. This makes it difficult to understand which specific assertion failed and why, especially when the test fails. Each assertion checks a different aspect of the computation, but without messages, it's not clear what each assertion is verifying. This lack of clarity can make the test harder to maintain and debug, as developers need to manually trace back the failure to understand the context.\n",
      "\n",
      "\n",
      "REFACTORED VERSION : @Test\n",
      "public void realCase() {\n",
      "    Point p34 = new Point(\"34\", 556506.667, 172513.91, 620.34, true);\n",
      "    Point p45 = new Point(\"45\", 556495.16, 172493.912, 623.37, true);\n",
      "    Point p47 = new Point(\"47\", 556612.21, 172489.274, 0.0, true);\n",
      "    Abriss a = new Abriss(p34, false);\n",
      "    a.removeDAO(CalculationsDataSource.getInstance());\n",
      "    a.getMeasures().add(new Measure(p45, 0.0, 91.6892, 23.277, 1.63));\n",
      "    a.getMeasures().add(new Measure(p47, 281.3521, 100.0471, 108.384, 1.63));\n",
      "\n",
      "    try {\n",
      "        a.compute();\n",
      "    } catch (CalculationException e) {\n",
      "        Assert.fail(\"Calculation failed: \" + e.getMessage());\n",
      "    }\n",
      "\n",
      "    // test intermediate values with point 45\n",
      "    Assert.assertEquals(\"Unknown orientation for point 45 is incorrect\", \"233.2405\",\n",
      "        this.df4.format(a.getResults().get(0).getUnknownOrientation()));\n",
      "    Assert.assertEquals(\"Oriented direction for point 45 is incorrect\", \"233.2435\",\n",
      "        this.df4.format(a.getResults().get(0).getOrientedDirection()));\n",
      "    Assert.assertEquals(\"Error in translation for point 45 is incorrect\", \"-0.1\", this.df1.format(\n",
      "        a.getResults().get(0).getErrTrans()));\n",
      "\n",
      "    // test intermediate values with point 47\n",
      "    Assert.assertEquals(\"Unknown orientation for point 47 is incorrect\", \"233.2466\",\n",
      "        this.df4.format(a.getResults().get(1).getUnknownOrientation()));\n",
      "    Assert.assertEquals(\"Oriented direction for point 47 is incorrect\", \"114.5956\",\n",
      "        this.df4.format(a.getResults().get(1).getOrientedDirection()));\n",
      "    Assert.assertEquals(\"Error in translation for point 47 is incorrect\", \"0.5\", this.df1.format(\n",
      "        a.getResults().get(1).getErrTrans()));\n",
      "\n",
      "    // test final results\n",
      "    Assert.assertEquals(\"Mean is incorrect\", \"233.2435\", this.df4.format(a.getMean()));\n",
      "    Assert.assertEquals(\"MSE is incorrect\", \"43\", this.df0.format(a.getMSE()));\n",
      "    Assert.assertEquals(\"Mean error compensation is incorrect\", \"30\", this.df0.format(a.getMeanErrComp()));\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "test_code = \"\"\"\n",
    "    @Test\n",
    "    public void realCase() {\n",
    "        Point p34 = new Point(\"34\", 556506.667, 172513.91, 620.34, true);\n",
    "        Point p45 = new Point(\"45\", 556495.16, 172493.912, 623.37, true);\n",
    "        Point p47 = new Point(\"47\", 556612.21, 172489.274, 0.0, true);\n",
    "        Abriss a = new Abriss(p34, false);\n",
    "        a.removeDAO(CalculationsDataSource.getInstance());\n",
    "        a.getMeasures().add(new Measure(p45, 0.0, 91.6892, 23.277, 1.63));\n",
    "        a.getMeasures().add(new Measure(p47, 281.3521, 100.0471, 108.384, 1.63));\n",
    "\n",
    "        try {\n",
    "            a.compute();\n",
    "        } catch (CalculationException e) {\n",
    "            Assert.fail(e.getMessage());\n",
    "        }\n",
    "\n",
    "        // test intermediate values with point 45\n",
    "        Assert.assertEquals(\"233.2405\",\n",
    "            this.df4.format(a.getResults().get(0).getUnknownOrientation()));\n",
    "        Assert.assertEquals(\"233.2435\",\n",
    "            this.df4.format(a.getResults().get(0).getOrientedDirection()));\n",
    "        Assert.assertEquals(\"-0.1\", this.df1.format(\n",
    "            a.getResults().get(0).getErrTrans()));\n",
    "\n",
    "        // test intermediate values with point 47\n",
    "        Assert.assertEquals(\"233.2466\",\n",
    "            this.df4.format(a.getResults().get(1).getUnknownOrientation()));\n",
    "        Assert.assertEquals(\"114.5956\",\n",
    "            this.df4.format(a.getResults().get(1).getOrientedDirection()));\n",
    "        Assert.assertEquals(\"0.5\", this.df1.format(\n",
    "            a.getResults().get(1).getErrTrans()));\n",
    "\n",
    "        // test final results\n",
    "        Assert.assertEquals(\"233.2435\", this.df4.format(a.getMean()));\n",
    "        Assert.assertEquals(\"43\", this.df0.format(a.getMSE()));\n",
    "        Assert.assertEquals(\"30\", this.df0.format(a.getMeanErrComp()));\n",
    "    }\n",
    "    \"\"\"\n",
    "production_code = \"not available\"\n",
    "\n",
    "smellAgent(test_code, production_code, \"Assertion Roulette\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE SMELL Assertion Roulettee DOES NOT EXISTS\n"
     ]
    }
   ],
   "source": [
    "smellAgent(test_code, production_code, \"Assertion Roulettee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE TEST CODE DO NOT CONTAIN Sleepy Test\n"
     ]
    }
   ],
   "source": [
    "smellAgent(test_code, production_code, \"Sleepy Test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
